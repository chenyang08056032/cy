2026-01-17 08:00:35,242 - INFO - Starting Ascend test suite: per-commit-1-npu-a3
Starting Ascend test suite: per-commit-1-npu-a3
2026-01-17 08:00:35,242 - INFO - Command args: Namespace(timeout_per_file=3600, suite='per-commit-1-npu-a3', auto_partition_id=None, auto_partition_size=None, continue_on_error=True, enable_retry=True, max_attempts=2, retry_wait_seconds=60, log_dir='/data/c30044170/log')
Command args: Namespace(timeout_per_file=3600, suite='per-commit-1-npu-a3', auto_partition_id=None, auto_partition_size=None, continue_on_error=True, enable_retry=True, max_attempts=2, retry_wait_seconds=60, log_dir='/data/c30044170/log')
2026-01-17 08:00:35,243 - INFO - Log directory: /data/c30044170/log/per-commit-1-npu-a3/20260117_080035
Log directory: /data/c30044170/log/per-commit-1-npu-a3/20260117_080035
2026-01-17 08:00:35,251 - INFO - ✅ Suite sanity check passed
✅ Suite sanity check passed
2026-01-17 08:00:35,252 - INFO - Total test files: 63
Total test files: 63
.
.
Begin (0/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_glm4_9b_chat.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:00:52] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat', tokenizer_path='/root/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=166897463, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
[2026-01-17 08:00:52] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:01:01] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:01:02] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:01:02] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:01:03] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:01:03] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  20% Completed | 2/10 [00:00<00:00,  8.28it/s]

Loading safetensors checkpoint shards:  30% Completed | 3/10 [00:00<00:00,  7.40it/s]

Loading safetensors checkpoint shards:  40% Completed | 4/10 [00:00<00:01,  4.13it/s]

Loading safetensors checkpoint shards:  50% Completed | 5/10 [00:01<00:01,  3.54it/s]

Loading safetensors checkpoint shards:  60% Completed | 6/10 [00:01<00:01,  3.41it/s]

Loading safetensors checkpoint shards:  70% Completed | 7/10 [00:01<00:00,  3.09it/s]

Loading safetensors checkpoint shards:  80% Completed | 8/10 [00:02<00:00,  3.35it/s]

Loading safetensors checkpoint shards:  90% Completed | 9/10 [00:02<00:00,  3.56it/s]

Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:02<00:00,  3.57it/s]

Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:02<00:00,  3.78it/s]

[2026-01-17 08:01:06] Load weight end. type=ChatGLMModel, dtype=torch.bfloat16, avail mem=43.27 GB, mem usage=17.54 GB.
[2026-01-17 08:01:06] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:01:06] The available memory for KV cache is 31.09 GB.
[2026-01-17 08:01:07] KV Cache is allocated. #tokens: 814848, K size: 15.54 GB, V size: 15.54 GB
[2026-01-17 08:01:07] Memory pool end. avail mem=10.61 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:01:08] max_total_num_tokens=814848, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=3183, context_len=131072, available_gpu_mem=10.61 GB
[2026-01-17 08:01:08] INFO:     Started server process [5912]
[2026-01-17 08:01:08] INFO:     Waiting for application startup.
[2026-01-17 08:01:08] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.8, 'top_k': 50, 'top_p': 0.8}
[2026-01-17 08:01:08] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.8, 'top_k': 50, 'top_p': 0.8}
[2026-01-17 08:01:08] INFO:     Application startup complete.
[2026-01-17 08:01:08] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:01:09] INFO:     127.0.0.1:54148 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:01:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:01:12] INFO:     127.0.0.1:54172 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:01:21] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:21] The server is fired up and ready to roll!
[2026-01-17 08:01:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:01:23] INFO:     127.0.0.1:38938 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:01:23] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:01:23] INFO:     127.0.0.1:38952 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:01:23] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:01:24] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestGLM49BChat.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:01:24] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:01:24] Prefill batch, #new-seq: 12, #new-token: 1792, #cached-token: 7680, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:01:25] Prefill batch, #new-seq: 61, #new-token: 8192, #cached-token: 39040, token usage: 0.00, #running-req: 13, #queue-req: 54,
[2026-01-17 08:01:25] Prefill batch, #new-seq: 54, #new-token: 7168, #cached-token: 34560, token usage: 0.01, #running-req: 74, #queue-req: 0,
[2026-01-17 08:01:27] Decode batch, #running-req: 128, #token: 23168, token usage: 0.03, npu graph: False, gen throughput (token/s): 150.81, #queue-req: 0,
[2026-01-17 08:01:27] INFO:     127.0.0.1:39682 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:03<11:16,  3.40s/it][2026-01-17 08:01:27] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 08:01:27] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:03<04:52,  1.47s/it][2026-01-17 08:01:27] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 08:01:27] INFO:     127.0.0.1:39948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:27] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 08:01:27] INFO:     127.0.0.1:39568 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:03<01:59,  1.64it/s][2026-01-17 08:01:27] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 08:01:27] INFO:     127.0.0.1:39610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:27] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:39520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] INFO:     127.0.0.1:39638 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:03<01:12,  2.69it/s]
  4%|▎         | 7/200 [00:03<00:42,  4.52it/s][2026-01-17 08:01:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:38986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] INFO:     127.0.0.1:39924 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:04<00:34,  5.59it/s][2026-01-17 08:01:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:39802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:39534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] INFO:     127.0.0.1:40012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:38970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] INFO:     127.0.0.1:40040 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:04<00:21,  8.80it/s]
  7%|▋         | 14/200 [00:04<00:14, 12.99it/s][2026-01-17 08:01:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:38998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] INFO:     127.0.0.1:39214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:39312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] INFO:     127.0.0.1:40024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.04, #running-req: 130, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:39746 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:04<00:11, 16.13it/s][2026-01-17 08:01:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:39796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:39380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:39270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:04<00:10, 16.50it/s]
 12%|█▏        | 24/200 [00:04<00:09, 18.06it/s][2026-01-17 08:01:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:39862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:01:28] INFO:     127.0.0.1:39464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:28] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:04<00:09, 17.51it/s][2026-01-17 08:01:28] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 08:01:29] INFO:     127.0.0.1:39716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] Decode batch, #running-req: 128, #token: 29696, token usage: 0.04, npu graph: False, gen throughput (token/s): 2771.89, #queue-req: 0,
[2026-01-17 08:01:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 08:01:29] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] INFO:     127.0.0.1:39442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] INFO:     127.0.0.1:39662 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:05<00:11, 15.52it/s]
 16%|█▌        | 31/200 [00:05<00:09, 16.96it/s]
 16%|█▌        | 31/200 [00:05<00:09, 16.96it/s][2026-01-17 08:01:29] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:01:29] INFO:     127.0.0.1:39336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] INFO:     127.0.0.1:39774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 131, #queue-req: 0,
[2026-01-17 08:01:29] INFO:     127.0.0.1:39136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] INFO:     127.0.0.1:39284 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] INFO:     127.0.0.1:39720 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:05<00:09, 16.99it/s]
 18%|█▊        | 36/200 [00:05<00:07, 21.39it/s]
 18%|█▊        | 36/200 [00:05<00:07, 21.39it/s][2026-01-17 08:01:29] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-17 08:01:29] INFO:     127.0.0.1:39546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:01:29] INFO:     127.0.0.1:39350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] INFO:     127.0.0.1:39734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 08:01:29] INFO:     127.0.0.1:39030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] INFO:     127.0.0.1:39714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:29] INFO:     127.0.0.1:40082 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:05<00:07, 20.94it/s]
 21%|██        | 42/200 [00:05<00:06, 24.57it/s]
 21%|██        | 42/200 [00:05<00:06, 24.57it/s][2026-01-17 08:01:29] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-17 08:01:30] INFO:     127.0.0.1:39142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:30] INFO:     127.0.0.1:39726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 08:01:30] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:06<00:20,  7.43it/s][2026-01-17 08:01:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:39998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:39260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:39450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:40016 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:07<00:19,  7.94it/s]
 24%|██▍       | 49/200 [00:07<00:12, 11.74it/s]
 24%|██▍       | 49/200 [00:07<00:12, 11.74it/s][2026-01-17 08:01:31] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.03, #running-req: 125, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:39904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:34210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:39436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:39926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:07<00:12, 12.29it/s]
 27%|██▋       | 54/200 [00:07<00:08, 16.40it/s]
 27%|██▋       | 54/200 [00:07<00:08, 16.40it/s][2026-01-17 08:01:31] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.03, #running-req: 125, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:39242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:39920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:39972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.03, #running-req: 125, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:39558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:39602 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:07<00:07, 18.55it/s]
 30%|███       | 60/200 [00:07<00:05, 26.42it/s]
 30%|███       | 60/200 [00:07<00:05, 26.42it/s][2026-01-17 08:01:31] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.03, #running-req: 125, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:39622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:39814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:39360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:07<00:05, 25.00it/s]
 32%|███▏      | 64/200 [00:07<00:05, 25.74it/s][2026-01-17 08:01:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:39564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:39382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:39856 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:07<00:06, 20.57it/s][2026-01-17 08:01:31] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:39018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:39088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] INFO:     127.0.0.1:34240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:31] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 08:01:31] INFO:     127.0.0.1:38994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 132, #queue-req: 0,
[2026-01-17 08:01:32] INFO:     127.0.0.1:39122 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:08<00:07, 16.86it/s][2026-01-17 08:01:32] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:39892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:34262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:39888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:39742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:34282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:34228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:39698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:39676 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:08<00:04, 26.20it/s][2026-01-17 08:01:32] INFO:     127.0.0.1:39614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:39332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] Decode batch, #running-req: 117, #token: 26368, token usage: 0.03, npu graph: False, gen throughput (token/s): 1444.98, #queue-req: 0,
[2026-01-17 08:01:32] INFO:     127.0.0.1:39418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:39510 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:08<00:04, 25.09it/s]
 44%|████▎     | 87/200 [00:08<00:04, 25.71it/s][2026-01-17 08:01:32] INFO:     127.0.0.1:39702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:39852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:39072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:39322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:34196 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:08<00:04, 22.94it/s]
 46%|████▋     | 93/200 [00:08<00:04, 22.55it/s][2026-01-17 08:01:32] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:32] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:40056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34338 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:08<00:04, 23.50it/s]
 48%|████▊     | 97/200 [00:08<00:03, 26.18it/s][2026-01-17 08:01:33] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39426 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [00:09<00:03, 28.86it/s][2026-01-17 08:01:33] INFO:     127.0.0.1:34406 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34574 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:09<00:03, 30.76it/s]
 53%|█████▎    | 106/200 [00:09<00:02, 34.63it/s][2026-01-17 08:01:33] INFO:     127.0.0.1:39882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:09<00:02, 43.01it/s]
 57%|█████▋    | 114/200 [00:09<00:01, 52.08it/s][2026-01-17 08:01:33] INFO:     127.0.0.1:39582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34238 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:09<00:01, 42.41it/s][2026-01-17 08:01:33] INFO:     127.0.0.1:34430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34482 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:09<00:01, 43.37it/s]
 63%|██████▎   | 126/200 [00:09<00:01, 46.53it/s][2026-01-17 08:01:33] INFO:     127.0.0.1:39938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [00:09<00:01, 47.27it/s]
 66%|██████▌   | 132/200 [00:09<00:01, 50.48it/s][2026-01-17 08:01:33] INFO:     127.0.0.1:34330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:33] Decode batch, #running-req: 63, #token: 17664, token usage: 0.02, npu graph: False, gen throughput (token/s): 2657.22, #queue-req: 0,
[2026-01-17 08:01:33] INFO:     127.0.0.1:34420 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:09<00:01, 39.74it/s][2026-01-17 08:01:34] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:34252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:39104 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:10<00:01, 32.72it/s][2026-01-17 08:01:34] INFO:     127.0.0.1:39988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:39982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:34314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:34226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:39154 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:10<00:01, 35.84it/s][2026-01-17 08:01:34] INFO:     127.0.0.1:34442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:34456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:34520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:10<00:02, 22.75it/s][2026-01-17 08:01:34] INFO:     127.0.0.1:39874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:34] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:35] INFO:     127.0.0.1:39300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:35] INFO:     127.0.0.1:39058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:35] INFO:     127.0.0.1:34592 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:10<00:02, 19.88it/s]
 80%|███████▉  | 159/200 [00:10<00:02, 19.17it/s][2026-01-17 08:01:35] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:35] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:35] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:35] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:11<00:01, 19.87it/s]
 82%|████████▏ | 163/200 [00:11<00:01, 22.07it/s][2026-01-17 08:01:35] Decode batch, #running-req: 37, #token: 11520, token usage: 0.01, npu graph: False, gen throughput (token/s): 1563.00, #queue-req: 0,
[2026-01-17 08:01:35] INFO:     127.0.0.1:34678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:35] INFO:     127.0.0.1:34288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:35] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:11<00:01, 20.34it/s][2026-01-17 08:01:35] INFO:     127.0.0.1:34180 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:35] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:35] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:11<00:01, 17.04it/s][2026-01-17 08:01:35] INFO:     127.0.0.1:34534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:36] INFO:     127.0.0.1:39358 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:11<00:02, 11.91it/s][2026-01-17 08:01:36] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:36] INFO:     127.0.0.1:34562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:36] INFO:     127.0.0.1:34390 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:12<00:01, 14.16it/s][2026-01-17 08:01:36] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:36] INFO:     127.0.0.1:39166 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:12<00:01, 13.42it/s][2026-01-17 08:01:36] Decode batch, #running-req: 24, #token: 9216, token usage: 0.01, npu graph: False, gen throughput (token/s): 1020.51, #queue-req: 0,
[2026-01-17 08:01:36] INFO:     127.0.0.1:34652 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:36] INFO:     127.0.0.1:34304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:36] INFO:     127.0.0.1:34354 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:12<00:01, 11.90it/s]
 90%|████████▉ | 179/200 [00:12<00:01, 12.27it/s][2026-01-17 08:01:36] INFO:     127.0.0.1:34272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:36] INFO:     127.0.0.1:34244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:36] INFO:     127.0.0.1:34612 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:12<00:01, 12.22it/s][2026-01-17 08:01:36] INFO:     127.0.0.1:39046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:37] INFO:     127.0.0.1:34378 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:13<00:01, 10.18it/s][2026-01-17 08:01:37] INFO:     127.0.0.1:34696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:37] INFO:     127.0.0.1:34770 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:13<00:01, 10.98it/s][2026-01-17 08:01:37] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:37] Decode batch, #running-req: 13, #token: 5888, token usage: 0.01, npu graph: False, gen throughput (token/s): 635.32, #queue-req: 0,
[2026-01-17 08:01:37] INFO:     127.0.0.1:40076 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:13<00:01,  6.24it/s][2026-01-17 08:01:38] INFO:     127.0.0.1:34576 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:13<00:01,  6.58it/s][2026-01-17 08:01:38] Decode batch, #running-req: 11, #token: 5504, token usage: 0.01, npu graph: False, gen throughput (token/s): 440.80, #queue-req: 0,
[2026-01-17 08:01:39] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:15<00:03,  2.72it/s][2026-01-17 08:01:39] Decode batch, #running-req: 10, #token: 5504, token usage: 0.01, npu graph: False, gen throughput (token/s): 398.73, #queue-req: 0,
[2026-01-17 08:01:40] INFO:     127.0.0.1:34674 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:16<00:04,  1.82it/s][2026-01-17 08:01:40] Decode batch, #running-req: 9, #token: 5120, token usage: 0.01, npu graph: False, gen throughput (token/s): 369.83, #queue-req: 0,
[2026-01-17 08:01:41] INFO:     127.0.0.1:34720 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:17<00:04,  1.67it/s][2026-01-17 08:01:41] Decode batch, #running-req: 8, #token: 4736, token usage: 0.01, npu graph: False, gen throughput (token/s): 323.41, #queue-req: 0,
[2026-01-17 08:01:41] INFO:     127.0.0.1:34640 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:17<00:03,  1.77it/s][2026-01-17 08:01:42] Decode batch, #running-req: 7, #token: 4992, token usage: 0.01, npu graph: False, gen throughput (token/s): 262.19, #queue-req: 0,
[2026-01-17 08:01:43] Decode batch, #running-req: 7, #token: 1152, token usage: 0.00, npu graph: False, gen throughput (token/s): 263.64, #queue-req: 0,
[2026-01-17 08:01:43] INFO:     127.0.0.1:39114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:43] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:43] INFO:     127.0.0.1:39588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:43] INFO:     127.0.0.1:39636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:43] INFO:     127.0.0.1:39648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:01:43] INFO:     127.0.0.1:39790 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:19<00:05,  1.02it/s]
100%|█████████▉| 199/200 [00:19<00:00,  2.07it/s]
100%|█████████▉| 199/200 [00:19<00:00,  2.07it/s]
100%|█████████▉| 199/200 [00:19<00:00,  2.07it/s]
100%|█████████▉| 199/200 [00:19<00:00,  2.07it/s]
100%|█████████▉| 199/200 [00:19<00:00,  2.07it/s][2026-01-17 08:01:44] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, npu graph: False, gen throughput (token/s): 46.03, #queue-req: 0,
[2026-01-17 08:01:45] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, npu graph: False, gen throughput (token/s): 40.16, #queue-req: 0,
[2026-01-17 08:01:46] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:22<00:00,  1.51it/s]
100%|██████████| 200/200 [00:22<00:00,  8.97it/s]
.
----------------------------------------------------------------------
Ran 1 test in 63.561s

OK
Accuracy: 0.790
Invalid: 0.000
Latency: 22.380 s
Output throughput: 1068.881 token/s
.
.
End (0/62):
filename='ascend/llm_models/test_ascend_glm4_9b_chat.py', elapsed=74, estimated_time=400
.
.

.
.
Begin (1/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_granite_3_1_8b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:02:06] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.1-8b-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.1-8b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=731389920, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.1-8b-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:02:06] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:02:15] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:02:16] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:02:16] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:02:16] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:02:17] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.38it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:11<00:13,  6.51s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:14<00:05,  5.10s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:25<00:00,  7.26s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:25<00:00,  6.30s/it]

[2026-01-17 08:02:43] Load weight end. type=GraniteForCausalLM, dtype=torch.bfloat16, avail mem=45.55 GB, mem usage=15.26 GB.
[2026-01-17 08:02:43] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:02:43] The available memory for KV cache is 33.35 GB.
[2026-01-17 08:02:44] KV Cache is allocated. #tokens: 218496, K size: 16.68 GB, V size: 16.68 GB
[2026-01-17 08:02:44] Memory pool end. avail mem=11.16 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:02:44] max_total_num_tokens=218496, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=11.16 GB
[2026-01-17 08:02:44] INFO:     Started server process [8262]
[2026-01-17 08:02:44] INFO:     Waiting for application startup.
[2026-01-17 08:02:44] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:02:44] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:02:44] INFO:     Application startup complete.
[2026-01-17 08:02:44] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:02:45] INFO:     127.0.0.1:35490 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:02:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:02:46] INFO:     127.0.0.1:35516 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:02:56] INFO:     127.0.0.1:42552 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:02:58] INFO:     127.0.0.1:35506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:02:58] The server is fired up and ready to roll!
[2026-01-17 08:03:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:03:07] INFO:     127.0.0.1:36508 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:03:07] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:03:07] INFO:     127.0.0.1:47472 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:03:07] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:03:07] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/ibm-granite/granite-3.1-8b-instruct --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:03:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:03:07] Prefill batch, #new-seq: 14, #new-token: 3584, #cached-token: 10752, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:03:09] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.02, #running-req: 15, #queue-req: 81,
[2026-01-17 08:03:10] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.06, #running-req: 47, #queue-req: 49,
[2026-01-17 08:03:10] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.10, #running-req: 79, #queue-req: 17,
[2026-01-17 08:03:11] Prefill batch, #new-seq: 17, #new-token: 4352, #cached-token: 13056, token usage: 0.13, #running-req: 111, #queue-req: 0,
[2026-01-17 08:03:13] Decode batch, #running-req: 128, #token: 34304, token usage: 0.16, npu graph: False, gen throughput (token/s): 67.66, #queue-req: 0,
[2026-01-17 08:03:13] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:06<19:59,  6.03s/it][2026-01-17 08:03:13] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:14] INFO:     127.0.0.1:48552 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:06<08:30,  2.58s/it][2026-01-17 08:03:14] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:14] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:14] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 128, #queue-req: 0,
[2026-01-17 08:03:14] INFO:     127.0.0.1:47986 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:06<03:18,  1.01s/it][2026-01-17 08:03:14] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:14] INFO:     127.0.0.1:47528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:14] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:14] INFO:     127.0.0.1:48330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:14] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.16, #running-req: 125, #queue-req: 0,
[2026-01-17 08:03:14] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:14] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:06<01:17,  2.48it/s]
  4%|▍         | 9/200 [00:06<00:40,  4.67it/s][2026-01-17 08:03:14] INFO:     127.0.0.1:47786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:14] INFO:     127.0.0.1:47850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:14] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.16, #running-req: 126, #queue-req: 0,
[2026-01-17 08:03:15] INFO:     127.0.0.1:48132 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:07<00:51,  3.65it/s][2026-01-17 08:03:15] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.16, #running-req: 128, #queue-req: 0,
[2026-01-17 08:03:15] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 130, #queue-req: 0,
[2026-01-17 08:03:15] INFO:     127.0.0.1:47940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:15] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:07<00:43,  4.32it/s][2026-01-17 08:03:15] INFO:     127.0.0.1:48288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:15] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.16, #running-req: 126, #queue-req: 0,
[2026-01-17 08:03:15] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:15] INFO:     127.0.0.1:48508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:15] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 128, #queue-req: 0,
[2026-01-17 08:03:15] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.17, #running-req: 129, #queue-req: 0,
[2026-01-17 08:03:16] INFO:     127.0.0.1:48170 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:08<00:29,  6.22it/s][2026-01-17 08:03:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:16] INFO:     127.0.0.1:48370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:16] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:16] INFO:     127.0.0.1:47760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:16] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:08<00:28,  6.30it/s]
 11%|█         | 22/200 [00:08<00:20,  8.65it/s]
 11%|█         | 22/200 [00:08<00:20,  8.65it/s][2026-01-17 08:03:16] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:16] INFO:     127.0.0.1:48606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:16] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.16, #running-req: 125, #queue-req: 0,
[2026-01-17 08:03:17] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.17, #running-req: 128, #queue-req: 0,
[2026-01-17 08:03:17] INFO:     127.0.0.1:47514 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:09<00:37,  4.73it/s][2026-01-17 08:03:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:17] Decode batch, #running-req: 127, #token: 37376, token usage: 0.17, npu graph: False, gen throughput (token/s): 1216.44, #queue-req: 0,
[2026-01-17 08:03:17] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:10<00:36,  4.76it/s][2026-01-17 08:03:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:18] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:18] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:18] INFO:     127.0.0.1:48210 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:10<00:29,  5.75it/s][2026-01-17 08:03:18] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.18, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:18] INFO:     127.0.0.1:47624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:18] INFO:     127.0.0.1:48214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:18] INFO:     127.0.0.1:48350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:18] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:10<00:29,  5.78it/s]
 16%|█▌        | 32/200 [00:10<00:13, 12.38it/s]
 16%|█▌        | 32/200 [00:10<00:13, 12.38it/s]
 16%|█▌        | 32/200 [00:10<00:13, 12.38it/s][2026-01-17 08:03:18] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 3072, token usage: 0.18, #running-req: 124, #queue-req: 0,
[2026-01-17 08:03:19] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:19] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.18, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:19] INFO:     127.0.0.1:47872 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:11<00:35,  4.63it/s][2026-01-17 08:03:19] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.18, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:19] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:19] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:19] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.18, #running-req: 126, #queue-req: 0,
[2026-01-17 08:03:20] INFO:     127.0.0.1:47686 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:20] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:20] INFO:     127.0.0.1:47806 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:12<00:46,  3.49it/s]
 20%|█▉        | 39/200 [00:12<00:45,  3.54it/s]
 20%|█▉        | 39/200 [00:12<00:45,  3.54it/s][2026-01-17 08:03:20] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.18, #running-req: 125, #queue-req: 0,
[2026-01-17 08:03:20] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:20] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:21] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:13<00:40,  3.90it/s][2026-01-17 08:03:21] INFO:     127.0.0.1:48516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:21] INFO:     127.0.0.1:47958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-17 08:03:21] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] INFO:     127.0.0.1:48380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.19, #running-req: 129, #queue-req: 0,
[2026-01-17 08:03:21] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:13<00:26,  5.85it/s][2026-01-17 08:03:21] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:21] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-17 08:03:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 129, #queue-req: 0,
[2026-01-17 08:03:21] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:13<00:21,  6.89it/s][2026-01-17 08:03:21] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:21] INFO:     127.0.0.1:48338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] INFO:     127.0.0.1:48434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-17 08:03:21] INFO:     127.0.0.1:48200 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:13<00:16,  8.78it/s][2026-01-17 08:03:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:21] INFO:     127.0.0.1:47700 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:21] INFO:     127.0.0.1:47928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:21] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:14<00:16,  8.99it/s]
 28%|██▊       | 57/200 [00:14<00:11, 12.36it/s]
 28%|██▊       | 57/200 [00:14<00:11, 12.36it/s][2026-01-17 08:03:22] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.19, #running-req: 125, #queue-req: 0,
[2026-01-17 08:03:22] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:22] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:22] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:14<00:12, 11.43it/s][2026-01-17 08:03:22] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:22] INFO:     127.0.0.1:48414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:22] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:22] Decode batch, #running-req: 128, #token: 42240, token usage: 0.19, npu graph: False, gen throughput (token/s): 1109.42, #queue-req: 0,
[2026-01-17 08:03:22] INFO:     127.0.0.1:48590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:22] INFO:     127.0.0.1:48686 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:14<00:12, 11.01it/s]
 31%|███       | 62/200 [00:14<00:11, 11.98it/s][2026-01-17 08:03:22] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:22] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.19, #running-req: 126, #queue-req: 0,
[2026-01-17 08:03:22] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-17 08:03:22] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:22] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:14<00:12, 11.29it/s]
 32%|███▎      | 65/200 [00:14<00:11, 12.13it/s][2026-01-17 08:03:22] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:22] INFO:     127.0.0.1:48698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:22] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.19, #running-req: 126, #queue-req: 0,
[2026-01-17 08:03:22] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:22] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-17 08:03:22] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 130, #queue-req: 0,
[2026-01-17 08:03:22] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:14<00:09, 14.04it/s][2026-01-17 08:03:22] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:22] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:22] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 08:03:22] INFO:     127.0.0.1:47702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:22] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:15<00:09, 14.17it/s]
 36%|███▌      | 72/200 [00:15<00:08, 15.98it/s][2026-01-17 08:03:22] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.19, #running-req: 126, #queue-req: 0,
[2026-01-17 08:03:23] INFO:     127.0.0.1:48838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:47600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48476 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:15<00:08, 15.56it/s]
 38%|███▊      | 76/200 [00:15<00:06, 20.28it/s]
 38%|███▊      | 76/200 [00:15<00:06, 20.28it/s][2026-01-17 08:03:23] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48420 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:15<00:05, 22.34it/s][2026-01-17 08:03:23] INFO:     127.0.0.1:47678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:47974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:47586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48566 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:15<00:04, 24.02it/s]
 42%|████▏     | 84/200 [00:15<00:03, 33.47it/s]
 42%|████▏     | 84/200 [00:15<00:03, 33.47it/s][2026-01-17 08:03:23] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:47838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:15<00:03, 35.14it/s]
 45%|████▌     | 90/200 [00:15<00:02, 44.92it/s]
 45%|████▌     | 90/200 [00:15<00:02, 44.92it/s][2026-01-17 08:03:23] INFO:     127.0.0.1:48308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:43124 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [00:15<00:02, 46.29it/s]
 48%|████▊     | 96/200 [00:15<00:02, 50.16it/s][2026-01-17 08:03:23] INFO:     127.0.0.1:47620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48118 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:47820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:23] INFO:     127.0.0.1:48442 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:15<00:02, 38.42it/s]
 52%|█████▏    | 103/200 [00:15<00:02, 35.14it/s][2026-01-17 08:03:24] Decode batch, #running-req: 97, #token: 33152, token usage: 0.15, npu graph: False, gen throughput (token/s): 2799.39, #queue-req: 0,
[2026-01-17 08:03:24] INFO:     127.0.0.1:48548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:47632 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:16<00:03, 24.99it/s]
 54%|█████▍    | 108/200 [00:16<00:04, 21.34it/s][2026-01-17 08:03:24] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43366 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:16<00:04, 21.77it/s]
 57%|█████▋    | 114/200 [00:16<00:02, 32.26it/s]
 57%|█████▋    | 114/200 [00:16<00:02, 32.26it/s]
 57%|█████▋    | 114/200 [00:16<00:02, 32.26it/s][2026-01-17 08:03:24] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48712 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48070 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:16<00:02, 32.30it/s][2026-01-17 08:03:24] INFO:     127.0.0.1:47526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43418 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:16<00:02, 36.33it/s][2026-01-17 08:03:24] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48794 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:16<00:02, 28.41it/s]
 65%|██████▌   | 130/200 [00:16<00:02, 25.71it/s][2026-01-17 08:03:24] INFO:     127.0.0.1:43456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43216 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:16<00:02, 23.30it/s][2026-01-17 08:03:24] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:24] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:43232 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:17<00:02, 28.41it/s]
 71%|███████   | 142/200 [00:17<00:01, 33.39it/s][2026-01-17 08:03:25] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:43534 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:17<00:01, 33.60it/s][2026-01-17 08:03:25] Decode batch, #running-req: 56, #token: 21248, token usage: 0.10, npu graph: False, gen throughput (token/s): 2461.19, #queue-req: 0,
[2026-01-17 08:03:25] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:43286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:17<00:01, 29.20it/s][2026-01-17 08:03:25] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:43338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:48800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:47718 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:17<00:01, 30.43it/s][2026-01-17 08:03:25] INFO:     127.0.0.1:43354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:48688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:17<00:01, 29.82it/s][2026-01-17 08:03:25] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:47640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:43440 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:17<00:01, 27.95it/s][2026-01-17 08:03:25] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:25] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:18<00:01, 27.67it/s][2026-01-17 08:03:25] INFO:     127.0.0.1:43134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:26] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:26] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:18<00:01, 22.13it/s][2026-01-17 08:03:26] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:26] INFO:     127.0.0.1:43132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:26] Decode batch, #running-req: 31, #token: 13056, token usage: 0.06, npu graph: False, gen throughput (token/s): 1472.37, #queue-req: 0,
[2026-01-17 08:03:26] INFO:     127.0.0.1:48406 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:18<00:01, 19.96it/s][2026-01-17 08:03:26] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:26] INFO:     127.0.0.1:48784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:26] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:18<00:01, 17.77it/s][2026-01-17 08:03:26] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:26] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:26] INFO:     127.0.0.1:43438 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:18<00:01, 19.55it/s][2026-01-17 08:03:26] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:27] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:27] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:19<00:01, 12.97it/s][2026-01-17 08:03:27] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:27] INFO:     127.0.0.1:48612 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:19<00:01, 11.98it/s][2026-01-17 08:03:27] INFO:     127.0.0.1:43530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:27] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:27] Decode batch, #running-req: 17, #token: 7680, token usage: 0.04, npu graph: False, gen throughput (token/s): 841.31, #queue-req: 0,
[2026-01-17 08:03:27] INFO:     127.0.0.1:47564 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:19<00:01, 13.43it/s][2026-01-17 08:03:27] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:27] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:19<00:00, 13.76it/s][2026-01-17 08:03:28] INFO:     127.0.0.1:43476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:28] INFO:     127.0.0.1:43294 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:20<00:01,  9.09it/s][2026-01-17 08:03:28] INFO:     127.0.0.1:43466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:28] INFO:     127.0.0.1:43300 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:20<00:00, 10.46it/s][2026-01-17 08:03:28] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:28] Decode batch, #running-req: 8, #token: 4736, token usage: 0.02, npu graph: False, gen throughput (token/s): 451.10, #queue-req: 0,
[2026-01-17 08:03:28] INFO:     127.0.0.1:43154 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:20<00:00,  7.91it/s][2026-01-17 08:03:28] INFO:     127.0.0.1:43492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:03:29] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:21<00:00,  5.62it/s][2026-01-17 08:03:29] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:21<00:00,  6.04it/s][2026-01-17 08:03:29] Decode batch, #running-req: 4, #token: 2816, token usage: 0.01, npu graph: False, gen throughput (token/s): 227.13, #queue-req: 0,
[2026-01-17 08:03:29] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:21<00:00,  4.48it/s][2026-01-17 08:03:30] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:22<00:00,  4.32it/s][2026-01-17 08:03:30] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:22<00:00,  4.20it/s][2026-01-17 08:03:30] Decode batch, #running-req: 1, #token: 1280, token usage: 0.01, npu graph: False, gen throughput (token/s): 100.14, #queue-req: 0,
[2026-01-17 08:03:31] Decode batch, #running-req: 1, #token: 1280, token usage: 0.01, npu graph: False, gen throughput (token/s): 40.18, #queue-req: 0,
[2026-01-17 08:03:32] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, npu graph: False, gen throughput (token/s): 38.48, #queue-req: 0,
[2026-01-17 08:03:33] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, npu graph: False, gen throughput (token/s): 37.85, #queue-req: 0,
[2026-01-17 08:03:34] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, npu graph: False, gen throughput (token/s): 37.85, #queue-req: 0,
[2026-01-17 08:03:35] Decode batch, #running-req: 1, #token: 1536, token usage: 0.01, npu graph: False, gen throughput (token/s): 37.98, #queue-req: 0,
[2026-01-17 08:03:35] INFO:     127.0.0.1:43200 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:27<00:00,  1.58s/it]
100%|██████████| 200/200 [00:28<00:00,  7.14it/s]
.
----------------------------------------------------------------------
Ran 1 test in 99.299s

OK
Accuracy: 0.705
Invalid: 0.000
Latency: 28.087 s
Output throughput: 899.732 token/s
.
.
End (1/62):
filename='ascend/llm_models/test_ascend_granite_3_1_8b.py', elapsed=110, estimated_time=400
.
.

.
.
Begin (2/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_mimo_7b_rl.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:03:55] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-7B-RL', tokenizer_path='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-7B-RL', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=283580199, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-7B-RL', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:03:56] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:04:05] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:04:06] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:04:06] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:04:06] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:04:06] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:17,  5.99s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:12<00:12,  6.33s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:18<00:06,  6.19s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:24<00:00,  6.07s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:24<00:00,  6.12s/it]

[2026-01-17 08:04:32] Load weight end. type=MiMoForCausalLM, dtype=torch.bfloat16, avail mem=46.59 GB, mem usage=14.22 GB.
[2026-01-17 08:04:32] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:04:32] The available memory for KV cache is 34.43 GB.
[2026-01-17 08:04:33] KV Cache is allocated. #tokens: 250624, K size: 17.22 GB, V size: 17.22 GB
[2026-01-17 08:04:33] Memory pool end. avail mem=11.67 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:04:33] max_total_num_tokens=250624, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=3917, context_len=32768, available_gpu_mem=11.67 GB
[2026-01-17 08:04:34] INFO:     Started server process [11289]
[2026-01-17 08:04:34] INFO:     Waiting for application startup.
[2026-01-17 08:04:34] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:04:34] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:04:34] INFO:     Application startup complete.
[2026-01-17 08:04:34] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:04:35] INFO:     127.0.0.1:52622 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:04:35] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:04:36] INFO:     127.0.0.1:52626 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:04:46] INFO:     127.0.0.1:35990 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:04:49] INFO:     127.0.0.1:52624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:04:49] The server is fired up and ready to roll!
[2026-01-17 08:04:56] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:04:57] INFO:     127.0.0.1:58934 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:04:57] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:04:57] INFO:     127.0.0.1:54462 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:04:57] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:04:57] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-7B-RL --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:04:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:04:57] Prefill batch, #new-seq: 12, #new-token: 1792, #cached-token: 9216, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:04:57] Prefill batch, #new-seq: 20, #new-token: 2560, #cached-token: 15360, token usage: 0.01, #running-req: 13, #queue-req: 0,
[2026-01-17 08:04:58] Prefill batch, #new-seq: 60, #new-token: 8192, #cached-token: 46080, token usage: 0.02, #running-req: 33, #queue-req: 35,
[2026-01-17 08:04:59] Prefill batch, #new-seq: 35, #new-token: 4480, #cached-token: 26880, token usage: 0.05, #running-req: 93, #queue-req: 0,
[2026-01-17 08:05:01] Decode batch, #running-req: 128, #token: 21376, token usage: 0.09, npu graph: False, gen throughput (token/s): 69.85, #queue-req: 0,
[2026-01-17 08:05:01] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:01] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:04<14:38,  4.41s/it]
  1%|          | 2/200 [00:04<09:10,  2.78s/it][2026-01-17 08:05:01] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-17 08:05:01] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:02] INFO:     127.0.0.1:54890 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:04<04:38,  1.42s/it][2026-01-17 08:05:02] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:02] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 08:05:02] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:02] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:04<02:17,  1.41it/s]
  4%|▍         | 8/200 [00:04<01:13,  2.62it/s][2026-01-17 08:05:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:05:03] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:03] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:03] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:06<01:31,  2.08it/s]
  6%|▌         | 11/200 [00:06<01:32,  2.04it/s][2026-01-17 08:05:03] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:03] INFO:     127.0.0.1:55090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:05:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:04] INFO:     127.0.0.1:54734 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:06<01:03,  2.95it/s][2026-01-17 08:05:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:04] Decode batch, #running-req: 127, #token: 31104, token usage: 0.12, npu graph: False, gen throughput (token/s): 1916.87, #queue-req: 0,
[2026-01-17 08:05:04] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,

  8%|▊         | 16/200 [00:06<00:50,  3.67it/s][2026-01-17 08:05:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:04] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:06<00:39,  4.63it/s][2026-01-17 08:05:04] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:05:04] INFO:     127.0.0.1:55428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-17 08:05:04] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:06<00:25,  7.01it/s][2026-01-17 08:05:04] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:04] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] INFO:     127.0.0.1:55110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:04] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-17 08:05:04] INFO:     127.0.0.1:54642 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:07<00:15, 11.24it/s][2026-01-17 08:05:04] INFO:     127.0.0.1:55034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:04] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-17 08:05:04] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:07<00:13, 12.12it/s][2026-01-17 08:05:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:04] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:05] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:07<00:13, 12.08it/s]
 18%|█▊        | 35/200 [00:07<00:12, 13.15it/s][2026-01-17 08:05:05] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:05:05] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-17 08:05:05] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:07<00:11, 14.20it/s]
 20%|█▉        | 39/200 [00:07<00:09, 16.50it/s][2026-01-17 08:05:05] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:05:05] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:05] INFO:     127.0.0.1:55006 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:07<00:10, 14.55it/s][2026-01-17 08:05:05] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:05] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] INFO:     127.0.0.1:54566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:05] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 131, #queue-req: 0,
[2026-01-17 08:05:05] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] INFO:     127.0.0.1:55150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:08<00:08, 18.32it/s]
 25%|██▌       | 50/200 [00:08<00:04, 30.47it/s]
 25%|██▌       | 50/200 [00:08<00:04, 30.47it/s]
 25%|██▌       | 50/200 [00:08<00:04, 30.47it/s][2026-01-17 08:05:05] INFO:     127.0.0.1:54750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.11, #running-req: 124, #queue-req: 0,
[2026-01-17 08:05:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:05] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:05] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:05] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:08<00:05, 27.40it/s]
 28%|██▊       | 55/200 [00:08<00:05, 26.56it/s][2026-01-17 08:05:05] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 08:05:05] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:05] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:08<00:05, 26.65it/s][2026-01-17 08:05:06] Decode batch, #running-req: 128, #token: 27776, token usage: 0.11, npu graph: False, gen throughput (token/s): 2655.36, #queue-req: 0,
[2026-01-17 08:05:06] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-17 08:05:06] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:06] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:06] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:08<00:06, 21.91it/s][2026-01-17 08:05:06] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:05:06] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:08<00:06, 21.75it/s]
 32%|███▎      | 65/200 [00:08<00:05, 23.39it/s][2026-01-17 08:05:06] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-17 08:05:06] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:06] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:08<00:05, 23.95it/s][2026-01-17 08:05:06] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:05:06] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:06] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:06] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:09<00:06, 19.86it/s][2026-01-17 08:05:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:05:06] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:05:06] INFO:     127.0.0.1:55064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:09<00:05, 21.30it/s]
 38%|███▊      | 75/200 [00:09<00:05, 24.61it/s][2026-01-17 08:05:06] INFO:     127.0.0.1:55412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:06] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:54930 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:09<00:06, 19.15it/s][2026-01-17 08:05:07] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:54708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:09<00:05, 20.66it/s][2026-01-17 08:05:07] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:55774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:55836 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:09<00:05, 21.91it/s]
 43%|████▎     | 86/200 [00:09<00:03, 29.91it/s]
 43%|████▎     | 86/200 [00:09<00:03, 29.91it/s][2026-01-17 08:05:07] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] Decode batch, #running-req: 112, #token: 28416, token usage: 0.11, npu graph: False, gen throughput (token/s): 3326.34, #queue-req: 0,
[2026-01-17 08:05:07] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:55630 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:10<00:05, 19.56it/s][2026-01-17 08:05:07] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:10<00:05, 20.84it/s][2026-01-17 08:05:07] INFO:     127.0.0.1:55166 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:07] INFO:     127.0.0.1:55768 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:10<00:03, 27.70it/s][2026-01-17 08:05:07] INFO:     127.0.0.1:54966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:10<00:03, 27.91it/s][2026-01-17 08:05:08] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:10<00:03, 29.72it/s][2026-01-17 08:05:08] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:10<00:03, 23.82it/s]
 56%|█████▋    | 113/200 [00:10<00:03, 22.99it/s]
 56%|█████▋    | 113/200 [00:10<00:03, 22.99it/s][2026-01-17 08:05:08] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:56154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:11<00:03, 23.67it/s][2026-01-17 08:05:08] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:54626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] Decode batch, #running-req: 82, #token: 25472, token usage: 0.10, npu graph: False, gen throughput (token/s): 3493.67, #queue-req: 0,
[2026-01-17 08:05:08] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:11<00:03, 24.38it/s][2026-01-17 08:05:08] INFO:     127.0.0.1:54582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:56314 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:11<00:03, 25.01it/s][2026-01-17 08:05:08] INFO:     127.0.0.1:56060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:54816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:55126 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:11<00:03, 22.76it/s]
 63%|██████▎   | 126/200 [00:11<00:03, 23.21it/s][2026-01-17 08:05:08] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:08] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:11<00:02, 24.09it/s]
 65%|██████▌   | 130/200 [00:11<00:02, 27.06it/s][2026-01-17 08:05:09] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:11<00:02, 25.56it/s]
 67%|██████▋   | 134/200 [00:11<00:02, 26.75it/s][2026-01-17 08:05:09] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:55654 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:11<00:02, 21.65it/s][2026-01-17 08:05:09] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:56354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:12<00:02, 23.73it/s]
 71%|███████   | 142/200 [00:12<00:02, 27.34it/s][2026-01-17 08:05:09] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] Decode batch, #running-req: 57, #token: 19200, token usage: 0.08, npu graph: False, gen throughput (token/s): 2525.21, #queue-req: 0,
[2026-01-17 08:05:09] INFO:     127.0.0.1:56350 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:12<00:02, 22.35it/s][2026-01-17 08:05:09] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:09] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:12<00:02, 21.27it/s][2026-01-17 08:05:09] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:10] INFO:     127.0.0.1:55992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:10] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:12<00:02, 17.29it/s][2026-01-17 08:05:10] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:10] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:12<00:03, 13.84it/s][2026-01-17 08:05:10] Decode batch, #running-req: 47, #token: 18560, token usage: 0.07, npu graph: False, gen throughput (token/s): 1921.74, #queue-req: 0,
[2026-01-17 08:05:10] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:10] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:13<00:04,  9.06it/s][2026-01-17 08:05:10] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:11] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:13<00:05,  8.49it/s][2026-01-17 08:05:11] INFO:     127.0.0.1:56266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:11] Decode batch, #running-req: 42, #token: 19200, token usage: 0.08, npu graph: False, gen throughput (token/s): 1713.80, #queue-req: 0,
[2026-01-17 08:05:11] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:14<00:06,  6.01it/s][2026-01-17 08:05:11] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:14<00:06,  6.38it/s][2026-01-17 08:05:11] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:14<00:05,  6.80it/s][2026-01-17 08:05:12] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:12] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:14<00:05,  7.11it/s][2026-01-17 08:05:12] INFO:     127.0.0.1:56250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:12] INFO:     127.0.0.1:56284 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:12] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:15<00:07,  5.06it/s]
 83%|████████▎ | 166/200 [00:15<00:06,  5.62it/s]
 83%|████████▎ | 166/200 [00:15<00:06,  5.62it/s][2026-01-17 08:05:12] Decode batch, #running-req: 34, #token: 17024, token usage: 0.07, npu graph: False, gen throughput (token/s): 1480.50, #queue-req: 0,
[2026-01-17 08:05:13] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:16<00:10,  3.01it/s][2026-01-17 08:05:13] Decode batch, #running-req: 33, #token: 17408, token usage: 0.07, npu graph: False, gen throughput (token/s): 1357.02, #queue-req: 0,
[2026-01-17 08:05:14] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:16<00:10,  3.02it/s][2026-01-17 08:05:14] Decode batch, #running-req: 32, #token: 19200, token usage: 0.08, npu graph: False, gen throughput (token/s): 1285.75, #queue-req: 0,
[2026-01-17 08:05:14] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:17<00:13,  2.24it/s][2026-01-17 08:05:15] Decode batch, #running-req: 31, #token: 19584, token usage: 0.08, npu graph: False, gen throughput (token/s): 1243.24, #queue-req: 0,
[2026-01-17 08:05:16] INFO:     127.0.0.1:55018 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:18<00:19,  1.52it/s][2026-01-17 08:05:16] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] Decode batch, #running-req: 30, #token: 7296, token usage: 0.03, npu graph: False, gen throughput (token/s): 1222.53, #queue-req: 0,
[2026-01-17 08:05:16] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:54796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:54850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:16] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:19<00:18,  1.58it/s]
 94%|█████████▍| 189/200 [00:19<00:00, 15.28it/s]
 94%|█████████▍| 189/200 [00:19<00:00, 15.28it/s]
 94%|█████████▍| 189/200 [00:19<00:00, 15.28it/s]
 94%|█████████▍| 189/200 [00:19<00:00, 15.28it/s]
 94%|█████████▍| 189/200 [00:19<00:00, 15.28it/s]
 94%|█████████▍| 189/200 [00:19<00:00, 15.28it/s]
 94%|█████████▍| 189/200 [00:19<00:00, 15.28it/s]
 94%|█████████▍| 189/200 [00:19<00:00, 15.28it/s]
 94%|█████████▍| 189/200 [00:19<00:00, 15.28it/s]
 94%|█████████▍| 189/200 [00:19<00:00, 15.28it/s][2026-01-17 08:05:17] Decode batch, #running-req: 11, #token: 7680, token usage: 0.03, npu graph: False, gen throughput (token/s): 474.69, #queue-req: 0,
[2026-01-17 08:05:18] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:18] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:20<00:01,  7.56it/s][2026-01-17 08:05:18] INFO:     127.0.0.1:55852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:18] INFO:     127.0.0.1:55858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:18] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:21<00:00,  7.70it/s]
 97%|█████████▋| 194/200 [00:21<00:00,  8.40it/s][2026-01-17 08:05:18] Decode batch, #running-req: 6, #token: 3328, token usage: 0.01, npu graph: False, gen throughput (token/s): 420.16, #queue-req: 0,
[2026-01-17 08:05:18] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:18] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:19] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:21<00:00,  8.25it/s][2026-01-17 08:05:19] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:05:19] INFO:     127.0.0.1:56270 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:22<00:00,  6.94it/s][2026-01-17 08:05:19] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, npu graph: False, gen throughput (token/s): 117.43, #queue-req: 0,
[2026-01-17 08:05:19] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:22<00:00,  6.19it/s]
100%|██████████| 200/200 [00:22<00:00,  8.94it/s]
.
----------------------------------------------------------------------
Ran 1 test in 93.734s

OK
Accuracy: 0.795
Invalid: 0.000
Latency: 22.462 s
Output throughput: 1653.545 token/s
.
.
End (2/62):
filename='ascend/llm_models/test_ascend_mimo_7b_rl.py', elapsed=104, estimated_time=400
.
.

.
.
Begin (3/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_persimmon_8b_chat.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:05:40] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Howeee/persimmon-8b-chat', tokenizer_path='/root/.cache/modelscope/hub/models/Howeee/persimmon-8b-chat', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=318118436, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Howeee/persimmon-8b-chat', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-17 08:05:43] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:05:52] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:05:53] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:05:53] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:05:54] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:05:54] Load weight begin. avail mem=60.81 GB

Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading pt checkpoint shards:  50% Completed | 1/2 [00:24<00:24, 24.42s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:51<00:00, 26.18s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:51<00:00, 25.92s/it]

[2026-01-17 08:06:47] Load weight end. type=PersimmonForCausalLM, dtype=torch.bfloat16, avail mem=43.29 GB, mem usage=17.53 GB.
[2026-01-17 08:06:47] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:06:47] The available memory for KV cache is 31.12 GB.
[2026-01-17 08:06:47] KV Cache is allocated. #tokens: 56576, K size: 15.57 GB, V size: 15.57 GB
[2026-01-17 08:06:47] Memory pool end. avail mem=12.02 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:06:52] max_total_num_tokens=56576, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=16384, available_gpu_mem=12.02 GB
[2026-01-17 08:06:53] INFO:     Started server process [14107]
[2026-01-17 08:06:53] INFO:     Waiting for application startup.
[2026-01-17 08:06:53] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:06:53] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:06:53] INFO:     Application startup complete.
[2026-01-17 08:06:53] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:06:54] INFO:     127.0.0.1:34954 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:06:54] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:07:00] INFO:     127.0.0.1:40796 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:07:07] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:07] The server is fired up and ready to roll!
[2026-01-17 08:07:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:07:11] INFO:     127.0.0.1:51824 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:07:11] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:07:11] INFO:     127.0.0.1:51830 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:07:11] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:07:11] INFO:     127.0.0.1:51832 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Howeee/persimmon-8b-chat --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:07:11] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 08:07:11] Prefill batch, #new-seq: 14, #new-token: 3584, #cached-token: 8960, token usage: 0.02, #running-req: 1, #queue-req: 0,
[2026-01-17 08:07:11] Prefill batch, #new-seq: 18, #new-token: 4608, #cached-token: 11520, token usage: 0.08, #running-req: 15, #queue-req: 0,
[2026-01-17 08:07:11] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 20480, token usage: 0.16, #running-req: 33, #queue-req: 28,
[2026-01-17 08:07:11] Prefill batch, #new-seq: 20, #new-token: 5120, #cached-token: 12800, token usage: 0.31, #running-req: 65, #queue-req: 43,
[2026-01-17 08:07:13] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<06:44,  2.03s/it][2026-01-17 08:07:13] Prefill batch, #new-seq: 8, #new-token: 2048, #cached-token: 5120, token usage: 0.39, #running-req: 84, #queue-req: 36,
[2026-01-17 08:07:14] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:03<05:32,  1.68s/it][2026-01-17 08:07:14] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.42, #running-req: 91, #queue-req: 34,
[2026-01-17 08:07:15] INFO:     127.0.0.1:52090 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:03<03:34,  1.09s/it][2026-01-17 08:07:15] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:15] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.43, #running-req: 93, #queue-req: 33,
[2026-01-17 08:07:15] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:15] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.43, #running-req: 93, #queue-req: 33,
[2026-01-17 08:07:15] Decode batch, #running-req: 95, #token: 24960, token usage: 0.44, npu graph: False, gen throughput (token/s): 31.56, #queue-req: 33,
[2026-01-17 08:07:15] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:04<01:29,  2.17it/s][2026-01-17 08:07:15] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.44, #running-req: 94, #queue-req: 32,
[2026-01-17 08:07:16] INFO:     127.0.0.1:51868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:16] INFO:     127.0.0.1:52556 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:04<01:31,  2.11it/s]
  4%|▍         | 8/200 [00:04<01:14,  2.59it/s][2026-01-17 08:07:16] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.44, #running-req: 94, #queue-req: 31,
[2026-01-17 08:07:16] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:16] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.45, #running-req: 96, #queue-req: 30,
[2026-01-17 08:07:16] INFO:     127.0.0.1:52468 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:05<00:58,  3.26it/s][2026-01-17 08:07:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.45, #running-req: 97, #queue-req: 30,
[2026-01-17 08:07:16] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:05<00:52,  3.58it/s][2026-01-17 08:07:16] INFO:     127.0.0.1:52138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:16] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:16] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.45, #running-req: 97, #queue-req: 29,
[2026-01-17 08:07:16] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:16] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:16] INFO:     127.0.0.1:52700 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:05<00:30,  6.12it/s]
  8%|▊         | 16/200 [00:05<00:11, 16.02it/s]
  8%|▊         | 16/200 [00:05<00:11, 16.02it/s][2026-01-17 08:07:16] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 2560, token usage: 0.44, #running-req: 94, #queue-req: 30,
[2026-01-17 08:07:16] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:16] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.45, #running-req: 97, #queue-req: 29,
[2026-01-17 08:07:16] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:16] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:05<00:13, 12.98it/s][2026-01-17 08:07:17] INFO:     127.0.0.1:51954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:17] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:17] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:17] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.44, #running-req: 97, #queue-req: 28,
[2026-01-17 08:07:17] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.46, #running-req: 97, #queue-req: 29,
[2026-01-17 08:07:17] INFO:     127.0.0.1:52550 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:06<00:13, 13.23it/s][2026-01-17 08:07:17] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.46, #running-req: 98, #queue-req: 28,
[2026-01-17 08:07:17] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.47, #running-req: 99, #queue-req: 28,
[2026-01-17 08:07:17] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:17] INFO:     127.0.0.1:52406 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:06<00:15, 11.01it/s][2026-01-17 08:07:17] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.47, #running-req: 98, #queue-req: 28,
[2026-01-17 08:07:17] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.48, #running-req: 99, #queue-req: 28,
[2026-01-17 08:07:17] Decode batch, #running-req: 99, #token: 26880, token usage: 0.48, npu graph: False, gen throughput (token/s): 1567.23, #queue-req: 28,
[2026-01-17 08:07:17] INFO:     127.0.0.1:52182 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:06<00:15, 11.06it/s][2026-01-17 08:07:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.48, #running-req: 99, #queue-req: 28,
[2026-01-17 08:07:18] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:18] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.48, #running-req: 99, #queue-req: 28,
[2026-01-17 08:07:18] INFO:     127.0.0.1:52004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:18] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:18] INFO:     127.0.0.1:52434 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:06<00:16, 10.01it/s]
 16%|█▌        | 32/200 [00:06<00:14, 11.83it/s]
 16%|█▌        | 32/200 [00:06<00:14, 11.83it/s][2026-01-17 08:07:18] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.47, #running-req: 97, #queue-req: 28,
[2026-01-17 08:07:18] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:18] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:18] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.47, #running-req: 98, #queue-req: 28,
[2026-01-17 08:07:18] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:07<00:15, 10.93it/s][2026-01-17 08:07:18] INFO:     127.0.0.1:52284 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:18] INFO:     127.0.0.1:52490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:18] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.47, #running-req: 99, #queue-req: 26,
[2026-01-17 08:07:18] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:18] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.48, #running-req: 99, #queue-req: 28,
[2026-01-17 08:07:18] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:07<00:12, 12.99it/s][2026-01-17 08:07:18] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.48, #running-req: 99, #queue-req: 28,
[2026-01-17 08:07:18] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:18] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.48, #running-req: 99, #queue-req: 28,
[2026-01-17 08:07:18] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:18] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:07<00:12, 12.77it/s]
 21%|██        | 42/200 [00:07<00:11, 13.99it/s][2026-01-17 08:07:18] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.48, #running-req: 98, #queue-req: 28,
[2026-01-17 08:07:19] INFO:     127.0.0.1:52416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:19] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.50, #running-req: 99, #queue-req: 28,
[2026-01-17 08:07:19] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:07<00:15,  9.90it/s][2026-01-17 08:07:19] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:19] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.50, #running-req: 99, #queue-req: 28,
[2026-01-17 08:07:19] INFO:     127.0.0.1:52678 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:08<00:13, 11.26it/s][2026-01-17 08:07:19] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.50, #running-req: 98, #queue-req: 29,
[2026-01-17 08:07:19] INFO:     127.0.0.1:52380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:19] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.52, #running-req: 98, #queue-req: 29,
[2026-01-17 08:07:19] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:08<00:16,  9.34it/s][2026-01-17 08:07:19] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.52, #running-req: 98, #queue-req: 29,
[2026-01-17 08:07:19] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:19] INFO:     127.0.0.1:52802 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:08<00:15,  9.86it/s][2026-01-17 08:07:19] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:19] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 97, #queue-req: 29,
[2026-01-17 08:07:19] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:19] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:19] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.51, #running-req: 96, #queue-req: 30,
[2026-01-17 08:07:19] INFO:     127.0.0.1:52896 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:08<00:11, 12.81it/s][2026-01-17 08:07:20] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.52, #running-req: 97, #queue-req: 30,
[2026-01-17 08:07:20] INFO:     127.0.0.1:52362 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:20] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 97, #queue-req: 30,
[2026-01-17 08:07:20] INFO:     127.0.0.1:52444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:20] INFO:     127.0.0.1:52486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:20] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:09<00:13, 11.07it/s]
 29%|██▉       | 58/200 [00:09<00:11, 12.56it/s]
 29%|██▉       | 58/200 [00:09<00:11, 12.56it/s][2026-01-17 08:07:20] INFO:     127.0.0.1:52782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:20] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 95, #queue-req: 31,
[2026-01-17 08:07:20] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:20] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 95, #queue-req: 31,
[2026-01-17 08:07:20] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:09<00:10, 13.28it/s][2026-01-17 08:07:20] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 96, #queue-req: 30,
[2026-01-17 08:07:20] INFO:     127.0.0.1:51946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:20] INFO:     127.0.0.1:52184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:20] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:20] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.51, #running-req: 95, #queue-req: 30,
[2026-01-17 08:07:20] INFO:     127.0.0.1:52204 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:09<00:08, 15.40it/s][2026-01-17 08:07:20] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.52, #running-req: 97, #queue-req: 30,
[2026-01-17 08:07:20] Decode batch, #running-req: 97, #token: 29056, token usage: 0.51, npu graph: False, gen throughput (token/s): 1358.51, #queue-req: 30,
[2026-01-17 08:07:20] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:20] INFO:     127.0.0.1:53052 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:09<00:08, 16.03it/s][2026-01-17 08:07:20] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.51, #running-req: 96, #queue-req: 30,
[2026-01-17 08:07:20] INFO:     127.0.0.1:52610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:20] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 97, #queue-req: 29,
[2026-01-17 08:07:21] INFO:     127.0.0.1:52766 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:09<00:10, 12.71it/s][2026-01-17 08:07:21] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:21] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 98, #queue-req: 28,
[2026-01-17 08:07:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 99, #queue-req: 28,
[2026-01-17 08:07:21] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:10<00:12, 10.05it/s][2026-01-17 08:07:21] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.52, #running-req: 99, #queue-req: 28,
[2026-01-17 08:07:21] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:21] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:10<00:11, 11.45it/s]
 37%|███▋      | 74/200 [00:10<00:08, 14.56it/s][2026-01-17 08:07:21] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 97, #queue-req: 27,
[2026-01-17 08:07:21] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:21] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 98, #queue-req: 25,
[2026-01-17 08:07:21] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:21] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [00:10<00:09, 13.61it/s]
 38%|███▊      | 77/200 [00:10<00:08, 14.66it/s][2026-01-17 08:07:21] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.53, #running-req: 98, #queue-req: 23,
[2026-01-17 08:07:21] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 99, #queue-req: 22,
[2026-01-17 08:07:21] INFO:     127.0.0.1:52086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:21] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:21] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:10<00:10, 11.72it/s]
 40%|████      | 81/200 [00:10<00:09, 12.55it/s]
 40%|████      | 81/200 [00:10<00:09, 12.55it/s][2026-01-17 08:07:21] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.52, #running-req: 97, #queue-req: 19,
[2026-01-17 08:07:21] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:21] INFO:     127.0.0.1:53048 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:10<00:08, 13.64it/s][2026-01-17 08:07:22] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.53, #running-req: 98, #queue-req: 17,
[2026-01-17 08:07:22] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:22] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:22] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 99, #queue-req: 15,
[2026-01-17 08:07:22] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:11<00:10, 10.93it/s]
 43%|████▎     | 86/200 [00:11<00:10, 10.62it/s][2026-01-17 08:07:22] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:22] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.53, #running-req: 98, #queue-req: 13,
[2026-01-17 08:07:22] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:11<00:09, 11.22it/s][2026-01-17 08:07:22] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.53, #running-req: 99, #queue-req: 11,
[2026-01-17 08:07:22] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:22] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 100, #queue-req: 10,
[2026-01-17 08:07:22] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:22] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:11<00:09, 11.04it/s]
 46%|████▌     | 91/200 [00:11<00:08, 12.37it/s][2026-01-17 08:07:22] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.53, #running-req: 99, #queue-req: 8,
[2026-01-17 08:07:22] INFO:     127.0.0.1:52640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:22] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.53, #running-req: 100, #queue-req: 6,
[2026-01-17 08:07:23] INFO:     127.0.0.1:52166 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:11<00:12,  8.25it/s][2026-01-17 08:07:23] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:23] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.54, #running-req: 101, #queue-req: 5,
[2026-01-17 08:07:23] INFO:     127.0.0.1:41010 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [00:11<00:10,  9.72it/s][2026-01-17 08:07:23] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.54, #running-req: 100, #queue-req: 4,
[2026-01-17 08:07:23] INFO:     127.0.0.1:52250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:23] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:12<00:10, 10.24it/s][2026-01-17 08:07:23] INFO:     127.0.0.1:52020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:23] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.52, #running-req: 99, #queue-req: 1,
[2026-01-17 08:07:23] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:23] INFO:     127.0.0.1:52316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:23] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:12<00:07, 13.41it/s]
 50%|█████     | 101/200 [00:12<00:05, 18.26it/s][2026-01-17 08:07:23] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 98, #queue-req: 0,
[2026-01-17 08:07:23] INFO:     127.0.0.1:51940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:23] Decode batch, #running-req: 98, #token: 29952, token usage: 0.53, npu graph: False, gen throughput (token/s): 1358.13, #queue-req: 0,
[2026-01-17 08:07:23] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:23] INFO:     127.0.0.1:52950 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:12<00:08, 11.49it/s][2026-01-17 08:07:24] INFO:     127.0.0.1:52918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:24] INFO:     127.0.0.1:41262 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:12<00:07, 11.91it/s][2026-01-17 08:07:24] INFO:     127.0.0.1:52196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:24] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:24] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:24] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:13<00:05, 15.27it/s][2026-01-17 08:07:24] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:24] INFO:     127.0.0.1:52852 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:13<00:05, 14.87it/s][2026-01-17 08:07:24] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:24] INFO:     127.0.0.1:41054 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:13<00:05, 14.56it/s][2026-01-17 08:07:24] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:24] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:24] INFO:     127.0.0.1:52934 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:13<00:05, 16.28it/s][2026-01-17 08:07:24] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:24] INFO:     127.0.0.1:52800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:24] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:13<00:05, 14.96it/s][2026-01-17 08:07:24] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:24] INFO:     127.0.0.1:52624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:40938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:40978 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:13<00:04, 18.61it/s][2026-01-17 08:07:25] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:14<00:06, 12.02it/s]
 66%|██████▌   | 131/200 [00:14<00:05, 12.21it/s]
 66%|██████▌   | 131/200 [00:14<00:05, 12.21it/s]
 66%|██████▌   | 131/200 [00:14<00:05, 12.21it/s]
 66%|██████▌   | 131/200 [00:14<00:05, 12.21it/s][2026-01-17 08:07:25] Decode batch, #running-req: 74, #token: 23040, token usage: 0.41, npu graph: False, gen throughput (token/s): 1808.70, #queue-req: 0,
[2026-01-17 08:07:25] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:41080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:41066 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:14<00:05, 13.09it/s][2026-01-17 08:07:25] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:40990 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [00:14<00:04, 12.83it/s][2026-01-17 08:07:25] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:41234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:25] INFO:     127.0.0.1:41304 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:14<00:04, 14.68it/s][2026-01-17 08:07:26] INFO:     127.0.0.1:52740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:26] INFO:     127.0.0.1:53180 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:14<00:03, 15.02it/s][2026-01-17 08:07:26] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:26] INFO:     127.0.0.1:41170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:26] INFO:     127.0.0.1:41036 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:14<00:03, 17.03it/s][2026-01-17 08:07:26] INFO:     127.0.0.1:41028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:26] INFO:     127.0.0.1:41134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:26] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:15<00:03, 17.52it/s][2026-01-17 08:07:26] INFO:     127.0.0.1:52838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:26] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:26] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:15<00:02, 17.99it/s][2026-01-17 08:07:26] INFO:     127.0.0.1:41272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:26] INFO:     127.0.0.1:41000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:26] INFO:     127.0.0.1:41184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:26] INFO:     127.0.0.1:53230 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:15<00:02, 17.91it/s][2026-01-17 08:07:26] INFO:     127.0.0.1:53158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:26] INFO:     127.0.0.1:41124 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:15<00:02, 15.83it/s][2026-01-17 08:07:26] INFO:     127.0.0.1:41280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:27] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:27] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:27] INFO:     127.0.0.1:41320 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:15<00:02, 18.09it/s]
 80%|████████  | 160/200 [00:15<00:01, 22.10it/s][2026-01-17 08:07:27] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:27] Decode batch, #running-req: 40, #token: 15872, token usage: 0.28, npu graph: False, gen throughput (token/s): 1388.69, #queue-req: 0,
[2026-01-17 08:07:27] INFO:     127.0.0.1:41114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:27] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:16<00:01, 19.22it/s][2026-01-17 08:07:27] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:27] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:27] INFO:     127.0.0.1:41032 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:16<00:01, 21.42it/s][2026-01-17 08:07:27] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:27] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:27] INFO:     127.0.0.1:40926 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:16<00:01, 17.02it/s][2026-01-17 08:07:27] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:27] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:28] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:16<00:02, 11.85it/s][2026-01-17 08:07:28] INFO:     127.0.0.1:41294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:28] Decode batch, #running-req: 27, #token: 12160, token usage: 0.21, npu graph: False, gen throughput (token/s): 987.48, #queue-req: 0,
[2026-01-17 08:07:28] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:17<00:02,  9.77it/s][2026-01-17 08:07:28] INFO:     127.0.0.1:41018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:28] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:17<00:02,  9.12it/s][2026-01-17 08:07:28] INFO:     127.0.0.1:41204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:28] INFO:     127.0.0.1:41150 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:17<00:02,  8.96it/s][2026-01-17 08:07:28] INFO:     127.0.0.1:40954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:29] Decode batch, #running-req: 21, #token: 10496, token usage: 0.19, npu graph: False, gen throughput (token/s): 780.49, #queue-req: 0,
[2026-01-17 08:07:29] INFO:     127.0.0.1:41344 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:18<00:03,  5.18it/s][2026-01-17 08:07:30] INFO:     127.0.0.1:41296 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:19<00:04,  3.99it/s][2026-01-17 08:07:30] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:19<00:04,  4.44it/s][2026-01-17 08:07:30] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:19<00:03,  4.69it/s][2026-01-17 08:07:30] Decode batch, #running-req: 17, #token: 9600, token usage: 0.17, npu graph: False, gen throughput (token/s): 678.19, #queue-req: 0,
[2026-01-17 08:07:31] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:20<00:05,  2.89it/s][2026-01-17 08:07:31] Decode batch, #running-req: 16, #token: 10112, token usage: 0.18, npu graph: False, gen throughput (token/s): 590.28, #queue-req: 0,
[2026-01-17 08:07:32] Decode batch, #running-req: 16, #token: 10240, token usage: 0.18, npu graph: False, gen throughput (token/s): 570.37, #queue-req: 0,
[2026-01-17 08:07:34] Decode batch, #running-req: 16, #token: 10880, token usage: 0.19, npu graph: False, gen throughput (token/s): 564.08, #queue-req: 0,
[2026-01-17 08:07:35] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:35] Decode batch, #running-req: 16, #token: 6656, token usage: 0.12, npu graph: False, gen throughput (token/s): 554.88, #queue-req: 0,
[2026-01-17 08:07:35] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:35] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:35] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:35] INFO:     127.0.0.1:51988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:35] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:35] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:24<00:18,  1.22s/it]
 96%|█████████▌| 191/200 [00:24<00:06,  1.38it/s]
 96%|█████████▌| 191/200 [00:24<00:06,  1.38it/s]
 96%|█████████▌| 191/200 [00:24<00:06,  1.38it/s]
 96%|█████████▌| 191/200 [00:24<00:06,  1.38it/s]
 96%|█████████▌| 191/200 [00:24<00:06,  1.38it/s]
 96%|█████████▌| 191/200 [00:24<00:06,  1.38it/s][2026-01-17 08:07:35] INFO:     127.0.0.1:52648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:35] INFO:     127.0.0.1:52652 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:24<00:05,  1.40it/s]
 96%|█████████▋| 193/200 [00:24<00:04,  1.58it/s][2026-01-17 08:07:35] INFO:     127.0.0.1:52736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:07:35] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:24<00:02,  1.93it/s][2026-01-17 08:07:36] Decode batch, #running-req: 5, #token: 3840, token usage: 0.07, npu graph: False, gen throughput (token/s): 305.41, #queue-req: 0,
[2026-01-17 08:07:37] Decode batch, #running-req: 5, #token: 4096, token usage: 0.07, npu graph: False, gen throughput (token/s): 186.49, #queue-req: 0,
[2026-01-17 08:07:38] INFO:     127.0.0.1:40968 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:26<00:02,  1.39it/s][2026-01-17 08:07:38] INFO:     127.0.0.1:41020 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:26<00:01,  1.60it/s][2026-01-17 08:07:38] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:27<00:01,  1.87it/s][2026-01-17 08:07:38] Decode batch, #running-req: 2, #token: 2048, token usage: 0.04, npu graph: False, gen throughput (token/s): 165.55, #queue-req: 0,
[2026-01-17 08:07:38] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:27<00:00,  2.02it/s][2026-01-17 08:07:38] INFO:     127.0.0.1:41220 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:27<00:00,  2.47it/s]
100%|██████████| 200/200 [00:27<00:00,  7.27it/s]
.
----------------------------------------------------------------------
Ran 1 test in 128.891s

OK
Accuracy: 0.180
Invalid: 0.010
Latency: 27.595 s
Output throughput: 956.923 token/s
.
.
End (3/62):
filename='ascend/llm_models/test_ascend_persimmon_8b_chat.py', elapsed=139, estimated_time=400
.
.

.
.
Begin (4/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_mistral_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:07:58] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/mistralai/Mistral-7B-Instruct-v0.2', tokenizer_path='/root/.cache/modelscope/hub/models/mistralai/Mistral-7B-Instruct-v0.2', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=687673985, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/mistralai/Mistral-7B-Instruct-v0.2', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:07:58] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:08:07] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:08:08] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:08:08] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:08:09] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:08:09] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:07<00:14,  7.08s/it]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:14<00:07,  7.42s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:22<00:00,  7.44s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:22<00:00,  7.40s/it]

[2026-01-17 08:08:32] Load weight end. type=MistralForCausalLM, dtype=torch.bfloat16, avail mem=47.29 GB, mem usage=13.52 GB.
[2026-01-17 08:08:32] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:08:32] The available memory for KV cache is 35.13 GB.
[2026-01-17 08:08:33] KV Cache is allocated. #tokens: 287744, K size: 17.57 GB, V size: 17.57 GB
[2026-01-17 08:08:33] Memory pool end. avail mem=11.65 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:08:33] max_total_num_tokens=287744, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.65 GB
[2026-01-17 08:08:33] INFO:     Started server process [16781]
[2026-01-17 08:08:33] INFO:     Waiting for application startup.
[2026-01-17 08:08:33] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:08:33] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:08:33] INFO:     Application startup complete.
[2026-01-17 08:08:33] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:08:34] INFO:     127.0.0.1:56202 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:08:35] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:08:38] INFO:     127.0.0.1:58342 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:08:48] INFO:     127.0.0.1:37746 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:08:54] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:08:54] The server is fired up and ready to roll!
[2026-01-17 08:08:58] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:08:59] INFO:     127.0.0.1:47452 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:08:59] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:08:59] INFO:     127.0.0.1:47468 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:08:59] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:08:59] INFO:     127.0.0.1:47480 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/mistralai/Mistral-7B-Instruct-v0.2 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:08:59] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:08:59] Prefill batch, #new-seq: 17, #new-token: 3840, #cached-token: 13056, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:08:59] Prefill batch, #new-seq: 33, #new-token: 6784, #cached-token: 25344, token usage: 0.02, #running-req: 18, #queue-req: 0,
[2026-01-17 08:09:01] Prefill batch, #new-seq: 38, #new-token: 8192, #cached-token: 29184, token usage: 0.04, #running-req: 51, #queue-req: 39,
[2026-01-17 08:09:01] Prefill batch, #new-seq: 40, #new-token: 8064, #cached-token: 29952, token usage: 0.07, #running-req: 88, #queue-req: 0,
[2026-01-17 08:09:05] Decode batch, #running-req: 128, #token: 33536, token usage: 0.12, npu graph: False, gen throughput (token/s): 68.79, #queue-req: 0,
[2026-01-17 08:09:05] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:05<18:01,  5.43s/it][2026-01-17 08:09:05] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:05] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:05] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:05] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:05<04:56,  1.51s/it][2026-01-17 08:09:05] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:05] INFO:     127.0.0.1:47666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:05] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:05] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:05<02:34,  1.26it/s]
  3%|▎         | 6/200 [00:05<01:21,  2.38it/s][2026-01-17 08:09:05] INFO:     127.0.0.1:47692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:05] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:09:05] INFO:     127.0.0.1:48288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-17 08:09:05] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:06<00:50,  3.78it/s][2026-01-17 08:09:06] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:06] INFO:     127.0.0.1:48548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-17 08:09:06] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:06<00:37,  5.02it/s][2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:06] INFO:     127.0.0.1:48092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:06] INFO:     127.0.0.1:48436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:06] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:06<00:29,  6.20it/s]
  8%|▊         | 15/200 [00:06<00:21,  8.60it/s][2026-01-17 08:09:06] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:06] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 125, #queue-req: 0,
[2026-01-17 08:09:06] Decode batch, #running-req: 125, #token: 33152, token usage: 0.12, npu graph: False, gen throughput (token/s): 3561.09, #queue-req: 0,
[2026-01-17 08:09:06] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:06] INFO:     127.0.0.1:48376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:06] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-17 08:09:06] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:06<00:16, 11.05it/s][2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:06] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:06] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:06<00:15, 11.20it/s][2026-01-17 08:09:06] INFO:     127.0.0.1:48534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:06] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:07<00:14, 11.85it/s][2026-01-17 08:09:06] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:06] INFO:     127.0.0.1:48076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:48206 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:07<00:12, 13.48it/s][2026-01-17 08:09:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:48684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:48668 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:07<00:12, 14.26it/s][2026-01-17 08:09:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:07<00:09, 16.94it/s][2026-01-17 08:09:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:47594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] INFO:     127.0.0.1:47742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] INFO:     127.0.0.1:47756 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] INFO:     127.0.0.1:48598 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:07<00:09, 16.88it/s]
 18%|█▊        | 35/200 [00:07<00:07, 22.96it/s]
 18%|█▊        | 35/200 [00:07<00:07, 22.96it/s][2026-01-17 08:09:07] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] Prefill batch, #new-seq: 4, #new-token: 896, #cached-token: 3072, token usage: 0.12, #running-req: 124, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:48054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:07<00:08, 20.24it/s][2026-01-17 08:09:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] INFO:     127.0.0.1:47926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 129, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] INFO:     127.0.0.1:48564 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:07<00:08, 18.45it/s]
 21%|██        | 42/200 [00:07<00:08, 19.10it/s][2026-01-17 08:09:07] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:09:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:48056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] INFO:     127.0.0.1:48146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:07] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-17 08:09:07] INFO:     127.0.0.1:48490 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:08<00:07, 21.74it/s][2026-01-17 08:09:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.13, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:08] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] INFO:     127.0.0.1:47804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] INFO:     127.0.0.1:48266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:08<00:06, 22.66it/s]
 26%|██▌       | 51/200 [00:08<00:05, 25.50it/s][2026-01-17 08:09:08] Prefill batch, #new-seq: 4, #new-token: 768, #cached-token: 3072, token usage: 0.12, #running-req: 124, #queue-req: 0,
[2026-01-17 08:09:08] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:08] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:08] INFO:     127.0.0.1:48238 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:08<00:07, 19.96it/s][2026-01-17 08:09:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.13, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:08] Decode batch, #running-req: 128, #token: 37760, token usage: 0.13, npu graph: False, gen throughput (token/s): 2533.18, #queue-req: 0,
[2026-01-17 08:09:08] INFO:     127.0.0.1:47640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.13, #running-req: 126, #queue-req: 0,

 28%|██▊       | 57/200 [00:08<00:07, 19.32it/s][2026-01-17 08:09:08] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 129, #queue-req: 0,
[2026-01-17 08:09:08] INFO:     127.0.0.1:47514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:08<00:07, 18.75it/s][2026-01-17 08:09:08] INFO:     127.0.0.1:47888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.13, #running-req: 126, #queue-req: 0,
[2026-01-17 08:09:08] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.13, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 130, #queue-req: 0,
[2026-01-17 08:09:08] INFO:     127.0.0.1:47524 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:08<00:06, 20.01it/s][2026-01-17 08:09:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:08] INFO:     127.0.0.1:48132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] INFO:     127.0.0.1:48360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:08] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.13, #running-req: 126, #queue-req: 0,
[2026-01-17 08:09:09] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:09<00:06, 19.11it/s][2026-01-17 08:09:09] INFO:     127.0.0.1:47550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:09] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:09<00:08, 16.26it/s][2026-01-17 08:09:09] INFO:     127.0.0.1:48638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-17 08:09:09] INFO:     127.0.0.1:47944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.14, #running-req: 128, #queue-req: 0,
[2026-01-17 08:09:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.14, #running-req: 129, #queue-req: 0,
[2026-01-17 08:09:09] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:09<00:07, 16.74it/s]
 36%|███▋      | 73/200 [00:09<00:06, 18.81it/s][2026-01-17 08:09:09] INFO:     127.0.0.1:47760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.13, #running-req: 126, #queue-req: 0,
[2026-01-17 08:09:09] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:48204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:09<00:05, 22.52it/s][2026-01-17 08:09:09] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:48004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:09<00:05, 22.13it/s]
 40%|████      | 81/200 [00:09<00:04, 23.87it/s][2026-01-17 08:09:09] INFO:     127.0.0.1:47806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:09<00:04, 24.75it/s][2026-01-17 08:09:09] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:47966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:10<00:04, 23.81it/s][2026-01-17 08:09:09] INFO:     127.0.0.1:47540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:48222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:09] Decode batch, #running-req: 112, #token: 33664, token usage: 0.12, npu graph: False, gen throughput (token/s): 3233.64, #queue-req: 0,
[2026-01-17 08:09:09] INFO:     127.0.0.1:48350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48854 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:10<00:03, 28.38it/s][2026-01-17 08:09:10] INFO:     127.0.0.1:47732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:10<00:03, 29.95it/s]
 50%|████▉     | 99/200 [00:10<00:03, 33.44it/s][2026-01-17 08:09:10] INFO:     127.0.0.1:47854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:10<00:02, 34.55it/s][2026-01-17 08:09:10] INFO:     127.0.0.1:47564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48724 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:10<00:02, 37.54it/s][2026-01-17 08:09:10] INFO:     127.0.0.1:59578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:47612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:47706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48874 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:10<00:02, 37.31it/s][2026-01-17 08:09:10] INFO:     127.0.0.1:47684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:47980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:47778 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:10<00:02, 37.33it/s][2026-01-17 08:09:10] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:10<00:02, 34.97it/s][2026-01-17 08:09:10] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:11<00:02, 33.67it/s][2026-01-17 08:09:10] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:48626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:10] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:48886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] Decode batch, #running-req: 71, #token: 24704, token usage: 0.09, npu graph: False, gen throughput (token/s): 3358.97, #queue-req: 0,
[2026-01-17 08:09:11] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:11<00:02, 33.19it/s][2026-01-17 08:09:11] INFO:     127.0.0.1:48308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:48610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:59430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:11<00:02, 32.69it/s][2026-01-17 08:09:11] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:47708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:48846 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:11<00:02, 24.87it/s]
 70%|██████▉   | 139/200 [00:11<00:02, 22.72it/s][2026-01-17 08:09:11] INFO:     127.0.0.1:47852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:48902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:11<00:02, 27.45it/s]
 72%|███████▎  | 145/200 [00:11<00:01, 33.83it/s][2026-01-17 08:09:11] INFO:     127.0.0.1:47502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:48880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:11<00:01, 35.00it/s]
 75%|███████▌  | 150/200 [00:11<00:01, 38.36it/s][2026-01-17 08:09:11] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:47526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:11] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:12<00:01, 28.68it/s]
 78%|███████▊  | 156/200 [00:12<00:01, 25.94it/s][2026-01-17 08:09:12] INFO:     127.0.0.1:48858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:59638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] Decode batch, #running-req: 44, #token: 16256, token usage: 0.06, npu graph: False, gen throughput (token/s): 2252.09, #queue-req: 0,
[2026-01-17 08:09:12] INFO:     127.0.0.1:48262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:59618 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:12<00:02, 19.51it/s][2026-01-17 08:09:12] INFO:     127.0.0.1:48708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:48980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:12<00:01, 20.35it/s][2026-01-17 08:09:12] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:59550 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:12<00:01, 21.16it/s][2026-01-17 08:09:12] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:48814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:48188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:12<00:01, 21.84it/s]
 86%|████████▌ | 171/200 [00:12<00:01, 24.03it/s][2026-01-17 08:09:12] INFO:     127.0.0.1:48034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:59654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:12] INFO:     127.0.0.1:59420 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:13<00:01, 20.55it/s][2026-01-17 08:09:12] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:13] Decode batch, #running-req: 25, #token: 11264, token usage: 0.04, npu graph: False, gen throughput (token/s): 1431.35, #queue-req: 0,
[2026-01-17 08:09:13] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:13] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:13<00:01, 15.21it/s][2026-01-17 08:09:13] INFO:     127.0.0.1:48964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:13] INFO:     127.0.0.1:48070 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:13] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:13<00:01, 16.90it/s][2026-01-17 08:09:13] INFO:     127.0.0.1:59446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:13] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:13] INFO:     127.0.0.1:48870 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:13<00:01, 16.01it/s][2026-01-17 08:09:13] INFO:     127.0.0.1:48944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:13] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:13<00:00, 16.18it/s][2026-01-17 08:09:13] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:13] INFO:     127.0.0.1:47628 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:14<00:00, 15.08it/s][2026-01-17 08:09:13] Decode batch, #running-req: 13, #token: 6656, token usage: 0.02, npu graph: False, gen throughput (token/s): 830.75, #queue-req: 0,
[2026-01-17 08:09:14] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:14] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:14] INFO:     127.0.0.1:48950 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:14<00:00, 13.40it/s][2026-01-17 08:09:14] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:14] INFO:     127.0.0.1:48914 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:14<00:00,  9.22it/s][2026-01-17 08:09:14] INFO:     127.0.0.1:47586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:14] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:14<00:00,  9.60it/s][2026-01-17 08:09:14] Decode batch, #running-req: 6, #token: 3328, token usage: 0.01, npu graph: False, gen throughput (token/s): 407.95, #queue-req: 0,
[2026-01-17 08:09:14] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:14] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:15<00:00, 10.52it/s][2026-01-17 08:09:15] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:15] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:15<00:00,  9.99it/s][2026-01-17 08:09:15] Decode batch, #running-req: 2, #token: 1792, token usage: 0.01, npu graph: False, gen throughput (token/s): 115.40, #queue-req: 0,
[2026-01-17 08:09:16] Decode batch, #running-req: 2, #token: 1920, token usage: 0.01, npu graph: False, gen throughput (token/s): 89.73, #queue-req: 0,
[2026-01-17 08:09:17] Decode batch, #running-req: 2, #token: 2048, token usage: 0.01, npu graph: False, gen throughput (token/s): 89.56, #queue-req: 0,
[2026-01-17 08:09:18] Decode batch, #running-req: 2, #token: 2048, token usage: 0.01, npu graph: False, gen throughput (token/s): 89.71, #queue-req: 0,
[2026-01-17 08:09:18] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:09:19] INFO:     127.0.0.1:59426 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:19<00:00,  1.60it/s]
100%|██████████| 200/200 [00:19<00:00, 10.41it/s]
.
----------------------------------------------------------------------
Ran 1 test in 90.566s

OK
Accuracy: 0.385
Invalid: 0.005
Latency: 19.314 s
Output throughput: 1447.114 token/s
.
.
End (4/62):
filename='ascend/llm_models/test_ascend_mistral_7b.py', elapsed=101, estimated_time=400
.
.

.
.
Begin (5/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_gemma_3_4b_it_llm.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:09:39] INFO model_config.py:150: Multimodal is disabled for gemma3. To enable it, set --enable-multimodal.
[2026-01-17 08:09:39] INFO model_config.py:987: For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 08:09:39] INFO model_config.py:1016: Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 08:09:39] WARNING server_args.py:1247: Disable hybrid SWA memory for Gemma3ForConditionalGeneration as it is not yet supported.
[2026-01-17 08:09:40] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', tokenizer_path='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=-1, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=True, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=277182143, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:09:40] Multimodal is disabled for gemma3. To enable it, set --enable-multimodal.
[2026-01-17 08:09:40] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 08:09:40] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 08:09:42] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:09:50] Multimodal is disabled for gemma3. To enable it, set --enable-multimodal.
[2026-01-17 08:09:50] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 08:09:50] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:09:52] Multimodal is disabled for gemma3. To enable it, set --enable-multimodal.
[2026-01-17 08:09:52] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 08:09:52] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 08:09:52] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:09:53] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:09:53] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:09:54] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:09:54] Load weight begin. avail mem=60.81 GB
[2026-01-17 08:09:54] Multimodal attention backend not set. Use sdpa.
[2026-01-17 08:09:54] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:08<00:08,  8.06s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:14<00:00,  7.09s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:14<00:00,  7.23s/it]

[2026-01-17 08:10:10] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=52.78 GB, mem usage=8.03 GB.
[2026-01-17 08:10:10] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:10:10] The available memory for KV cache is 40.62 GB.
[2026-01-17 08:10:11] KV Cache is allocated. #tokens: 313088, K size: 20.31 GB, V size: 20.31 GB
[2026-01-17 08:10:11] Memory pool end. avail mem=4.16 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:10:13] max_total_num_tokens=313088, chunked_prefill_size=-1, max_prefill_tokens=16384, max_running_requests=2048, context_len=1048576, available_gpu_mem=4.15 GB
[2026-01-17 08:10:15] INFO:     Started server process [19616]
[2026-01-17 08:10:15] INFO:     Waiting for application startup.
[2026-01-17 08:10:15] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 64, 'top_p': 0.95}
[2026-01-17 08:10:15] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 64, 'top_p': 0.95}
[2026-01-17 08:10:15] INFO:     Application startup complete.
[2026-01-17 08:10:15] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:10:16] INFO:     127.0.0.1:43996 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:10:16] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:10:20] INFO:     127.0.0.1:36424 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:10:30] INFO:     127.0.0.1:39688 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:10:30] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:30] The server is fired up and ready to roll!
[2026-01-17 08:10:40] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:10:41] INFO:     127.0.0.1:44806 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:10:41] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:10:41] INFO:     127.0.0.1:44818 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:10:41] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:10:41] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/google/gemma-3-4b-it --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --disable-radix-cache --chunked-prefill-size -1 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:10:41] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:10:41] Prefill batch, #new-seq: 17, #new-token: 15616, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 10,
[2026-01-17 08:10:42] Prefill batch, #new-seq: 18, #new-token: 16128, #cached-token: 0, token usage: 0.05, #running-req: 18, #queue-req: 92,
[2026-01-17 08:10:43] Prefill batch, #new-seq: 17, #new-token: 15488, #cached-token: 0, token usage: 0.10, #running-req: 36, #queue-req: 75,
[2026-01-17 08:10:43] Prefill batch, #new-seq: 17, #new-token: 15488, #cached-token: 0, token usage: 0.15, #running-req: 53, #queue-req: 58,
[2026-01-17 08:10:45] Prefill batch, #new-seq: 18, #new-token: 16256, #cached-token: 0, token usage: 0.20, #running-req: 70, #queue-req: 40,
[2026-01-17 08:10:45] Prefill batch, #new-seq: 17, #new-token: 15488, #cached-token: 0, token usage: 0.26, #running-req: 88, #queue-req: 23,
[2026-01-17 08:10:46] Prefill batch, #new-seq: 18, #new-token: 16128, #cached-token: 0, token usage: 0.30, #running-req: 105, #queue-req: 5,
[2026-01-17 08:10:46] Prefill batch, #new-seq: 5, #new-token: 4480, #cached-token: 0, token usage: 0.36, #running-req: 123, #queue-req: 0,
[2026-01-17 08:10:49] Decode batch, #running-req: 128, #token: 121216, token usage: 0.39, npu graph: False, gen throughput (token/s): 67.68, #queue-req: 0,
[2026-01-17 08:10:50] INFO:     127.0.0.1:45798 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:09<30:32,  9.21s/it][2026-01-17 08:10:50] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.40, #running-req: 127, #queue-req: 0,
[2026-01-17 08:10:50] INFO:     127.0.0.1:44840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:50] INFO:     127.0.0.1:45184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:50] INFO:     127.0.0.1:45282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:50] INFO:     127.0.0.1:45354 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:09<12:43,  3.85s/it]
  2%|▎         | 5/200 [00:09<01:03,  3.08it/s]
  2%|▎         | 5/200 [00:09<01:03,  3.08it/s]
  2%|▎         | 5/200 [00:09<01:03,  3.08it/s][2026-01-17 08:10:50] Prefill batch, #new-seq: 4, #new-token: 3584, #cached-token: 0, token usage: 0.39, #running-req: 124, #queue-req: 0,
[2026-01-17 08:10:52] INFO:     127.0.0.1:46022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:52] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:52] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.40, #running-req: 127, #queue-req: 0,
[2026-01-17 08:10:52] INFO:     127.0.0.1:45234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:52] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.40, #running-req: 128, #queue-req: 0,
[2026-01-17 08:10:52] INFO:     127.0.0.1:45948 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:10<01:18,  2.46it/s]
  4%|▍         | 9/200 [00:10<01:18,  2.43it/s][2026-01-17 08:10:52] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.40, #running-req: 129, #queue-req: 0,
[2026-01-17 08:10:53] INFO:     127.0.0.1:45738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:53] INFO:     127.0.0.1:45862 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:12<01:29,  2.11it/s][2026-01-17 08:10:53] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:53] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.40, #running-req: 126, #queue-req: 0,
[2026-01-17 08:10:54] INFO:     127.0.0.1:45914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:54] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.40, #running-req: 128, #queue-req: 0,

  6%|▋         | 13/200 [00:13<01:36,  1.95it/s][2026-01-17 08:10:54] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.41, #running-req: 129, #queue-req: 0,
[2026-01-17 08:10:56] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:56] INFO:     127.0.0.1:45644 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:14<01:57,  1.58it/s]
  8%|▊         | 15/200 [00:14<01:58,  1.56it/s][2026-01-17 08:10:56] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:56] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.40, #running-req: 126, #queue-req: 0,
[2026-01-17 08:10:56] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.41, #running-req: 128, #queue-req: 0,
[2026-01-17 08:10:56] INFO:     127.0.0.1:45078 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:15<01:31,  2.01it/s][2026-01-17 08:10:56] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:56] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.40, #running-req: 127, #queue-req: 0,
[2026-01-17 08:10:56] INFO:     127.0.0.1:45424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:56] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:56] INFO:     127.0.0.1:45976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:56] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:56] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.39, #running-req: 128, #queue-req: 0,

 10%|█         | 20/200 [00:15<00:57,  3.15it/s]
 11%|█         | 22/200 [00:15<00:21,  8.40it/s]
 11%|█         | 22/200 [00:15<00:21,  8.40it/s][2026-01-17 08:10:56] Prefill batch, #new-seq: 3, #new-token: 2816, #cached-token: 0, token usage: 0.40, #running-req: 130, #queue-req: 0,
[2026-01-17 08:10:58] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:58] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:58] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:58] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.40, #running-req: 126, #queue-req: 0,

 12%|█▎        | 25/200 [00:16<00:39,  4.43it/s][2026-01-17 08:10:58] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.41, #running-req: 128, #queue-req: 0,
[2026-01-17 08:10:58] INFO:     127.0.0.1:45422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:58] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:58] Decode batch, #running-req: 128, #token: 126336, token usage: 0.40, npu graph: False, gen throughput (token/s): 585.12, #queue-req: 0,
[2026-01-17 08:10:58] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.40, #running-req: 126, #queue-req: 0,
[2026-01-17 08:10:58] INFO:     127.0.0.1:45732 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:17<00:33,  5.08it/s][2026-01-17 08:10:58] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.41, #running-req: 127, #queue-req: 0,
[2026-01-17 08:10:58] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:58] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:17<00:28,  5.95it/s][2026-01-17 08:10:58] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.40, #running-req: 126, #queue-req: 0,
[2026-01-17 08:10:58] INFO:     127.0.0.1:45918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:58] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.41, #running-req: 127, #queue-req: 0,
[2026-01-17 08:10:58] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:58] INFO:     127.0.0.1:45594 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:17<00:25,  6.48it/s]
 16%|█▋        | 33/200 [00:17<00:21,  7.86it/s][2026-01-17 08:10:58] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.40, #running-req: 126, #queue-req: 0,
[2026-01-17 08:10:59] INFO:     127.0.0.1:45042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:59] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.41, #running-req: 127, #queue-req: 0,
[2026-01-17 08:10:59] INFO:     127.0.0.1:45686 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:17<00:22,  7.19it/s][2026-01-17 08:10:59] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.41, #running-req: 127, #queue-req: 0,
[2026-01-17 08:10:59] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:59] INFO:     127.0.0.1:45172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:59] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.40, #running-req: 126, #queue-req: 0,
[2026-01-17 08:10:59] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:59] INFO:     127.0.0.1:45676 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:18<00:18,  8.74it/s]
 20%|█▉        | 39/200 [00:18<00:14, 11.24it/s][2026-01-17 08:10:59] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.40, #running-req: 126, #queue-req: 0,
[2026-01-17 08:10:59] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:59] INFO:     127.0.0.1:45932 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:18<00:13, 12.17it/s][2026-01-17 08:10:59] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.40, #running-req: 126, #queue-req: 0,
[2026-01-17 08:10:59] INFO:     127.0.0.1:44878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:59] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:18<00:14, 10.49it/s][2026-01-17 08:10:59] INFO:     127.0.0.1:45718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:59] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.40, #running-req: 126, #queue-req: 0,
[2026-01-17 08:10:59] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:10:59] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.40, #running-req: 128, #queue-req: 0,

 22%|██▎       | 45/200 [00:18<00:13, 11.89it/s][2026-01-17 08:10:59] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.40, #running-req: 129, #queue-req: 0,
[2026-01-17 08:11:00] INFO:     127.0.0.1:46012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.39, #running-req: 127, #queue-req: 0,
[2026-01-17 08:11:00] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] INFO:     127.0.0.1:45854 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:18<00:14, 10.42it/s]
 24%|██▍       | 49/200 [00:18<00:12, 12.37it/s]
 24%|██▍       | 49/200 [00:18<00:12, 12.37it/s][2026-01-17 08:11:00] INFO:     127.0.0.1:45412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] INFO:     127.0.0.1:45584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] Prefill batch, #new-seq: 3, #new-token: 2816, #cached-token: 0, token usage: 0.39, #running-req: 128, #queue-req: 0,
[2026-01-17 08:11:00] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.40, #running-req: 131, #queue-req: 0,
[2026-01-17 08:11:00] INFO:     127.0.0.1:45298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] INFO:     127.0.0.1:45900 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:19<00:13, 11.23it/s]
 27%|██▋       | 54/200 [00:19<00:11, 12.90it/s]
 27%|██▋       | 54/200 [00:19<00:11, 12.90it/s][2026-01-17 08:11:00] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] Prefill batch, #new-seq: 3, #new-token: 2816, #cached-token: 0, token usage: 0.39, #running-req: 125, #queue-req: 0,
[2026-01-17 08:11:00] INFO:     127.0.0.1:45212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.40, #running-req: 128, #queue-req: 0,

 28%|██▊       | 56/200 [00:19<00:10, 13.69it/s][2026-01-17 08:11:00] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.40, #running-req: 129, #queue-req: 0,
[2026-01-17 08:11:00] INFO:     127.0.0.1:45582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.40, #running-req: 127, #queue-req: 0,
[2026-01-17 08:11:00] INFO:     127.0.0.1:45008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] INFO:     127.0.0.1:45030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:00] INFO:     127.0.0.1:45908 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:19<00:13, 10.20it/s]
 30%|███       | 60/200 [00:19<00:14,  9.67it/s]
 30%|███       | 60/200 [00:19<00:14,  9.67it/s][2026-01-17 08:11:00] Prefill batch, #new-seq: 3, #new-token: 2816, #cached-token: 0, token usage: 0.40, #running-req: 125, #queue-req: 0,
[2026-01-17 08:11:01] INFO:     127.0.0.1:45512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:01] INFO:     127.0.0.1:45778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:01] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.39, #running-req: 127, #queue-req: 0,
[2026-01-17 08:11:01] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:01] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:20<00:16,  8.59it/s]
 32%|███▏      | 64/200 [00:20<00:13,  9.74it/s]
 32%|███▏      | 64/200 [00:20<00:13,  9.74it/s][2026-01-17 08:11:01] INFO:     127.0.0.1:45150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:01] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, token usage: 0.39, #running-req: 128, #queue-req: 0,
[2026-01-17 08:11:01] INFO:     127.0.0.1:45646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:01] INFO:     127.0.0.1:46038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:01] Prefill batch, #new-seq: 3, #new-token: 2816, #cached-token: 0, token usage: 0.39, #running-req: 131, #queue-req: 0,
[2026-01-17 08:11:01] INFO:     127.0.0.1:44892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:01] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:01] INFO:     127.0.0.1:45816 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:20<00:13,  9.87it/s]
 35%|███▌      | 70/200 [00:20<00:10, 12.17it/s]
 35%|███▌      | 70/200 [00:20<00:10, 12.17it/s][2026-01-17 08:11:01] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, token usage: 0.39, #running-req: 125, #queue-req: 0,
[2026-01-17 08:11:01] INFO:     127.0.0.1:44956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:01] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.40, #running-req: 127, #queue-req: 0,
[2026-01-17 08:11:01] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:01] INFO:     127.0.0.1:36842 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:20<00:11, 11.29it/s]
 36%|███▋      | 73/200 [00:20<00:11, 11.52it/s][2026-01-17 08:11:01] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.40, #running-req: 126, #queue-req: 0,
[2026-01-17 08:11:01] INFO:     127.0.0.1:45694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:01] INFO:     127.0.0.1:45804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:45396 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [00:20<00:09, 12.42it/s][2026-01-17 08:11:02] Decode batch, #running-req: 125, #token: 119680, token usage: 0.38, npu graph: False, gen throughput (token/s): 1366.32, #queue-req: 0,
[2026-01-17 08:11:02] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:45996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:36828 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:21<00:07, 15.51it/s]
 41%|████      | 82/200 [00:21<00:06, 19.32it/s][2026-01-17 08:11:02] INFO:     127.0.0.1:36862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:45496 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:21<00:06, 18.37it/s][2026-01-17 08:11:02] INFO:     127.0.0.1:45292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:45570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:45494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:46052 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:21<00:05, 18.83it/s]
 45%|████▌     | 90/200 [00:21<00:04, 24.21it/s]
 45%|████▌     | 90/200 [00:21<00:04, 24.21it/s][2026-01-17 08:11:02] INFO:     127.0.0.1:45340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:45382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:45660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:46040 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:21<00:05, 19.95it/s]
 47%|████▋     | 94/200 [00:21<00:05, 18.89it/s][2026-01-17 08:11:02] INFO:     127.0.0.1:44872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:45092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:45780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:36878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:02] INFO:     127.0.0.1:44994 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:21<00:04, 22.57it/s][2026-01-17 08:11:03] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:45132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:45558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:45874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:22<00:04, 21.96it/s]
 52%|█████▎    | 105/200 [00:22<00:03, 25.49it/s]
 52%|█████▎    | 105/200 [00:22<00:03, 25.49it/s][2026-01-17 08:11:03] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:45108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:22<00:04, 22.61it/s][2026-01-17 08:11:03] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:44930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:22<00:04, 20.52it/s]
 56%|█████▌    | 112/200 [00:22<00:04, 20.62it/s][2026-01-17 08:11:03] INFO:     127.0.0.1:36788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:22<00:04, 17.99it/s][2026-01-17 08:11:03] INFO:     127.0.0.1:45762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:45924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:45072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:45130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:45606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:03] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:22<00:04, 17.28it/s]
 60%|██████    | 121/200 [00:22<00:03, 23.65it/s]
 60%|██████    | 121/200 [00:22<00:03, 23.65it/s]
 60%|██████    | 121/200 [00:22<00:03, 23.65it/s][2026-01-17 08:11:04] Decode batch, #running-req: 83, #token: 77312, token usage: 0.25, npu graph: False, gen throughput (token/s): 2121.29, #queue-req: 0,
[2026-01-17 08:11:04] INFO:     127.0.0.1:45648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:58142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:22<00:02, 26.50it/s][2026-01-17 08:11:04] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:58172 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:23<00:02, 25.07it/s][2026-01-17 08:11:04] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:45198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:23<00:02, 23.89it/s]
 67%|██████▋   | 134/200 [00:23<00:02, 27.62it/s]
 67%|██████▋   | 134/200 [00:23<00:02, 27.62it/s][2026-01-17 08:11:04] INFO:     127.0.0.1:45890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:23<00:03, 17.88it/s][2026-01-17 08:11:04] INFO:     127.0.0.1:36964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:36804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:04] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:23<00:03, 17.37it/s]
 70%|███████   | 141/200 [00:23<00:03, 18.47it/s][2026-01-17 08:11:04] INFO:     127.0.0.1:44970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:05] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:05] INFO:     127.0.0.1:45064 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:24<00:03, 16.70it/s][2026-01-17 08:11:05] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:05] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:05] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:05] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:24<00:02, 19.25it/s][2026-01-17 08:11:05] INFO:     127.0.0.1:45706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:05] INFO:     127.0.0.1:36938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:05] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:05] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:24<00:02, 18.61it/s][2026-01-17 08:11:05] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:05] INFO:     127.0.0.1:45228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:05] Decode batch, #running-req: 46, #token: 49152, token usage: 0.16, npu graph: False, gen throughput (token/s): 1305.75, #queue-req: 0,
[2026-01-17 08:11:05] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:24<00:03, 13.44it/s][2026-01-17 08:11:06] INFO:     127.0.0.1:45754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:06] INFO:     127.0.0.1:45822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:06] INFO:     127.0.0.1:36800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:06] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:06] INFO:     127.0.0.1:36902 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:24<00:02, 16.65it/s][2026-01-17 08:11:06] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:06] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:25<00:03, 10.79it/s][2026-01-17 08:11:06] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:06] INFO:     127.0.0.1:44850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:06] INFO:     127.0.0.1:36856 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:25<00:03, 11.36it/s]
 82%|████████▎ | 165/200 [00:25<00:02, 13.27it/s][2026-01-17 08:11:07] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:07] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:25<00:03, 10.75it/s][2026-01-17 08:11:07] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:07] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:07] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:26<00:03,  9.24it/s]
 85%|████████▌ | 170/200 [00:26<00:03,  9.37it/s][2026-01-17 08:11:07] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:07] INFO:     127.0.0.1:44902 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:26<00:02, 10.28it/s][2026-01-17 08:11:07] Decode batch, #running-req: 28, #token: 31232, token usage: 0.10, npu graph: False, gen throughput (token/s): 822.60, #queue-req: 0,
[2026-01-17 08:11:07] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:07] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:07] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:26<00:03,  8.09it/s]
 88%|████████▊ | 175/200 [00:26<00:03,  7.88it/s][2026-01-17 08:11:08] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:26<00:03,  7.83it/s][2026-01-17 08:11:08] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:27<00:03,  7.32it/s][2026-01-17 08:11:08] INFO:     127.0.0.1:36784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:08] INFO:     127.0.0.1:36850 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:27<00:03,  6.51it/s]
 90%|████████▉ | 179/200 [00:27<00:02,  7.23it/s][2026-01-17 08:11:08] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:27<00:02,  7.28it/s][2026-01-17 08:11:08] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:27<00:02,  7.31it/s][2026-01-17 08:11:08] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:08] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:27<00:02,  7.32it/s]
 92%|█████████▏| 183/200 [00:27<00:01,  9.22it/s][2026-01-17 08:11:08] INFO:     127.0.0.1:58216 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:27<00:01,  8.78it/s][2026-01-17 08:11:09] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:09] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:27<00:01, 10.41it/s][2026-01-17 08:11:09] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:09] Decode batch, #running-req: 13, #token: 15104, token usage: 0.05, npu graph: False, gen throughput (token/s): 469.59, #queue-req: 0,
[2026-01-17 08:11:10] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:28<00:02,  4.57it/s][2026-01-17 08:11:11] Decode batch, #running-req: 12, #token: 14848, token usage: 0.05, npu graph: False, gen throughput (token/s): 283.66, #queue-req: 0,
[2026-01-17 08:11:12] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:31<00:08,  1.35it/s][2026-01-17 08:11:12] Decode batch, #running-req: 11, #token: 14080, token usage: 0.04, npu graph: False, gen throughput (token/s): 276.23, #queue-req: 0,
[2026-01-17 08:11:12] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:31<00:06,  1.63it/s][2026-01-17 08:11:13] INFO:     127.0.0.1:44948 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:32<00:05,  1.79it/s][2026-01-17 08:11:13] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:32<00:04,  1.94it/s][2026-01-17 08:11:14] Decode batch, #running-req: 8, #token: 10368, token usage: 0.03, npu graph: False, gen throughput (token/s): 204.90, #queue-req: 0,
[2026-01-17 08:11:16] Decode batch, #running-req: 8, #token: 11136, token usage: 0.04, npu graph: False, gen throughput (token/s): 185.71, #queue-req: 0,
[2026-01-17 08:11:18] Decode batch, #running-req: 8, #token: 11264, token usage: 0.04, npu graph: False, gen throughput (token/s): 187.19, #queue-req: 0,
[2026-01-17 08:11:19] INFO:     127.0.0.1:44880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:19] Decode batch, #running-req: 8, #token: 2816, token usage: 0.01, npu graph: False, gen throughput (token/s): 187.17, #queue-req: 0,
[2026-01-17 08:11:19] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:19] INFO:     127.0.0.1:45020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:19] INFO:     127.0.0.1:45166 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:19] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:11:19] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:38<00:14,  2.01s/it]
 99%|█████████▉| 198/200 [00:38<00:02,  1.33s/it]
 99%|█████████▉| 198/200 [00:38<00:02,  1.33s/it]
 99%|█████████▉| 198/200 [00:38<00:02,  1.33s/it]
 99%|█████████▉| 198/200 [00:38<00:02,  1.33s/it]
 99%|█████████▉| 198/200 [00:38<00:02,  1.33s/it][2026-01-17 08:11:21] Decode batch, #running-req: 2, #token: 2944, token usage: 0.01, npu graph: False, gen throughput (token/s): 50.09, #queue-req: 0,
[2026-01-17 08:11:22] INFO:     127.0.0.1:36920 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:41<00:01,  1.46s/it][2026-01-17 08:11:22] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:41<00:00,  1.32s/it]
100%|██████████| 200/200 [00:41<00:00,  4.80it/s]
.
----------------------------------------------------------------------
Ran 1 test in 112.951s

OK
Accuracy: 0.710
Invalid: 0.000
Latency: 41.763 s
Output throughput: 608.333 token/s
.
.
End (5/62):
filename='ascend/llm_models/test_ascend_gemma_3_4b_it_llm.py', elapsed=123, estimated_time=400
.
.

.
.
Begin (6/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_granite_3_0_3b_a800m.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:11:42] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.0-3b-a800m-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.0-3b-a800m-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=759492282, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.0-3b-a800m-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:11:42] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:11:52] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:11:53] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:11:54] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:11:54] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.14it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.64it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.29it/s]

[2026-01-17 08:12:02] Parameter lm_head.weight not found in params_dict
[2026-01-17 08:12:18] Load weight end. type=GraniteMoeForCausalLM, dtype=torch.bfloat16, avail mem=54.63 GB, mem usage=6.18 GB.
[2026-01-17 08:12:18] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:12:18] The available memory for KV cache is 42.47 GB.
[2026-01-17 08:12:19] KV Cache is allocated. #tokens: 695808, K size: 21.24 GB, V size: 21.24 GB
[2026-01-17 08:12:19] Memory pool end. avail mem=12.09 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:12:19] max_total_num_tokens=695808, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4096, available_gpu_mem=12.09 GB
[2026-01-17 08:12:20] INFO:     Started server process [22789]
[2026-01-17 08:12:20] INFO:     Waiting for application startup.
[2026-01-17 08:12:20] INFO:     Application startup complete.
[2026-01-17 08:12:20] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:12:21] INFO:     127.0.0.1:59276 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:12:21] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:12:22] INFO:     127.0.0.1:59296 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
.[2026-01-17 08:12:30] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:30] The server is fired up and ready to roll!
[2026-01-17 08:12:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:12:33] INFO:     127.0.0.1:52066 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:12:33] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:12:33] INFO:     127.0.0.1:52076 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:12:33] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:12:33] INFO:     127.0.0.1:52084 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/ibm-granite/granite-3.0-3b-a800m-instruct --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:12:33] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:12:33] Prefill batch, #new-seq: 24, #new-token: 6144, #cached-token: 18432, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:12:33] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.01, #running-req: 25, #queue-req: 3,
[2026-01-17 08:12:33] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.02, #running-req: 57, #queue-req: 0,
[2026-01-17 08:12:33] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.02, #running-req: 60, #queue-req: 16,
[2026-01-17 08:12:35] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.03, #running-req: 92, #queue-req: 4,
[2026-01-17 08:12:35] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 3072, token usage: 0.05, #running-req: 124, #queue-req: 0,
[2026-01-17 08:12:36] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<09:55,  2.99s/it][2026-01-17 08:12:36] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:37] Decode batch, #running-req: 128, #token: 34304, token usage: 0.05, npu graph: False, gen throughput (token/s): 87.56, #queue-req: 0,
[2026-01-17 08:12:37] INFO:     127.0.0.1:52284 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:03<05:28,  1.66s/it][2026-01-17 08:12:37] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:37] INFO:     127.0.0.1:52340 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:04<03:24,  1.04s/it][2026-01-17 08:12:37] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:37] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:04<02:21,  1.39it/s][2026-01-17 08:12:37] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:38] INFO:     127.0.0.1:52392 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:04<01:37,  2.01it/s][2026-01-17 08:12:38] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:38] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:04<01:16,  2.54it/s][2026-01-17 08:12:38] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:38] INFO:     127.0.0.1:52102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:38] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:38] INFO:     127.0.0.1:52780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:38] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:04<00:52,  3.63it/s]
  4%|▍         | 9/200 [00:04<00:35,  5.34it/s][2026-01-17 08:12:38] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:38] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 08:12:38] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 08:12:38] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:05<00:31,  6.07it/s][2026-01-17 08:12:38] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:39] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:39] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:05<00:34,  5.40it/s]
  6%|▋         | 13/200 [00:05<00:31,  5.95it/s][2026-01-17 08:12:39] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:39] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 08:12:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 08:12:39] INFO:     127.0.0.1:52958 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:05<00:27,  6.65it/s][2026-01-17 08:12:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:39] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:39] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:05<00:30,  6.04it/s]
  8%|▊         | 17/200 [00:05<00:26,  6.79it/s][2026-01-17 08:12:39] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 08:12:39] Decode batch, #running-req: 126, #token: 37760, token usage: 0.05, npu graph: False, gen throughput (token/s): 2151.37, #queue-req: 0,
[2026-01-17 08:12:39] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:39] INFO:     127.0.0.1:52420 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:06<00:28,  6.45it/s]
 10%|▉         | 19/200 [00:06<00:23,  7.57it/s][2026-01-17 08:12:39] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 08:12:39] INFO:     127.0.0.1:52390 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:06<00:27,  6.55it/s][2026-01-17 08:12:39] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:39] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:39] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:39] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:39] INFO:     127.0.0.1:53162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:39] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:39] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.05, #running-req: 128, #queue-req: 0,

 12%|█▏        | 23/200 [00:06<00:17, 10.37it/s]
 13%|█▎        | 26/200 [00:06<00:05, 31.72it/s]
 13%|█▎        | 26/200 [00:06<00:05, 31.72it/s]
 13%|█▎        | 26/200 [00:06<00:05, 31.72it/s][2026-01-17 08:12:40] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 3072, token usage: 0.06, #running-req: 130, #queue-req: 0,
[2026-01-17 08:12:40] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:40] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:40] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:40] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:40] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:40] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:40] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,

 15%|█▌        | 30/200 [00:06<00:09, 17.75it/s][2026-01-17 08:12:40] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 08:12:40] INFO:     127.0.0.1:52882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:40] INFO:     127.0.0.1:52918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:40] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:40] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,

 16%|█▋        | 33/200 [00:07<00:10, 16.36it/s][2026-01-17 08:12:40] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:12:40] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:40] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:40] INFO:     127.0.0.1:52718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:40] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:40] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:07<00:11, 14.36it/s]
 18%|█▊        | 37/200 [00:07<00:11, 14.28it/s][2026-01-17 08:12:41] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.06, #running-req: 125, #queue-req: 0,
[2026-01-17 08:12:41] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:41] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:41] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:41] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:07<00:13, 11.95it/s]
 20%|██        | 40/200 [00:07<00:13, 11.51it/s][2026-01-17 08:12:41] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:41] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:12:41] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:41] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,

 21%|██        | 42/200 [00:07<00:12, 12.65it/s][2026-01-17 08:12:41] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 08:12:41] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:41] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:07<00:12, 12.29it/s][2026-01-17 08:12:41] INFO:     127.0.0.1:53044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:41] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:12:41] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:12:41] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:41] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:41] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:08<00:16,  9.54it/s]
 24%|██▍       | 48/200 [00:08<00:15,  9.98it/s]
 24%|██▍       | 48/200 [00:08<00:15,  9.98it/s][2026-01-17 08:12:41] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.06, #running-req: 125, #queue-req: 0,
[2026-01-17 08:12:42] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:42] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,

 25%|██▌       | 50/200 [00:08<00:15,  9.75it/s][2026-01-17 08:12:42] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:12:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 08:12:42] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:08<00:16,  9.10it/s][2026-01-17 08:12:42] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:42] Decode batch, #running-req: 127, #token: 42880, token usage: 0.06, npu graph: False, gen throughput (token/s): 1767.74, #queue-req: 0,
[2026-01-17 08:12:42] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:12:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 08:12:42] INFO:     127.0.0.1:52896 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:09<00:16,  8.97it/s][2026-01-17 08:12:42] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:42] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:42] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:42] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:12:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 130, #queue-req: 0,
[2026-01-17 08:12:42] INFO:     127.0.0.1:42004 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:09<00:13, 10.66it/s][2026-01-17 08:12:43] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:43] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:12:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 08:12:43] INFO:     127.0.0.1:52182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:43] INFO:     127.0.0.1:42014 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:09<00:12, 10.83it/s]
 32%|███▏      | 63/200 [00:09<00:11, 12.04it/s][2026-01-17 08:12:43] INFO:     127.0.0.1:52142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:43] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:12:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:12:43] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:09<00:12, 10.82it/s][2026-01-17 08:12:43] INFO:     127.0.0.1:52126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:12:43] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:43] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:43] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:12:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 130, #queue-req: 0,
[2026-01-17 08:12:43] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:43] INFO:     127.0.0.1:42028 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:10<00:10, 12.07it/s]
 35%|███▌      | 70/200 [00:10<00:09, 14.07it/s][2026-01-17 08:12:43] INFO:     127.0.0.1:42060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:43] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:12:43] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:12:43] INFO:     127.0.0.1:41976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 08:12:44] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:10<00:08, 14.42it/s][2026-01-17 08:12:44] INFO:     127.0.0.1:53190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:41990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:52518 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:10<00:07, 16.19it/s][2026-01-17 08:12:44] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:52676 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:10<00:08, 14.98it/s][2026-01-17 08:12:44] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:52840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:52662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:52744 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:10<00:06, 17.66it/s]
 42%|████▎     | 85/200 [00:10<00:05, 21.55it/s][2026-01-17 08:12:44] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:52424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:11<00:05, 18.71it/s][2026-01-17 08:12:44] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] Decode batch, #running-req: 112, #token: 36992, token usage: 0.05, npu graph: False, gen throughput (token/s): 1937.43, #queue-req: 0,
[2026-01-17 08:12:44] INFO:     127.0.0.1:53224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:41998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:44] INFO:     127.0.0.1:42298 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:11<00:06, 16.88it/s]
 46%|████▌     | 92/200 [00:11<00:06, 17.11it/s][2026-01-17 08:12:45] INFO:     127.0.0.1:42496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:52600 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [00:11<00:06, 15.01it/s][2026-01-17 08:12:45] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:11<00:05, 17.89it/s]
 50%|█████     | 100/200 [00:11<00:04, 22.06it/s][2026-01-17 08:12:45] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:42212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:42170 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:11<00:04, 22.15it/s][2026-01-17 08:12:45] INFO:     127.0.0.1:42228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:52236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:52824 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:12<00:04, 22.05it/s]
 54%|█████▍    | 108/200 [00:12<00:03, 23.85it/s][2026-01-17 08:12:45] INFO:     127.0.0.1:42258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:52528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:12<00:04, 21.42it/s][2026-01-17 08:12:45] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:45] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:46] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:12<00:04, 19.77it/s][2026-01-17 08:12:46] INFO:     127.0.0.1:42456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:46] INFO:     127.0.0.1:42202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:46] INFO:     127.0.0.1:52380 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:12<00:04, 17.42it/s][2026-01-17 08:12:46] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:46] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:46] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:12<00:04, 18.53it/s][2026-01-17 08:12:46] INFO:     127.0.0.1:42470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:46] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:46] INFO:     127.0.0.1:42280 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:13<00:05, 15.21it/s]
 62%|██████▏   | 123/200 [00:13<00:05, 14.70it/s][2026-01-17 08:12:46] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:46] INFO:     127.0.0.1:42264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:46] INFO:     127.0.0.1:42402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:46] Decode batch, #running-req: 76, #token: 28160, token usage: 0.04, npu graph: False, gen throughput (token/s): 2029.49, #queue-req: 0,
[2026-01-17 08:12:46] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:13<00:04, 15.73it/s][2026-01-17 08:12:46] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:41994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:42370 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:13<00:04, 15.51it/s]
 66%|██████▌   | 131/200 [00:13<00:03, 20.23it/s]
 66%|██████▌   | 131/200 [00:13<00:03, 20.23it/s][2026-01-17 08:12:47] INFO:     127.0.0.1:52498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:42414 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:13<00:02, 22.65it/s]
 68%|██████▊   | 136/200 [00:13<00:02, 26.50it/s][2026-01-17 08:12:47] INFO:     127.0.0.1:52614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:42316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:53308 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:13<00:03, 18.71it/s][2026-01-17 08:12:47] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:52108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:42252 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:14<00:03, 15.11it/s]
 72%|███████▏  | 143/200 [00:14<00:03, 14.35it/s][2026-01-17 08:12:47] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:47] INFO:     127.0.0.1:53192 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:14<00:03, 14.47it/s][2026-01-17 08:12:48] INFO:     127.0.0.1:42208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:48] INFO:     127.0.0.1:42110 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:14<00:04, 12.88it/s][2026-01-17 08:12:48] INFO:     127.0.0.1:42422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:48] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:14<00:04, 11.76it/s][2026-01-17 08:12:48] INFO:     127.0.0.1:42304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:48] INFO:     127.0.0.1:42250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:48] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:48] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:48] INFO:     127.0.0.1:42556 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:14<00:03, 13.02it/s]
 77%|███████▋  | 154/200 [00:14<00:02, 18.87it/s]
 77%|███████▋  | 154/200 [00:14<00:02, 18.87it/s][2026-01-17 08:12:48] Decode batch, #running-req: 49, #token: 19840, token usage: 0.03, npu graph: False, gen throughput (token/s): 1366.82, #queue-req: 0,
[2026-01-17 08:12:48] INFO:     127.0.0.1:42498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:48] INFO:     127.0.0.1:42126 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:15<00:02, 14.82it/s][2026-01-17 08:12:48] INFO:     127.0.0.1:42172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:48] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:48] INFO:     127.0.0.1:42392 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:15<00:02, 16.36it/s][2026-01-17 08:12:48] INFO:     127.0.0.1:42544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:49] INFO:     127.0.0.1:52430 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:15<00:02, 13.19it/s][2026-01-17 08:12:49] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:49] INFO:     127.0.0.1:42520 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:15<00:03, 11.98it/s][2026-01-17 08:12:49] INFO:     127.0.0.1:42296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:49] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:16<00:03,  9.05it/s][2026-01-17 08:12:49] INFO:     127.0.0.1:42340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:49] INFO:     127.0.0.1:41964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:49] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:16<00:02, 11.39it/s][2026-01-17 08:12:50] INFO:     127.0.0.1:52246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:50] Decode batch, #running-req: 31, #token: 14336, token usage: 0.02, npu graph: False, gen throughput (token/s): 878.72, #queue-req: 0,
[2026-01-17 08:12:50] INFO:     127.0.0.1:52766 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:16<00:03,  8.82it/s][2026-01-17 08:12:50] INFO:     127.0.0.1:42356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:50] INFO:     127.0.0.1:52224 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:16<00:02,  9.90it/s][2026-01-17 08:12:50] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:50] INFO:     127.0.0.1:42206 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:17<00:03,  7.61it/s][2026-01-17 08:12:50] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:51] INFO:     127.0.0.1:52336 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:17<00:03,  7.11it/s][2026-01-17 08:12:51] INFO:     127.0.0.1:42328 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:17<00:03,  7.30it/s][2026-01-17 08:12:51] INFO:     127.0.0.1:42474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:51] INFO:     127.0.0.1:42522 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:17<00:02,  8.41it/s][2026-01-17 08:12:51] INFO:     127.0.0.1:42418 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:18<00:02,  7.50it/s][2026-01-17 08:12:51] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:51] INFO:     127.0.0.1:42104 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:18<00:02,  7.67it/s]
 91%|█████████ | 182/200 [00:18<00:01,  9.64it/s][2026-01-17 08:12:51] Decode batch, #running-req: 18, #token: 8704, token usage: 0.01, npu graph: False, gen throughput (token/s): 597.01, #queue-req: 0,
[2026-01-17 08:12:51] INFO:     127.0.0.1:42032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:52] INFO:     127.0.0.1:42182 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:18<00:01,  9.18it/s][2026-01-17 08:12:52] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:52] INFO:     127.0.0.1:42244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:52] INFO:     127.0.0.1:42136 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:19<00:02,  6.36it/s][2026-01-17 08:12:52] INFO:     127.0.0.1:42528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:52] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:19<00:01,  7.72it/s][2026-01-17 08:12:52] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:53] INFO:     127.0.0.1:42164 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:19<00:01,  7.93it/s][2026-01-17 08:12:53] INFO:     127.0.0.1:42424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:53] INFO:     127.0.0.1:42486 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:19<00:01,  6.58it/s][2026-01-17 08:12:53] Decode batch, #running-req: 8, #token: 4608, token usage: 0.01, npu graph: False, gen throughput (token/s): 307.08, #queue-req: 0,
[2026-01-17 08:12:53] INFO:     127.0.0.1:42378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:53] INFO:     127.0.0.1:42510 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:20<00:01,  5.62it/s]
 98%|█████████▊| 195/200 [00:20<00:00,  5.92it/s][2026-01-17 08:12:53] INFO:     127.0.0.1:42176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:53] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:20<00:00,  6.29it/s]
 98%|█████████▊| 197/200 [00:20<00:00,  8.10it/s][2026-01-17 08:12:54] INFO:     127.0.0.1:42440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:55] Decode batch, #running-req: 2, #token: 2048, token usage: 0.00, npu graph: False, gen throughput (token/s): 89.26, #queue-req: 0,
[2026-01-17 08:12:56] Decode batch, #running-req: 2, #token: 2048, token usage: 0.00, npu graph: False, gen throughput (token/s): 53.65, #queue-req: 0,
[2026-01-17 08:12:57] Decode batch, #running-req: 2, #token: 2176, token usage: 0.00, npu graph: False, gen throughput (token/s): 53.63, #queue-req: 0,
[2026-01-17 08:12:59] Decode batch, #running-req: 2, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 54.29, #queue-req: 0,
[2026-01-17 08:12:59] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:12:59] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:25<00:01,  1.02s/it]
100%|██████████| 200/200 [00:25<00:00,  1.37s/it]
100%|██████████| 200/200 [00:25<00:00,  7.74it/s]
.
----------------------------------------------------------------------
Ran 1 test in 87.045s

OK
Accuracy: 0.490
Invalid: 0.000
Latency: 25.883 s
Output throughput: 1102.492 token/s
.
.
End (6/62):
filename='ascend/llm_models/test_ascend_granite_3_0_3b_a800m.py', elapsed=97, estimated_time=400
.
.

.
.
Begin (7/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_internlm2_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:13:18] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b', tokenizer_path='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=936406990, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:13:19] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:13:29] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:13:30] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:13:30] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:13:31] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:13:31] Load weight begin. avail mem=60.81 GB

Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading pt checkpoint shards:  50% Completed | 1/2 [00:16<00:16, 16.08s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:45<00:00, 24.14s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:45<00:00, 22.93s/it]

[2026-01-17 08:14:17] Load weight end. type=InternLM2ForCausalLM, dtype=torch.bfloat16, avail mem=46.38 GB, mem usage=14.44 GB.
[2026-01-17 08:14:17] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:14:17] The available memory for KV cache is 34.21 GB.
[2026-01-17 08:14:18] KV Cache is allocated. #tokens: 280192, K size: 17.11 GB, V size: 17.11 GB
[2026-01-17 08:14:18] Memory pool end. avail mem=11.65 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:14:19] max_total_num_tokens=280192, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.65 GB
[2026-01-17 08:14:20] INFO:     Started server process [25679]
[2026-01-17 08:14:20] INFO:     Waiting for application startup.
[2026-01-17 08:14:20] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:14:20] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:14:20] INFO:     Application startup complete.
[2026-01-17 08:14:20] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:14:21] INFO:     127.0.0.1:39850 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:14:21] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:14:29] INFO:     127.0.0.1:57178 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:14:33] INFO:     127.0.0.1:39854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:33] The server is fired up and ready to roll!
[2026-01-17 08:14:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:14:40] INFO:     127.0.0.1:46070 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:14:40] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:14:40] INFO:     127.0.0.1:46082 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:14:40] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:14:40] INFO:     127.0.0.1:46090 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:14:40] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:14:40] Prefill batch, #new-seq: 18, #new-token: 2688, #cached-token: 11520, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:14:40] Prefill batch, #new-seq: 23, #new-token: 3072, #cached-token: 14720, token usage: 0.01, #running-req: 19, #queue-req: 0,
[2026-01-17 08:14:40] Prefill batch, #new-seq: 23, #new-token: 3456, #cached-token: 14720, token usage: 0.02, #running-req: 42, #queue-req: 0,
[2026-01-17 08:14:40] Prefill batch, #new-seq: 57, #new-token: 8192, #cached-token: 36480, token usage: 0.04, #running-req: 65, #queue-req: 6,
[2026-01-17 08:14:40] Prefill batch, #new-seq: 6, #new-token: 768, #cached-token: 3840, token usage: 0.06, #running-req: 122, #queue-req: 0,
[2026-01-17 08:14:41] INFO:     127.0.0.1:46690 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:01<04:46,  1.44s/it][2026-01-17 08:14:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 08:14:41] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:01<02:16,  1.45it/s][2026-01-17 08:14:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 08:14:42] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:01<01:23,  2.37it/s][2026-01-17 08:14:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:14:42] Decode batch, #running-req: 127, #token: 24832, token usage: 0.09, npu graph: False, gen throughput (token/s): 53.31, #queue-req: 0,
[2026-01-17 08:14:42] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:02<01:12,  2.69it/s][2026-01-17 08:14:42] INFO:     127.0.0.1:46114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:14:42] INFO:     127.0.0.1:46716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:42] INFO:     127.0.0.1:46736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:42] INFO:     127.0.0.1:46768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:42] INFO:     127.0.0.1:46942 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:42] INFO:     127.0.0.1:47046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:14:42] Prefill batch, #new-seq: 5, #new-token: 640, #cached-token: 3200, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-17 08:14:43] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:03<00:43,  4.30it/s][2026-01-17 08:14:43] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:14:43] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] INFO:     127.0.0.1:46624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:14:43] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-17 08:14:43] INFO:     127.0.0.1:46288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] INFO:     127.0.0.1:46354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:03<00:26,  6.98it/s]
  9%|▉         | 18/200 [00:03<00:11, 15.56it/s]
  9%|▉         | 18/200 [00:03<00:11, 15.56it/s][2026-01-17 08:14:43] INFO:     127.0.0.1:46386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] INFO:     127.0.0.1:46980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.10, #running-req: 125, #queue-req: 0,
[2026-01-17 08:14:43] INFO:     127.0.0.1:46104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] INFO:     127.0.0.1:46246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:14:43] INFO:     127.0.0.1:46506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:43] Prefill batch, #new-seq: 5, #new-token: 896, #cached-token: 3200, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-17 08:14:44] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:03<00:08, 20.54it/s][2026-01-17 08:14:44] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:14:44] INFO:     127.0.0.1:46458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:14:44] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-17 08:14:44] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:03<00:08, 20.84it/s][2026-01-17 08:14:44] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:14:44] INFO:     127.0.0.1:46766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:14:44] INFO:     127.0.0.1:46096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] INFO:     127.0.0.1:46346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] INFO:     127.0.0.1:46958 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:04<00:07, 20.91it/s]
 18%|█▊        | 35/200 [00:04<00:06, 25.52it/s]
 18%|█▊        | 35/200 [00:04<00:06, 25.52it/s][2026-01-17 08:14:44] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.10, #running-req: 125, #queue-req: 0,
[2026-01-17 08:14:44] INFO:     127.0.0.1:46910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:14:44] INFO:     127.0.0.1:46870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 128, #queue-req: 0,

 19%|█▉        | 38/200 [00:04<00:07, 21.38it/s][2026-01-17 08:14:44] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 08:14:44] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] INFO:     127.0.0.1:46132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] INFO:     127.0.0.1:46526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] INFO:     127.0.0.1:46924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,

 20%|██        | 41/200 [00:04<00:07, 21.56it/s]
 22%|██▏       | 43/200 [00:04<00:05, 27.03it/s]
 22%|██▏       | 43/200 [00:04<00:05, 27.03it/s][2026-01-17 08:14:44] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:14:44] INFO:     127.0.0.1:46236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] INFO:     127.0.0.1:46696 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:04<00:05, 26.84it/s][2026-01-17 08:14:44] INFO:     127.0.0.1:47054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] INFO:     127.0.0.1:47130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:44] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.10, #running-req: 125, #queue-req: 0,
[2026-01-17 08:14:44] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:04<00:06, 25.04it/s][2026-01-17 08:14:45] Decode batch, #running-req: 128, #token: 29184, token usage: 0.10, npu graph: False, gen throughput (token/s): 1722.94, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:14:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46764 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:04<00:06, 23.09it/s]
 26%|██▋       | 53/200 [00:04<00:06, 23.78it/s][2026-01-17 08:14:45] INFO:     127.0.0.1:46604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:46650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:14:45] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:47100 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:05<00:05, 25.68it/s][2026-01-17 08:14:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:46178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:46796 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:05<00:05, 26.12it/s][2026-01-17 08:14:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:47184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:47272 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:05<00:05, 24.90it/s]
 32%|███▎      | 65/200 [00:05<00:05, 26.17it/s][2026-01-17 08:14:45] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.10, #running-req: 124, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:47142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:47432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.10, #running-req: 131, #queue-req: 0,
[2026-01-17 08:14:45] INFO:     127.0.0.1:46174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:47030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:47292 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:05<00:04, 30.64it/s]
 38%|███▊      | 75/200 [00:05<00:03, 38.87it/s]
 38%|███▊      | 75/200 [00:05<00:03, 38.87it/s][2026-01-17 08:14:45] INFO:     127.0.0.1:46700 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:47228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:05<00:02, 44.00it/s]
 42%|████▏     | 83/200 [00:05<00:02, 50.63it/s][2026-01-17 08:14:45] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:45] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46118 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46410 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:05<00:02, 49.02it/s]
 46%|████▌     | 91/200 [00:05<00:02, 52.82it/s]
 46%|████▌     | 91/200 [00:05<00:02, 52.82it/s][2026-01-17 08:14:46] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:05<00:01, 53.02it/s][2026-01-17 08:14:46] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46752 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:05<00:01, 61.09it/s][2026-01-17 08:14:46] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47528 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:06<00:01, 59.10it/s]
 58%|█████▊    | 116/200 [00:06<00:01, 63.01it/s]
 58%|█████▊    | 116/200 [00:06<00:01, 63.01it/s][2026-01-17 08:14:46] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] Decode batch, #running-req: 84, #token: 20096, token usage: 0.07, npu graph: False, gen throughput (token/s): 3219.95, #queue-req: 0,
[2026-01-17 08:14:46] INFO:     127.0.0.1:47444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46712 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:06<00:01, 57.58it/s]
 62%|██████▎   | 125/200 [00:06<00:01, 57.40it/s]
 62%|██████▎   | 125/200 [00:06<00:01, 57.40it/s][2026-01-17 08:14:46] INFO:     127.0.0.1:47376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [00:06<00:01, 52.71it/s][2026-01-17 08:14:46] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:46780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47720 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:06<00:01, 51.47it/s]
 70%|███████   | 140/200 [00:06<00:01, 55.61it/s]
 70%|███████   | 140/200 [00:06<00:01, 55.61it/s][2026-01-17 08:14:46] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:46] INFO:     127.0.0.1:47484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47794 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:06<00:00, 54.44it/s]
 74%|███████▎  | 147/200 [00:06<00:00, 55.84it/s][2026-01-17 08:14:47] INFO:     127.0.0.1:47282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:46110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:06<00:00, 54.76it/s]
 77%|███████▋  | 154/200 [00:06<00:00, 56.38it/s][2026-01-17 08:14:47] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:46670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47582 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:06<00:00, 52.39it/s]
 80%|████████  | 161/200 [00:06<00:00, 52.04it/s][2026-01-17 08:14:47] INFO:     127.0.0.1:47330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] Decode batch, #running-req: 37, #token: 11264, token usage: 0.04, npu graph: False, gen throughput (token/s): 2480.67, #queue-req: 0,
[2026-01-17 08:14:47] INFO:     127.0.0.1:46192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47640 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:07<00:00, 34.60it/s][2026-01-17 08:14:47] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:46148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:46426 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:07<00:00, 30.73it/s][2026-01-17 08:14:47] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:47] INFO:     127.0.0.1:47614 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:07<00:00, 28.68it/s]
 88%|████████▊ | 177/200 [00:07<00:00, 28.76it/s][2026-01-17 08:14:48] INFO:     127.0.0.1:47304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:48] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:48] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:48] INFO:     127.0.0.1:47552 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:07<00:00, 24.68it/s][2026-01-17 08:14:48] INFO:     127.0.0.1:46734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:48] Decode batch, #running-req: 19, #token: 6400, token usage: 0.02, npu graph: False, gen throughput (token/s): 1272.39, #queue-req: 0,
[2026-01-17 08:14:48] INFO:     127.0.0.1:47502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:48] INFO:     127.0.0.1:47706 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:07<00:00, 24.47it/s][2026-01-17 08:14:48] INFO:     127.0.0.1:47280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:48] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:48] INFO:     127.0.0.1:47778 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:08<00:00, 19.12it/s][2026-01-17 08:14:48] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:48] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:48] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:48] INFO:     127.0.0.1:47334 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:08<00:00, 15.77it/s]
 96%|█████████▌| 191/200 [00:08<00:00, 15.04it/s][2026-01-17 08:14:48] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:48] INFO:     127.0.0.1:47770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:49] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:08<00:00, 17.09it/s][2026-01-17 08:14:49] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:49] Decode batch, #running-req: 5, #token: 2560, token usage: 0.01, npu graph: False, gen throughput (token/s): 578.32, #queue-req: 0,
[2026-01-17 08:14:49] INFO:     127.0.0.1:46268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:49] INFO:     127.0.0.1:46652 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:09<00:00,  8.05it/s][2026-01-17 08:14:49] Decode batch, #running-req: 4, #token: 1792, token usage: 0.01, npu graph: False, gen throughput (token/s): 201.97, #queue-req: 0,
[2026-01-17 08:14:49] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:14:50] INFO:     127.0.0.1:47604 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:10<00:00,  6.64it/s][2026-01-17 08:14:50] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, npu graph: False, gen throughput (token/s): 85.20, #queue-req: 0,
[2026-01-17 08:14:50] INFO:     127.0.0.1:47656 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:10<00:00, 19.05it/s]
.
----------------------------------------------------------------------
Ran 1 test in 101.825s

OK
Accuracy: 0.660
Invalid: 0.000
Latency: 10.581 s
Output throughput: 1680.249 token/s
.
.
End (7/62):
filename='ascend/llm_models/test_ascend_internlm2_7b.py', elapsed=111, estimated_time=400
.
.

.
.
Begin (8/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_ling_lite.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:15:10] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/inclusionAI/Ling-lite', tokenizer_path='/root/.cache/modelscope/hub/models/inclusionAI/Ling-lite', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=103732344, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/inclusionAI/Ling-lite', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:15:10] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:15:20] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:15:21] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:15:22] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:15:22] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/66 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   2% Completed | 1/66 [00:00<00:06,  9.85it/s]

Loading safetensors checkpoint shards:   3% Completed | 2/66 [00:00<00:14,  4.57it/s]

Loading safetensors checkpoint shards:   5% Completed | 3/66 [00:00<00:13,  4.71it/s]

Loading safetensors checkpoint shards:   6% Completed | 4/66 [00:00<00:13,  4.45it/s]

Loading safetensors checkpoint shards:   8% Completed | 5/66 [00:01<00:12,  5.04it/s]

Loading safetensors checkpoint shards:   9% Completed | 6/66 [00:01<00:09,  6.06it/s]

Loading safetensors checkpoint shards:  12% Completed | 8/66 [00:01<00:08,  7.24it/s]

Loading safetensors checkpoint shards:  14% Completed | 9/66 [00:01<00:08,  6.91it/s]

Loading safetensors checkpoint shards:  15% Completed | 10/66 [00:01<00:08,  6.72it/s]

Loading safetensors checkpoint shards:  18% Completed | 12/66 [00:01<00:06,  8.58it/s]

Loading safetensors checkpoint shards:  20% Completed | 13/66 [00:01<00:06,  8.29it/s]

Loading safetensors checkpoint shards:  21% Completed | 14/66 [00:02<00:06,  7.83it/s]

Loading safetensors checkpoint shards:  23% Completed | 15/66 [00:02<00:06,  7.52it/s]

Loading safetensors checkpoint shards:  24% Completed | 16/66 [00:02<00:06,  8.06it/s]

Loading safetensors checkpoint shards:  27% Completed | 18/66 [00:02<00:05,  8.77it/s]

Loading safetensors checkpoint shards:  29% Completed | 19/66 [00:02<00:05,  8.18it/s]

Loading safetensors checkpoint shards:  32% Completed | 21/66 [00:02<00:05,  8.59it/s]

Loading safetensors checkpoint shards:  35% Completed | 23/66 [00:03<00:04,  9.66it/s]

Loading safetensors checkpoint shards:  38% Completed | 25/66 [00:03<00:04,  9.72it/s]

Loading safetensors checkpoint shards:  39% Completed | 26/66 [00:03<00:04,  9.08it/s]

Loading safetensors checkpoint shards:  41% Completed | 27/66 [00:03<00:04,  8.57it/s]

Loading safetensors checkpoint shards:  44% Completed | 29/66 [00:03<00:04,  8.82it/s]

Loading safetensors checkpoint shards:  47% Completed | 31/66 [00:03<00:03, 10.74it/s]

Loading safetensors checkpoint shards:  50% Completed | 33/66 [00:04<00:03, 10.46it/s]

Loading safetensors checkpoint shards:  53% Completed | 35/66 [00:04<00:03,  9.51it/s]

Loading safetensors checkpoint shards:  56% Completed | 37/66 [00:04<00:03,  9.50it/s]

Loading safetensors checkpoint shards:  58% Completed | 38/66 [00:04<00:03,  8.50it/s]

Loading safetensors checkpoint shards:  59% Completed | 39/66 [00:04<00:03,  7.54it/s]

Loading safetensors checkpoint shards:  61% Completed | 40/66 [00:05<00:03,  7.72it/s]

Loading safetensors checkpoint shards:  64% Completed | 42/66 [00:05<00:03,  7.75it/s]

Loading safetensors checkpoint shards:  65% Completed | 43/66 [00:05<00:03,  7.09it/s]

Loading safetensors checkpoint shards:  67% Completed | 44/66 [00:05<00:03,  6.53it/s]

Loading safetensors checkpoint shards:  68% Completed | 45/66 [00:05<00:03,  6.20it/s]

Loading safetensors checkpoint shards:  70% Completed | 46/66 [00:06<00:03,  6.05it/s]

Loading safetensors checkpoint shards:  71% Completed | 47/66 [00:06<00:02,  6.62it/s]

Loading safetensors checkpoint shards:  74% Completed | 49/66 [00:06<00:02,  7.17it/s]

Loading safetensors checkpoint shards:  76% Completed | 50/66 [00:06<00:02,  7.49it/s]

Loading safetensors checkpoint shards:  79% Completed | 52/66 [00:06<00:01,  7.61it/s]

Loading safetensors checkpoint shards:  80% Completed | 53/66 [00:06<00:01,  7.64it/s]

Loading safetensors checkpoint shards:  83% Completed | 55/66 [00:07<00:01,  8.61it/s]

Loading safetensors checkpoint shards:  86% Completed | 57/66 [00:07<00:00,  9.28it/s]

Loading safetensors checkpoint shards:  89% Completed | 59/66 [00:07<00:00,  8.76it/s]

Loading safetensors checkpoint shards:  92% Completed | 61/66 [00:07<00:00, 10.61it/s]

Loading safetensors checkpoint shards:  95% Completed | 63/66 [00:10<00:01,  1.74it/s]

Loading safetensors checkpoint shards:  98% Completed | 65/66 [00:11<00:00,  2.35it/s]

Loading safetensors checkpoint shards: 100% Completed | 66/66 [00:11<00:00,  5.97it/s]

[2026-01-17 08:15:49] Load weight end. type=BailingMoeForCausalLM, dtype=torch.bfloat16, avail mem=29.49 GB, mem usage=31.32 GB.
[2026-01-17 08:15:49] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:15:49] The available memory for KV cache is 17.31 GB.
[2026-01-17 08:15:49] KV Cache is allocated. #tokens: 323968, K size: 8.65 GB, V size: 8.65 GB
[2026-01-17 08:15:49] Memory pool end. avail mem=11.68 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:15:50] max_total_num_tokens=323968, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.68 GB
[2026-01-17 08:15:51] INFO:     Started server process [28278]
[2026-01-17 08:15:51] INFO:     Waiting for application startup.
[2026-01-17 08:15:51] INFO:     Application startup complete.
[2026-01-17 08:15:51] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:15:52] INFO:     127.0.0.1:49120 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:15:52] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
.[2026-01-17 08:15:56] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:15:56] The server is fired up and ready to roll!
[2026-01-17 08:16:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:16:01] INFO:     127.0.0.1:34322 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:16:01] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:16:01] INFO:     127.0.0.1:34332 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:16:01] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:16:03] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/inclusionAI/Ling-lite --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:16:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:16:03] Prefill batch, #new-seq: 18, #new-token: 2688, #cached-token: 13824, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:16:03] Prefill batch, #new-seq: 34, #new-token: 4736, #cached-token: 26112, token usage: 0.01, #running-req: 19, #queue-req: 0,
[2026-01-17 08:16:04] Prefill batch, #new-seq: 58, #new-token: 8192, #cached-token: 44544, token usage: 0.03, #running-req: 53, #queue-req: 17,
[2026-01-17 08:16:04] Prefill batch, #new-seq: 18, #new-token: 2432, #cached-token: 13056, token usage: 0.05, #running-req: 110, #queue-req: 0,
[2026-01-17 08:16:06] Decode batch, #running-req: 128, #token: 25728, token usage: 0.08, npu graph: False, gen throughput (token/s): 98.50, #queue-req: 0,
[2026-01-17 08:16:06] INFO:     127.0.0.1:34362 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:06] INFO:     127.0.0.1:34564 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:03<12:10,  3.67s/it]
  1%|          | 2/200 [00:03<07:37,  2.31s/it][2026-01-17 08:16:06] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-17 08:16:07] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:07] INFO:     127.0.0.1:34922 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:03<05:04,  1.55s/it]
  2%|▏         | 4/200 [00:03<02:37,  1.25it/s][2026-01-17 08:16:07] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-17 08:16:07] INFO:     127.0.0.1:35066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:07] INFO:     127.0.0.1:34976 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:04<01:44,  1.85it/s][2026-01-17 08:16:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:07] INFO:     127.0.0.1:35102 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:04<01:29,  2.16it/s][2026-01-17 08:16:07] INFO:     127.0.0.1:34604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:07] INFO:     127.0.0.1:35020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:07] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:16:07] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:04<00:52,  3.64it/s][2026-01-17 08:16:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:07] INFO:     127.0.0.1:34906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:07] INFO:     127.0.0.1:35460 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:04<00:47,  3.95it/s]
  6%|▌         | 12/200 [00:04<00:36,  5.19it/s][2026-01-17 08:16:08] INFO:     127.0.0.1:35032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-17 08:16:08] INFO:     127.0.0.1:34658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:16:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-17 08:16:08] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:05<00:27,  6.66it/s][2026-01-17 08:16:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:08] INFO:     127.0.0.1:35036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-17 08:16:08] INFO:     127.0.0.1:34636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] INFO:     127.0.0.1:34708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:05<00:20,  8.76it/s]
 10%|█         | 21/200 [00:05<00:08, 21.02it/s]
 10%|█         | 21/200 [00:05<00:08, 21.02it/s]
 10%|█         | 21/200 [00:05<00:08, 21.02it/s][2026-01-17 08:16:08] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 3072, token usage: 0.09, #running-req: 124, #queue-req: 0,
[2026-01-17 08:16:08] INFO:     127.0.0.1:35160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] INFO:     127.0.0.1:35216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-17 08:16:08] Decode batch, #running-req: 128, #token: 31104, token usage: 0.10, npu graph: False, gen throughput (token/s): 2351.66, #queue-req: 0,
[2026-01-17 08:16:08] INFO:     127.0.0.1:34502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:05<00:10, 16.99it/s]
 12%|█▎        | 25/200 [00:05<00:11, 15.89it/s][2026-01-17 08:16:08] INFO:     127.0.0.1:35298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-17 08:16:08] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:16:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 129, #queue-req: 0,
[2026-01-17 08:16:08] INFO:     127.0.0.1:35320 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:05<00:11, 14.80it/s][2026-01-17 08:16:09] INFO:     127.0.0.1:34442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:09] INFO:     127.0.0.1:34792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:16:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 129, #queue-req: 0,
[2026-01-17 08:16:09] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:05<00:12, 14.08it/s][2026-01-17 08:16:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:09] INFO:     127.0.0.1:34548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:09] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:09] INFO:     127.0.0.1:35450 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:06<00:14, 11.27it/s]
 17%|█▋        | 34/200 [00:06<00:15, 10.63it/s][2026-01-17 08:16:09] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:09] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.09, #running-req: 125, #queue-req: 0,
[2026-01-17 08:16:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:16:09] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:06<00:16,  9.98it/s][2026-01-17 08:16:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:09] INFO:     127.0.0.1:34672 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:10] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:06<00:17,  9.52it/s][2026-01-17 08:16:10] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:10] INFO:     127.0.0.1:35140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:10] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:16:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 129, #queue-req: 0,
[2026-01-17 08:16:10] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:10] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:07<00:15, 10.29it/s]
 21%|██        | 42/200 [00:07<00:13, 12.02it/s][2026-01-17 08:16:10] INFO:     127.0.0.1:35252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-17 08:16:10] INFO:     127.0.0.1:35358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:10] INFO:     127.0.0.1:35228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:16:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 130, #queue-req: 0,
[2026-01-17 08:16:10] INFO:     127.0.0.1:34588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:10] INFO:     127.0.0.1:34984 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:07<00:11, 13.27it/s]
 24%|██▎       | 47/200 [00:07<00:09, 15.36it/s][2026-01-17 08:16:10] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:10] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-17 08:16:10] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:16:10] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:07<00:10, 14.48it/s][2026-01-17 08:16:10] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:10] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:10] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:10] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:07<00:11, 12.64it/s]
 26%|██▋       | 53/200 [00:07<00:11, 12.56it/s][2026-01-17 08:16:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-17 08:16:11] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:11] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:11] INFO:     127.0.0.1:35434 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:07<00:11, 12.48it/s][2026-01-17 08:16:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:11] Decode batch, #running-req: 127, #token: 30336, token usage: 0.09, npu graph: False, gen throughput (token/s): 1997.42, #queue-req: 0,
[2026-01-17 08:16:11] INFO:     127.0.0.1:34868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:11] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:11] INFO:     127.0.0.1:34650 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:08<00:11, 12.46it/s][2026-01-17 08:16:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:11] INFO:     127.0.0.1:35026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:11] INFO:     127.0.0.1:35330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:11] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-17 08:16:11] INFO:     127.0.0.1:35444 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:08<00:09, 14.06it/s][2026-01-17 08:16:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:11] INFO:     127.0.0.1:34746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:11] INFO:     127.0.0.1:34956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:11] INFO:     127.0.0.1:35004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:11] INFO:     127.0.0.1:35194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:11] INFO:     127.0.0.1:41480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:11] Prefill batch, #new-seq: 5, #new-token: 640, #cached-token: 3840, token usage: 0.09, #running-req: 123, #queue-req: 0,
[2026-01-17 08:16:13] INFO:     127.0.0.1:41492 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:09<00:22,  6.00it/s][2026-01-17 08:16:13] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:13] INFO:     127.0.0.1:35086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:13] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:13] INFO:     127.0.0.1:35390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:13] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:16:13] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:10<00:17,  7.34it/s][2026-01-17 08:16:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:16:13] INFO:     127.0.0.1:34440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:13] INFO:     127.0.0.1:35180 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:10<00:16,  7.73it/s][2026-01-17 08:16:13] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:13] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-17 08:16:13] INFO:     127.0.0.1:35062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:13] INFO:     127.0.0.1:35176 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:10<00:13,  9.29it/s][2026-01-17 08:16:13] INFO:     127.0.0.1:34458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:13] INFO:     127.0.0.1:35050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:13] INFO:     127.0.0.1:35240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:13] INFO:     127.0.0.1:41498 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:10<00:10, 11.94it/s][2026-01-17 08:16:13] INFO:     127.0.0.1:34900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:13] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:10<00:09, 12.64it/s][2026-01-17 08:16:14] INFO:     127.0.0.1:35514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:41478 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:10<00:09, 12.63it/s][2026-01-17 08:16:14] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:34562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:34940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:11<00:08, 13.35it/s]
 44%|████▍     | 88/200 [00:11<00:04, 25.06it/s]
 44%|████▍     | 88/200 [00:11<00:04, 25.06it/s]
 44%|████▍     | 88/200 [00:11<00:04, 25.06it/s][2026-01-17 08:16:14] INFO:     127.0.0.1:34770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:35230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:11<00:05, 21.62it/s][2026-01-17 08:16:14] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:34882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:41794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:41824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] Decode batch, #running-req: 108, #token: 28416, token usage: 0.09, npu graph: False, gen throughput (token/s): 1451.21, #queue-req: 0,
[2026-01-17 08:16:14] INFO:     127.0.0.1:34678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:34812 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:11<00:03, 26.35it/s]
 49%|████▉     | 98/200 [00:11<00:03, 31.51it/s][2026-01-17 08:16:14] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:34950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:11<00:03, 27.64it/s][2026-01-17 08:16:14] INFO:     127.0.0.1:35038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:14] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:35406 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:11<00:04, 23.63it/s][2026-01-17 08:16:15] INFO:     127.0.0.1:35250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:42038 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:11<00:03, 23.89it/s]
 55%|█████▍    | 109/200 [00:11<00:03, 26.12it/s][2026-01-17 08:16:15] INFO:     127.0.0.1:34498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:34782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:34888 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:12<00:03, 25.73it/s][2026-01-17 08:16:15] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:41716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:34396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:41568 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:12<00:03, 23.66it/s]
 58%|█████▊    | 116/200 [00:12<00:03, 24.30it/s][2026-01-17 08:16:15] INFO:     127.0.0.1:35380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:35268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:12<00:04, 18.62it/s][2026-01-17 08:16:15] INFO:     127.0.0.1:41776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:35354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:12<00:04, 18.73it/s][2026-01-17 08:16:15] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:15] INFO:     127.0.0.1:41590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:16] INFO:     127.0.0.1:34618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:16] INFO:     127.0.0.1:35082 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:12<00:04, 17.72it/s]
 63%|██████▎   | 126/200 [00:12<00:03, 18.67it/s][2026-01-17 08:16:16] Decode batch, #running-req: 74, #token: 23424, token usage: 0.07, npu graph: False, gen throughput (token/s): 2261.18, #queue-req: 0,
[2026-01-17 08:16:16] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:16] INFO:     127.0.0.1:34846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:16] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:16] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:13<00:04, 14.43it/s]
 65%|██████▌   | 130/200 [00:13<00:05, 13.91it/s]
 65%|██████▌   | 130/200 [00:13<00:05, 13.91it/s][2026-01-17 08:16:16] INFO:     127.0.0.1:35184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:16] INFO:     127.0.0.1:41634 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:13<00:05, 12.43it/s][2026-01-17 08:16:16] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:16] INFO:     127.0.0.1:35476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:16] INFO:     127.0.0.1:41532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:16] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:16] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [00:13<00:04, 15.31it/s]
 68%|██████▊   | 137/200 [00:13<00:03, 19.25it/s][2026-01-17 08:16:16] INFO:     127.0.0.1:34622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:16] INFO:     127.0.0.1:41986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:17] INFO:     127.0.0.1:34876 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:13<00:04, 14.92it/s][2026-01-17 08:16:17] INFO:     127.0.0.1:35350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:17] INFO:     127.0.0.1:34646 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:13<00:04, 13.79it/s][2026-01-17 08:16:17] INFO:     127.0.0.1:35326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:17] INFO:     127.0.0.1:42104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:17] INFO:     127.0.0.1:34486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:17] INFO:     127.0.0.1:41808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:17] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:14<00:02, 17.98it/s][2026-01-17 08:16:17] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:17] INFO:     127.0.0.1:41666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:17] INFO:     127.0.0.1:42024 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:14<00:02, 18.39it/s][2026-01-17 08:16:17] INFO:     127.0.0.1:41884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:17] INFO:     127.0.0.1:41920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:17] Decode batch, #running-req: 50, #token: 16768, token usage: 0.05, npu graph: False, gen throughput (token/s): 1630.17, #queue-req: 0,
[2026-01-17 08:16:17] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:14<00:03, 15.05it/s][2026-01-17 08:16:17] INFO:     127.0.0.1:41780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:18] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:14<00:03, 13.16it/s][2026-01-17 08:16:18] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:18] INFO:     127.0.0.1:41706 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:14<00:03, 13.86it/s][2026-01-17 08:16:18] INFO:     127.0.0.1:41548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:18] INFO:     127.0.0.1:35370 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:15<00:03, 12.93it/s][2026-01-17 08:16:18] INFO:     127.0.0.1:35024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:18] INFO:     127.0.0.1:41690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:18] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:15<00:03, 12.44it/s][2026-01-17 08:16:18] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:18] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:18] INFO:     127.0.0.1:42008 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:15<00:03, 11.33it/s]
 82%|████████▎ | 165/200 [00:15<00:02, 11.96it/s][2026-01-17 08:16:19] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:19] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:15<00:03, 10.57it/s][2026-01-17 08:16:19] INFO:     127.0.0.1:41860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:19] Decode batch, #running-req: 33, #token: 12800, token usage: 0.04, npu graph: False, gen throughput (token/s): 1096.74, #queue-req: 0,
[2026-01-17 08:16:19] INFO:     127.0.0.1:35318 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:15<00:02, 11.77it/s][2026-01-17 08:16:19] INFO:     127.0.0.1:34720 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:19] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:16<00:02, 12.92it/s][2026-01-17 08:16:19] INFO:     127.0.0.1:34488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:19] INFO:     127.0.0.1:41564 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:16<00:02, 12.21it/s][2026-01-17 08:16:19] INFO:     127.0.0.1:42134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:19] INFO:     127.0.0.1:42060 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:16<00:02, 10.02it/s][2026-01-17 08:16:19] INFO:     127.0.0.1:41830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:19] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:20] INFO:     127.0.0.1:41856 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:16<00:02,  9.73it/s][2026-01-17 08:16:20] INFO:     127.0.0.1:41526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:20] INFO:     127.0.0.1:41934 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:17<00:02,  9.17it/s][2026-01-17 08:16:20] Decode batch, #running-req: 20, #token: 8960, token usage: 0.03, npu graph: False, gen throughput (token/s): 684.64, #queue-req: 0,
[2026-01-17 08:16:20] INFO:     127.0.0.1:41512 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:17<00:02,  7.01it/s][2026-01-17 08:16:20] INFO:     127.0.0.1:35484 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:17<00:02,  6.14it/s][2026-01-17 08:16:20] INFO:     127.0.0.1:41664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:20] INFO:     127.0.0.1:41756 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:21] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:17<00:01,  8.50it/s][2026-01-17 08:16:21] INFO:     127.0.0.1:41768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:21] INFO:     127.0.0.1:41932 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:18<00:01,  7.96it/s][2026-01-17 08:16:21] INFO:     127.0.0.1:41904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:21] INFO:     127.0.0.1:41992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:16:21] INFO:     127.0.0.1:41602 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:18<00:01,  8.39it/s][2026-01-17 08:16:21] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:18<00:01,  7.45it/s][2026-01-17 08:16:22] Decode batch, #running-req: 9, #token: 4864, token usage: 0.02, npu graph: False, gen throughput (token/s): 392.83, #queue-req: 0,
[2026-01-17 08:16:22] INFO:     127.0.0.1:41874 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:18<00:01,  7.01it/s][2026-01-17 08:16:22] INFO:     127.0.0.1:41800 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:19<00:01,  6.07it/s][2026-01-17 08:16:22] INFO:     127.0.0.1:41900 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:19<00:01,  4.82it/s][2026-01-17 08:16:23] INFO:     127.0.0.1:41946 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:20<00:01,  3.21it/s][2026-01-17 08:16:23] Decode batch, #running-req: 5, #token: 3200, token usage: 0.01, npu graph: False, gen throughput (token/s): 191.90, #queue-req: 0,
[2026-01-17 08:16:24] INFO:     127.0.0.1:41960 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:21<00:02,  1.73it/s][2026-01-17 08:16:24] Decode batch, #running-req: 4, #token: 3072, token usage: 0.01, npu graph: False, gen throughput (token/s): 139.06, #queue-req: 0,
[2026-01-17 08:16:25] INFO:     127.0.0.1:42146 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:22<00:02,  1.43it/s][2026-01-17 08:16:26] Decode batch, #running-req: 3, #token: 2560, token usage: 0.01, npu graph: False, gen throughput (token/s): 102.80, #queue-req: 0,
[2026-01-17 08:16:26] INFO:     127.0.0.1:34472 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:23<00:01,  1.18it/s][2026-01-17 08:16:27] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:24<00:00,  1.38it/s][2026-01-17 08:16:27] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 66.19, #queue-req: 0,
[2026-01-17 08:16:29] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 29.74, #queue-req: 0,
[2026-01-17 08:16:30] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 29.67, #queue-req: 0,
[2026-01-17 08:16:31] INFO:     127.0.0.1:41976 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:28<00:00,  1.72s/it]
100%|██████████| 200/200 [00:28<00:00,  7.07it/s]
.
----------------------------------------------------------------------
Ran 1 test in 91.056s

OK
Accuracy: 0.875
Invalid: 0.000
Latency: 29.867 s
Output throughput: 995.593 token/s
.
.
End (8/62):
filename='ascend/llm_models/test_ascend_ling_lite.py', elapsed=101, estimated_time=400
.
.

.
.
Begin (9/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_afm_4_5b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
The explicitly set RoPE scaling factor (config.rope_scaling['factor'] = 20.0) does not match the ratio implicitly set by other parameters (implicit factor = post-yarn context length / pre-yarn context length = config.max_position_embeddings / config.rope_scaling['original_max_position_embeddings'] = 16.0). Using the explicit factor (20.0) in YaRN. This may cause unexpected behaviour in model usage, please correct the 'max_position_embeddings' fields in the model config.
[2026-01-17 08:16:51] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/arcee-ai/AFM-4.5B-Base', tokenizer_path='/root/.cache/modelscope/hub/models/arcee-ai/AFM-4.5B-Base', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=636072971, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/arcee-ai/AFM-4.5B-Base', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:16:51] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
The explicitly set RoPE scaling factor (config.rope_scaling['factor'] = 20.0) does not match the ratio implicitly set by other parameters (implicit factor = post-yarn context length / pre-yarn context length = config.max_position_embeddings / config.rope_scaling['original_max_position_embeddings'] = 16.0). Using the explicit factor (20.0) in YaRN. This may cause unexpected behaviour in model usage, please correct the 'max_position_embeddings' fields in the model config.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:17:00] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:17:01] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:17:01] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:17:02] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:17:02] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:07<00:07,  7.08s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:13<00:00,  6.71s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:13<00:00,  6.77s/it]

[2026-01-17 08:17:16] Load weight end. type=ArceeForCausalLM, dtype=torch.bfloat16, avail mem=52.18 GB, mem usage=8.64 GB.
[2026-01-17 08:17:16] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:17:16] The available memory for KV cache is 40.01 GB.
[2026-01-17 08:17:17] KV Cache is allocated. #tokens: 582656, K size: 20.01 GB, V size: 20.01 GB
[2026-01-17 08:17:17] Memory pool end. avail mem=11.16 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:17:18] max_total_num_tokens=582656, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=65536, available_gpu_mem=11.14 GB
[2026-01-17 08:17:19] INFO:     Started server process [31237]
[2026-01-17 08:17:19] INFO:     Waiting for application startup.
[2026-01-17 08:17:19] INFO:     Application startup complete.
[2026-01-17 08:17:19] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:17:20] INFO:     127.0.0.1:52072 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:17:20] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:17:21] INFO:     127.0.0.1:52090 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:17:31] INFO:     127.0.0.1:46240 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:17:35] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:35] The server is fired up and ready to roll!
[2026-01-17 08:17:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:17:42] INFO:     127.0.0.1:52632 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:17:42] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:17:42] INFO:     127.0.0.1:52644 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:17:43] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:17:43] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/arcee-ai/AFM-4.5B-Base --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:17:43] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:17:43] Prefill batch, #new-seq: 16, #new-token: 2304, #cached-token: 10240, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:17:43] Prefill batch, #new-seq: 21, #new-token: 2688, #cached-token: 13440, token usage: 0.01, #running-req: 17, #queue-req: 0,
[2026-01-17 08:17:43] Prefill batch, #new-seq: 22, #new-token: 3072, #cached-token: 14080, token usage: 0.01, #running-req: 38, #queue-req: 0,
[2026-01-17 08:17:43] Prefill batch, #new-seq: 20, #new-token: 2688, #cached-token: 12800, token usage: 0.02, #running-req: 60, #queue-req: 0,
[2026-01-17 08:17:44] Prefill batch, #new-seq: 48, #new-token: 6272, #cached-token: 30720, token usage: 0.02, #running-req: 80, #queue-req: 0,
[2026-01-17 08:17:45] INFO:     127.0.0.1:52970 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<07:46,  2.34s/it][2026-01-17 08:17:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 08:17:45] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:02<04:00,  1.21s/it][2026-01-17 08:17:45] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 08:17:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:17:45] Decode batch, #running-req: 128, #token: 21504, token usage: 0.04, npu graph: False, gen throughput (token/s): 86.57, #queue-req: 0,
[2026-01-17 08:17:45] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:45] INFO:     127.0.0.1:53632 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:02<01:38,  1.99it/s]
  2%|▎         | 5/200 [00:02<00:47,  4.09it/s][2026-01-17 08:17:46] INFO:     127.0.0.1:53200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:46] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 08:17:46] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:46] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:17:46] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 08:17:47] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:04<01:09,  2.74it/s][2026-01-17 08:17:47] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 08:17:47] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:53816 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:04<01:01,  3.10it/s]
  6%|▌         | 11/200 [00:04<00:29,  6.47it/s]
  6%|▌         | 11/200 [00:04<00:29,  6.47it/s][2026-01-17 08:17:47] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:53186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-17 08:17:47] Prefill batch, #new-seq: 7, #new-token: 1024, #cached-token: 4480, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:17:47] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:53706 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:04<00:13, 13.16it/s]
 10%|█         | 20/200 [00:04<00:08, 20.01it/s][2026-01-17 08:17:47] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 08:17:47] INFO:     127.0.0.1:53792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:47] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:17:47] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 131, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:04<00:07, 22.47it/s][2026-01-17 08:17:48] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 08:17:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:05<00:07, 21.69it/s][2026-01-17 08:17:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.05, #running-req: 125, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:05<00:07, 23.25it/s]
 18%|█▊        | 35/200 [00:05<00:05, 30.58it/s]
 18%|█▊        | 35/200 [00:05<00:05, 30.58it/s][2026-01-17 08:17:48] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.05, #running-req: 125, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 2560, token usage: 0.05, #running-req: 124, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:52918 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:05<00:04, 32.07it/s][2026-01-17 08:17:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:05<00:05, 28.12it/s]
 23%|██▎       | 46/200 [00:05<00:05, 28.49it/s]
 23%|██▎       | 46/200 [00:05<00:05, 28.49it/s][2026-01-17 08:17:48] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.05, #running-req: 125, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:05<00:06, 24.54it/s][2026-01-17 08:17:48] INFO:     127.0.0.1:53372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 08:17:48] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] INFO:     127.0.0.1:53690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 08:17:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.05, #running-req: 130, #queue-req: 0,
[2026-01-17 08:17:49] INFO:     127.0.0.1:53266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:49] INFO:     127.0.0.1:53598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:49] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:06<00:05, 24.97it/s]
 28%|██▊       | 56/200 [00:06<00:04, 29.73it/s]
 28%|██▊       | 56/200 [00:06<00:04, 29.73it/s][2026-01-17 08:17:49] INFO:     127.0.0.1:53776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:49] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.05, #running-req: 125, #queue-req: 0,
[2026-01-17 08:17:49] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:49] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:49] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:49] INFO:     127.0.0.1:53360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:49] INFO:     127.0.0.1:53450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:49] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:17:49] Prefill batch, #new-seq: 5, #new-token: 768, #cached-token: 3200, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 08:17:49] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:49] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:06<00:04, 31.27it/s]
 32%|███▏      | 64/200 [00:06<00:04, 33.84it/s][2026-01-17 08:17:49] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 08:17:49] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:49] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:50] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:50] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:07<00:12, 10.43it/s][2026-01-17 08:17:50] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.04, #running-req: 128, #queue-req: 0,

 34%|███▍      | 69/200 [00:07<00:19,  6.86it/s][2026-01-17 08:17:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 131, #queue-req: 0,
[2026-01-17 08:17:50] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:50] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:50] INFO:     127.0.0.1:53878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:50] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,

 36%|███▌      | 71/200 [00:07<00:17,  7.28it/s]
 36%|███▌      | 72/200 [00:07<00:15,  8.41it/s][2026-01-17 08:17:50] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:50] INFO:     127.0.0.1:53310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 08:17:50] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:50] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:50] Decode batch, #running-req: 124, #token: 25216, token usage: 0.04, npu graph: False, gen throughput (token/s): 1018.64, #queue-req: 0,
[2026-01-17 08:17:50] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:50] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:50] INFO:     127.0.0.1:48486 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:07<00:10, 11.52it/s]
 40%|███▉      | 79/200 [00:07<00:05, 21.86it/s]
 40%|███▉      | 79/200 [00:07<00:05, 21.86it/s][2026-01-17 08:17:51] INFO:     127.0.0.1:52752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:52740 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:08<00:04, 25.84it/s][2026-01-17 08:17:51] INFO:     127.0.0.1:53082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:08<00:03, 28.37it/s]
 45%|████▌     | 90/200 [00:08<00:03, 32.30it/s][2026-01-17 08:17:51] INFO:     127.0.0.1:53236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [00:08<00:02, 35.82it/s][2026-01-17 08:17:51] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:48592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:08<00:03, 30.24it/s]
 51%|█████     | 102/200 [00:08<00:03, 29.36it/s]
 51%|█████     | 102/200 [00:08<00:03, 29.36it/s][2026-01-17 08:17:51] INFO:     127.0.0.1:48578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:49024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:52944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:08<00:02, 36.85it/s][2026-01-17 08:17:51] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:48910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:52758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:08<00:02, 32.59it/s]
 58%|█████▊    | 116/200 [00:08<00:02, 31.40it/s][2026-01-17 08:17:51] INFO:     127.0.0.1:52954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:51] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [00:08<00:01, 39.21it/s][2026-01-17 08:17:52] INFO:     127.0.0.1:53820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:53494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:09<00:01, 40.06it/s][2026-01-17 08:17:52] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:49012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] Decode batch, #running-req: 63, #token: 14592, token usage: 0.03, npu graph: False, gen throughput (token/s): 3044.89, #queue-req: 0,
[2026-01-17 08:17:52] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48892 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:09<00:01, 50.80it/s]
 70%|███████   | 141/200 [00:09<00:00, 71.71it/s]
 70%|███████   | 141/200 [00:09<00:00, 71.71it/s][2026-01-17 08:17:52] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:53064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48708 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:09<00:00, 58.93it/s]
 75%|███████▌  | 150/200 [00:09<00:00, 53.66it/s][2026-01-17 08:17:52] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:09<00:00, 46.57it/s]
 78%|███████▊  | 157/200 [00:09<00:00, 43.57it/s][2026-01-17 08:17:52] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48914 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:09<00:00, 43.94it/s][2026-01-17 08:17:52] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:09<00:00, 40.21it/s][2026-01-17 08:17:52] INFO:     127.0.0.1:53158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:52] INFO:     127.0.0.1:49032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:53] INFO:     127.0.0.1:52806 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:09<00:00, 37.58it/s][2026-01-17 08:17:53] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:53] INFO:     127.0.0.1:53884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:53] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:53] Decode batch, #running-req: 25, #token: 7424, token usage: 0.01, npu graph: False, gen throughput (token/s): 1472.89, #queue-req: 0,
[2026-01-17 08:17:53] INFO:     127.0.0.1:48902 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:10<00:00, 27.62it/s][2026-01-17 08:17:53] INFO:     127.0.0.1:48612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:53] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:53] INFO:     127.0.0.1:48600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:53] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:10<00:00, 21.87it/s][2026-01-17 08:17:53] INFO:     127.0.0.1:48514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:53] INFO:     127.0.0.1:48992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:53] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:10<00:00, 18.30it/s][2026-01-17 08:17:53] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:54] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:54] Decode batch, #running-req: 15, #token: 5504, token usage: 0.01, npu graph: False, gen throughput (token/s): 796.30, #queue-req: 0,
[2026-01-17 08:17:54] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:11<00:01, 12.50it/s][2026-01-17 08:17:54] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:54] INFO:     127.0.0.1:53852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:54] INFO:     127.0.0.1:48656 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:11<00:00, 11.96it/s][2026-01-17 08:17:54] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:54] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:11<00:00, 12.63it/s][2026-01-17 08:17:54] INFO:     127.0.0.1:48870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:55] Decode batch, #running-req: 8, #token: 3584, token usage: 0.01, npu graph: False, gen throughput (token/s): 456.75, #queue-req: 0,
[2026-01-17 08:17:55] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:17:55] INFO:     127.0.0.1:48906 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:12<00:01,  6.41it/s]
 97%|█████████▋| 194/200 [00:12<00:01,  5.02it/s][2026-01-17 08:17:55] INFO:     127.0.0.1:48940 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:12<00:01,  4.85it/s][2026-01-17 08:17:56] Decode batch, #running-req: 5, #token: 2688, token usage: 0.00, npu graph: False, gen throughput (token/s): 297.10, #queue-req: 0,
[2026-01-17 08:17:57] Decode batch, #running-req: 5, #token: 3072, token usage: 0.01, npu graph: False, gen throughput (token/s): 217.67, #queue-req: 0,
[2026-01-17 08:17:57] Decode batch, #running-req: 5, #token: 3200, token usage: 0.01, npu graph: False, gen throughput (token/s): 218.63, #queue-req: 0,
[2026-01-17 08:17:58] Decode batch, #running-req: 5, #token: 3200, token usage: 0.01, npu graph: False, gen throughput (token/s): 219.33, #queue-req: 0,
[2026-01-17 08:17:59] Decode batch, #running-req: 5, #token: 3584, token usage: 0.01, npu graph: False, gen throughput (token/s): 218.18, #queue-req: 0,
[2026-01-17 08:18:00] Decode batch, #running-req: 5, #token: 3840, token usage: 0.01, npu graph: False, gen throughput (token/s): 220.00, #queue-req: 0,
[2026-01-17 08:18:01] Decode batch, #running-req: 5, #token: 1920, token usage: 0.00, npu graph: False, gen throughput (token/s): 217.64, #queue-req: 0,
[2026-01-17 08:18:01] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:18:01] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:18:01] INFO:     127.0.0.1:53416 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:18<00:04,  1.07s/it]
 99%|█████████▉| 198/200 [00:18<00:03,  1.73s/it]
 99%|█████████▉| 198/200 [00:18<00:03,  1.73s/it][2026-01-17 08:18:02] Decode batch, #running-req: 2, #token: 1280, token usage: 0.00, npu graph: False, gen throughput (token/s): 90.85, #queue-req: 0,
[2026-01-17 08:18:02] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:19<00:01,  1.60s/it][2026-01-17 08:18:02] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:19<00:00,  1.33s/it]
100%|██████████| 200/200 [00:19<00:00, 10.19it/s]
.
----------------------------------------------------------------------
Ran 1 test in 80.902s

OK
Accuracy: 0.535
Invalid: 0.005
Latency: 19.686 s
Output throughput: 875.363 token/s
.
.
End (9/62):
filename='ascend/llm_models/test_ascend_afm_4_5b.py', elapsed=91, estimated_time=400
.
.

.
.
Begin (10/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_chatglm2_6b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:18:23] WARNING model_config.py:1020: Casting torch.float16 to torch.bfloat16.
[2026-01-17 08:18:24] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/ZhipuAI/chatglm2-6b', tokenizer_path='/root/.cache/modelscope/hub/models/ZhipuAI/chatglm2-6b', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=15146087, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/ZhipuAI/chatglm2-6b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:18:24] Casting torch.float16 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
[2026-01-17 08:18:24] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:18:34] Casting torch.float16 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:18:35] Casting torch.float16 to torch.bfloat16.
[2026-01-17 08:18:35] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:18:36] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:18:36] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:18:36] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:18:36] Load weight begin. avail mem=60.81 GB

Loading pt checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]

Loading pt checkpoint shards:  14% Completed | 1/7 [00:05<00:31,  5.26s/it]

Loading pt checkpoint shards:  29% Completed | 2/7 [00:10<00:27,  5.43s/it]

Loading pt checkpoint shards:  43% Completed | 3/7 [00:16<00:21,  5.41s/it]

Loading pt checkpoint shards:  57% Completed | 4/7 [00:21<00:15,  5.33s/it]

Loading pt checkpoint shards:  71% Completed | 5/7 [00:26<00:10,  5.29s/it]

Loading pt checkpoint shards:  86% Completed | 6/7 [00:31<00:05,  5.16s/it]

Loading pt checkpoint shards: 100% Completed | 7/7 [00:34<00:00,  4.36s/it]

Loading pt checkpoint shards: 100% Completed | 7/7 [00:34<00:00,  4.89s/it]

[2026-01-17 08:19:11] Load weight end. type=ChatGLMModel, dtype=torch.bfloat16, avail mem=49.17 GB, mem usage=11.64 GB.
[2026-01-17 08:19:11] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:19:11] The available memory for KV cache is 37.01 GB.
[2026-01-17 08:19:12] KV Cache is allocated. #tokens: 1385728, K size: 18.50 GB, V size: 18.50 GB
[2026-01-17 08:19:12] Memory pool end. avail mem=11.65 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:19:12] max_total_num_tokens=1385728, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.65 GB
[2026-01-17 08:19:13] INFO:     Started server process [34054]
[2026-01-17 08:19:13] INFO:     Waiting for application startup.
[2026-01-17 08:19:13] INFO:     Application startup complete.
[2026-01-17 08:19:13] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:19:13] INFO:     127.0.0.1:40652 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:19:14] INFO:     127.0.0.1:40668 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:19:14] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:19:23] INFO:     127.0.0.1:37592 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:19:26] INFO:     127.0.0.1:40678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:26] The server is fired up and ready to roll!
[2026-01-17 08:19:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:19:34] INFO:     127.0.0.1:44372 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:19:34] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:19:34] INFO:     127.0.0.1:44386 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:19:34] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:19:34] INFO:     127.0.0.1:44400 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/ZhipuAI/chatglm2-6b --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --dtype bfloat16 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:19:34] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:19:34] Prefill batch, #new-seq: 6, #new-token: 896, #cached-token: 4608, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:19:35] Prefill batch, #new-seq: 43, #new-token: 8192, #cached-token: 33024, token usage: 0.00, #running-req: 7, #queue-req: 78,
[2026-01-17 08:19:36] Prefill batch, #new-seq: 41, #new-token: 8192, #cached-token: 31488, token usage: 0.01, #running-req: 50, #queue-req: 37,
[2026-01-17 08:19:36] Prefill batch, #new-seq: 37, #new-token: 6528, #cached-token: 28416, token usage: 0.01, #running-req: 91, #queue-req: 0,
[2026-01-17 08:19:37] INFO:     127.0.0.1:45062 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<09:15,  2.79s/it][2026-01-17 08:19:37] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:37] Decode batch, #running-req: 128, #token: 33536, token usage: 0.02, npu graph: False, gen throughput (token/s): 72.93, #queue-req: 0,
[2026-01-17 08:19:37] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:03<04:18,  1.30s/it][2026-01-17 08:19:37] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:37] INFO:     127.0.0.1:44956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:37] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:37] INFO:     127.0.0.1:45040 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:03<01:48,  1.81it/s][2026-01-17 08:19:37] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:44444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] INFO:     127.0.0.1:44868 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:03<01:20,  2.42it/s]
  3%|▎         | 6/200 [00:03<00:47,  4.11it/s][2026-01-17 08:19:38] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:45290 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:03<00:33,  5.69it/s][2026-01-17 08:19:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:44684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] INFO:     127.0.0.1:45182 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:03<00:25,  7.34it/s]
  6%|▌         | 11/200 [00:03<00:18, 10.40it/s][2026-01-17 08:19:38] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:44916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:44756 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] INFO:     127.0.0.1:45096 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:03<00:16, 11.04it/s]
  7%|▋         | 14/200 [00:03<00:14, 13.26it/s][2026-01-17 08:19:38] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:45358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:44718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] INFO:     127.0.0.1:44902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:45400 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:04<00:11, 15.66it/s][2026-01-17 08:19:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:45524 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:44612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:44876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] INFO:     127.0.0.1:45142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:38] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:04<00:11, 15.86it/s]
 12%|█▏        | 24/200 [00:04<00:08, 19.66it/s]
 12%|█▏        | 24/200 [00:04<00:08, 19.66it/s][2026-01-17 08:19:38] Prefill batch, #new-seq: 3, #new-token: 640, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 08:19:38] INFO:     127.0.0.1:45094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:45508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Decode batch, #running-req: 128, #token: 32896, token usage: 0.02, npu graph: False, gen throughput (token/s): 3650.08, #queue-req: 0,
[2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:44548 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:04<00:09, 18.96it/s][2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:45314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:45272 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:04<00:09, 18.48it/s][2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] INFO:     127.0.0.1:44882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:44458 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:04<00:08, 19.72it/s][2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:45110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:04<00:08, 19.34it/s][2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:45472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:44926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:04<00:08, 19.77it/s][2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:44460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] INFO:     127.0.0.1:44792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] INFO:     127.0.0.1:45262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:44754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] INFO:     127.0.0.1:45458 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:05<00:06, 24.11it/s]
 22%|██▎       | 45/200 [00:05<00:04, 35.20it/s]
 22%|██▎       | 45/200 [00:05<00:04, 35.20it/s][2026-01-17 08:19:39] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:45150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:39] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:46264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:44596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:44814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45218 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:05<00:06, 22.95it/s][2026-01-17 08:19:40] INFO:     127.0.0.1:44704 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:05<00:08, 18.28it/s]
 26%|██▌       | 51/200 [00:05<00:08, 18.28it/s][2026-01-17 08:19:40] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 3072, token usage: 0.02, #running-req: 124, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:44632 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:44670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:46244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:46420 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:05<00:07, 18.41it/s]
 28%|██▊       | 56/200 [00:05<00:06, 22.63it/s]
 28%|██▊       | 56/200 [00:05<00:06, 22.63it/s][2026-01-17 08:19:40] Prefill batch, #new-seq: 3, #new-token: 640, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:45192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.02, #running-req: 131, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:44852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:46528 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:05<00:05, 25.98it/s]
 32%|███▏      | 64/200 [00:05<00:03, 34.90it/s]
 32%|███▏      | 64/200 [00:05<00:03, 34.90it/s][2026-01-17 08:19:40] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:44454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:44986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 130, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:44698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:05<00:03, 34.01it/s]
 35%|███▌      | 70/200 [00:05<00:03, 35.11it/s][2026-01-17 08:19:40] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:44906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:45220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] Decode batch, #running-req: 125, #token: 32768, token usage: 0.02, npu graph: False, gen throughput (token/s): 3022.49, #queue-req: 0,
[2026-01-17 08:19:40] INFO:     127.0.0.1:44766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:44846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [00:06<00:03, 36.69it/s]
 40%|███▉      | 79/200 [00:06<00:02, 49.00it/s]
 40%|███▉      | 79/200 [00:06<00:02, 49.00it/s]
 40%|███▉      | 79/200 [00:06<00:02, 49.00it/s][2026-01-17 08:19:40] INFO:     127.0.0.1:45258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:44972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:45340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:40] INFO:     127.0.0.1:46328 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:06<00:02, 44.77it/s][2026-01-17 08:19:40] INFO:     127.0.0.1:44498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:46254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:46270 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:06<00:02, 43.96it/s]
 45%|████▌     | 90/200 [00:06<00:02, 45.55it/s][2026-01-17 08:19:41] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:45378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:45054 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [00:06<00:02, 42.15it/s][2026-01-17 08:19:41] INFO:     127.0.0.1:46350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:45130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:46290 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:06<00:02, 41.34it/s]
 50%|█████     | 101/200 [00:06<00:02, 43.01it/s][2026-01-17 08:19:41] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44764 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:06<00:02, 42.57it/s]
 54%|█████▍    | 108/200 [00:06<00:02, 44.43it/s][2026-01-17 08:19:41] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:46534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:45328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44508 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:06<00:02, 39.87it/s][2026-01-17 08:19:41] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:46576 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:44568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] Decode batch, #running-req: 83, #token: 25984, token usage: 0.02, npu graph: False, gen throughput (token/s): 4299.95, #queue-req: 0,
[2026-01-17 08:19:41] INFO:     127.0.0.1:44798 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:07<00:02, 38.50it/s][2026-01-17 08:19:41] INFO:     127.0.0.1:44524 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:45144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:46768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:46300 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:07<00:02, 36.43it/s][2026-01-17 08:19:41] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:41] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:45048 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:07<00:01, 40.22it/s][2026-01-17 08:19:42] INFO:     127.0.0.1:44650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:45428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:44586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46772 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:07<00:02, 32.82it/s]
 68%|██████▊   | 135/200 [00:07<00:02, 30.53it/s][2026-01-17 08:19:42] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:45072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46380 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:07<00:01, 31.77it/s][2026-01-17 08:19:42] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:45162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:07<00:01, 34.81it/s][2026-01-17 08:19:42] INFO:     127.0.0.1:46586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:44488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:07<00:01, 37.04it/s][2026-01-17 08:19:42] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] Decode batch, #running-req: 51, #token: 17920, token usage: 0.01, npu graph: False, gen throughput (token/s): 3020.41, #queue-req: 0,
[2026-01-17 08:19:42] INFO:     127.0.0.1:45416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:08<00:01, 37.49it/s][2026-01-17 08:19:42] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:08<00:01, 39.54it/s][2026-01-17 08:19:42] INFO:     127.0.0.1:44600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:44994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:42] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:08<00:00, 37.49it/s][2026-01-17 08:19:43] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:08<00:00, 37.56it/s][2026-01-17 08:19:43] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:45194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:08<00:00, 35.52it/s]
 86%|████████▋ | 173/200 [00:08<00:00, 36.47it/s][2026-01-17 08:19:43] INFO:     127.0.0.1:46376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:44538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] Decode batch, #running-req: 24, #token: 10112, token usage: 0.01, npu graph: False, gen throughput (token/s): 1661.55, #queue-req: 0,
[2026-01-17 08:19:43] INFO:     127.0.0.1:44896 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:08<00:00, 25.70it/s][2026-01-17 08:19:43] INFO:     127.0.0.1:46592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:45298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:09<00:00, 23.73it/s][2026-01-17 08:19:43] INFO:     127.0.0.1:46690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:09<00:00, 23.03it/s][2026-01-17 08:19:43] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:43] INFO:     127.0.0.1:46846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:44] INFO:     127.0.0.1:46486 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:09<00:00, 23.52it/s][2026-01-17 08:19:44] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:44] INFO:     127.0.0.1:46736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:44] INFO:     127.0.0.1:46844 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:09<00:00, 20.87it/s][2026-01-17 08:19:44] Decode batch, #running-req: 9, #token: 4480, token usage: 0.00, npu graph: False, gen throughput (token/s): 813.91, #queue-req: 0,
[2026-01-17 08:19:44] INFO:     127.0.0.1:44932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:44] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:45] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:10<00:00,  9.41it/s][2026-01-17 08:19:45] Decode batch, #running-req: 7, #token: 3584, token usage: 0.00, npu graph: False, gen throughput (token/s): 440.95, #queue-req: 0,
[2026-01-17 08:19:45] INFO:     127.0.0.1:46308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:45] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:10<00:00,  7.29it/s][2026-01-17 08:19:45] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:19:45] Decode batch, #running-req: 3, #token: 2048, token usage: 0.00, npu graph: False, gen throughput (token/s): 247.40, #queue-req: 0,
[2026-01-17 08:19:46] Decode batch, #running-req: 3, #token: 2304, token usage: 0.00, npu graph: False, gen throughput (token/s): 160.73, #queue-req: 0,
[2026-01-17 08:19:47] Decode batch, #running-req: 3, #token: 2432, token usage: 0.00, npu graph: False, gen throughput (token/s): 161.96, #queue-req: 0,
[2026-01-17 08:19:47] INFO:     127.0.0.1:46708 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:12<00:00,  3.06it/s][2026-01-17 08:19:48] Decode batch, #running-req: 2, #token: 1920, token usage: 0.00, npu graph: False, gen throughput (token/s): 127.48, #queue-req: 0,
[2026-01-17 08:19:48] INFO:     127.0.0.1:46790 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:13<00:00,  2.43it/s][2026-01-17 08:19:48] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 90.16, #queue-req: 0,
[2026-01-17 08:19:49] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 55.80, #queue-req: 0,
[2026-01-17 08:19:50] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 56.05, #queue-req: 0,
[2026-01-17 08:19:50] INFO:     127.0.0.1:46540 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:15<00:00,  1.59it/s]
100%|██████████| 200/200 [00:15<00:00, 12.86it/s]
.
----------------------------------------------------------------------
Ran 1 test in 96.829s

OK
Accuracy: 0.305
Invalid: 0.000
Latency: 15.649 s
Output throughput: 1580.815 token/s
.
.
End (10/62):
filename='ascend/llm_models/test_ascend_chatglm2_6b.py', elapsed=107, estimated_time=400
.
.

.
.
Begin (11/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_exaone_3.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:20:09] INFO model_config.py:1016: Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 08:20:09] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=1041502329, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:20:09] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 08:20:10] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:20:18] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:20:19] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 08:20:19] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:20:20] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:20:20] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:20:20] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:20:21] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:01<00:11,  1.99s/it]

Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:04<00:11,  2.35s/it]

Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:07<00:10,  2.53s/it]

Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:10<00:07,  2.64s/it]

Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:11<00:04,  2.11s/it]

Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:13<00:02,  2.22s/it]

Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:16<00:00,  2.38s/it]

Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:16<00:00,  2.35s/it]

[2026-01-17 08:20:38] Load weight end. type=ExaoneForCausalLM, dtype=torch.bfloat16, avail mem=46.22 GB, mem usage=14.59 GB.
[2026-01-17 08:20:38] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:20:38] The available memory for KV cache is 34.06 GB.
[2026-01-17 08:20:39] KV Cache is allocated. #tokens: 278912, K size: 17.03 GB, V size: 17.03 GB
[2026-01-17 08:20:39] Memory pool end. avail mem=11.65 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:20:39] max_total_num_tokens=278912, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.65 GB
[2026-01-17 08:20:40] INFO:     Started server process [36335]
[2026-01-17 08:20:40] INFO:     Waiting for application startup.
[2026-01-17 08:20:40] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:20:40] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:20:40] INFO:     Application startup complete.
[2026-01-17 08:20:40] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:20:40] INFO:     127.0.0.1:58186 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:20:41] INFO:     127.0.0.1:58188 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:20:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:20:50] INFO:     127.0.0.1:34572 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:20:54] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:20:54] The server is fired up and ready to roll!
[2026-01-17 08:21:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:21:01] INFO:     127.0.0.1:37926 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:21:01] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:21:01] INFO:     127.0.0.1:37930 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:21:01] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:21:01] INFO:     127.0.0.1:37940 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --dtype bfloat16 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:21:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:21:01] Prefill batch, #new-seq: 9, #new-token: 1408, #cached-token: 6912, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:21:02] Prefill batch, #new-seq: 56, #new-token: 8192, #cached-token: 43008, token usage: 0.01, #running-req: 10, #queue-req: 62,
[2026-01-17 08:21:03] Prefill batch, #new-seq: 53, #new-token: 8192, #cached-token: 39936, token usage: 0.04, #running-req: 65, #queue-req: 10,
[2026-01-17 08:21:03] Prefill batch, #new-seq: 10, #new-token: 1408, #cached-token: 7680, token usage: 0.07, #running-req: 118, #queue-req: 0,
[2026-01-17 08:21:05] Decode batch, #running-req: 128, #token: 27520, token usage: 0.10, npu graph: False, gen throughput (token/s): 84.30, #queue-req: 0,
[2026-01-17 08:21:05] INFO:     127.0.0.1:38154 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:04<14:42,  4.44s/it][2026-01-17 08:21:05] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:06] INFO:     127.0.0.1:39034 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:04<06:37,  2.01s/it][2026-01-17 08:21:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:06] INFO:     127.0.0.1:37956 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:04<03:44,  1.14s/it][2026-01-17 08:21:06] INFO:     127.0.0.1:38100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:06] INFO:     127.0.0.1:38504 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:05<01:49,  1.78it/s][2026-01-17 08:21:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:06] INFO:     127.0.0.1:39012 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:05<01:23,  2.32it/s][2026-01-17 08:21:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:06] INFO:     127.0.0.1:38234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:06] INFO:     127.0.0.1:38858 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:05<00:50,  3.80it/s][2026-01-17 08:21:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:06] INFO:     127.0.0.1:37972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:05<00:38,  4.95it/s][2026-01-17 08:21:07] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38156 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:05<00:21,  8.54it/s][2026-01-17 08:21:07] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:07] Decode batch, #running-req: 128, #token: 31616, token usage: 0.11, npu graph: False, gen throughput (token/s): 3466.77, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:05<00:18,  9.70it/s][2026-01-17 08:21:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] INFO:     127.0.0.1:38582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:06<00:15, 11.57it/s]
 10%|█         | 20/200 [00:06<00:12, 14.56it/s][2026-01-17 08:21:07] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] INFO:     127.0.0.1:38906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:06<00:10, 16.72it/s][2026-01-17 08:21:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38324 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] INFO:     127.0.0.1:38990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38648 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:06<00:10, 16.25it/s][2026-01-17 08:21:07] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] INFO:     127.0.0.1:38938 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:06<00:10, 15.88it/s]
 14%|█▍        | 29/200 [00:06<00:09, 17.47it/s][2026-01-17 08:21:07] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:21:07] INFO:     127.0.0.1:38900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:07] INFO:     127.0.0.1:39050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:21:08] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:06<00:10, 15.80it/s][2026-01-17 08:21:08] INFO:     127.0.0.1:38374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:08] INFO:     127.0.0.1:38120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] INFO:     127.0.0.1:38194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:06<00:10, 15.54it/s]
 18%|█▊        | 36/200 [00:06<00:07, 20.55it/s]
 18%|█▊        | 36/200 [00:06<00:07, 20.55it/s][2026-01-17 08:21:08] INFO:     127.0.0.1:37976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] INFO:     127.0.0.1:38754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-17 08:21:08] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:08] INFO:     127.0.0.1:39024 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:07<00:09, 17.36it/s][2026-01-17 08:21:08] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:08] INFO:     127.0.0.1:39088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] INFO:     127.0.0.1:38654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 130, #queue-req: 0,
[2026-01-17 08:21:08] INFO:     127.0.0.1:38190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] INFO:     127.0.0.1:38742 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:07<00:08, 19.40it/s]
 22%|██▏       | 44/200 [00:07<00:06, 22.65it/s][2026-01-17 08:21:08] INFO:     127.0.0.1:38384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:21:08] INFO:     127.0.0.1:38546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 130, #queue-req: 0,
[2026-01-17 08:21:08] INFO:     127.0.0.1:38842 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:07<00:07, 19.94it/s][2026-01-17 08:21:08] INFO:     127.0.0.1:38108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:08] INFO:     127.0.0.1:38746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] Decode batch, #running-req: 127, #token: 31360, token usage: 0.11, npu graph: False, gen throughput (token/s): 2992.23, #queue-req: 0,
[2026-01-17 08:21:09] INFO:     127.0.0.1:38338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 130, #queue-req: 0,
[2026-01-17 08:21:09] INFO:     127.0.0.1:38416 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:07<00:07, 21.08it/s][2026-01-17 08:21:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:09] INFO:     127.0.0.1:38346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:21:09] INFO:     127.0.0.1:38650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:07<00:07, 18.51it/s][2026-01-17 08:21:09] INFO:     127.0.0.1:39134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:21:09] INFO:     127.0.0.1:38070 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] INFO:     127.0.0.1:38398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] INFO:     127.0.0.1:38452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:09] INFO:     127.0.0.1:38484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 3072, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 08:21:09] INFO:     127.0.0.1:37978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] INFO:     127.0.0.1:38304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] INFO:     127.0.0.1:39126 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:08<00:06, 21.56it/s]
 32%|███▏      | 64/200 [00:08<00:04, 32.95it/s]
 32%|███▏      | 64/200 [00:08<00:04, 32.95it/s]
 32%|███▏      | 64/200 [00:08<00:04, 32.95it/s][2026-01-17 08:21:09] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.12, #running-req: 124, #queue-req: 0,
[2026-01-17 08:21:09] INFO:     127.0.0.1:38280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] INFO:     127.0.0.1:38892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] INFO:     127.0.0.1:38146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:21:09] INFO:     127.0.0.1:38812 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:08<00:04, 31.56it/s][2026-01-17 08:21:09] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-17 08:21:09] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] INFO:     127.0.0.1:39036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] INFO:     127.0.0.1:38538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:09] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,

 36%|███▌      | 72/200 [00:08<00:05, 24.81it/s][2026-01-17 08:21:10] INFO:     127.0.0.1:38008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:21:10] INFO:     127.0.0.1:38174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:38980 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:08<00:05, 23.19it/s][2026-01-17 08:21:10] INFO:     127.0.0.1:38054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:38262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:38568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:08<00:05, 23.78it/s]
 40%|███▉      | 79/200 [00:08<00:04, 26.20it/s][2026-01-17 08:21:10] INFO:     127.0.0.1:38362 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:38252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:08<00:04, 24.96it/s]
 42%|████▏     | 83/200 [00:08<00:04, 26.06it/s][2026-01-17 08:21:10] INFO:     127.0.0.1:38158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] Decode batch, #running-req: 116, #token: 32768, token usage: 0.12, npu graph: False, gen throughput (token/s): 3266.84, #queue-req: 0,
[2026-01-17 08:21:10] INFO:     127.0.0.1:37966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:38370 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:09<00:04, 23.42it/s][2026-01-17 08:21:10] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:38244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:09<00:04, 23.60it/s][2026-01-17 08:21:10] INFO:     127.0.0.1:39084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:09<00:04, 22.91it/s][2026-01-17 08:21:10] INFO:     127.0.0.1:38512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:10] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:09<00:05, 20.24it/s][2026-01-17 08:21:11] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:38210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:38680 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:09<00:04, 23.71it/s][2026-01-17 08:21:11] INFO:     127.0.0.1:38414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:09<00:03, 24.55it/s][2026-01-17 08:21:11] INFO:     127.0.0.1:39182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:38124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:38816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:09<00:02, 32.33it/s][2026-01-17 08:21:11] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:58256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] Decode batch, #running-req: 88, #token: 27648, token usage: 0.10, npu graph: False, gen throughput (token/s): 3711.04, #queue-req: 0,
[2026-01-17 08:21:11] INFO:     127.0.0.1:38706 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:10<00:03, 25.47it/s][2026-01-17 08:21:11] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:38880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:10<00:03, 23.50it/s][2026-01-17 08:21:11] INFO:     127.0.0.1:38748 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:58354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:11] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:10<00:03, 22.24it/s][2026-01-17 08:21:12] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:39072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:38438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:39112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [00:10<00:02, 27.79it/s][2026-01-17 08:21:12] INFO:     127.0.0.1:39104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:38592 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:10<00:02, 24.77it/s][2026-01-17 08:21:12] INFO:     127.0.0.1:38830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:38214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:39166 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:10<00:02, 27.92it/s][2026-01-17 08:21:12] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:38090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [00:11<00:02, 26.22it/s][2026-01-17 08:21:12] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] Decode batch, #running-req: 63, #token: 22272, token usage: 0.08, npu graph: False, gen throughput (token/s): 2966.42, #queue-req: 0,
[2026-01-17 08:21:12] INFO:     127.0.0.1:39130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:58284 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:11<00:02, 20.56it/s]
 70%|███████   | 140/200 [00:11<00:03, 19.03it/s][2026-01-17 08:21:12] INFO:     127.0.0.1:38462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:38728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:38868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:39156 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:11<00:02, 21.07it/s][2026-01-17 08:21:12] INFO:     127.0.0.1:39150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:12] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:38406 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:11<00:02, 22.00it/s][2026-01-17 08:21:13] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:11<00:02, 17.91it/s][2026-01-17 08:21:13] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:38798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:12<00:02, 17.19it/s][2026-01-17 08:21:13] Decode batch, #running-req: 46, #token: 19072, token usage: 0.07, npu graph: False, gen throughput (token/s): 2277.32, #queue-req: 0,
[2026-01-17 08:21:13] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:38536 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:12<00:02, 17.27it/s][2026-01-17 08:21:13] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:39010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:12<00:02, 17.33it/s]
 80%|███████▉  | 159/200 [00:12<00:02, 19.43it/s][2026-01-17 08:21:13] INFO:     127.0.0.1:38294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:13] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:12<00:01, 22.46it/s][2026-01-17 08:21:13] INFO:     127.0.0.1:38918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:14] INFO:     127.0.0.1:38608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:14] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:12<00:01, 22.53it/s][2026-01-17 08:21:14] INFO:     127.0.0.1:39158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:14] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:14] INFO:     127.0.0.1:38720 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:12<00:02, 14.54it/s][2026-01-17 08:21:14] Decode batch, #running-req: 31, #token: 14464, token usage: 0.05, npu graph: False, gen throughput (token/s): 1693.83, #queue-req: 0,
[2026-01-17 08:21:14] INFO:     127.0.0.1:38348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:14] INFO:     127.0.0.1:39030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:14] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:13<00:02, 13.14it/s]
 86%|████████▌ | 172/200 [00:13<00:02, 13.58it/s][2026-01-17 08:21:14] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:14] INFO:     127.0.0.1:58554 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:13<00:01, 13.28it/s][2026-01-17 08:21:14] INFO:     127.0.0.1:39060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:14] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:15] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:13<00:02, 10.59it/s][2026-01-17 08:21:15] Decode batch, #running-req: 23, #token: 11264, token usage: 0.04, npu graph: False, gen throughput (token/s): 1304.50, #queue-req: 0,
[2026-01-17 08:21:15] INFO:     127.0.0.1:38218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:15] INFO:     127.0.0.1:38788 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:13<00:01, 11.87it/s][2026-01-17 08:21:15] INFO:     127.0.0.1:38696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:15] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:14<00:01,  9.69it/s][2026-01-17 08:21:15] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:15] INFO:     127.0.0.1:38074 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:14<00:01, 10.58it/s][2026-01-17 08:21:15] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:15] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:14<00:01, 12.13it/s][2026-01-17 08:21:15] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:16] Decode batch, #running-req: 14, #token: 7040, token usage: 0.03, npu graph: False, gen throughput (token/s): 907.18, #queue-req: 0,
[2026-01-17 08:21:16] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:16] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:14<00:01, 11.08it/s]
 94%|█████████▍| 188/200 [00:14<00:01, 11.93it/s][2026-01-17 08:21:16] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:16] INFO:     127.0.0.1:38786 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:14<00:01,  8.91it/s][2026-01-17 08:21:16] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:16] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:15<00:00,  8.88it/s][2026-01-17 08:21:16] Decode batch, #running-req: 8, #token: 5248, token usage: 0.02, npu graph: False, gen throughput (token/s): 537.73, #queue-req: 0,
[2026-01-17 08:21:17] Decode batch, #running-req: 8, #token: 5376, token usage: 0.02, npu graph: False, gen throughput (token/s): 431.82, #queue-req: 0,
[2026-01-17 08:21:17] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:17] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:16<00:01,  4.02it/s][2026-01-17 08:21:17] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:18] INFO:     127.0.0.1:38298 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:16<00:00,  4.33it/s][2026-01-17 08:21:18] Decode batch, #running-req: 4, #token: 2560, token usage: 0.01, npu graph: False, gen throughput (token/s): 327.28, #queue-req: 0,
[2026-01-17 08:21:18] INFO:     127.0.0.1:38140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:18] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:17<00:00,  4.64it/s][2026-01-17 08:21:18] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:21:19] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, npu graph: False, gen throughput (token/s): 104.24, #queue-req: 0,
[2026-01-17 08:21:19] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, npu graph: False, gen throughput (token/s): 56.06, #queue-req: 0,
[2026-01-17 08:21:20] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:18<00:00,  2.63it/s]
100%|██████████| 200/200 [00:18<00:00, 10.70it/s]
.
----------------------------------------------------------------------
Ran 1 test in 80.065s

OK
Accuracy: 0.810
Invalid: 0.000
Latency: 18.791 s
Output throughput: 1746.265 token/s
.
.
End (11/62):
filename='ascend/llm_models/test_ascend_exaone_3.py', elapsed=90, estimated_time=400
.
.

.
.
Begin (12/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_phi_4_multimodal.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:21:40] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/LLM-Research/Phi-4-multimodal-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/LLM-Research/Phi-4-multimodal-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=934134662, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/LLM-Research/Phi-4-multimodal-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-17 08:21:43] Inferred chat template from model path: phi-4-mm
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:21:51] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:21:52] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:21:52] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:21:53] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:21:53] Load weight begin. avail mem=60.81 GB
[2026-01-17 08:21:53] Multimodal attention backend not set. Use sdpa.
[2026-01-17 08:21:53] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:03,  1.62s/it]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:10<00:05,  5.88s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:18<00:00,  6.95s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:18<00:00,  6.24s/it]

[2026-01-17 08:22:12] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=51.96 GB, mem usage=8.85 GB.
[2026-01-17 08:22:12] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:22:12] The available memory for KV cache is 39.80 GB.
[2026-01-17 08:22:13] KV Cache is allocated. #tokens: 325888, K size: 19.90 GB, V size: 19.90 GB
[2026-01-17 08:22:13] Memory pool end. avail mem=11.16 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:22:16] max_total_num_tokens=325888, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=11.16 GB
[2026-01-17 08:22:17] INFO:     Started server process [39006]
[2026-01-17 08:22:17] INFO:     Waiting for application startup.
[2026-01-17 08:22:17] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:22:17] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:22:17] INFO:     Application startup complete.
[2026-01-17 08:22:17] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:22:18] INFO:     127.0.0.1:39804 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 08:22:18] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 08:22:20] INFO:     127.0.0.1:39814 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:22:30] INFO:     127.0.0.1:43864 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:22:33] INFO:     127.0.0.1:39810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 08:22:33] The server is fired up and ready to roll!
[2026-01-17 08:22:40] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:22:41] INFO:     127.0.0.1:34408 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:22:41] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:22:41] INFO:     127.0.0.1:34414 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:22:41] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:22:41] INFO:     127.0.0.1:34420 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/LLM-Research/Phi-4-multimodal-instruct --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:22:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:22:41] Prefill batch, #new-seq: 13, #new-token: 1920, #cached-token: 8320, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:22:41] Prefill batch, #new-seq: 20, #new-token: 2560, #cached-token: 12800, token usage: 0.01, #running-req: 14, #queue-req: 0,
[2026-01-17 08:22:41] Prefill batch, #new-seq: 20, #new-token: 2688, #cached-token: 12800, token usage: 0.02, #running-req: 34, #queue-req: 0,
[2026-01-17 08:22:41] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 54, #queue-req: 0,
[2026-01-17 08:22:42] Prefill batch, #new-seq: 31, #new-token: 4096, #cached-token: 19840, token usage: 0.03, #running-req: 57, #queue-req: 0,
[2026-01-17 08:22:42] Prefill batch, #new-seq: 24, #new-token: 3200, #cached-token: 15360, token usage: 0.04, #running-req: 88, #queue-req: 0,
[2026-01-17 08:22:42] Prefill batch, #new-seq: 16, #new-token: 2176, #cached-token: 10240, token usage: 0.05, #running-req: 112, #queue-req: 0,
[2026-01-17 08:22:43] Decode batch, #running-req: 128, #token: 21248, token usage: 0.07, npu graph: False, gen throughput (token/s): 73.78, #queue-req: 0,
[2026-01-17 08:22:43] INFO:     127.0.0.1:34452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:43] INFO:     127.0.0.1:34988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:43] INFO:     127.0.0.1:35238 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<06:42,  2.02s/it]
  2%|▏         | 3/200 [00:02<02:58,  1.10it/s]
  2%|▏         | 3/200 [00:02<02:58,  1.10it/s][2026-01-17 08:22:43] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-17 08:22:43] INFO:     127.0.0.1:34614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:43] INFO:     127.0.0.1:34618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:43] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:43] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-17 08:22:43] INFO:     127.0.0.1:34584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:43] INFO:     127.0.0.1:35150 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:02<01:24,  2.28it/s]
  4%|▍         | 8/200 [00:02<00:45,  4.18it/s][2026-01-17 08:22:43] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 08:22:44] INFO:     127.0.0.1:35220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] INFO:     127.0.0.1:34766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] INFO:     127.0.0.1:35148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] INFO:     127.0.0.1:35312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,

  5%|▌         | 10/200 [00:02<00:40,  4.73it/s]
  6%|▋         | 13/200 [00:02<00:17, 10.77it/s]
  6%|▋         | 13/200 [00:02<00:17, 10.77it/s]
  6%|▋         | 13/200 [00:02<00:17, 10.77it/s][2026-01-17 08:22:44] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-17 08:22:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 132, #queue-req: 0,
[2026-01-17 08:22:44] INFO:     127.0.0.1:34674 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:02<00:17, 10.61it/s][2026-01-17 08:22:44] INFO:     127.0.0.1:34800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 08:22:44] INFO:     127.0.0.1:34960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] INFO:     127.0.0.1:35382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-17 08:22:44] INFO:     127.0.0.1:35330 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:02<00:13, 13.05it/s][2026-01-17 08:22:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 08:22:44] INFO:     127.0.0.1:34826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.08, #running-req: 126, #queue-req: 0,
[2026-01-17 08:22:44] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:02<00:12, 14.37it/s][2026-01-17 08:22:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 08:22:44] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 08:22:44] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:03<00:12, 14.16it/s][2026-01-17 08:22:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:22:44] INFO:     127.0.0.1:35478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] INFO:     127.0.0.1:34536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,

 13%|█▎        | 26/200 [00:03<00:13, 13.36it/s][2026-01-17 08:22:45] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:22:45] INFO:     127.0.0.1:34460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] INFO:     127.0.0.1:34654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] INFO:     127.0.0.1:34866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] INFO:     127.0.0.1:35076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] INFO:     127.0.0.1:35100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:03<00:13, 12.53it/s]
 16%|█▋        | 33/200 [00:03<00:06, 26.83it/s]
 16%|█▋        | 33/200 [00:03<00:06, 26.83it/s]
 16%|█▋        | 33/200 [00:03<00:06, 26.83it/s]
 16%|█▋        | 33/200 [00:03<00:06, 26.83it/s]
 16%|█▋        | 33/200 [00:03<00:06, 26.83it/s][2026-01-17 08:22:45] Prefill batch, #new-seq: 7, #new-token: 896, #cached-token: 4480, token usage: 0.09, #running-req: 121, #queue-req: 0,
[2026-01-17 08:22:45] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] INFO:     127.0.0.1:35494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:22:45] INFO:     127.0.0.1:34874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:22:45] INFO:     127.0.0.1:35066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:03<00:07, 22.06it/s]
 19%|█▉        | 38/200 [00:03<00:07, 20.79it/s]
 19%|█▉        | 38/200 [00:03<00:07, 20.79it/s][2026-01-17 08:22:45] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.09, #running-req: 129, #queue-req: 0,
[2026-01-17 08:22:45] INFO:     127.0.0.1:35354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] Decode batch, #running-req: 128, #token: 28032, token usage: 0.09, npu graph: False, gen throughput (token/s): 2310.17, #queue-req: 0,
[2026-01-17 08:22:45] INFO:     127.0.0.1:35104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:22:45] INFO:     127.0.0.1:35132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] INFO:     127.0.0.1:35344 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:03<00:08, 18.69it/s]
 21%|██        | 42/200 [00:03<00:08, 18.59it/s][2026-01-17 08:22:45] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-17 08:22:45] INFO:     127.0.0.1:35502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.09, #running-req: 131, #queue-req: 0,
[2026-01-17 08:22:45] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:04<00:08, 17.54it/s][2026-01-17 08:22:45] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:22:45] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:22:45] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 129, #queue-req: 0,
[2026-01-17 08:22:46] INFO:     127.0.0.1:35370 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:04<00:09, 16.46it/s][2026-01-17 08:22:46] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:22:46] INFO:     127.0.0.1:34484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] INFO:     127.0.0.1:34508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-17 08:22:46] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] INFO:     127.0.0.1:34664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] INFO:     127.0.0.1:34728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] INFO:     127.0.0.1:34778 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:04<00:09, 15.58it/s]
 28%|██▊       | 55/200 [00:04<00:06, 20.77it/s]
 28%|██▊       | 55/200 [00:04<00:06, 20.77it/s]
 28%|██▊       | 55/200 [00:04<00:06, 20.77it/s][2026-01-17 08:22:46] INFO:     127.0.0.1:35176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.08, #running-req: 124, #queue-req: 0,
[2026-01-17 08:22:46] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-17 08:22:46] INFO:     127.0.0.1:35314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.08, #running-req: 129, #queue-req: 0,
[2026-01-17 08:22:46] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] INFO:     127.0.0.1:34932 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:04<00:07, 19.88it/s]
 30%|███       | 60/200 [00:04<00:06, 20.59it/s][2026-01-17 08:22:46] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.08, #running-req: 126, #queue-req: 0,
[2026-01-17 08:22:46] INFO:     127.0.0.1:34940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] INFO:     127.0.0.1:34714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] INFO:     127.0.0.1:35448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-17 08:22:46] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.08, #running-req: 130, #queue-req: 0,
[2026-01-17 08:22:46] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] INFO:     127.0.0.1:35590 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:05<00:06, 21.18it/s]
 33%|███▎      | 66/200 [00:05<00:05, 22.94it/s][2026-01-17 08:22:46] INFO:     127.0.0.1:34462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] INFO:     127.0.0.1:35598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.08, #running-req: 126, #queue-req: 0,
[2026-01-17 08:22:46] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-17 08:22:46] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 130, #queue-req: 0,
[2026-01-17 08:22:46] INFO:     127.0.0.1:34806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] INFO:     127.0.0.1:35052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:46] INFO:     127.0.0.1:35134 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:05<00:06, 21.51it/s]
 36%|███▌      | 72/200 [00:05<00:05, 23.46it/s]
 36%|███▌      | 72/200 [00:05<00:05, 23.46it/s][2026-01-17 08:22:47] INFO:     127.0.0.1:34550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.08, #running-req: 125, #queue-req: 0,
[2026-01-17 08:22:47] INFO:     127.0.0.1:34650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:34916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35118 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:05<00:03, 30.32it/s][2026-01-17 08:22:47] INFO:     127.0.0.1:35196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:05<00:03, 32.93it/s][2026-01-17 08:22:47] INFO:     127.0.0.1:34556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:34600 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:05<00:03, 35.25it/s][2026-01-17 08:22:47] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:05<00:03, 32.78it/s][2026-01-17 08:22:47] INFO:     127.0.0.1:34900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:34744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35464 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:05<00:03, 33.39it/s][2026-01-17 08:22:47] INFO:     127.0.0.1:34502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35600 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:06<00:02, 33.75it/s]
 52%|█████▏    | 103/200 [00:06<00:02, 36.36it/s][2026-01-17 08:22:47] Decode batch, #running-req: 99, #token: 21376, token usage: 0.07, npu graph: False, gen throughput (token/s): 2216.51, #queue-req: 0,
[2026-01-17 08:22:47] INFO:     127.0.0.1:35054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:06<00:03, 30.87it/s][2026-01-17 08:22:47] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:47] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:06<00:02, 36.29it/s]
 57%|█████▊    | 115/200 [00:06<00:01, 50.20it/s]
 57%|█████▊    | 115/200 [00:06<00:01, 50.20it/s][2026-01-17 08:22:48] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:34630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:06<00:02, 33.72it/s]
 61%|██████    | 122/200 [00:06<00:02, 28.63it/s][2026-01-17 08:22:48] INFO:     127.0.0.1:34794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:06<00:02, 29.99it/s][2026-01-17 08:22:48] INFO:     127.0.0.1:35856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:06<00:01, 34.95it/s][2026-01-17 08:22:48] INFO:     127.0.0.1:35752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:34472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:36038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35940 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:07<00:01, 34.97it/s]
 69%|██████▉   | 138/200 [00:07<00:01, 36.93it/s][2026-01-17 08:22:48] INFO:     127.0.0.1:35230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:48] INFO:     127.0.0.1:35164 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:07<00:02, 28.42it/s][2026-01-17 08:22:49] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:35114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:35614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:07<00:01, 26.69it/s][2026-01-17 08:22:49] Decode batch, #running-req: 55, #token: 14080, token usage: 0.04, npu graph: False, gen throughput (token/s): 2058.98, #queue-req: 0,
[2026-01-17 08:22:49] INFO:     127.0.0.1:35816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:34578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:36082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:36084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:35880 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:07<00:01, 32.27it/s][2026-01-17 08:22:49] INFO:     127.0.0.1:35914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:35952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:36116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:36016 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:07<00:01, 26.38it/s][2026-01-17 08:22:49] INFO:     127.0.0.1:35904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:36112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:34504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:35582 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:08<00:01, 25.28it/s][2026-01-17 08:22:49] INFO:     127.0.0.1:35736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:35930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:35656 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:08<00:01, 22.94it/s][2026-01-17 08:22:49] INFO:     127.0.0.1:35802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:49] INFO:     127.0.0.1:35836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:35642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:35988 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:08<00:01, 22.52it/s]
 84%|████████▍ | 169/200 [00:08<00:01, 24.00it/s][2026-01-17 08:22:50] INFO:     127.0.0.1:34760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:35542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:35844 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:08<00:01, 21.91it/s]
 86%|████████▋ | 173/200 [00:08<00:01, 22.25it/s][2026-01-17 08:22:50] INFO:     127.0.0.1:34820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:35268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:36024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:36000 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:08<00:01, 22.51it/s]
 89%|████████▉ | 178/200 [00:08<00:00, 24.43it/s][2026-01-17 08:22:50] INFO:     127.0.0.1:34752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:35670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:35734 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:08<00:00, 22.25it/s][2026-01-17 08:22:50] Decode batch, #running-req: 20, #token: 5760, token usage: 0.02, npu graph: False, gen throughput (token/s): 999.37, #queue-req: 0,
[2026-01-17 08:22:50] INFO:     127.0.0.1:35776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:50] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:51] INFO:     127.0.0.1:35932 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:09<00:01, 13.15it/s][2026-01-17 08:22:51] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:51] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:51] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:09<00:00, 15.34it/s][2026-01-17 08:22:51] INFO:     127.0.0.1:35804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:51] INFO:     127.0.0.1:35870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:51] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:09<00:00, 15.06it/s][2026-01-17 08:22:51] INFO:     127.0.0.1:36060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:51] INFO:     127.0.0.1:36150 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:09<00:00, 14.20it/s][2026-01-17 08:22:51] INFO:     127.0.0.1:35612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:51] INFO:     127.0.0.1:36072 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:10<00:00, 11.73it/s][2026-01-17 08:22:51] Decode batch, #running-req: 6, #token: 2304, token usage: 0.01, npu graph: False, gen throughput (token/s): 380.50, #queue-req: 0,
[2026-01-17 08:22:51] INFO:     127.0.0.1:36048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:52] INFO:     127.0.0.1:35828 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:10<00:00, 10.25it/s][2026-01-17 08:22:52] INFO:     127.0.0.1:36096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:22:52] INFO:     127.0.0.1:36052 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:11<00:00,  5.32it/s][2026-01-17 08:22:53] INFO:     127.0.0.1:35766 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:11<00:00,  4.79it/s][2026-01-17 08:22:53] Decode batch, #running-req: 1, #token: 1024, token usage: 0.00, npu graph: False, gen throughput (token/s): 86.85, #queue-req: 0,
[2026-01-17 08:22:53] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:11<00:00,  5.27it/s]
100%|██████████| 200/200 [00:11<00:00, 17.06it/s]
.
----------------------------------------------------------------------
Ran 1 test in 83.038s

OK
Accuracy: 0.840
Invalid: 0.000
Latency: 11.768 s
Output throughput: 1605.055 token/s
.
.
End (12/62):
filename='ascend/llm_models/test_ascend_phi_4_multimodal.py', elapsed=92, estimated_time=400
.
.

.
.
Begin (13/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_smollm_1_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:23:11] INFO model_config.py:1016: Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 08:23:12] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/HuggingFaceTB/SmolLM-1.7B', tokenizer_path='/root/.cache/modelscope/hub/models/HuggingFaceTB/SmolLM-1.7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=692592006, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/HuggingFaceTB/SmolLM-1.7B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:23:12] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 08:23:12] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:23:21] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:23:21] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 08:23:21] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:23:22] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:23:22] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:23:23] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:23:23] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:30<00:30, 30.16s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:35<00:00, 15.46s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:35<00:00, 17.66s/it]

[2026-01-17 08:23:59] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=57.61 GB, mem usage=3.21 GB.
[2026-01-17 08:23:59] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:23:59] The available memory for KV cache is 45.44 GB.
[2026-01-17 08:24:00] KV Cache is allocated. #tokens: 248064, K size: 22.72 GB, V size: 22.72 GB
[2026-01-17 08:24:00] Memory pool end. avail mem=12.14 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:24:00] max_total_num_tokens=248064, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=2048, available_gpu_mem=12.12 GB
[2026-01-17 08:24:01] INFO:     Started server process [41915]
[2026-01-17 08:24:01] INFO:     Waiting for application startup.
[2026-01-17 08:24:01] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:24:01] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:24:01] INFO:     Application startup complete.
[2026-01-17 08:24:01] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:24:02] INFO:     127.0.0.1:40158 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:24:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:24:03] INFO:     127.0.0.1:40180 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:24:13] INFO:     127.0.0.1:37000 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:24:19] INFO:     127.0.0.1:40164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:19] The server is fired up and ready to roll!
[2026-01-17 08:24:23] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:24:24] INFO:     127.0.0.1:40286 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:24:24] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:24:24] INFO:     127.0.0.1:40298 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:24:24] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:24:24] INFO:     127.0.0.1:40314 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/HuggingFaceTB/SmolLM-1.7B --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --dtype bfloat16 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:24:24] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:24:24] Prefill batch, #new-seq: 9, #new-token: 1408, #cached-token: 6912, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:24:24] Prefill batch, #new-seq: 12, #new-token: 1664, #cached-token: 9216, token usage: 0.01, #running-req: 10, #queue-req: 0,
[2026-01-17 08:24:25] Prefill batch, #new-seq: 54, #new-token: 8192, #cached-token: 41472, token usage: 0.02, #running-req: 22, #queue-req: 52,
[2026-01-17 08:24:25] Prefill batch, #new-seq: 52, #new-token: 7680, #cached-token: 39936, token usage: 0.05, #running-req: 76, #queue-req: 0,
[2026-01-17 08:24:27] INFO:     127.0.0.1:40770 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:03<12:02,  3.63s/it][2026-01-17 08:24:27] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:27] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:27] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:03<03:25,  1.04s/it][2026-01-17 08:24:28] INFO:     127.0.0.1:40980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:41156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:24:28] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:40500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] INFO:     127.0.0.1:41084 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:04<00:50,  3.75it/s]
  5%|▌         | 10/200 [00:04<00:24,  7.77it/s][2026-01-17 08:24:28] INFO:     127.0.0.1:41306 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-17 08:24:28] Decode batch, #running-req: 126, #token: 25344, token usage: 0.10, npu graph: False, gen throughput (token/s): 59.39, #queue-req: 0,
[2026-01-17 08:24:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:41004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] INFO:     127.0.0.1:40476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:40590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,

  7%|▋         | 14/200 [00:04<00:18,  9.96it/s][2026-01-17 08:24:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:40550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] INFO:     127.0.0.1:40566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] INFO:     127.0.0.1:41276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:04<00:14, 12.87it/s][2026-01-17 08:24:28] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 3072, token usage: 0.11, #running-req: 124, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:40450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:04<00:14, 12.71it/s][2026-01-17 08:24:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:40326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:40494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] INFO:     127.0.0.1:40628 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:04<00:12, 14.31it/s]
 13%|█▎        | 26/200 [00:04<00:10, 17.29it/s][2026-01-17 08:24:28] INFO:     127.0.0.1:40516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] INFO:     127.0.0.1:41134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:24:28] INFO:     127.0.0.1:41288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:24:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:04<00:08, 20.08it/s][2026-01-17 08:24:29] INFO:     127.0.0.1:41196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:41476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:41322 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:05<00:06, 25.03it/s][2026-01-17 08:24:29] INFO:     127.0.0.1:40336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:41234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:24:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:40486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:05<00:06, 26.48it/s][2026-01-17 08:24:29] INFO:     127.0.0.1:40912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:40384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:40782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:24:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:41210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:41348 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:05<00:05, 27.38it/s]
 23%|██▎       | 46/200 [00:05<00:05, 29.79it/s][2026-01-17 08:24:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:40674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:41046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Decode batch, #running-req: 128, #token: 30080, token usage: 0.12, npu graph: False, gen throughput (token/s): 3711.62, #queue-req: 0,
[2026-01-17 08:24:29] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.12, #running-req: 125, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:40618 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:05<00:05, 28.85it/s][2026-01-17 08:24:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:40602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:41228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:41474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.12, #running-req: 125, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:40700 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:05<00:04, 33.29it/s][2026-01-17 08:24:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:40946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:41302 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:41342 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:05<00:04, 32.64it/s][2026-01-17 08:24:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:40888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] INFO:     127.0.0.1:33050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:24:29] INFO:     127.0.0.1:40690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:30] INFO:     127.0.0.1:40668 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:05<00:04, 32.36it/s][2026-01-17 08:24:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:30] INFO:     127.0.0.1:40712 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:24:30] INFO:     127.0.0.1:41000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:40858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:24:30] INFO:     127.0.0.1:41058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:41172 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:06<00:04, 29.34it/s]
 35%|███▌      | 70/200 [00:06<00:04, 31.26it/s]
 35%|███▌      | 70/200 [00:06<00:04, 31.26it/s][2026-01-17 08:24:30] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:24:30] INFO:     127.0.0.1:40360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:40582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:40984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:40454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 125, #queue-req: 0,

 37%|███▋      | 74/200 [00:06<00:04, 31.08it/s][2026-01-17 08:24:30] INFO:     127.0.0.1:40728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:41100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:40942 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:41154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:33008 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:06<00:03, 34.45it/s]
 40%|████      | 80/200 [00:06<00:03, 39.46it/s][2026-01-17 08:24:30] INFO:     127.0.0.1:40670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:41182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:40780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:06<00:02, 39.62it/s][2026-01-17 08:24:30] INFO:     127.0.0.1:40754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:32904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:40806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:41426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:06<00:02, 46.49it/s][2026-01-17 08:24:30] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:40796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:33414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:41060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] Decode batch, #running-req: 102, #token: 25344, token usage: 0.10, npu graph: False, gen throughput (token/s): 4075.62, #queue-req: 0,
[2026-01-17 08:24:30] INFO:     127.0.0.1:40352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:06<00:02, 48.59it/s]
 51%|█████     | 102/200 [00:06<00:01, 57.25it/s]
 51%|█████     | 102/200 [00:06<00:01, 57.25it/s][2026-01-17 08:24:30] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:40584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:40710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:33378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:41038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:30] INFO:     127.0.0.1:41044 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:06<00:01, 51.99it/s]
 55%|█████▍    | 109/200 [00:06<00:01, 50.68it/s][2026-01-17 08:24:30] INFO:     127.0.0.1:41388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:40818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:41068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:40436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:40402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:07<00:02, 39.36it/s]
 58%|█████▊    | 116/200 [00:07<00:02, 35.22it/s][2026-01-17 08:24:31] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:40970 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:07<00:02, 36.38it/s][2026-01-17 08:24:31] INFO:     127.0.0.1:40378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:40682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:32892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:40388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:41254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:07<00:01, 42.81it/s][2026-01-17 08:24:31] INFO:     127.0.0.1:33432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:40904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:07<00:01, 42.18it/s]
 68%|██████▊   | 135/200 [00:07<00:01, 43.80it/s][2026-01-17 08:24:31] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] Decode batch, #running-req: 65, #token: 19072, token usage: 0.08, npu graph: False, gen throughput (token/s): 4072.78, #queue-req: 0,
[2026-01-17 08:24:31] INFO:     127.0.0.1:41120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:41022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33494 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:07<00:01, 43.47it/s][2026-01-17 08:24:31] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:40596 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:07<00:01, 41.08it/s][2026-01-17 08:24:31] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:31] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:33338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:33186 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:07<00:01, 36.57it/s][2026-01-17 08:24:32] INFO:     127.0.0.1:33020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:33168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:08<00:01, 34.29it/s][2026-01-17 08:24:32] INFO:     127.0.0.1:33346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:40532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] Decode batch, #running-req: 43, #token: 15360, token usage: 0.06, npu graph: False, gen throughput (token/s): 2637.79, #queue-req: 0,
[2026-01-17 08:24:32] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:08<00:01, 25.74it/s][2026-01-17 08:24:32] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:33056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:33258 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:08<00:02, 19.27it/s]
 81%|████████  | 162/200 [00:08<00:02, 17.09it/s][2026-01-17 08:24:32] INFO:     127.0.0.1:32960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:32] INFO:     127.0.0.1:40328 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:08<00:01, 17.67it/s][2026-01-17 08:24:33] INFO:     127.0.0.1:33364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:33] Decode batch, #running-req: 35, #token: 13440, token usage: 0.05, npu graph: False, gen throughput (token/s): 2048.40, #queue-req: 0,
[2026-01-17 08:24:33] INFO:     127.0.0.1:32896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:33] INFO:     127.0.0.1:33198 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:09<00:04,  7.96it/s][2026-01-17 08:24:33] Decode batch, #running-req: 33, #token: 13824, token usage: 0.06, npu graph: False, gen throughput (token/s): 1840.64, #queue-req: 0,
[2026-01-17 08:24:33] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:34] Decode batch, #running-req: 31, #token: 15360, token usage: 0.06, npu graph: False, gen throughput (token/s): 1721.24, #queue-req: 0,
[2026-01-17 08:24:34] INFO:     127.0.0.1:33058 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:10<00:06,  4.96it/s][2026-01-17 08:24:35] Decode batch, #running-req: 30, #token: 15616, token usage: 0.06, npu graph: False, gen throughput (token/s): 1678.86, #queue-req: 0,
[2026-01-17 08:24:35] INFO:     127.0.0.1:33312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:35] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:11<00:06,  4.22it/s][2026-01-17 08:24:36] Decode batch, #running-req: 28, #token: 15744, token usage: 0.06, npu graph: False, gen throughput (token/s): 1575.59, #queue-req: 0,
[2026-01-17 08:24:36] Decode batch, #running-req: 28, #token: 17408, token usage: 0.07, npu graph: False, gen throughput (token/s): 1476.20, #queue-req: 0,
[2026-01-17 08:24:37] Decode batch, #running-req: 28, #token: 18048, token usage: 0.07, npu graph: False, gen throughput (token/s): 1454.36, #queue-req: 0,
[2026-01-17 08:24:38] INFO:     127.0.0.1:40356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:40410 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] Decode batch, #running-req: 28, #token: 7040, token usage: 0.03, npu graph: False, gen throughput (token/s): 1421.94, #queue-req: 0,
[2026-01-17 08:24:38] INFO:     127.0.0.1:40424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:40542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:40640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:40732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:40872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:40956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:41018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:41344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:41362 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:41414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:38] INFO:     127.0.0.1:41464 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:14<00:15,  1.73it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s]
 95%|█████████▌| 190/200 [00:14<00:02,  3.90it/s][2026-01-17 08:24:38] INFO:     127.0.0.1:32944 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:14<00:02,  3.73it/s][2026-01-17 08:24:39] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:14<00:02,  3.83it/s][2026-01-17 08:24:39] Decode batch, #running-req: 8, #token: 5888, token usage: 0.02, npu graph: False, gen throughput (token/s): 539.85, #queue-req: 0,
[2026-01-17 08:24:39] INFO:     127.0.0.1:33042 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:15<00:01,  3.90it/s][2026-01-17 08:24:39] INFO:     127.0.0.1:33266 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:15<00:01,  3.74it/s][2026-01-17 08:24:39] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:39] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:15<00:00,  4.46it/s][2026-01-17 08:24:39] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:39] INFO:     127.0.0.1:33394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:24:39] Decode batch, #running-req: 3, #token: 1408, token usage: 0.01, npu graph: False, gen throughput (token/s): 347.83, #queue-req: 0,
[2026-01-17 08:24:39] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:15<00:00,  5.95it/s][2026-01-17 08:24:40] INFO:     127.0.0.1:33454 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:15<00:00, 12.60it/s]
.
----------------------------------------------------------------------
Ran 1 test in 97.189s

OK
Accuracy: 0.095
Invalid: 0.000
Latency: 15.924 s
Output throughput: 1875.950 token/s
.
.
End (13/62):
filename='ascend/llm_models/test_ascend_smollm_1_7b.py', elapsed=108, estimated_time=400
.
.

.
.
Begin (14/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_llm_models_stablelm-2-1_6b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:24:59] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/stabilityai/stablelm-2-1_6b', tokenizer_path='/root/.cache/modelscope/hub/models/stabilityai/stablelm-2-1_6b', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=206045620, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/stabilityai/stablelm-2-1_6b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:25:00] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:25:09] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:25:10] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:25:10] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:25:11] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:25:11] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:32<00:00, 32.10s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:32<00:00, 32.10s/it]

[2026-01-17 08:25:43] Load weight end. type=StableLmForCausalLM, dtype=torch.float16, avail mem=57.74 GB, mem usage=3.07 GB.
[2026-01-17 08:25:43] Using KV cache dtype: torch.float16
[2026-01-17 08:25:43] The available memory for KV cache is 45.58 GB.
[2026-01-17 08:25:44] KV Cache is allocated. #tokens: 248832, K size: 22.79 GB, V size: 22.79 GB
[2026-01-17 08:25:44] Memory pool end. avail mem=12.08 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:25:45] max_total_num_tokens=248832, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4096, available_gpu_mem=12.08 GB
[2026-01-17 08:25:45] INFO:     Started server process [44805]
[2026-01-17 08:25:45] INFO:     Waiting for application startup.
[2026-01-17 08:25:45] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:25:45] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:25:45] INFO:     Application startup complete.
[2026-01-17 08:25:45] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:25:46] INFO:     127.0.0.1:55486 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:25:46] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:25:50] INFO:     127.0.0.1:33370 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:25:59] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:25:59] The server is fired up and ready to roll!
[2026-01-17 08:26:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:26:01] INFO:     127.0.0.1:42296 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:26:01] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:26:01] INFO:     127.0.0.1:42308 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:26:01] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:26:01] INFO:     127.0.0.1:42316 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/stabilityai/stablelm-2-1_6b --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 1 --enable-torch-compile --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestStablelm.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:26:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:26:01] Prefill batch, #new-seq: 12, #new-token: 1792, #cached-token: 9216, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:26:01] Prefill batch, #new-seq: 17, #new-token: 2176, #cached-token: 13056, token usage: 0.01, #running-req: 13, #queue-req: 0,
[2026-01-17 08:26:01] Prefill batch, #new-seq: 19, #new-token: 2560, #cached-token: 14592, token usage: 0.02, #running-req: 30, #queue-req: 0,
[2026-01-17 08:26:01] Prefill batch, #new-seq: 11, #new-token: 1408, #cached-token: 8448, token usage: 0.03, #running-req: 49, #queue-req: 0,
[2026-01-17 08:26:01] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.04, #running-req: 60, #queue-req: 0,
[2026-01-17 08:26:01] Prefill batch, #new-seq: 14, #new-token: 1792, #cached-token: 10752, token usage: 0.04, #running-req: 63, #queue-req: 0,
[2026-01-17 08:26:01] Prefill batch, #new-seq: 13, #new-token: 1792, #cached-token: 9984, token usage: 0.04, #running-req: 77, #queue-req: 0,
[2026-01-17 08:26:01] Prefill batch, #new-seq: 17, #new-token: 2304, #cached-token: 13056, token usage: 0.05, #running-req: 90, #queue-req: 0,
[2026-01-17 08:26:01] Prefill batch, #new-seq: 14, #new-token: 1792, #cached-token: 10752, token usage: 0.06, #running-req: 107, #queue-req: 0,
[2026-01-17 08:26:01] Prefill batch, #new-seq: 7, #new-token: 896, #cached-token: 5376, token usage: 0.07, #running-req: 121, #queue-req: 0,
[2026-01-17 08:26:02] INFO:     127.0.0.1:42820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:02] INFO:     127.0.0.1:43266 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:01<03:37,  1.09s/it]
  1%|          | 2/200 [00:01<02:16,  1.46it/s][2026-01-17 08:26:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.08, #running-req: 126, #queue-req: 0,
[2026-01-17 08:26:02] Decode batch, #running-req: 128, #token: 21376, token usage: 0.09, npu graph: False, gen throughput (token/s): 72.55, #queue-req: 0,
[2026-01-17 08:26:02] INFO:     127.0.0.1:43358 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:01<01:40,  1.95it/s][2026-01-17 08:26:02] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:26:02] INFO:     127.0.0.1:43256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:26:03] INFO:     127.0.0.1:42510 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:01<00:46,  4.21it/s][2026-01-17 08:26:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:26:03] INFO:     127.0.0.1:42342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:03] INFO:     127.0.0.1:42360 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:01<00:47,  4.07it/s]
  4%|▍         | 8/200 [00:01<00:39,  4.90it/s][2026-01-17 08:26:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-17 08:26:03] INFO:     127.0.0.1:43022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:03] INFO:     127.0.0.1:43302 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-17 08:26:03] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:01<00:27,  6.81it/s][2026-01-17 08:26:03] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:26:03] INFO:     127.0.0.1:42982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:26:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 08:26:03] INFO:     127.0.0.1:43106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:03] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:02<00:20,  9.13it/s]
  8%|▊         | 15/200 [00:02<00:14, 12.62it/s][2026-01-17 08:26:03] INFO:     127.0.0.1:42670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:03] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:26:03] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:26:03] INFO:     127.0.0.1:42326 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:02<00:12, 14.76it/s][2026-01-17 08:26:03] INFO:     127.0.0.1:42606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:26:03] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:03] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:26:04] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43312 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:02<00:11, 15.27it/s]
 11%|█         | 22/200 [00:02<00:10, 17.25it/s][2026-01-17 08:26:04] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:26:04] INFO:     127.0.0.1:42992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:42884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,

 12%|█▏        | 24/200 [00:02<00:10, 16.16it/s]
 13%|█▎        | 26/200 [00:02<00:08, 19.58it/s]
 13%|█▎        | 26/200 [00:02<00:08, 19.58it/s][2026-01-17 08:26:04] Decode batch, #running-req: 127, #token: 29568, token usage: 0.12, npu graph: False, gen throughput (token/s): 3742.92, #queue-req: 0,
[2026-01-17 08:26:04] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:26:04] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 131, #queue-req: 0,
[2026-01-17 08:26:04] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:02<00:08, 20.28it/s][2026-01-17 08:26:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:26:04] INFO:     127.0.0.1:42930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:26:04] INFO:     127.0.0.1:42564 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:02<00:07, 22.06it/s][2026-01-17 08:26:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:26:04] INFO:     127.0.0.1:42964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43140 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:02<00:08, 20.20it/s]
 18%|█▊        | 36/200 [00:02<00:07, 20.92it/s][2026-01-17 08:26:04] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.11, #running-req: 124, #queue-req: 0,
[2026-01-17 08:26:04] INFO:     127.0.0.1:43282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 3072, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:26:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 132, #queue-req: 0,
[2026-01-17 08:26:04] INFO:     127.0.0.1:43104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:03<00:06, 25.87it/s]
 22%|██▏       | 43/200 [00:03<00:05, 31.20it/s][2026-01-17 08:26:04] INFO:     127.0.0.1:42572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:26:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:26:04] INFO:     127.0.0.1:42558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:26:04] INFO:     127.0.0.1:42728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:04] INFO:     127.0.0.1:43200 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:03<00:05, 26.96it/s]
 24%|██▍       | 48/200 [00:03<00:05, 26.01it/s][2026-01-17 08:26:04] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 125, #queue-req: 0,
[2026-01-17 08:26:05] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:26:05] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:26:05] INFO:     127.0.0.1:42376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:43114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:43276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:43410 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:03<00:06, 22.27it/s]
 27%|██▋       | 54/200 [00:03<00:06, 24.30it/s]
 27%|██▋       | 54/200 [00:03<00:06, 24.30it/s]
 27%|██▋       | 54/200 [00:03<00:06, 24.30it/s][2026-01-17 08:26:05] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.11, #running-req: 124, #queue-req: 0,
[2026-01-17 08:26:05] INFO:     127.0.0.1:42688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:42792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-17 08:26:05] INFO:     127.0.0.1:42912 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:03<00:05, 23.84it/s][2026-01-17 08:26:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:26:05] INFO:     127.0.0.1:42744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:42896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:26:05] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:42830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:43242 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:03<00:06, 22.65it/s]
 32%|███▏      | 63/200 [00:03<00:05, 25.80it/s]
 32%|███▏      | 63/200 [00:03<00:05, 25.80it/s][2026-01-17 08:26:05] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-17 08:26:05] INFO:     127.0.0.1:42646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:26:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 08:26:05] INFO:     127.0.0.1:42620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:42760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:43524 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:43682 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:04<00:05, 23.79it/s]
 35%|███▌      | 70/200 [00:04<00:03, 33.84it/s]
 35%|███▌      | 70/200 [00:04<00:03, 33.84it/s]
 35%|███▌      | 70/200 [00:04<00:03, 33.84it/s]
 35%|███▌      | 70/200 [00:04<00:03, 33.84it/s][2026-01-17 08:26:05] Prefill batch, #new-seq: 5, #new-token: 768, #cached-token: 3840, token usage: 0.11, #running-req: 123, #queue-req: 0,
[2026-01-17 08:26:05] INFO:     127.0.0.1:42372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:43574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:43204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 125, #queue-req: 0,

 37%|███▋      | 74/200 [00:04<00:03, 32.76it/s]
 38%|███▊      | 75/200 [00:04<00:03, 33.96it/s][2026-01-17 08:26:05] Decode batch, #running-req: 125, #token: 27008, token usage: 0.11, npu graph: False, gen throughput (token/s): 3043.52, #queue-req: 0,
[2026-01-17 08:26:05] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:42630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:42390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:05] INFO:     127.0.0.1:42422 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:04<00:03, 35.18it/s][2026-01-17 08:26:05] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43604 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:04<00:03, 30.39it/s]
 42%|████▎     | 85/200 [00:04<00:03, 30.66it/s]
 42%|████▎     | 85/200 [00:04<00:03, 30.66it/s][2026-01-17 08:26:06] INFO:     127.0.0.1:42724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42374 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:04<00:03, 34.33it/s][2026-01-17 08:26:06] INFO:     127.0.0.1:42450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42808 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:04<00:03, 35.15it/s][2026-01-17 08:26:06] INFO:     127.0.0.1:43552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:04<00:02, 40.07it/s]
 50%|█████     | 101/200 [00:04<00:02, 46.28it/s][2026-01-17 08:26:06] INFO:     127.0.0.1:42974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42412 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:05<00:02, 32.68it/s][2026-01-17 08:26:06] INFO:     127.0.0.1:43880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:42488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43172 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:05<00:02, 36.08it/s][2026-01-17 08:26:06] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] Decode batch, #running-req: 87, #token: 22144, token usage: 0.09, npu graph: False, gen throughput (token/s): 4048.16, #queue-req: 0,
[2026-01-17 08:26:06] INFO:     127.0.0.1:44118 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:06] INFO:     127.0.0.1:44088 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:05<00:02, 35.06it/s][2026-01-17 08:26:07] INFO:     127.0.0.1:42784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:42532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43816 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:05<00:02, 32.90it/s]
 61%|██████    | 122/200 [00:05<00:02, 33.33it/s][2026-01-17 08:26:07] INFO:     127.0.0.1:42466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:42472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:05<00:02, 35.59it/s][2026-01-17 08:26:07] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43624 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:05<00:01, 37.01it/s][2026-01-17 08:26:07] INFO:     127.0.0.1:43844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:42852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:42502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:42846 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:05<00:01, 34.52it/s]
 70%|██████▉   | 139/200 [00:05<00:01, 34.87it/s][2026-01-17 08:26:07] INFO:     127.0.0.1:43244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:44014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:06<00:01, 36.10it/s][2026-01-17 08:26:07] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:44126 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:06<00:01, 29.69it/s]
 74%|███████▍  | 149/200 [00:06<00:01, 27.69it/s][2026-01-17 08:26:07] Decode batch, #running-req: 53, #token: 15872, token usage: 0.06, npu graph: False, gen throughput (token/s): 2755.72, #queue-req: 0,
[2026-01-17 08:26:07] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:44108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:42588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:07] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:06<00:01, 29.72it/s]
 77%|███████▋  | 154/200 [00:06<00:01, 33.63it/s][2026-01-17 08:26:08] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:43026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:06<00:01, 31.28it/s][2026-01-17 08:26:08] INFO:     127.0.0.1:43988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:42948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:42574 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:06<00:01, 24.08it/s][2026-01-17 08:26:08] INFO:     127.0.0.1:43762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:07<00:01, 20.46it/s][2026-01-17 08:26:08] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:42406 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:43482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:07<00:01, 21.21it/s]
 84%|████████▍ | 169/200 [00:07<00:01, 23.73it/s][2026-01-17 08:26:08] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:43940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:08] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:07<00:01, 23.92it/s][2026-01-17 08:26:08] Decode batch, #running-req: 29, #token: 9728, token usage: 0.04, npu graph: False, gen throughput (token/s): 1568.30, #queue-req: 0,
[2026-01-17 08:26:09] INFO:     127.0.0.1:43818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:09] INFO:     127.0.0.1:44186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:09] INFO:     127.0.0.1:43846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:09] INFO:     127.0.0.1:44038 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:07<00:01, 17.66it/s]
 88%|████████▊ | 176/200 [00:07<00:01, 16.09it/s][2026-01-17 08:26:09] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:09] Decode batch, #running-req: 23, #token: 9216, token usage: 0.04, npu graph: False, gen throughput (token/s): 1030.04, #queue-req: 0,
[2026-01-17 08:26:09] INFO:     127.0.0.1:43698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:09] INFO:     127.0.0.1:44024 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:08<00:02,  8.14it/s]
 90%|████████▉ | 179/200 [00:08<00:03,  6.24it/s][2026-01-17 08:26:10] INFO:     127.0.0.1:43746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:10] INFO:     127.0.0.1:43660 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:08<00:03,  6.30it/s][2026-01-17 08:26:10] INFO:     127.0.0.1:43926 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:08<00:02,  6.27it/s][2026-01-17 08:26:10] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:10] INFO:     127.0.0.1:43774 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:08<00:02,  7.58it/s][2026-01-17 08:26:10] INFO:     127.0.0.1:42772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:10] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:09<00:01,  8.02it/s][2026-01-17 08:26:10] INFO:     127.0.0.1:43986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:10] Decode batch, #running-req: 14, #token: 6400, token usage: 0.03, npu graph: False, gen throughput (token/s): 789.04, #queue-req: 0,
[2026-01-17 08:26:11] Decode batch, #running-req: 13, #token: 6656, token usage: 0.03, npu graph: False, gen throughput (token/s): 567.20, #queue-req: 0,
[2026-01-17 08:26:11] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:10<00:02,  4.02it/s][2026-01-17 08:26:12] INFO:     127.0.0.1:44096 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:10<00:02,  3.95it/s][2026-01-17 08:26:12] Decode batch, #running-req: 11, #token: 6144, token usage: 0.02, npu graph: False, gen throughput (token/s): 504.57, #queue-req: 0,
[2026-01-17 08:26:12] INFO:     127.0.0.1:44052 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:11<00:03,  3.12it/s][2026-01-17 08:26:13] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:11<00:03,  2.32it/s][2026-01-17 08:26:13] Decode batch, #running-req: 10, #token: 5760, token usage: 0.02, npu graph: False, gen throughput (token/s): 439.88, #queue-req: 0,
[2026-01-17 08:26:14] Decode batch, #running-req: 9, #token: 5888, token usage: 0.02, npu graph: False, gen throughput (token/s): 396.01, #queue-req: 0,
[2026-01-17 08:26:15] Decode batch, #running-req: 9, #token: 4992, token usage: 0.02, npu graph: False, gen throughput (token/s): 392.32, #queue-req: 0,
[2026-01-17 08:26:15] INFO:     127.0.0.1:42840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:15] INFO:     127.0.0.1:42898 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:13<00:06,  1.30it/s]
 96%|█████████▋| 193/200 [00:13<00:05,  1.20it/s][2026-01-17 08:26:15] INFO:     127.0.0.1:43170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:15] INFO:     127.0.0.1:43260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:16] Decode batch, #running-req: 5, #token: 3968, token usage: 0.02, npu graph: False, gen throughput (token/s): 225.91, #queue-req: 0,
[2026-01-17 08:26:16] INFO:     127.0.0.1:43676 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:15<00:02,  1.47it/s][2026-01-17 08:26:17] Decode batch, #running-req: 4, #token: 3328, token usage: 0.01, npu graph: False, gen throughput (token/s): 208.73, #queue-req: 0,
[2026-01-17 08:26:17] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:15<00:01,  1.54it/s][2026-01-17 08:26:17] INFO:     127.0.0.1:44028 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:15<00:01,  1.83it/s][2026-01-17 08:26:17] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:26:17] INFO:     127.0.0.1:44144 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:16<00:00,  2.57it/s]
100%|██████████| 200/200 [00:16<00:00, 12.36it/s]
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/llm_models/test_ascend_llm_models_stablelm-2-1_6b.py", line 75, in test_gsm8k
    self.assertGreater(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1271, in assertGreater
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: np.float64(0.195) not greater than 0.195 : Accuracy of /root/.cache/modelscope/hub/models/stabilityai/stablelm-2-1_6b is 0.195, is lower than 0.195
E
======================================================================
ERROR: test_gsm8k (__main__.TestStablelm.test_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: np.float64(0.195) not greater than 0.195 : Accuracy of /root/.cache/modelscope/hub/models/stabilityai/stablelm-2-1_6b is 0.195, is lower than 0.195

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 87.521s

FAILED (errors=1)
Accuracy: 0.195
Invalid: 0.000
Latency: 16.223 s
Output throughput: 1657.579 token/s
.
.
End (14/62):
filename='ascend/llm_models/test_ascend_llm_models_stablelm-2-1_6b.py', elapsed=98, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_ascend_llm_models_stablelm-2-1_6b.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/llm_models/test_ascend_llm_models_stablelm-2-1_6b.py

.
.
Begin (14/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_llm_models_stablelm-2-1_6b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:27:37] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/stabilityai/stablelm-2-1_6b', tokenizer_path='/root/.cache/modelscope/hub/models/stabilityai/stablelm-2-1_6b', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=632602894, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/stabilityai/stablelm-2-1_6b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:27:37] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:27:47] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:27:47] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:27:48] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:27:48] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:27:48] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.22s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.22s/it]

[2026-01-17 08:27:52] Load weight end. type=StableLmForCausalLM, dtype=torch.float16, avail mem=57.74 GB, mem usage=3.07 GB.
[2026-01-17 08:27:52] Using KV cache dtype: torch.float16
[2026-01-17 08:27:52] The available memory for KV cache is 45.58 GB.
[2026-01-17 08:27:53] KV Cache is allocated. #tokens: 248832, K size: 22.79 GB, V size: 22.79 GB
[2026-01-17 08:27:53] Memory pool end. avail mem=12.08 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:27:54] max_total_num_tokens=248832, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4096, available_gpu_mem=12.08 GB
[2026-01-17 08:27:54] INFO:     Started server process [47334]
[2026-01-17 08:27:54] INFO:     Waiting for application startup.
[2026-01-17 08:27:54] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:27:54] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:27:54] INFO:     Application startup complete.
[2026-01-17 08:27:54] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:27:55] INFO:     127.0.0.1:43924 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:27:55] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:27:58] INFO:     127.0.0.1:49886 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:28:08] INFO:     127.0.0.1:40534 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:28:08] INFO:     127.0.0.1:43926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:08] The server is fired up and ready to roll!
[2026-01-17 08:28:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:28:19] INFO:     127.0.0.1:50562 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:28:19] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:28:19] INFO:     127.0.0.1:50564 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:28:19] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:28:19] INFO:     127.0.0.1:50574 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/stabilityai/stablelm-2-1_6b --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 1 --enable-torch-compile --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestStablelm.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:28:19] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:28:19] Prefill batch, #new-seq: 12, #new-token: 1792, #cached-token: 9216, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:28:19] Prefill batch, #new-seq: 18, #new-token: 2304, #cached-token: 13824, token usage: 0.01, #running-req: 13, #queue-req: 0,
[2026-01-17 08:28:19] Prefill batch, #new-seq: 18, #new-token: 2432, #cached-token: 13824, token usage: 0.02, #running-req: 31, #queue-req: 0,
[2026-01-17 08:28:19] Prefill batch, #new-seq: 11, #new-token: 1408, #cached-token: 8448, token usage: 0.03, #running-req: 49, #queue-req: 0,
[2026-01-17 08:28:19] Prefill batch, #new-seq: 9, #new-token: 1152, #cached-token: 6912, token usage: 0.04, #running-req: 60, #queue-req: 0,
[2026-01-17 08:28:19] Prefill batch, #new-seq: 14, #new-token: 1920, #cached-token: 10752, token usage: 0.04, #running-req: 69, #queue-req: 0,
[2026-01-17 08:28:19] Prefill batch, #new-seq: 16, #new-token: 2304, #cached-token: 12288, token usage: 0.05, #running-req: 83, #queue-req: 0,
[2026-01-17 08:28:19] Prefill batch, #new-seq: 16, #new-token: 2048, #cached-token: 12288, token usage: 0.06, #running-req: 99, #queue-req: 0,
[2026-01-17 08:28:19] Prefill batch, #new-seq: 13, #new-token: 1664, #cached-token: 9984, token usage: 0.07, #running-req: 115, #queue-req: 0,
[2026-01-17 08:28:20] INFO:     127.0.0.1:51004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:20] INFO:     127.0.0.1:51272 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:01<03:33,  1.07s/it]
  1%|          | 2/200 [00:01<02:14,  1.48it/s][2026-01-17 08:28:20] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.08, #running-req: 126, #queue-req: 0,
[2026-01-17 08:28:20] Decode batch, #running-req: 128, #token: 21376, token usage: 0.09, npu graph: False, gen throughput (token/s): 114.19, #queue-req: 0,
[2026-01-17 08:28:20] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:01<01:39,  1.98it/s][2026-01-17 08:28:20] INFO:     127.0.0.1:50862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:20] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:28:20] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:20] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-17 08:28:20] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:01<00:45,  4.27it/s][2026-01-17 08:28:20] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-17 08:28:21] INFO:     127.0.0.1:50594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:01<00:46,  4.12it/s]
  4%|▍         | 8/200 [00:01<00:38,  4.96it/s][2026-01-17 08:28:21] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-17 08:28:21] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 08:28:21] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-17 08:28:21] INFO:     127.0.0.1:51402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:01<00:28,  6.54it/s]
  6%|▌         | 12/200 [00:01<00:21,  8.71it/s][2026-01-17 08:28:21] INFO:     127.0.0.1:51054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] INFO:     127.0.0.1:51568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:28:21] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:28:21] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 130, #queue-req: 0,
[2026-01-17 08:28:21] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] INFO:     127.0.0.1:50902 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:02<00:15, 11.85it/s]
  8%|▊         | 17/200 [00:02<00:11, 16.11it/s][2026-01-17 08:28:21] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:28:21] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:28:21] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:02<00:09, 18.16it/s][2026-01-17 08:28:21] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:28:21] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:21] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:28:21] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:02<00:11, 15.35it/s][2026-01-17 08:28:22] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:51596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] Decode batch, #running-req: 127, #token: 29568, token usage: 0.12, npu graph: False, gen throughput (token/s): 3776.90, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:28:22] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:02<00:08, 20.65it/s][2026-01-17 08:28:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:02<00:07, 22.17it/s][2026-01-17 08:28:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:51090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51696 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:02<00:08, 20.57it/s]
 18%|█▊        | 37/200 [00:02<00:06, 23.34it/s]
 18%|█▊        | 37/200 [00:02<00:06, 23.34it/s][2026-01-17 08:28:22] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] Prefill batch, #new-seq: 5, #new-token: 640, #cached-token: 3840, token usage: 0.12, #running-req: 123, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:28:22] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:03<00:06, 25.37it/s]
 22%|██▏       | 43/200 [00:03<00:05, 28.61it/s][2026-01-17 08:28:22] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-17 08:28:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:50952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:03<00:06, 23.79it/s]
 24%|██▍       | 48/200 [00:03<00:06, 22.95it/s]
 24%|██▍       | 48/200 [00:03<00:06, 22.95it/s][2026-01-17 08:28:22] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 125, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-17 08:28:22] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:22] INFO:     127.0.0.1:51618 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:03<00:07, 20.35it/s]
 27%|██▋       | 54/200 [00:03<00:06, 23.78it/s]
 27%|██▋       | 54/200 [00:03<00:06, 23.78it/s]
 27%|██▋       | 54/200 [00:03<00:06, 23.78it/s][2026-01-17 08:28:23] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.11, #running-req: 124, #queue-req: 0,
[2026-01-17 08:28:23] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-17 08:28:23] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:03<00:06, 21.90it/s][2026-01-17 08:28:23] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 08:28:23] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:28:23] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 08:28:23] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:03<00:07, 19.75it/s]
 32%|███▏      | 63/200 [00:03<00:06, 21.07it/s]
 32%|███▏      | 63/200 [00:03<00:06, 21.07it/s][2026-01-17 08:28:23] INFO:     127.0.0.1:51704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-17 08:28:23] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 08:28:23] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 08:28:23] INFO:     127.0.0.1:50804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:50968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:51924 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:04<00:06, 20.18it/s]
 35%|███▌      | 70/200 [00:04<00:04, 30.93it/s]
 35%|███▌      | 70/200 [00:04<00:04, 30.93it/s]
 35%|███▌      | 70/200 [00:04<00:04, 30.93it/s]
 35%|███▌      | 70/200 [00:04<00:04, 30.93it/s][2026-01-17 08:28:23] Prefill batch, #new-seq: 5, #new-token: 768, #cached-token: 3840, token usage: 0.11, #running-req: 123, #queue-req: 0,
[2026-01-17 08:28:23] INFO:     127.0.0.1:50618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 08:28:23] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:04<00:04, 29.11it/s][2026-01-17 08:28:23] Decode batch, #running-req: 126, #token: 27264, token usage: 0.11, npu graph: False, gen throughput (token/s): 2860.22, #queue-req: 0,
[2026-01-17 08:28:23] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:23] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:04<00:03, 32.73it/s][2026-01-17 08:28:23] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:04<00:03, 30.01it/s]
 42%|████▏     | 84/200 [00:04<00:03, 30.05it/s][2026-01-17 08:28:24] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:50748 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:04<00:03, 35.27it/s][2026-01-17 08:28:24] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:04<00:03, 35.33it/s][2026-01-17 08:28:24] INFO:     127.0.0.1:51796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:04<00:02, 39.64it/s]
 50%|█████     | 101/200 [00:04<00:02, 45.26it/s][2026-01-17 08:28:24] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:50660 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:05<00:03, 31.19it/s][2026-01-17 08:28:24] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:50704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:50738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:05<00:02, 34.16it/s][2026-01-17 08:28:24] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:52206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] Decode batch, #running-req: 87, #token: 22144, token usage: 0.09, npu graph: False, gen throughput (token/s): 3781.50, #queue-req: 0,
[2026-01-17 08:28:24] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:24] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:05<00:02, 31.41it/s]
 58%|█████▊    | 117/200 [00:05<00:02, 31.40it/s][2026-01-17 08:28:25] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52064 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:05<00:02, 29.75it/s]
 61%|██████    | 122/200 [00:05<00:02, 30.34it/s][2026-01-17 08:28:25] INFO:     127.0.0.1:50726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:51706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:05<00:02, 33.77it/s]
 64%|██████▍   | 128/200 [00:05<00:01, 38.87it/s][2026-01-17 08:28:25] INFO:     127.0.0.1:51420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:51844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:05<00:01, 36.55it/s]
 67%|██████▋   | 134/200 [00:05<00:01, 37.07it/s][2026-01-17 08:28:25] INFO:     127.0.0.1:52086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:06<00:01, 35.23it/s][2026-01-17 08:28:25] INFO:     127.0.0.1:50740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:51958 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:06<00:01, 34.15it/s][2026-01-17 08:28:25] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:06<00:01, 34.77it/s][2026-01-17 08:28:25] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] INFO:     127.0.0.1:52316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:25] Decode batch, #running-req: 52, #token: 15616, token usage: 0.06, npu graph: False, gen throughput (token/s): 2561.93, #queue-req: 0,
[2026-01-17 08:28:25] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:06<00:01, 28.34it/s][2026-01-17 08:28:26] INFO:     127.0.0.1:52290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:50794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:51954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:06<00:01, 29.74it/s][2026-01-17 08:28:26] INFO:     127.0.0.1:51832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:52134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:52190 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:06<00:01, 28.88it/s][2026-01-17 08:28:26] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:52000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:50788 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:07<00:01, 20.92it/s][2026-01-17 08:28:26] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:51524 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:51326 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:07<00:01, 17.62it/s][2026-01-17 08:28:26] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:26] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:07<00:01, 18.43it/s][2026-01-17 08:28:26] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:27] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:27] INFO:     127.0.0.1:52152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:27] INFO:     127.0.0.1:52384 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:07<00:01, 20.72it/s][2026-01-17 08:28:27] Decode batch, #running-req: 29, #token: 9728, token usage: 0.04, npu graph: False, gen throughput (token/s): 1360.37, #queue-req: 0,
[2026-01-17 08:28:27] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:27] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:27] INFO:     127.0.0.1:52090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:27] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:07<00:01, 15.47it/s]
 88%|████████▊ | 176/200 [00:07<00:01, 14.13it/s][2026-01-17 08:28:27] INFO:     127.0.0.1:52140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:27] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:08<00:01, 14.70it/s][2026-01-17 08:28:28] Decode batch, #running-req: 22, #token: 8832, token usage: 0.04, npu graph: False, gen throughput (token/s): 875.26, #queue-req: 0,
[2026-01-17 08:28:28] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:28] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:09<00:03,  6.36it/s][2026-01-17 08:28:28] INFO:     127.0.0.1:51990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:28] INFO:     127.0.0.1:51886 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:09<00:02,  7.33it/s][2026-01-17 08:28:28] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:28] INFO:     127.0.0.1:52398 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:09<00:02,  7.35it/s][2026-01-17 08:28:28] INFO:     127.0.0.1:52016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:29] INFO:     127.0.0.1:50976 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:09<00:01,  8.70it/s][2026-01-17 08:28:29] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:29] INFO:     127.0.0.1:52184 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:09<00:01,  8.58it/s][2026-01-17 08:28:29] Decode batch, #running-req: 13, #token: 6016, token usage: 0.02, npu graph: False, gen throughput (token/s): 661.91, #queue-req: 0,
[2026-01-17 08:28:30] Decode batch, #running-req: 12, #token: 6272, token usage: 0.03, npu graph: False, gen throughput (token/s): 457.10, #queue-req: 0,
[2026-01-17 08:28:30] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:30] INFO:     127.0.0.1:52298 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:11<00:02,  3.35it/s][2026-01-17 08:28:31] Decode batch, #running-req: 10, #token: 5760, token usage: 0.02, npu graph: False, gen throughput (token/s): 406.91, #queue-req: 0,
[2026-01-17 08:28:32] INFO:     127.0.0.1:51058 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:12<00:04,  2.01it/s][2026-01-17 08:28:32] Decode batch, #running-req: 9, #token: 5760, token usage: 0.02, npu graph: False, gen throughput (token/s): 420.49, #queue-req: 0,
[2026-01-17 08:28:33] Decode batch, #running-req: 9, #token: 5888, token usage: 0.02, npu graph: False, gen throughput (token/s): 389.00, #queue-req: 0,
[2026-01-17 08:28:34] Decode batch, #running-req: 9, #token: 5632, token usage: 0.02, npu graph: False, gen throughput (token/s): 389.92, #queue-req: 0,
[2026-01-17 08:28:34] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:14<00:06,  1.30it/s][2026-01-17 08:28:34] INFO:     127.0.0.1:51130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:34] INFO:     127.0.0.1:51212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:34] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:35] Decode batch, #running-req: 5, #token: 3968, token usage: 0.02, npu graph: False, gen throughput (token/s): 221.81, #queue-req: 0,
[2026-01-17 08:28:35] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:16<00:02,  1.75it/s][2026-01-17 08:28:36] Decode batch, #running-req: 4, #token: 3328, token usage: 0.01, npu graph: False, gen throughput (token/s): 202.38, #queue-req: 0,
[2026-01-17 08:28:36] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:16<00:01,  1.77it/s][2026-01-17 08:28:36] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:16<00:00,  2.05it/s][2026-01-17 08:28:36] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:28:36] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:17<00:00,  2.75it/s]
100%|██████████| 200/200 [00:17<00:00, 11.62it/s]
.
----------------------------------------------------------------------
Ran 1 test in 68.512s

OK
Accuracy: 0.200
Invalid: 0.000
Latency: 17.250 s
Output throughput: 1549.597 token/s
.
.
End (14/62):
filename='ascend/llm_models/test_ascend_llm_models_stablelm-2-1_6b.py', elapsed=78, estimated_time=400
.
.


✓ PASSED on retry (attempt 2): ascend/llm_models/test_ascend_llm_models_stablelm-2-1_6b.py

.
.
Begin (15/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_c4ai_command_r_v01.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:28:55] WARNING model_config.py:1020: Casting torch.float16 to torch.bfloat16.
[2026-01-17 08:28:55] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/CohereForAI/c4ai-command-r-v01', tokenizer_path='/root/.cache/modelscope/hub/models/CohereForAI/c4ai-command-r-v01', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=327464786, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/CohereForAI/c4ai-command-r-v01', weight_version='default', chat_template='tool_chat_template_c4ai_command_r_v01.jinja', completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:28:55] Casting torch.float16 to torch.bfloat16.
[2026-01-17 08:28:56] Loading chat template from argument: tool_chat_template_c4ai_command_r_v01.jinja
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 32, in <module>
    run_server(server_args)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 25, in run_server
    launch_server(server_args)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/http_server.py", line 1704, in launch_server
    launch_subprocesses_func(server_args=server_args)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/engine.py", line 151, in _launch_subprocesses
    tokenizer_manager, template_manager = _init_tokenizer_manager(
                                          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/engine.py", line 891, in _init_tokenizer_manager
    template_manager.initialize_templates(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/template_manager.py", line 217, in initialize_templates
    self.load_chat_template(tokenizer_manager, chat_template, model_path)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/template_manager.py", line 114, in load_chat_template
    self._load_explicit_chat_template(tokenizer_manager, chat_template_arg)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/template_manager.py", line 157, in _load_explicit_chat_template
    raise RuntimeError(
RuntimeError: Chat template tool_chat_template_c4ai_command_r_v01.jinja is not a built-in template name or a valid chat template file path.
[ERROR] 2026-01-17-08:28:57 (PID:49863, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
E
======================================================================
ERROR: setUpClass (__main__.TestC4AI)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/llm_models/test_ascend_c4ai_command_r_v01.py", line 60, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 20.005s

FAILED (errors=1)
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/CohereForAI/c4ai-command-r-v01 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --chat-template tool_chat_template_c4ai_command_r_v01.jinja --tp-size 2 --dtype bfloat16 --device npu --host 127.0.0.1 --port 21000
.
.
End (15/62):
filename='ascend/llm_models/test_ascend_c4ai_command_r_v01.py', elapsed=29, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_ascend_c4ai_command_r_v01.py failed with non-retriable error: RuntimeError - not retrying


✗ FAILED: ascend/llm_models/test_ascend_c4ai_command_r_v01.py returned exit code 1

.
.
Begin (16/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:29:25] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/allenai/OLMoE-1B-7B-0924', tokenizer_path='/root/.cache/modelscope/hub/models/allenai/OLMoE-1B-7B-0924', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=746060187, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/allenai/OLMoE-1B-7B-0924', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:29:25] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:29:34] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:29:35] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:29:35] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:29:36] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:11<00:22, 11.14s/it]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:25<00:13, 13.22s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:41<00:00, 14.13s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:41<00:00, 13.68s/it]

[2026-01-17 08:30:30] Load weight end. type=OlmoeForCausalLM, dtype=torch.bfloat16, avail mem=47.88 GB, mem usage=12.94 GB.
[2026-01-17 08:30:30] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:30:30] The available memory for KV cache is 35.71 GB.
[2026-01-17 08:30:31] KV Cache is allocated. #tokens: 292480, K size: 17.86 GB, V size: 17.86 GB
[2026-01-17 08:30:31] Memory pool end. avail mem=12.09 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:30:32] max_total_num_tokens=292480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4096, available_gpu_mem=12.09 GB
[2026-01-17 08:30:32] INFO:     Started server process [50598]
[2026-01-17 08:30:32] INFO:     Waiting for application startup.
[2026-01-17 08:30:32] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:30:32] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:30:32] INFO:     Application startup complete.
[2026-01-17 08:30:32] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:30:33] INFO:     127.0.0.1:40328 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:30:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:30:35] INFO:     127.0.0.1:40344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:35] The server is fired up and ready to roll!
[2026-01-17 08:30:35] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:30:36] INFO:     127.0.0.1:40346 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:30:36] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:30:36] INFO:     127.0.0.1:40356 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:30:36] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:30:36] INFO:     127.0.0.1:40358 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/allenai/OLMoE-1B-7B-0924 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMiMo.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:30:36] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:30:36] Prefill batch, #new-seq: 13, #new-token: 1664, #cached-token: 8320, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:30:36] Prefill batch, #new-seq: 16, #new-token: 2048, #cached-token: 10240, token usage: 0.01, #running-req: 14, #queue-req: 0,
[2026-01-17 08:30:36] Prefill batch, #new-seq: 22, #new-token: 2944, #cached-token: 14080, token usage: 0.02, #running-req: 30, #queue-req: 0,
[2026-01-17 08:30:36] Prefill batch, #new-seq: 8, #new-token: 1024, #cached-token: 5120, token usage: 0.03, #running-req: 52, #queue-req: 0,
[2026-01-17 08:30:36] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.03, #running-req: 60, #queue-req: 0,
[2026-01-17 08:30:37] Prefill batch, #new-seq: 15, #new-token: 1920, #cached-token: 9600, token usage: 0.03, #running-req: 63, #queue-req: 0,
[2026-01-17 08:30:37] Prefill batch, #new-seq: 15, #new-token: 1920, #cached-token: 9600, token usage: 0.04, #running-req: 78, #queue-req: 0,
[2026-01-17 08:30:37] Prefill batch, #new-seq: 15, #new-token: 1920, #cached-token: 9600, token usage: 0.04, #running-req: 93, #queue-req: 0,
[2026-01-17 08:30:37] Prefill batch, #new-seq: 16, #new-token: 2048, #cached-token: 10240, token usage: 0.05, #running-req: 108, #queue-req: 0,
[2026-01-17 08:30:37] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 2560, token usage: 0.06, #running-req: 124, #queue-req: 0,
[2026-01-17 08:30:37] INFO:     127.0.0.1:40594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] INFO:     127.0.0.1:41018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] INFO:     127.0.0.1:41416 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:00<03:03,  1.09it/s]
  2%|▏         | 3/200 [00:00<01:21,  2.43it/s]
  2%|▏         | 3/200 [00:00<01:21,  2.43it/s][2026-01-17 08:30:37] INFO:     127.0.0.1:40904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.06, #running-req: 125, #queue-req: 0,
[2026-01-17 08:30:37] INFO:     127.0.0.1:40650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] INFO:     127.0.0.1:40688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:30:37] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.06, #running-req: 130, #queue-req: 0,
[2026-01-17 08:30:37] INFO:     127.0.0.1:40694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] INFO:     127.0.0.1:41038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] INFO:     127.0.0.1:41326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] INFO:     127.0.0.1:41362 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:01<00:36,  5.31it/s]
  6%|▌         | 11/200 [00:01<00:09, 20.05it/s]
  6%|▌         | 11/200 [00:01<00:09, 20.05it/s]
  6%|▌         | 11/200 [00:01<00:09, 20.05it/s][2026-01-17 08:30:37] INFO:     127.0.0.1:40740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] INFO:     127.0.0.1:40994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.06, #running-req: 124, #queue-req: 0,
[2026-01-17 08:30:37] INFO:     127.0.0.1:40838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] INFO:     127.0.0.1:40876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:37] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:30:37] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.06, #running-req: 130, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:01<00:08, 22.69it/s][2026-01-17 08:30:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:40536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:40662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:41336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:30:38] Decode batch, #running-req: 126, #token: 18944, token usage: 0.06, npu graph: False, gen throughput (token/s): 61.76, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:41010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:01<00:07, 23.57it/s]
 10%|█         | 21/200 [00:01<00:06, 25.87it/s][2026-01-17 08:30:38] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:41290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:41388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:40788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:40916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:41142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:41194 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:01<00:07, 24.78it/s]
 14%|█▎        | 27/200 [00:01<00:06, 27.78it/s]
 14%|█▎        | 27/200 [00:01<00:06, 27.78it/s][2026-01-17 08:30:38] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-17 08:30:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:40384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:40558 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:01<00:06, 27.47it/s][2026-01-17 08:30:38] INFO:     127.0.0.1:40454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:40830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:30:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 129, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:40512 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:01<00:06, 25.01it/s][2026-01-17 08:30:38] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:41134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:41414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:41314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:30:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 131, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:41340 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:01<00:05, 28.22it/s][2026-01-17 08:30:38] INFO:     127.0.0.1:40382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:40582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:30:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 130, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:60018 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:02<00:05, 28.53it/s]
 22%|██▏       | 44/200 [00:02<00:04, 34.60it/s]
 22%|██▏       | 44/200 [00:02<00:04, 34.60it/s][2026-01-17 08:30:38] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-17 08:30:38] INFO:     127.0.0.1:40570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:41156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] INFO:     127.0.0.1:40784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:38] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:30:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 130, #queue-req: 0,
[2026-01-17 08:30:39] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40932 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:02<00:04, 31.48it/s]
 24%|██▍       | 49/200 [00:02<00:04, 31.40it/s][2026-01-17 08:30:39] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 08:30:39] INFO:     127.0.0.1:41426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:30:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 129, #queue-req: 0,
[2026-01-17 08:30:39] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:41078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:41390 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:02<00:04, 30.73it/s]
 27%|██▋       | 54/200 [00:02<00:04, 32.31it/s][2026-01-17 08:30:39] INFO:     127.0.0.1:40438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-17 08:30:39] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:41256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:30:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 131, #queue-req: 0,
[2026-01-17 08:30:39] INFO:     127.0.0.1:40792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:41022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:02<00:04, 33.50it/s]
 31%|███       | 62/200 [00:02<00:03, 45.92it/s]
 31%|███       | 62/200 [00:02<00:03, 45.92it/s]
 31%|███       | 62/200 [00:02<00:03, 45.92it/s][2026-01-17 08:30:39] INFO:     127.0.0.1:41396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.07, #running-req: 124, #queue-req: 0,
[2026-01-17 08:30:39] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-17 08:30:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 129, #queue-req: 0,
[2026-01-17 08:30:39] INFO:     127.0.0.1:40954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 08:30:39] INFO:     127.0.0.1:40620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:41008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:41276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:30:39] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:02<00:03, 39.13it/s]
 36%|███▌      | 72/200 [00:02<00:02, 45.95it/s]
 36%|███▌      | 72/200 [00:02<00:02, 45.95it/s]
 36%|███▌      | 72/200 [00:02<00:02, 45.95it/s]
 36%|███▌      | 72/200 [00:02<00:02, 45.95it/s]
 36%|███▌      | 72/200 [00:02<00:02, 45.95it/s][2026-01-17 08:30:39] Prefill batch, #new-seq: 6, #new-token: 768, #cached-token: 3840, token usage: 0.07, #running-req: 129, #queue-req: 0,
[2026-01-17 08:30:39] INFO:     127.0.0.1:40940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:41304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:59938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:60082 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:02<00:03, 40.41it/s]
 39%|███▉      | 78/200 [00:02<00:03, 38.21it/s][2026-01-17 08:30:39] INFO:     127.0.0.1:41184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] Decode batch, #running-req: 121, #token: 23040, token usage: 0.08, npu graph: False, gen throughput (token/s): 3177.06, #queue-req: 0,
[2026-01-17 08:30:39] INFO:     127.0.0.1:40390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:03<00:02, 42.87it/s][2026-01-17 08:30:39] INFO:     127.0.0.1:40890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:60274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:41126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:41114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:41240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:60126 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:03<00:02, 48.28it/s]
 47%|████▋     | 94/200 [00:03<00:01, 54.69it/s][2026-01-17 08:30:39] INFO:     127.0.0.1:40412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:39] INFO:     127.0.0.1:40826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:41110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40804 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:03<00:01, 54.42it/s][2026-01-17 08:30:40] INFO:     127.0.0.1:40440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40686 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40388 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:03<00:01, 60.15it/s][2026-01-17 08:30:40] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:41178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40780 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:03<00:01, 54.56it/s]
 59%|█████▉    | 118/200 [00:03<00:01, 53.03it/s][2026-01-17 08:30:40] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40506 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:03<00:01, 57.08it/s][2026-01-17 08:30:40] INFO:     127.0.0.1:60108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:41048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:03<00:01, 55.41it/s][2026-01-17 08:30:40] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] Decode batch, #running-req: 67, #token: 14976, token usage: 0.05, npu graph: False, gen throughput (token/s): 3931.43, #queue-req: 0,
[2026-01-17 08:30:40] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:03<00:01, 49.97it/s]
 70%|██████▉   | 139/200 [00:03<00:01, 48.54it/s][2026-01-17 08:30:40] INFO:     127.0.0.1:41350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:59998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:04<00:01, 48.09it/s][2026-01-17 08:30:40] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:40] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:40368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:04<00:01, 41.69it/s][2026-01-17 08:30:41] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60596 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:04<00:01, 39.40it/s]
 78%|███████▊  | 155/200 [00:04<00:01, 39.88it/s][2026-01-17 08:30:41] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:40870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60152 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:04<00:01, 39.78it/s][2026-01-17 08:30:41] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:40980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:04<00:00, 41.58it/s][2026-01-17 08:30:41] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] Decode batch, #running-req: 33, #token: 9088, token usage: 0.03, npu graph: False, gen throughput (token/s): 2278.89, #queue-req: 0,
[2026-01-17 08:30:41] INFO:     127.0.0.1:40456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:41332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60364 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:04<00:01, 27.26it/s][2026-01-17 08:30:41] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:41] INFO:     127.0.0.1:60550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:42] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:05<00:01, 18.88it/s][2026-01-17 08:30:42] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:42] INFO:     127.0.0.1:60438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:42] Decode batch, #running-req: 25, #token: 8192, token usage: 0.03, npu graph: False, gen throughput (token/s): 1428.38, #queue-req: 0,
[2026-01-17 08:30:43] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:06<00:02,  9.64it/s][2026-01-17 08:30:43] Decode batch, #running-req: 23, #token: 8960, token usage: 0.03, npu graph: False, gen throughput (token/s): 1249.42, #queue-req: 0,
[2026-01-17 08:30:43] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:43] INFO:     127.0.0.1:60188 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:06<00:02,  7.39it/s][2026-01-17 08:30:43] Decode batch, #running-req: 21, #token: 8960, token usage: 0.03, npu graph: False, gen throughput (token/s): 1153.51, #queue-req: 0,
[2026-01-17 08:30:44] INFO:     127.0.0.1:60352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:44] Decode batch, #running-req: 20, #token: 9472, token usage: 0.03, npu graph: False, gen throughput (token/s): 1055.41, #queue-req: 0,
[2026-01-17 08:30:45] Decode batch, #running-req: 20, #token: 10240, token usage: 0.04, npu graph: False, gen throughput (token/s): 1046.35, #queue-req: 0,
[2026-01-17 08:30:46] Decode batch, #running-req: 20, #token: 11008, token usage: 0.04, npu graph: False, gen throughput (token/s): 1048.85, #queue-req: 0,
[2026-01-17 08:30:46] Decode batch, #running-req: 20, #token: 11904, token usage: 0.04, npu graph: False, gen throughput (token/s): 1045.19, #queue-req: 0,
[2026-01-17 08:30:47] Decode batch, #running-req: 20, #token: 12800, token usage: 0.04, npu graph: False, gen throughput (token/s): 1047.08, #queue-req: 0,
[2026-01-17 08:30:48] INFO:     127.0.0.1:40406 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] INFO:     127.0.0.1:40670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] Decode batch, #running-req: 20, #token: 4864, token usage: 0.02, npu graph: False, gen throughput (token/s): 1038.77, #queue-req: 0,
[2026-01-17 08:30:48] INFO:     127.0.0.1:40716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] INFO:     127.0.0.1:40938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] INFO:     127.0.0.1:41208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] INFO:     127.0.0.1:41214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] INFO:     127.0.0.1:41260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:48] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:11<00:10,  1.75it/s]
 96%|█████████▋| 193/200 [00:11<00:03,  1.87it/s]
 96%|█████████▋| 193/200 [00:11<00:03,  1.87it/s]
 96%|█████████▋| 193/200 [00:11<00:03,  1.87it/s]
 96%|█████████▋| 193/200 [00:11<00:03,  1.87it/s]
 96%|█████████▋| 193/200 [00:11<00:03,  1.87it/s]
 96%|█████████▋| 193/200 [00:11<00:03,  1.87it/s]
 96%|█████████▋| 193/200 [00:11<00:03,  1.87it/s]
 96%|█████████▋| 193/200 [00:11<00:03,  1.87it/s]
 96%|█████████▋| 193/200 [00:11<00:03,  1.87it/s][2026-01-17 08:30:48] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:12<00:03,  1.88it/s][2026-01-17 08:30:49] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:12<00:02,  1.97it/s][2026-01-17 08:30:49] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:49] Decode batch, #running-req: 4, #token: 3200, token usage: 0.01, npu graph: False, gen throughput (token/s): 352.26, #queue-req: 0,
[2026-01-17 08:30:49] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:12<00:01,  2.23it/s][2026-01-17 08:30:49] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:12<00:00,  2.34it/s][2026-01-17 08:30:49] INFO:     127.0.0.1:60466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:30:49] INFO:     127.0.0.1:60568 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:12<00:00,  2.88it/s]
100%|██████████| 200/200 [00:12<00:00, 15.55it/s]
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py", line 73, in test_gsm8k
    self.assertGreaterEqual(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1277, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: np.float64(0.125) not greater than or equal to 0.13 : Accuracy of /root/.cache/modelscope/hub/models/allenai/OLMoE-1B-7B-0924 is 0.125, is lower than 0.13
E
======================================================================
ERROR: test_gsm8k (__main__.TestMiMo.test_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: np.float64(0.125) not greater than or equal to 0.13 : Accuracy of /root/.cache/modelscope/hub/models/allenai/OLMoE-1B-7B-0924 is 0.125, is lower than 0.13

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 94.173s

FAILED (errors=1)
Accuracy: 0.125
Invalid: 0.005
Latency: 12.907 s
Output throughput: 1759.280 token/s
.
.
End (16/62):
filename='ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py', elapsed=103, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py

.
.
Begin (16/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:32:08] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/allenai/OLMoE-1B-7B-0924', tokenizer_path='/root/.cache/modelscope/hub/models/allenai/OLMoE-1B-7B-0924', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=147176819, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/allenai/OLMoE-1B-7B-0924', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:32:08] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:32:18] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:32:18] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:32:19] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:32:19] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.70it/s]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.18it/s]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.05it/s]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.11it/s]

[2026-01-17 08:32:34] Load weight end. type=OlmoeForCausalLM, dtype=torch.bfloat16, avail mem=47.88 GB, mem usage=12.94 GB.
[2026-01-17 08:32:34] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:32:34] The available memory for KV cache is 35.71 GB.
[2026-01-17 08:32:35] KV Cache is allocated. #tokens: 292480, K size: 17.86 GB, V size: 17.86 GB
[2026-01-17 08:32:35] Memory pool end. avail mem=12.09 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:32:35] max_total_num_tokens=292480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4096, available_gpu_mem=12.09 GB
[2026-01-17 08:32:36] INFO:     Started server process [53201]
[2026-01-17 08:32:36] INFO:     Waiting for application startup.
[2026-01-17 08:32:36] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:32:36] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 08:32:36] INFO:     Application startup complete.
[2026-01-17 08:32:36] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:32:37] INFO:     127.0.0.1:46822 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:32:37] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:32:38] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:38] The server is fired up and ready to roll!
[2026-01-17 08:32:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:32:40] INFO:     127.0.0.1:49366 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:32:40] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:32:40] INFO:     127.0.0.1:49372 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:32:40] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:32:40] INFO:     127.0.0.1:49386 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/allenai/OLMoE-1B-7B-0924 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMiMo.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:32:40] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:32:40] Prefill batch, #new-seq: 11, #new-token: 1408, #cached-token: 7040, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 08:32:40] Prefill batch, #new-seq: 13, #new-token: 1664, #cached-token: 8320, token usage: 0.01, #running-req: 12, #queue-req: 0,
[2026-01-17 08:32:40] Prefill batch, #new-seq: 14, #new-token: 1792, #cached-token: 8960, token usage: 0.01, #running-req: 25, #queue-req: 0,
[2026-01-17 08:32:40] Prefill batch, #new-seq: 13, #new-token: 1792, #cached-token: 8320, token usage: 0.02, #running-req: 39, #queue-req: 0,
[2026-01-17 08:32:40] Prefill batch, #new-seq: 8, #new-token: 1024, #cached-token: 5120, token usage: 0.03, #running-req: 52, #queue-req: 0,
[2026-01-17 08:32:40] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 60, #queue-req: 0,
[2026-01-17 08:32:40] Prefill batch, #new-seq: 11, #new-token: 1536, #cached-token: 7040, token usage: 0.03, #running-req: 62, #queue-req: 0,
[2026-01-17 08:32:40] Prefill batch, #new-seq: 10, #new-token: 1280, #cached-token: 6400, token usage: 0.04, #running-req: 73, #queue-req: 0,
[2026-01-17 08:32:40] Prefill batch, #new-seq: 14, #new-token: 1792, #cached-token: 8960, token usage: 0.04, #running-req: 83, #queue-req: 0,
[2026-01-17 08:32:40] Prefill batch, #new-seq: 16, #new-token: 2048, #cached-token: 10240, token usage: 0.05, #running-req: 97, #queue-req: 0,
[2026-01-17 08:32:40] Prefill batch, #new-seq: 15, #new-token: 1920, #cached-token: 9600, token usage: 0.05, #running-req: 113, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:00<02:36,  1.27it/s]
  1%|          | 2/200 [00:00<01:37,  2.02it/s][2026-01-17 08:32:41] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:49698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.06, #running-req: 125, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:49736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:00<00:26,  7.32it/s]
  4%|▍         | 9/200 [00:00<00:13, 13.94it/s][2026-01-17 08:32:41] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.06, #running-req: 128, #queue-req: 0,

  6%|▋         | 13/200 [00:01<00:11, 16.82it/s]
  7%|▋         | 14/200 [00:01<00:08, 21.06it/s][2026-01-17 08:32:41] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.06, #running-req: 130, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:49616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:32:41] Decode batch, #running-req: 127, #token: 18944, token usage: 0.06, npu graph: False, gen throughput (token/s): 159.74, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:50010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:01<00:08, 21.04it/s][2026-01-17 08:32:41] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:32:41] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.07, #running-req: 130, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:49714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:01<00:08, 22.22it/s]
 11%|█         | 22/200 [00:01<00:07, 25.24it/s][2026-01-17 08:32:41] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:49920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:50400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:49482 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:01<00:05, 30.73it/s][2026-01-17 08:32:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:49866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:49964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 08:32:41] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:41] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:01<00:06, 26.04it/s]
 16%|█▋        | 33/200 [00:01<00:06, 25.04it/s][2026-01-17 08:32:41] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:49392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:49642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:01<00:06, 25.83it/s]
 20%|██        | 41/200 [00:01<00:04, 35.14it/s]
 20%|██        | 41/200 [00:01<00:04, 35.14it/s]
 20%|██        | 41/200 [00:01<00:04, 35.14it/s][2026-01-17 08:32:42] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.07, #running-req: 124, #queue-req: 0,
[2026-01-17 08:32:42] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:49880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:01<00:04, 35.59it/s][2026-01-17 08:32:42] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:32:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 129, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:49876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:02<00:04, 33.08it/s]
 25%|██▌       | 50/200 [00:02<00:04, 33.21it/s][2026-01-17 08:32:42] INFO:     127.0.0.1:49894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.07, #running-req: 130, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50554 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:02<00:04, 33.84it/s]
 28%|██▊       | 56/200 [00:02<00:03, 41.16it/s]
 28%|██▊       | 56/200 [00:02<00:03, 41.16it/s][2026-01-17 08:32:42] INFO:     127.0.0.1:50544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-17 08:32:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 129, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:02<00:03, 38.23it/s][2026-01-17 08:32:42] INFO:     127.0.0.1:49688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.07, #running-req: 130, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:02<00:03, 37.84it/s][2026-01-17 08:32:42] INFO:     127.0.0.1:50042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] Prefill batch, #new-seq: 5, #new-token: 768, #cached-token: 3200, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 08:32:42] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.08, #running-req: 133, #queue-req: 0,
[2026-01-17 08:32:42] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:02<00:02, 44.30it/s][2026-01-17 08:32:42] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:42] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] Decode batch, #running-req: 125, #token: 22656, token usage: 0.08, npu graph: False, gen throughput (token/s): 3267.18, #queue-req: 0,
[2026-01-17 08:32:43] INFO:     127.0.0.1:49436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:02<00:02, 49.33it/s]
 42%|████▎     | 85/200 [00:02<00:01, 66.16it/s]
 42%|████▎     | 85/200 [00:02<00:01, 66.16it/s]
 42%|████▎     | 85/200 [00:02<00:01, 66.16it/s][2026-01-17 08:32:43] INFO:     127.0.0.1:49570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:51074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:02<00:01, 64.74it/s]
 47%|████▋     | 94/200 [00:02<00:01, 69.16it/s]
 47%|████▋     | 94/200 [00:02<00:01, 69.16it/s][2026-01-17 08:32:43] INFO:     127.0.0.1:49600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [00:03<00:01, 68.03it/s]
 51%|█████     | 102/200 [00:03<00:01, 69.73it/s][2026-01-17 08:32:43] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:03<00:01, 56.07it/s]
 55%|█████▌    | 110/200 [00:03<00:01, 50.78it/s][2026-01-17 08:32:43] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50794 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:03<00:01, 51.71it/s]
 60%|█████▉    | 119/200 [00:03<00:01, 54.35it/s][2026-01-17 08:32:43] INFO:     127.0.0.1:49548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:03<00:01, 56.85it/s][2026-01-17 08:32:43] INFO:     127.0.0.1:50890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] Decode batch, #running-req: 71, #token: 16512, token usage: 0.06, npu graph: False, gen throughput (token/s): 4359.61, #queue-req: 0,
[2026-01-17 08:32:43] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:49530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:03<00:01, 49.38it/s]
 66%|██████▋   | 133/200 [00:03<00:01, 46.64it/s][2026-01-17 08:32:43] INFO:     127.0.0.1:51094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:43] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:49512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:03<00:01, 43.58it/s][2026-01-17 08:32:44] INFO:     127.0.0.1:51242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51044 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:03<00:01, 46.91it/s][2026-01-17 08:32:44] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:50882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:04<00:01, 47.67it/s]
 76%|███████▌  | 151/200 [00:04<00:00, 50.57it/s][2026-01-17 08:32:44] INFO:     127.0.0.1:50702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:04<00:00, 45.57it/s][2026-01-17 08:32:44] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:04<00:00, 45.88it/s][2026-01-17 08:32:44] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] Decode batch, #running-req: 34, #token: 9344, token usage: 0.03, npu graph: False, gen throughput (token/s): 2549.14, #queue-req: 0,
[2026-01-17 08:32:44] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:04<00:00, 43.01it/s][2026-01-17 08:32:44] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:50468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:44] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:04<00:00, 30.97it/s][2026-01-17 08:32:45] INFO:     127.0.0.1:51250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:45] INFO:     127.0.0.1:51090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:45] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:45] Decode batch, #running-req: 25, #token: 8448, token usage: 0.03, npu graph: False, gen throughput (token/s): 1589.46, #queue-req: 0,
[2026-01-17 08:32:45] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:05<00:01, 18.60it/s][2026-01-17 08:32:45] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:46] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:46] Decode batch, #running-req: 23, #token: 8576, token usage: 0.03, npu graph: False, gen throughput (token/s): 1339.65, #queue-req: 0,
[2026-01-17 08:32:46] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:06<00:02,  9.70it/s][2026-01-17 08:32:46] Decode batch, #running-req: 21, #token: 8960, token usage: 0.03, npu graph: False, gen throughput (token/s): 1216.77, #queue-req: 0,
[2026-01-17 08:32:47] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:47] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:47] Decode batch, #running-req: 19, #token: 8960, token usage: 0.03, npu graph: False, gen throughput (token/s): 1132.89, #queue-req: 0,
[2026-01-17 08:32:48] Decode batch, #running-req: 19, #token: 9984, token usage: 0.03, npu graph: False, gen throughput (token/s): 1036.61, #queue-req: 0,
[2026-01-17 08:32:49] Decode batch, #running-req: 19, #token: 10496, token usage: 0.04, npu graph: False, gen throughput (token/s): 1041.35, #queue-req: 0,
[2026-01-17 08:32:49] Decode batch, #running-req: 19, #token: 11264, token usage: 0.04, npu graph: False, gen throughput (token/s): 1039.05, #queue-req: 0,
[2026-01-17 08:32:50] Decode batch, #running-req: 19, #token: 12416, token usage: 0.04, npu graph: False, gen throughput (token/s): 1043.75, #queue-req: 0,
[2026-01-17 08:32:51] Decode batch, #running-req: 19, #token: 10112, token usage: 0.03, npu graph: False, gen throughput (token/s): 1041.33, #queue-req: 0,
[2026-01-17 08:32:51] INFO:     127.0.0.1:49452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:10<00:08,  2.25it/s]
 92%|█████████▎| 185/200 [00:10<00:11,  1.27it/s]
 92%|█████████▎| 185/200 [00:10<00:11,  1.27it/s]
 92%|█████████▎| 185/200 [00:10<00:11,  1.27it/s][2026-01-17 08:32:51] INFO:     127.0.0.1:49976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:50178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:50336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:50650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:11<00:01,  2.37it/s][2026-01-17 08:32:51] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:51] INFO:     127.0.0.1:50870 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:11<00:00,  2.60it/s][2026-01-17 08:32:51] Decode batch, #running-req: 2, #token: 1920, token usage: 0.01, npu graph: False, gen throughput (token/s): 285.03, #queue-req: 0,
[2026-01-17 08:32:52] INFO:     127.0.0.1:50982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:32:52] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:12<00:00,  2.74it/s]
100%|██████████| 200/200 [00:12<00:00, 16.48it/s]
.
----------------------------------------------------------------------
Ran 1 test in 53.406s

OK
Accuracy: 0.145
Invalid: 0.005
Latency: 12.176 s
Output throughput: 1855.924 token/s
.
.
End (16/62):
filename='ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py', elapsed=63, estimated_time=400
.
.


✓ PASSED on retry (attempt 2): ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py

.
.
Begin (17/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_llama_2_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:33:11] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/LLM-Research/Llama-2-7B', tokenizer_path='/root/.cache/modelscope/hub/models/LLM-Research/Llama-2-7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=617609227, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/LLM-Research/Llama-2-7B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:33:12] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:33:21] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:33:22] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:33:22] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:33:22] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:33:23] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:08<00:16,  8.43s/it]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:20<00:10, 10.55s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:30<00:00, 10.48s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:30<00:00, 10.29s/it]

[2026-01-17 08:33:54] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=48.25 GB, mem usage=12.56 GB.
[2026-01-17 08:33:54] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:33:54] The available memory for KV cache is 36.09 GB.
[2026-01-17 08:33:55] KV Cache is allocated. #tokens: 73856, K size: 18.06 GB, V size: 18.06 GB
[2026-01-17 08:33:55] Memory pool end. avail mem=12.06 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:33:55] max_total_num_tokens=73856, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4096, available_gpu_mem=12.04 GB
[2026-01-17 08:33:56] INFO:     Started server process [55804]
[2026-01-17 08:33:56] INFO:     Waiting for application startup.
[2026-01-17 08:33:56] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-17 08:33:56] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-17 08:33:56] INFO:     Application startup complete.
[2026-01-17 08:33:56] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:33:57] INFO:     127.0.0.1:56534 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:33:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:34:02] INFO:     127.0.0.1:60542 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:34:12] INFO:     127.0.0.1:59088 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:34:13] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:13] The server is fired up and ready to roll!
[2026-01-17 08:34:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:34:23] INFO:     127.0.0.1:38264 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:34:23] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:34:23] INFO:     127.0.0.1:38272 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:34:23] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:34:23] INFO:     127.0.0.1:38278 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/LLM-Research/Llama-2-7B --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:34:23] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 08:34:23] Prefill batch, #new-seq: 17, #new-token: 4352, #cached-token: 13056, token usage: 0.01, #running-req: 1, #queue-req: 0,
[2026-01-17 08:34:24] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.07, #running-req: 18, #queue-req: 78,
[2026-01-17 08:34:25] Prefill batch, #new-seq: 33, #new-token: 8192, #cached-token: 24576, token usage: 0.18, #running-req: 49, #queue-req: 46,
[2026-01-17 08:34:25] Prefill batch, #new-seq: 30, #new-token: 7552, #cached-token: 22272, token usage: 0.29, #running-req: 81, #queue-req: 17,
[2026-01-17 08:34:27] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:04<14:28,  4.37s/it][2026-01-17 08:34:27] Prefill batch, #new-seq: 11, #new-token: 2816, #cached-token: 8448, token usage: 0.40, #running-req: 110, #queue-req: 7,
[2026-01-17 08:34:28] Decode batch, #running-req: 121, #token: 32512, token usage: 0.44, npu graph: False, gen throughput (token/s): 51.26, #queue-req: 7,
[2026-01-17 08:34:28] INFO:     127.0.0.1:39034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:28] INFO:     127.0.0.1:39126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:28] INFO:     127.0.0.1:39304 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:04<06:42,  2.03s/it]
  2%|▏         | 4/200 [00:04<01:28,  2.20it/s]
  2%|▏         | 4/200 [00:04<01:28,  2.20it/s][2026-01-17 08:34:28] Prefill batch, #new-seq: 5, #new-token: 1280, #cached-token: 3840, token usage: 0.43, #running-req: 118, #queue-req: 5,
[2026-01-17 08:34:28] INFO:     127.0.0.1:38564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:28] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.45, #running-req: 122, #queue-req: 4,
[2026-01-17 08:34:28] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:04<01:04,  3.00it/s][2026-01-17 08:34:28] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.45, #running-req: 123, #queue-req: 3,
[2026-01-17 08:34:28] INFO:     127.0.0.1:38578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:28] INFO:     127.0.0.1:39032 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:05<00:58,  3.31it/s]
  4%|▍         | 8/200 [00:05<00:42,  4.47it/s][2026-01-17 08:34:28] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:28] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.45, #running-req: 123, #queue-req: 3,
[2026-01-17 08:34:28] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.46, #running-req: 124, #queue-req: 3,
[2026-01-17 08:34:28] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:05<00:34,  5.57it/s][2026-01-17 08:34:28] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.46, #running-req: 124, #queue-req: 3,
[2026-01-17 08:34:29] INFO:     127.0.0.1:38724 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:05<00:34,  5.41it/s][2026-01-17 08:34:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.47, #running-req: 124, #queue-req: 3,
[2026-01-17 08:34:29] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:29] INFO:     127.0.0.1:38834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:29] INFO:     127.0.0.1:39318 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:05<00:40,  4.62it/s]
  7%|▋         | 14/200 [00:05<00:29,  6.34it/s]
  7%|▋         | 14/200 [00:05<00:29,  6.34it/s][2026-01-17 08:34:29] INFO:     127.0.0.1:39282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:29] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 3072, token usage: 0.46, #running-req: 122, #queue-req: 2,
[2026-01-17 08:34:29] INFO:     127.0.0.1:38294 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:05<00:23,  7.97it/s][2026-01-17 08:34:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.47, #running-req: 124, #queue-req: 3,
[2026-01-17 08:34:29] INFO:     127.0.0.1:38524 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:29] INFO:     127.0.0.1:39142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:29] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:29] INFO:     127.0.0.1:39230 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:06<00:22,  8.09it/s]
 10%|█         | 20/200 [00:06<00:15, 11.57it/s]
 10%|█         | 20/200 [00:06<00:15, 11.57it/s][2026-01-17 08:34:29] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 3072, token usage: 0.47, #running-req: 121, #queue-req: 3,
[2026-01-17 08:34:30] INFO:     127.0.0.1:38850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:30] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:06<00:17, 10.37it/s][2026-01-17 08:34:30] Prefill batch, #new-seq: 2, #new-token: 640, #cached-token: 1536, token usage: 0.48, #running-req: 123, #queue-req: 3,
[2026-01-17 08:34:30] INFO:     127.0.0.1:38814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:30] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:30] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.48, #running-req: 124, #queue-req: 2,

 12%|█▏        | 24/200 [00:06<00:18,  9.68it/s][2026-01-17 08:34:30] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.49, #running-req: 125, #queue-req: 2,
[2026-01-17 08:34:30] INFO:     127.0.0.1:38430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:30] INFO:     127.0.0.1:38866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:30] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.49, #running-req: 125, #queue-req: 1,

 13%|█▎        | 26/200 [00:07<00:19,  8.88it/s][2026-01-17 08:34:30] INFO:     127.0.0.1:38286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:30] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.49, #running-req: 125, #queue-req: 1,
[2026-01-17 08:34:30] Decode batch, #running-req: 127, #token: 37120, token usage: 0.50, npu graph: False, gen throughput (token/s): 1876.80, #queue-req: 1,
[2026-01-17 08:34:30] INFO:     127.0.0.1:38806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:30] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:07<00:21,  8.17it/s]
 14%|█▍        | 29/200 [00:07<00:19,  8.80it/s][2026-01-17 08:34:30] INFO:     127.0.0.1:39340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:30] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.50, #running-req: 125, #queue-req: 2,
[2026-01-17 08:34:30] INFO:     127.0.0.1:38626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:30] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:07<00:16, 10.23it/s]
 16%|█▌        | 32/200 [00:07<00:12, 13.28it/s][2026-01-17 08:34:31] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.50, #running-req: 123, #queue-req: 3,
[2026-01-17 08:34:31] INFO:     127.0.0.1:38598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:07<00:12, 12.84it/s]
 18%|█▊        | 35/200 [00:07<00:11, 14.18it/s][2026-01-17 08:34:31] INFO:     127.0.0.1:38602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] INFO:     127.0.0.1:38676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 3072, token usage: 0.49, #running-req: 122, #queue-req: 2,
[2026-01-17 08:34:31] INFO:     127.0.0.1:38890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] INFO:     127.0.0.1:38792 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:07<00:08, 19.09it/s][2026-01-17 08:34:31] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.50, #running-req: 122, #queue-req: 4,
[2026-01-17 08:34:31] INFO:     127.0.0.1:38988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] INFO:     127.0.0.1:39192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:07<00:09, 17.52it/s]
 22%|██▏       | 43/200 [00:07<00:08, 18.24it/s][2026-01-17 08:34:31] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.51, #running-req: 120, #queue-req: 5,
[2026-01-17 08:34:31] INFO:     127.0.0.1:38666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] INFO:     127.0.0.1:38954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] INFO:     127.0.0.1:39098 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:08<00:09, 15.76it/s][2026-01-17 08:34:31] INFO:     127.0.0.1:38788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.51, #running-req: 120, #queue-req: 5,
[2026-01-17 08:34:31] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:08<00:09, 16.48it/s][2026-01-17 08:34:31] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.52, #running-req: 121, #queue-req: 5,
[2026-01-17 08:34:31] INFO:     127.0.0.1:38822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.53, #running-req: 122, #queue-req: 5,
[2026-01-17 08:34:32] INFO:     127.0.0.1:38470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:32] INFO:     127.0.0.1:39066 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:08<00:11, 13.00it/s]
 26%|██▌       | 51/200 [00:08<00:11, 12.45it/s][2026-01-17 08:34:32] INFO:     127.0.0.1:38660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:32] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.53, #running-req: 121, #queue-req: 5,
[2026-01-17 08:34:32] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:32] INFO:     127.0.0.1:39458 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:08<00:10, 13.65it/s]
 27%|██▋       | 54/200 [00:08<00:08, 16.68it/s][2026-01-17 08:34:32] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.53, #running-req: 120, #queue-req: 6,
[2026-01-17 08:34:32] INFO:     127.0.0.1:39392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:32] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.53, #running-req: 121, #queue-req: 5,
[2026-01-17 08:34:32] INFO:     127.0.0.1:38982 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:08<00:09, 14.84it/s][2026-01-17 08:34:32] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.53, #running-req: 122, #queue-req: 5,
[2026-01-17 08:34:32] INFO:     127.0.0.1:38592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:32] INFO:     127.0.0.1:38750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:32] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.53, #running-req: 121, #queue-req: 5,
[2026-01-17 08:34:32] INFO:     127.0.0.1:39018 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:08<00:08, 15.72it/s][2026-01-17 08:34:32] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.54, #running-req: 122, #queue-req: 5,
[2026-01-17 08:34:32] INFO:     127.0.0.1:38360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:32] INFO:     127.0.0.1:39384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:32] INFO:     127.0.0.1:59490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:32] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.53, #running-req: 122, #queue-req: 4,

 30%|███       | 61/200 [00:09<00:10, 13.33it/s]
 31%|███       | 62/200 [00:09<00:10, 13.43it/s][2026-01-17 08:34:32] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:32] INFO:     127.0.0.1:39122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:32] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.53, #running-req: 120, #queue-req: 6,
[2026-01-17 08:34:33] INFO:     127.0.0.1:38628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] INFO:     127.0.0.1:38694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] INFO:     127.0.0.1:39076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] INFO:     127.0.0.1:39244 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:09<00:11, 11.53it/s]
 34%|███▍      | 68/200 [00:09<00:09, 14.53it/s]
 34%|███▍      | 68/200 [00:09<00:09, 14.53it/s]
 34%|███▍      | 68/200 [00:09<00:09, 14.53it/s][2026-01-17 08:34:33] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 3072, token usage: 0.53, #running-req: 118, #queue-req: 6,
[2026-01-17 08:34:33] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.54, #running-req: 121, #queue-req: 5,
[2026-01-17 08:34:33] INFO:     127.0.0.1:39058 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:09<00:10, 12.10it/s][2026-01-17 08:34:33] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.54, #running-req: 122, #queue-req: 4,
[2026-01-17 08:34:33] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:09<00:09, 13.05it/s][2026-01-17 08:34:33] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.54, #running-req: 122, #queue-req: 4,
[2026-01-17 08:34:33] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.54, #running-req: 122, #queue-req: 2,
[2026-01-17 08:34:33] Decode batch, #running-req: 124, #token: 40064, token usage: 0.54, npu graph: False, gen throughput (token/s): 1694.86, #queue-req: 2,
[2026-01-17 08:34:33] INFO:     127.0.0.1:38736 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:10<00:10, 12.43it/s][2026-01-17 08:34:33] INFO:     127.0.0.1:39474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.54, #running-req: 123, #queue-req: 0,
[2026-01-17 08:34:33] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] INFO:     127.0.0.1:39252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:10<00:09, 13.42it/s]
 40%|████      | 80/200 [00:10<00:04, 26.59it/s]
 40%|████      | 80/200 [00:10<00:04, 26.59it/s]
 40%|████      | 80/200 [00:10<00:04, 26.59it/s][2026-01-17 08:34:33] INFO:     127.0.0.1:38356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:33] INFO:     127.0.0.1:38386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:38802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:10<00:04, 27.38it/s][2026-01-17 08:34:34] INFO:     127.0.0.1:39160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:59522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:39436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:10<00:04, 24.76it/s][2026-01-17 08:34:34] INFO:     127.0.0.1:38760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:38344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:38372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:38612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:38720 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:10<00:04, 23.36it/s]
 46%|████▋     | 93/200 [00:10<00:03, 26.95it/s]
 46%|████▋     | 93/200 [00:10<00:03, 26.95it/s][2026-01-17 08:34:34] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:10<00:03, 27.56it/s][2026-01-17 08:34:34] INFO:     127.0.0.1:38334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:38854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:39352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:38358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:39168 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:11<00:03, 26.86it/s][2026-01-17 08:34:34] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:38586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:34] INFO:     127.0.0.1:39290 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:11<00:03, 25.65it/s][2026-01-17 08:34:34] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:11<00:04, 19.49it/s]
 55%|█████▍    | 109/200 [00:11<00:05, 17.74it/s][2026-01-17 08:34:35] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:38938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:11<00:04, 18.77it/s][2026-01-17 08:34:35] INFO:     127.0.0.1:39372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:39110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:11<00:04, 18.25it/s]
 58%|█████▊    | 116/200 [00:11<00:04, 19.51it/s][2026-01-17 08:34:35] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:12<00:04, 18.80it/s]
 60%|██████    | 120/200 [00:12<00:04, 19.95it/s][2026-01-17 08:34:35] Decode batch, #running-req: 80, #token: 28032, token usage: 0.38, npu graph: False, gen throughput (token/s): 2115.90, #queue-req: 0,
[2026-01-17 08:34:35] INFO:     127.0.0.1:38716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:39136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:59726 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:12<00:03, 19.47it/s][2026-01-17 08:34:35] INFO:     127.0.0.1:38710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:39422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:38892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:38534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:59762 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:12<00:02, 24.44it/s]
 65%|██████▌   | 130/200 [00:12<00:01, 37.63it/s]
 65%|██████▌   | 130/200 [00:12<00:01, 37.63it/s][2026-01-17 08:34:35] INFO:     127.0.0.1:38398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:39044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:35] INFO:     127.0.0.1:39238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:38446 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:12<00:02, 27.49it/s][2026-01-17 08:34:36] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:59520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:38422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:59496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:12<00:02, 28.95it/s]
 70%|███████   | 140/200 [00:12<00:01, 32.20it/s][2026-01-17 08:34:36] INFO:     127.0.0.1:38962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:38976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:59962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:12<00:02, 25.32it/s][2026-01-17 08:34:36] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:59618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:59686 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:13<00:02, 23.51it/s][2026-01-17 08:34:36] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:13<00:02, 23.82it/s][2026-01-17 08:34:36] INFO:     127.0.0.1:59832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:36] INFO:     127.0.0.1:60018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] Decode batch, #running-req: 46, #token: 18048, token usage: 0.24, npu graph: False, gen throughput (token/s): 1682.75, #queue-req: 0,
[2026-01-17 08:34:37] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:13<00:02, 20.36it/s][2026-01-17 08:34:37] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:59754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:39444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:13<00:01, 22.22it/s]
 80%|████████  | 161/200 [00:13<00:01, 25.12it/s][2026-01-17 08:34:37] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:59968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:39266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:13<00:01, 20.53it/s]
 82%|████████▎ | 165/200 [00:13<00:01, 19.10it/s][2026-01-17 08:34:37] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:59592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:38318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:14<00:01, 19.30it/s]
 85%|████████▌ | 170/200 [00:14<00:01, 20.88it/s][2026-01-17 08:34:37] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:59990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:37] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:14<00:01, 19.70it/s][2026-01-17 08:34:37] INFO:     127.0.0.1:59918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:38] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:38] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:14<00:01, 21.32it/s][2026-01-17 08:34:38] INFO:     127.0.0.1:39344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:38] INFO:     127.0.0.1:59708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:38] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:14<00:00, 21.79it/s][2026-01-17 08:34:38] Decode batch, #running-req: 21, #token: 10368, token usage: 0.14, npu graph: False, gen throughput (token/s): 1166.97, #queue-req: 0,
[2026-01-17 08:34:38] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:38] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:38] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:15<00:01, 11.31it/s][2026-01-17 08:34:38] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:39] Decode batch, #running-req: 17, #token: 9088, token usage: 0.12, npu graph: False, gen throughput (token/s): 766.95, #queue-req: 0,
[2026-01-17 08:34:39] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:15<00:02,  7.70it/s][2026-01-17 08:34:39] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:39] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:40] Decode batch, #running-req: 14, #token: 8064, token usage: 0.11, npu graph: False, gen throughput (token/s): 611.35, #queue-req: 0,
[2026-01-17 08:34:40] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:17<00:03,  3.99it/s][2026-01-17 08:34:41] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:41] Decode batch, #running-req: 12, #token: 8064, token usage: 0.11, npu graph: False, gen throughput (token/s): 574.28, #queue-req: 0,
[2026-01-17 08:34:42] Decode batch, #running-req: 12, #token: 8192, token usage: 0.11, npu graph: False, gen throughput (token/s): 503.45, #queue-req: 0,
[2026-01-17 08:34:42] INFO:     127.0.0.1:38324 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:18<00:03,  2.83it/s][2026-01-17 08:34:43] Decode batch, #running-req: 11, #token: 7680, token usage: 0.10, npu graph: False, gen throughput (token/s): 468.04, #queue-req: 0,
[2026-01-17 08:34:44] Decode batch, #running-req: 11, #token: 8448, token usage: 0.11, npu graph: False, gen throughput (token/s): 452.90, #queue-req: 0,
[2026-01-17 08:34:44] INFO:     127.0.0.1:38420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:44] Decode batch, #running-req: 11, #token: 2816, token usage: 0.04, npu graph: False, gen throughput (token/s): 454.50, #queue-req: 0,
[2026-01-17 08:34:44] INFO:     127.0.0.1:38484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:44] INFO:     127.0.0.1:38506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:44] INFO:     127.0.0.1:38532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:44] INFO:     127.0.0.1:38616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:44] INFO:     127.0.0.1:38644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:44] INFO:     127.0.0.1:38904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:34:44] INFO:     127.0.0.1:39232 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:21<00:06,  1.50it/s]
 98%|█████████▊| 197/200 [00:21<00:01,  2.35it/s]
 98%|█████████▊| 197/200 [00:21<00:01,  2.35it/s]
 98%|█████████▊| 197/200 [00:21<00:01,  2.35it/s]
 98%|█████████▊| 197/200 [00:21<00:01,  2.35it/s]
 98%|█████████▊| 197/200 [00:21<00:01,  2.35it/s]
 98%|█████████▊| 197/200 [00:21<00:01,  2.35it/s]
 98%|█████████▊| 197/200 [00:21<00:01,  2.35it/s][2026-01-17 08:34:45] Decode batch, #running-req: 3, #token: 2816, token usage: 0.04, npu graph: False, gen throughput (token/s): 134.79, #queue-req: 0,
[2026-01-17 08:34:46] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:23<00:01,  1.98it/s][2026-01-17 08:34:46] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:23<00:00,  2.08it/s][2026-01-17 08:34:46] Decode batch, #running-req: 1, #token: 1536, token usage: 0.02, npu graph: False, gen throughput (token/s): 114.54, #queue-req: 0,
[2026-01-17 08:34:47] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:23<00:00,  2.09it/s]
100%|██████████| 200/200 [00:23<00:00,  8.42it/s]
.
----------------------------------------------------------------------
Ran 1 test in 105.095s

OK
Accuracy: 0.195
Invalid: 0.005
Latency: 23.834 s
Output throughput: 1050.033 token/s
.
.
End (17/62):
filename='ascend/llm_models/test_ascend_llama_2_7b.py', elapsed=115, estimated_time=400
.
.

.
.
Begin (18/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_qwen3_next_models.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:35:06] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-Next-80B-A3B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-Next-80B-A3B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=335666847, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=10, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-Next-80B-A3B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:35:06] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:35:16 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:35:16 TP0] Init torch distributed begin.
[2026-01-17 08:35:17 TP2] Init torch distributed begin.
[2026-01-17 08:35:17 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 08:35:26 TP3] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 08:35:26 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:35:26 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:35:26 TP2] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 08:35:27 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:35:27 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:35:27 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:35:27 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:35:28 TP0] Load weight begin. avail mem=60.78 GB
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 08:35:28 TP0] using attn output gate!
[2026-01-17 08:35:28 TP3] Load weight begin. avail mem=61.04 GB
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 08:35:28 TP3] using attn output gate!
[2026-01-17 08:35:28 TP1] Load weight begin. avail mem=61.03 GB
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 08:35:28 TP2] Load weight begin. avail mem=60.76 GB
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 08:35:28 TP1] using attn output gate!
[2026-01-17 08:35:28 TP2] using attn output gate!

Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:05<03:24,  5.10s/it]

Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:12<04:02,  6.23s/it]

Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:18<03:53,  6.16s/it]

Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:24<03:48,  6.17s/it]

Loading safetensors checkpoint shards:  12% Completed | 5/41 [00:30<03:38,  6.06s/it]

Loading safetensors checkpoint shards:  15% Completed | 6/41 [00:36<03:30,  6.01s/it]

Loading safetensors checkpoint shards:  17% Completed | 7/41 [00:42<03:29,  6.16s/it]

Loading safetensors checkpoint shards:  20% Completed | 8/41 [00:49<03:27,  6.30s/it]

Loading safetensors checkpoint shards:  22% Completed | 9/41 [00:55<03:25,  6.42s/it]

Loading safetensors checkpoint shards:  24% Completed | 10/41 [01:01<03:11,  6.19s/it]

Loading safetensors checkpoint shards:  27% Completed | 11/41 [01:07<03:02,  6.07s/it]

Loading safetensors checkpoint shards:  29% Completed | 12/41 [01:13<02:52,  5.95s/it]

Loading safetensors checkpoint shards:  32% Completed | 13/41 [01:18<02:45,  5.92s/it]

Loading safetensors checkpoint shards:  34% Completed | 14/41 [01:25<02:44,  6.08s/it]

Loading safetensors checkpoint shards:  37% Completed | 15/41 [01:30<02:32,  5.85s/it]

Loading safetensors checkpoint shards:  39% Completed | 16/41 [01:32<01:59,  4.79s/it]

Loading safetensors checkpoint shards:  41% Completed | 17/41 [01:39<02:06,  5.25s/it]

Loading safetensors checkpoint shards:  44% Completed | 18/41 [01:44<02:02,  5.31s/it]

Loading safetensors checkpoint shards:  46% Completed | 19/41 [01:50<02:00,  5.46s/it]

Loading safetensors checkpoint shards:  49% Completed | 20/41 [01:56<01:55,  5.51s/it]

Loading safetensors checkpoint shards:  51% Completed | 21/41 [02:01<01:51,  5.56s/it]

Loading safetensors checkpoint shards:  54% Completed | 22/41 [02:07<01:46,  5.62s/it]

Loading safetensors checkpoint shards:  56% Completed | 23/41 [02:14<01:46,  5.91s/it]

Loading safetensors checkpoint shards:  59% Completed | 24/41 [02:19<01:39,  5.83s/it]

Loading safetensors checkpoint shards:  61% Completed | 25/41 [02:25<01:33,  5.84s/it]

Loading safetensors checkpoint shards:  63% Completed | 26/41 [02:31<01:29,  5.95s/it]

Loading safetensors checkpoint shards:  66% Completed | 27/41 [02:37<01:23,  5.97s/it]

Loading safetensors checkpoint shards:  68% Completed | 28/41 [02:43<01:16,  5.89s/it]

Loading safetensors checkpoint shards:  71% Completed | 29/41 [02:49<01:10,  5.84s/it]

Loading safetensors checkpoint shards:  73% Completed | 30/41 [02:54<01:03,  5.74s/it]

Loading safetensors checkpoint shards:  76% Completed | 31/41 [03:01<01:00,  6.00s/it]

Loading safetensors checkpoint shards:  78% Completed | 32/41 [03:07<00:53,  5.90s/it]

Loading safetensors checkpoint shards:  80% Completed | 33/41 [03:15<00:52,  6.57s/it]

Loading safetensors checkpoint shards:  83% Completed | 34/41 [03:22<00:47,  6.72s/it]

Loading safetensors checkpoint shards:  85% Completed | 35/41 [03:29<00:40,  6.83s/it]

Loading safetensors checkpoint shards:  88% Completed | 36/41 [03:37<00:35,  7.06s/it]

Loading safetensors checkpoint shards:  90% Completed | 37/41 [03:44<00:28,  7.09s/it]

Loading safetensors checkpoint shards:  93% Completed | 38/41 [03:52<00:22,  7.42s/it]

Loading safetensors checkpoint shards:  95% Completed | 39/41 [03:59<00:14,  7.18s/it]

Loading safetensors checkpoint shards:  98% Completed | 40/41 [04:05<00:06,  6.91s/it]

Loading safetensors checkpoint shards: 100% Completed | 41/41 [04:11<00:00,  6.68s/it]

Loading safetensors checkpoint shards: 100% Completed | 41/41 [04:11<00:00,  6.13s/it]

[2026-01-17 08:39:53 TP1] Load weight end. type=Qwen3NextForCausalLM, dtype=torch.bfloat16, avail mem=23.12 GB, mem usage=37.91 GB.
[2026-01-17 08:39:53 TP0] Load weight end. type=Qwen3NextForCausalLM, dtype=torch.bfloat16, avail mem=22.87 GB, mem usage=37.91 GB.
[2026-01-17 08:39:54 TP3] Load weight end. type=Qwen3NextForCausalLM, dtype=torch.bfloat16, avail mem=23.13 GB, mem usage=37.91 GB.
[2026-01-17 08:39:54 TP2] Load weight end. type=Qwen3NextForCausalLM, dtype=torch.bfloat16, avail mem=22.85 GB, mem usage=37.91 GB.
[2026-01-17 08:39:54 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:39:54 TP0] The available memory for KV cache is 1.47 GB.
[2026-01-17 08:39:54 TP1] The available memory for KV cache is 1.47 GB.
[2026-01-17 08:39:54 TP3] The available memory for KV cache is 1.47 GB.
[2026-01-17 08:39:54 TP2] The available memory for KV cache is 1.47 GB.
[2026-01-17 08:39:54 TP0] Mamba Cache is allocated. max_mamba_cache_size: 512, conv_state size: 0.21GB, ssm_state size: 9.02GB
[2026-01-17 08:39:54 TP3] Mamba Cache is allocated. max_mamba_cache_size: 512, conv_state size: 0.21GB, ssm_state size: 9.02GB
[2026-01-17 08:39:54 TP1] Mamba Cache is allocated. max_mamba_cache_size: 512, conv_state size: 0.21GB, ssm_state size: 9.02GB
[2026-01-17 08:39:54 TP2] Mamba Cache is allocated. max_mamba_cache_size: 512, conv_state size: 0.21GB, ssm_state size: 9.02GB
[2026-01-17 08:39:54 TP0] KV Cache is allocated. #tokens: 128256, K size: 0.73 GB, V size: 0.73 GB
[2026-01-17 08:39:54 TP0] Memory pool end. avail mem=11.87 GB
[2026-01-17 08:39:54 TP3] KV Cache is allocated. #tokens: 128256, K size: 0.73 GB, V size: 0.73 GB
[2026-01-17 08:39:54 TP3] Memory pool end. avail mem=12.14 GB
[2026-01-17 08:39:54 TP1] KV Cache is allocated. #tokens: 128256, K size: 0.73 GB, V size: 0.73 GB
[2026-01-17 08:39:54 TP1] Memory pool end. avail mem=12.12 GB
[2026-01-17 08:39:54 TP2] KV Cache is allocated. #tokens: 128256, K size: 0.73 GB, V size: 0.73 GB
[2026-01-17 08:39:54 TP2] Memory pool end. avail mem=11.86 GB
[2026-01-17 08:39:54 TP1] Using hybrid linear attention backend for hybrid GDN models.
[2026-01-17 08:39:54 TP0] Using hybrid linear attention backend for hybrid GDN models.
[2026-01-17 08:39:54 TP3] Using hybrid linear attention backend for hybrid GDN models.
[2026-01-17 08:39:54 TP2] Using hybrid linear attention backend for hybrid GDN models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:39:55 TP0] max_total_num_tokens=128256, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=512, context_len=262144, available_gpu_mem=11.87 GB
[2026-01-17 08:39:55] INFO:     Started server process [58551]
[2026-01-17 08:39:55] INFO:     Waiting for application startup.
[2026-01-17 08:39:55] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 08:39:55] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 08:39:55] INFO:     Application startup complete.
[2026-01-17 08:39:55] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:39:56] INFO:     127.0.0.1:49338 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:39:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, full token usage: 0.00, mamba usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:39:57] INFO:     127.0.0.1:49348 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[rank0]:[W117 08:39:58.275896452 compiler_depend.ts:3136] Warning: The indexFromRank 0is not equal indexFromCurDevice 10 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[rank3]:[W117 08:39:59.471108964 compiler_depend.ts:3136] Warning: The indexFromRank 3is not equal indexFromCurDevice 13 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[rank1]:[W117 08:39:59.642010475 compiler_depend.ts:3136] Warning: The indexFromRank 1is not equal indexFromCurDevice 11 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[rank2]:[W117 08:39:59.648913819 compiler_depend.ts:3136] Warning: The indexFromRank 2is not equal indexFromCurDevice 12 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[2026-01-17 08:40:07] INFO:     127.0.0.1:33106 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:40:17] INFO:     127.0.0.1:43108 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:40:27] INFO:     127.0.0.1:38178 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:40:37] INFO:     127.0.0.1:33338 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
....[2026-01-17 08:40:47] INFO:     127.0.0.1:45880 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:40:52] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:40:52] The server is fired up and ready to roll!
[2026-01-17 08:40:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, full token usage: 0.00, mamba usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:40:58] INFO:     127.0.0.1:42304 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:40:58] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:40:58] INFO:     127.0.0.1:56654 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:40:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.00, mamba usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:41:01] INFO:     127.0.0.1:56656 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-Next-80B-A3B-Instruct --tp-size 4 --disable-cuda-graph --attention-backend ascend --base-gpu-id 10 --mem-fraction-static 0.8 --disable-radix-cache --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestQwen3Next.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:41:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.00, mamba usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:41:02 TP0] Prefill batch, #new-seq: 9, #new-token: 8192, #cached-token: 0, full token usage: 0.01, mamba usage: 0.00, #running-req: 1, #queue-req: 48,
[2026-01-17 08:41:05 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, full token usage: 0.07, mamba usage: 0.02, #running-req: 9, #queue-req: 109,
....[2026-01-17 08:41:08 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, full token usage: 0.13, mamba usage: 0.04, #running-req: 19, #queue-req: 99,
[2026-01-17 08:41:08 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, full token usage: 0.20, mamba usage: 0.06, #running-req: 28, #queue-req: 90,
[2026-01-17 08:41:09 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, full token usage: 0.26, mamba usage: 0.07, #running-req: 37, #queue-req: 81,
[2026-01-17 08:41:09 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, full token usage: 0.33, mamba usage: 0.09, #running-req: 46, #queue-req: 72,
[2026-01-17 08:41:12 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, full token usage: 0.39, mamba usage: 0.11, #running-req: 55, #queue-req: 63,
[2026-01-17 08:41:12 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, full token usage: 0.45, mamba usage: 0.13, #running-req: 64, #queue-req: 54,
[2026-01-17 08:41:16 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, full token usage: 0.52, mamba usage: 0.14, #running-req: 73, #queue-req: 45,
[2026-01-17 08:41:17 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, full token usage: 0.58, mamba usage: 0.16, #running-req: 82, #queue-req: 36,
[2026-01-17 08:41:17 TP0] Prefill batch, #new-seq: 9, #new-token: 7424, #cached-token: 0, full token usage: 0.65, mamba usage: 0.18, #running-req: 91, #queue-req: 28,
[2026-01-17 08:41:22 TP0] Decode batch, #running-req: 100, #full token: 92800, full token usage: 0.72, mamba num: 100, mamba usage: 0.20, npu graph: False, gen throughput (token/s): 8.49, #queue-req: 28,
[2026-01-17 08:41:23] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:21<1:12:24, 21.83s/it][2026-01-17 08:41:23 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, full token usage: 0.74, mamba usage: 0.19, #running-req: 99, #queue-req: 27,
[2026-01-17 08:41:24] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:22<31:32,  9.56s/it]  [2026-01-17 08:41:24] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:22<17:14,  5.25s/it][2026-01-17 08:41:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.75, mamba usage: 0.19, #running-req: 100, #queue-req: 27,
[2026-01-17 08:41:24] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:23<10:40,  3.27s/it][2026-01-17 08:41:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.75, mamba usage: 0.19, #running-req: 99, #queue-req: 28,
[2026-01-17 08:41:26] INFO:     127.0.0.1:56674 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:25<09:00,  2.77s/it][2026-01-17 08:41:27] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:27] INFO:     127.0.0.1:57530 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:26<07:05,  2.19s/it]
  4%|▎         | 7/200 [00:26<04:24,  1.37s/it][2026-01-17 08:41:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.77, mamba usage: 0.19, #running-req: 97, #queue-req: 30,
[2026-01-17 08:41:28] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:26<03:33,  1.11s/it][2026-01-17 08:41:28 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.77, mamba usage: 0.19, #running-req: 97, #queue-req: 29,
[2026-01-17 08:41:28 TP0] Decode batch, #running-req: 97, #full token: 100096, full token usage: 0.78, mamba num: 99, mamba usage: 0.19, npu graph: False, gen throughput (token/s): 622.82, #queue-req: 29,
[2026-01-17 08:41:28] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:27<03:07,  1.02it/s][2026-01-17 08:41:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.78, mamba usage: 0.19, #running-req: 98, #queue-req: 29,
[2026-01-17 08:41:29] INFO:     127.0.0.1:56788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:29] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:27<02:52,  1.10it/s]
  6%|▌         | 11/200 [00:27<02:05,  1.50it/s][2026-01-17 08:41:29 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.77, mamba usage: 0.19, #running-req: 97, #queue-req: 29,
[2026-01-17 08:41:30] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:28<02:09,  1.45it/s][2026-01-17 08:41:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.77, mamba usage: 0.19, #running-req: 98, #queue-req: 29,
[2026-01-17 08:41:31] INFO:     127.0.0.1:56838 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:29<02:11,  1.42it/s][2026-01-17 08:41:31 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.77, mamba usage: 0.19, #running-req: 98, #queue-req: 28,
[2026-01-17 08:41:33] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:31<03:25,  1.10s/it][2026-01-17 08:41:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.78, mamba usage: 0.19, #running-req: 99, #queue-req: 28,
[2026-01-17 08:41:33] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:32<02:58,  1.04it/s][2026-01-17 08:41:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.78, mamba usage: 0.19, #running-req: 99, #queue-req: 28,
[2026-01-17 08:41:34] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:33<02:52,  1.06it/s][2026-01-17 08:41:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.78, mamba usage: 0.19, #running-req: 99, #queue-req: 28,
[2026-01-17 08:41:35] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:33<02:34,  1.18it/s][2026-01-17 08:41:35 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.78, mamba usage: 0.19, #running-req: 99, #queue-req: 27,
[2026-01-17 08:41:36] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:34<02:23,  1.27it/s][2026-01-17 08:41:36] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:36] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:34<01:46,  1.69it/s]
 10%|█         | 20/200 [00:34<01:02,  2.87it/s][2026-01-17 08:41:36 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.77, mamba usage: 0.19, #running-req: 100, #queue-req: 26,
[2026-01-17 08:41:36] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:34<00:59,  3.02it/s][2026-01-17 08:41:36 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.78, mamba usage: 0.19, #running-req: 99, #queue-req: 27,
[2026-01-17 08:41:37] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:35<01:21,  2.19it/s][2026-01-17 08:41:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.78, mamba usage: 0.20, #running-req: 100, #queue-req: 27,
[2026-01-17 08:41:37] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:35<01:17,  2.29it/s][2026-01-17 08:41:37 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.78, mamba usage: 0.20, #running-req: 100, #queue-req: 26,
[2026-01-17 08:41:38] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:38] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:36<01:28,  1.99it/s]
 12%|█▎        | 25/200 [00:36<01:15,  2.33it/s][2026-01-17 08:41:38] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:36<01:02,  2.78it/s][2026-01-17 08:41:38 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.77, mamba usage: 0.19, #running-req: 100, #queue-req: 26,
[2026-01-17 08:41:38] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:38] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:38] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:36<00:57,  3.00it/s]
 14%|█▍        | 29/200 [00:36<00:28,  5.91it/s]
 14%|█▍        | 29/200 [00:36<00:28,  5.91it/s][2026-01-17 08:41:38 TP0] Prefill batch, #new-seq: 4, #new-token: 3584, #cached-token: 0, full token usage: 0.76, mamba usage: 0.19, #running-req: 98, #queue-req: 26,
[2026-01-17 08:41:39 TP0] Decode batch, #running-req: 98, #full token: 100480, full token usage: 0.78, mamba num: 101, mamba usage: 0.20, npu graph: False, gen throughput (token/s): 377.87, #queue-req: 26,
[2026-01-17 08:41:39] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:37<00:38,  4.46it/s][2026-01-17 08:41:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.78, mamba usage: 0.20, #running-req: 101, #queue-req: 26,
[2026-01-17 08:41:39] INFO:     127.0.0.1:57422 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:37<00:43,  3.89it/s][2026-01-17 08:41:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.78, mamba usage: 0.20, #running-req: 101, #queue-req: 26,
[2026-01-17 08:41:40] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:38<01:03,  2.65it/s][2026-01-17 08:41:40] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:38<00:53,  3.13it/s][2026-01-17 08:41:40 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.78, mamba usage: 0.20, #running-req: 101, #queue-req: 25,
[2026-01-17 08:41:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.79, mamba usage: 0.20, #running-req: 102, #queue-req: 25,
[2026-01-17 08:41:41] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:41] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:39<01:18,  2.13it/s]
 18%|█▊        | 35/200 [00:39<01:16,  2.17it/s][2026-01-17 08:41:41] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:39<01:03,  2.59it/s][2026-01-17 08:41:41 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, full token usage: 0.77, mamba usage: 0.20, #running-req: 101, #queue-req: 24,
[2026-01-17 08:41:42] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:42] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:42] INFO:     127.0.0.1:57582 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:40<01:20,  2.03it/s]
 20%|█▉        | 39/200 [00:40<00:59,  2.71it/s]
 20%|█▉        | 39/200 [00:40<00:59,  2.71it/s][2026-01-17 08:41:42 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, full token usage: 0.77, mamba usage: 0.20, #running-req: 100, #queue-req: 25,
[2026-01-17 08:41:42] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:40<01:00,  2.63it/s][2026-01-17 08:41:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.79, mamba usage: 0.20, #running-req: 102, #queue-req: 25,
[2026-01-17 08:41:43] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:41<00:59,  2.65it/s][2026-01-17 08:41:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.79, mamba usage: 0.20, #running-req: 102, #queue-req: 25,
[2026-01-17 08:41:43] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:43] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:41<01:08,  2.30it/s]
 22%|██▏       | 43/200 [00:41<01:00,  2.58it/s][2026-01-17 08:41:43] INFO:     127.0.0.1:57258 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:42<00:51,  3.02it/s][2026-01-17 08:41:43 TP0] Prefill batch, #new-seq: 3, #new-token: 2816, #cached-token: 0, full token usage: 0.77, mamba usage: 0.20, #running-req: 101, #queue-req: 24,
[2026-01-17 08:41:44] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:44] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:42<00:53,  2.87it/s]
 23%|██▎       | 46/200 [00:42<00:44,  3.47it/s][2026-01-17 08:41:44 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, full token usage: 0.78, mamba usage: 0.20, #running-req: 101, #queue-req: 25,
[2026-01-17 08:41:44] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:44] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:43<00:51,  2.94it/s]
 24%|██▍       | 48/200 [00:43<00:46,  3.25it/s][2026-01-17 08:41:44 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.78, mamba usage: 0.20, #running-req: 101, #queue-req: 25,
[2026-01-17 08:41:45] INFO:     127.0.0.1:57092 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:43<01:01,  2.45it/s][2026-01-17 08:41:45] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:45] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:45] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:43<00:51,  2.90it/s]
 26%|██▌       | 52/200 [00:43<00:21,  6.85it/s]
 26%|██▌       | 52/200 [00:43<00:21,  6.85it/s][2026-01-17 08:41:45 TP0] Prefill batch, #new-seq: 4, #new-token: 3584, #cached-token: 0, full token usage: 0.76, mamba usage: 0.19, #running-req: 102, #queue-req: 22,
[2026-01-17 08:41:46] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:46] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:44<00:24,  6.03it/s][2026-01-17 08:41:46 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.78, mamba usage: 0.20, #running-req: 101, #queue-req: 25,
[2026-01-17 08:41:46] INFO:     127.0.0.1:56918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:46] INFO:     127.0.0.1:57374 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:45<00:39,  3.64it/s]
 28%|██▊       | 56/200 [00:45<00:44,  3.21it/s][2026-01-17 08:41:47 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.78, mamba usage: 0.20, #running-req: 101, #queue-req: 25,
[2026-01-17 08:41:47] INFO:     127.0.0.1:56752 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:46<01:01,  2.33it/s][2026-01-17 08:41:47] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:47] INFO:     127.0.0.1:57480 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:46<00:51,  2.73it/s]
 30%|██▉       | 59/200 [00:46<00:35,  3.98it/s][2026-01-17 08:41:47 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, full token usage: 0.77, mamba usage: 0.20, #running-req: 102, #queue-req: 23,
[2026-01-17 08:41:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.79, mamba usage: 0.20, #running-req: 103, #queue-req: 24,
[2026-01-17 08:41:49] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:48<01:28,  1.58it/s][2026-01-17 08:41:50] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:48<01:12,  1.93it/s][2026-01-17 08:41:50 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.79, mamba usage: 0.20, #running-req: 103, #queue-req: 23,
[2026-01-17 08:41:50 TP0] Decode batch, #running-req: 103, #full token: 102656, full token usage: 0.80, mamba num: 104, mamba usage: 0.20, npu graph: False, gen throughput (token/s): 361.03, #queue-req: 23,
[2026-01-17 08:41:50] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:49<01:22,  1.67it/s][2026-01-17 08:41:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, full token usage: 0.80, mamba usage: 0.20, #running-req: 103, #queue-req: 24,
[2026-01-17 08:41:51] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:49<01:18,  1.74it/s][2026-01-17 08:41:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.80, mamba usage: 0.20, #running-req: 103, #queue-req: 24,
[2026-01-17 08:41:52] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:52] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:51<01:49,  1.24it/s]
 32%|███▎      | 65/200 [00:51<01:43,  1.30it/s][2026-01-17 08:41:53 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.80, mamba usage: 0.20, #running-req: 102, #queue-req: 24,
[2026-01-17 08:41:53] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:51<01:36,  1.39it/s][2026-01-17 08:41:53] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:51<01:20,  1.66it/s][2026-01-17 08:41:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.80, mamba usage: 0.20, #running-req: 102, #queue-req: 25,
[2026-01-17 08:41:54] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:53<01:40,  1.32it/s][2026-01-17 08:41:55] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:53<01:17,  1.70it/s][2026-01-17 08:41:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.80, mamba usage: 0.20, #running-req: 102, #queue-req: 25,
[2026-01-17 08:41:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.81, mamba usage: 0.20, #running-req: 102, #queue-req: 25,
[2026-01-17 08:41:55] INFO:     127.0.0.1:57286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:55] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:55] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:55] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:54<01:26,  1.50it/s]
 36%|███▋      | 73/200 [00:54<00:41,  3.05it/s]
 36%|███▋      | 73/200 [00:54<00:41,  3.05it/s]
 36%|███▋      | 73/200 [00:54<00:41,  3.05it/s][2026-01-17 08:41:56 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, full token usage: 0.79, mamba usage: 0.19, #running-req: 99, #queue-req: 25,
[2026-01-17 08:41:57] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:55<01:03,  2.00it/s][2026-01-17 08:41:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.81, mamba usage: 0.20, #running-req: 101, #queue-req: 24,
[2026-01-17 08:41:58] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:58] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:56<01:05,  1.90it/s]
 38%|███▊      | 76/200 [00:56<00:57,  2.17it/s][2026-01-17 08:41:58] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:56<00:48,  2.51it/s][2026-01-17 08:41:58 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, full token usage: 0.80, mamba usage: 0.19, #running-req: 100, #queue-req: 21,
[2026-01-17 08:41:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.82, mamba usage: 0.20, #running-req: 102, #queue-req: 20,
[2026-01-17 08:41:59] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:41:59] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:57<01:02,  1.96it/s]
 40%|███▉      | 79/200 [00:57<00:59,  2.03it/s][2026-01-17 08:41:59 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.81, mamba usage: 0.20, #running-req: 101, #queue-req: 18,
[2026-01-17 08:41:59] INFO:     127.0.0.1:57760 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:57<01:00,  2.00it/s][2026-01-17 08:41:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, full token usage: 0.81, mamba usage: 0.20, #running-req: 102, #queue-req: 17,
[2026-01-17 08:42:00] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:00] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:58<01:10,  1.68it/s]
 41%|████      | 82/200 [00:58<01:03,  1.86it/s][2026-01-17 08:42:00] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:58<00:52,  2.22it/s][2026-01-17 08:42:00 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, full token usage: 0.80, mamba usage: 0.20, #running-req: 101, #queue-req: 15,
[2026-01-17 08:42:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.82, mamba usage: 0.20, #running-req: 102, #queue-req: 14,
[2026-01-17 08:42:01] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:59<01:01,  1.88it/s][2026-01-17 08:42:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, full token usage: 0.82, mamba usage: 0.20, #running-req: 102, #queue-req: 13,
[2026-01-17 08:42:01 TP0] Decode batch, #running-req: 102, #full token: 104960, full token usage: 0.82, mamba num: 102, mamba usage: 0.20, npu graph: False, gen throughput (token/s): 356.70, #queue-req: 13,
[2026-01-17 08:42:01] INFO:     127.0.0.1:50900 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [01:00<00:59,  1.92it/s][2026-01-17 08:42:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.82, mamba usage: 0.20, #running-req: 102, #queue-req: 12,
[2026-01-17 08:42:02] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [01:00<00:58,  1.94it/s][2026-01-17 08:42:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.82, mamba usage: 0.20, #running-req: 102, #queue-req: 11,
[2026-01-17 08:42:02] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [01:01<00:57,  1.97it/s][2026-01-17 08:42:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.82, mamba usage: 0.20, #running-req: 102, #queue-req: 10,
[2026-01-17 08:42:03] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:03] INFO:     127.0.0.1:57806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:03] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [01:01<01:04,  1.72it/s]
 45%|████▌     | 90/200 [01:01<00:41,  2.66it/s]
 45%|████▌     | 90/200 [01:01<00:41,  2.66it/s][2026-01-17 08:42:03 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, full token usage: 0.80, mamba usage: 0.20, #running-req: 100, #queue-req: 7,
[2026-01-17 08:42:04] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [01:02<00:44,  2.46it/s][2026-01-17 08:42:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, full token usage: 0.82, mamba usage: 0.20, #running-req: 102, #queue-req: 6,
[2026-01-17 08:42:05] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [01:03<01:07,  1.60it/s][2026-01-17 08:42:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.82, mamba usage: 0.20, #running-req: 102, #queue-req: 5,
[2026-01-17 08:42:06] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:06] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [01:04<01:10,  1.52it/s]
 47%|████▋     | 94/200 [01:04<00:58,  1.83it/s][2026-01-17 08:42:06] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [01:04<00:48,  2.18it/s][2026-01-17 08:42:06 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, full token usage: 0.80, mamba usage: 0.20, #running-req: 101, #queue-req: 2,
[2026-01-17 08:42:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.82, mamba usage: 0.20, #running-req: 103, #queue-req: 1,
[2026-01-17 08:42:07] INFO:     127.0.0.1:51008 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [01:05<00:58,  1.78it/s][2026-01-17 08:42:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, full token usage: 0.82, mamba usage: 0.20, #running-req: 103, #queue-req: 0,
[2026-01-17 08:42:08] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [01:07<01:18,  1.31it/s][2026-01-17 08:42:09] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [01:07<01:00,  1.67it/s][2026-01-17 08:42:09] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [01:07<00:47,  2.13it/s][2026-01-17 08:42:09] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:09] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [01:07<00:41,  2.42it/s]
 50%|█████     | 101/200 [01:07<00:28,  3.46it/s][2026-01-17 08:42:09] INFO:     127.0.0.1:57778 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [01:07<00:24,  3.96it/s][2026-01-17 08:42:09] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [01:07<00:21,  4.52it/s][2026-01-17 08:42:09] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [01:08<00:22,  4.27it/s][2026-01-17 08:42:10] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:10] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [01:08<00:22,  4.29it/s]
 53%|█████▎    | 106/200 [01:08<00:16,  5.53it/s][2026-01-17 08:42:10] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [01:08<00:18,  5.16it/s][2026-01-17 08:42:10] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [01:08<00:19,  4.84it/s][2026-01-17 08:42:10] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [01:09<00:19,  4.65it/s][2026-01-17 08:42:11] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [01:09<00:19,  4.53it/s][2026-01-17 08:42:11] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [01:09<00:20,  4.44it/s][2026-01-17 08:42:11] INFO:     127.0.0.1:57128 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [01:09<00:20,  4.36it/s][2026-01-17 08:42:12 TP0] Decode batch, #running-req: 88, #full token: 90368, full token usage: 0.70, mamba num: 88, mamba usage: 0.17, npu graph: False, gen throughput (token/s): 396.13, #queue-req: 0,
[2026-01-17 08:42:13] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:13] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:13] INFO:     127.0.0.1:35098 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [01:11<01:01,  1.41it/s]
 57%|█████▊    | 115/200 [01:11<01:01,  1.39it/s]
 57%|█████▊    | 115/200 [01:11<01:01,  1.39it/s][2026-01-17 08:42:14] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:14] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [01:12<01:03,  1.31it/s]
 58%|█████▊    | 117/200 [01:12<00:54,  1.52it/s][2026-01-17 08:42:15] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:15] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:15] INFO:     127.0.0.1:35176 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [01:13<00:54,  1.50it/s]
 60%|██████    | 120/200 [01:13<00:32,  2.43it/s]
 60%|██████    | 120/200 [01:13<00:32,  2.43it/s][2026-01-17 08:42:15] INFO:     127.0.0.1:57144 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [01:13<00:30,  2.63it/s][2026-01-17 08:42:15] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:15] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [01:13<00:27,  2.85it/s]
 62%|██████▏   | 123/200 [01:13<00:20,  3.75it/s][2026-01-17 08:42:15] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [01:14<00:19,  3.86it/s][2026-01-17 08:42:16] INFO:     127.0.0.1:35042 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [01:14<00:18,  3.95it/s][2026-01-17 08:42:16] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [01:14<00:18,  4.02it/s][2026-01-17 08:42:16] INFO:     127.0.0.1:57540 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [01:14<00:17,  4.10it/s][2026-01-17 08:42:16] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [01:14<00:17,  4.12it/s][2026-01-17 08:42:17] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:17] INFO:     127.0.0.1:41872 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [01:15<00:21,  3.31it/s]
 65%|██████▌   | 130/200 [01:15<00:18,  3.71it/s][2026-01-17 08:42:17] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [01:16<00:25,  2.68it/s][2026-01-17 08:42:18] INFO:     127.0.0.1:35322 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [01:16<00:30,  2.20it/s][2026-01-17 08:42:19] INFO:     127.0.0.1:50886 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [01:17<00:38,  1.73it/s][2026-01-17 08:42:19] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [01:18<00:36,  1.82it/s][2026-01-17 08:42:20] INFO:     127.0.0.1:50928 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [01:18<00:38,  1.70it/s][2026-01-17 08:42:21 TP0] Decode batch, #running-req: 65, #full token: 68224, full token usage: 0.53, mamba num: 64, mamba usage: 0.12, npu graph: False, gen throughput (token/s): 326.77, #queue-req: 0,
[2026-01-17 08:42:21] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [01:19<00:39,  1.62it/s][2026-01-17 08:42:21] INFO:     127.0.0.1:35106 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [01:19<00:31,  1.98it/s][2026-01-17 08:42:22] INFO:     127.0.0.1:35302 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [01:20<00:40,  1.54it/s][2026-01-17 08:42:23] INFO:     127.0.0.1:41918 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [01:21<00:36,  1.65it/s][2026-01-17 08:42:23] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [01:21<00:29,  2.00it/s][2026-01-17 08:42:24] INFO:     127.0.0.1:35214 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [01:22<00:38,  1.54it/s][2026-01-17 08:42:24] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [01:22<00:30,  1.88it/s][2026-01-17 08:42:24] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [01:23<00:25,  2.27it/s][2026-01-17 08:42:25] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:25] INFO:     127.0.0.1:41958 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [01:24<00:36,  1.53it/s]
 72%|███████▎  | 145/200 [01:24<00:33,  1.62it/s][2026-01-17 08:42:26] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [01:24<00:31,  1.74it/s][2026-01-17 08:42:26] INFO:     127.0.0.1:35218 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [01:24<00:25,  2.05it/s][2026-01-17 08:42:26] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [01:25<00:21,  2.38it/s][2026-01-17 08:42:27] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:27] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [01:25<00:18,  2.69it/s]
 75%|███████▌  | 150/200 [01:25<00:13,  3.83it/s][2026-01-17 08:42:27] INFO:     127.0.0.1:41972 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [01:26<00:18,  2.67it/s][2026-01-17 08:42:29] INFO:     127.0.0.1:35082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:29] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [01:27<00:34,  1.40it/s]
 76%|███████▋  | 153/200 [01:27<00:36,  1.29it/s][2026-01-17 08:42:30] INFO:     127.0.0.1:41980 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [01:28<00:35,  1.31it/s][2026-01-17 08:42:30] INFO:     127.0.0.1:35040 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [01:29<00:31,  1.43it/s][2026-01-17 08:42:31 TP0] Decode batch, #running-req: 46, #full token: 48768, full token usage: 0.38, mamba num: 44, mamba usage: 0.09, npu graph: False, gen throughput (token/s): 226.99, #queue-req: 0,
[2026-01-17 08:42:31] INFO:     127.0.0.1:35240 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [01:29<00:25,  1.73it/s][2026-01-17 08:42:31] INFO:     127.0.0.1:50950 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [01:29<00:20,  2.05it/s][2026-01-17 08:42:31] INFO:     127.0.0.1:35146 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [01:30<00:20,  2.05it/s][2026-01-17 08:42:32] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:32] INFO:     127.0.0.1:35292 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [01:30<00:17,  2.40it/s]
 80%|████████  | 160/200 [01:30<00:11,  3.54it/s][2026-01-17 08:42:33] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [01:31<00:17,  2.23it/s][2026-01-17 08:42:34] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [01:32<00:24,  1.56it/s][2026-01-17 08:42:34] INFO:     127.0.0.1:35076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:42:34] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [01:32<00:19,  1.87it/s]
 82%|████████▏ | 164/200 [01:32<00:12,  2.82it/s][2026-01-17 08:42:34] INFO:     127.0.0.1:50964 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [01:32<00:11,  3.05it/s][2026-01-17 08:42:35] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [01:33<00:12,  2.73it/s][2026-01-17 08:42:38] INFO:     127.0.0.1:41924 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [01:36<00:35,  1.09s/it][2026-01-17 08:42:38] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [01:36<00:29,  1.09it/s][2026-01-17 08:42:39] INFO:     127.0.0.1:35084 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [01:37<00:26,  1.16it/s][2026-01-17 08:42:39] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [01:37<00:20,  1.47it/s][2026-01-17 08:42:39] INFO:     127.0.0.1:35330 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [01:38<00:16,  1.81it/s][2026-01-17 08:42:40] INFO:     127.0.0.1:57598 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [01:38<00:12,  2.17it/s][2026-01-17 08:42:40 TP0] Decode batch, #running-req: 28, #full token: 31616, full token usage: 0.25, mamba num: 28, mamba usage: 0.05, npu graph: False, gen throughput (token/s): 150.42, #queue-req: 0,
[2026-01-17 08:42:41] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [01:40<00:22,  1.22it/s][2026-01-17 08:42:43] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [01:41<00:25,  1.01it/s][2026-01-17 08:42:44] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [01:42<00:27,  1.11s/it][2026-01-17 08:42:45] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [01:43<00:22,  1.09it/s][2026-01-17 08:42:45] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [01:43<00:16,  1.40it/s][2026-01-17 08:42:45] INFO:     127.0.0.1:41982 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [01:44<00:14,  1.57it/s][2026-01-17 08:42:47] INFO:     127.0.0.1:35024 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [01:45<00:20,  1.00it/s][2026-01-17 08:42:49] INFO:     127.0.0.1:41946 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [01:47<00:23,  1.18s/it][2026-01-17 08:42:49 TP0] Decode batch, #running-req: 20, #full token: 22400, full token usage: 0.17, mamba num: 19, mamba usage: 0.04, npu graph: False, gen throughput (token/s): 104.54, #queue-req: 0,
[2026-01-17 08:42:49] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [01:48<00:19,  1.03s/it][2026-01-17 08:42:51] INFO:     127.0.0.1:41934 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [01:49<00:21,  1.20s/it][2026-01-17 08:42:51] INFO:     127.0.0.1:35128 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [01:50<00:16,  1.02it/s][2026-01-17 08:42:54] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [01:52<00:22,  1.38s/it][2026-01-17 08:42:55] INFO:     127.0.0.1:35262 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [01:53<00:19,  1.32s/it][2026-01-17 08:42:55] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [01:54<00:14,  1.07s/it][2026-01-17 08:42:58] INFO:     127.0.0.1:41902 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [01:56<00:19,  1.51s/it][2026-01-17 08:42:59 TP0] Decode batch, #running-req: 13, #full token: 16000, full token usage: 0.12, mamba num: 13, mamba usage: 0.03, npu graph: False, gen throughput (token/s): 69.94, #queue-req: 0,
[2026-01-17 08:43:01] INFO:     127.0.0.1:41994 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [01:59<00:21,  1.82s/it][2026-01-17 08:43:01] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [01:59<00:16,  1.48s/it][2026-01-17 08:43:01] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [02:00<00:11,  1.11s/it][2026-01-17 08:43:03] INFO:     127.0.0.1:41904 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [02:01<00:10,  1.19s/it][2026-01-17 08:43:05] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [02:03<00:11,  1.38s/it][2026-01-17 08:43:08 TP0] Decode batch, #running-req: 8, #full token: 9856, full token usage: 0.08, mamba num: 8, mamba usage: 0.02, npu graph: False, gen throughput (token/s): 42.97, #queue-req: 0,
[2026-01-17 08:43:08] INFO:     127.0.0.1:41868 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [02:07<00:14,  2.12s/it][2026-01-17 08:43:09] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [02:07<00:09,  1.63s/it][2026-01-17 08:43:12] INFO:     127.0.0.1:41888 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [02:10<00:09,  1.99s/it][2026-01-17 08:43:13] INFO:     127.0.0.1:35126 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [02:11<00:07,  1.76s/it][2026-01-17 08:43:13] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [02:12<00:03,  1.31s/it][2026-01-17 08:43:15] INFO:     127.0.0.1:41880 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [02:14<00:03,  1.56s/it][2026-01-17 08:43:18] INFO:     127.0.0.1:35064 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [02:16<00:01,  1.92s/it][2026-01-17 08:43:20 TP0] Decode batch, #running-req: 1, #full token: 1280, full token usage: 0.01, mamba num: 1, mamba usage: 0.00, npu graph: False, gen throughput (token/s): 13.76, #queue-req: 0,
[2026-01-17 08:43:25 TP0] Decode batch, #running-req: 1, #full token: 1280, full token usage: 0.01, mamba num: 1, mamba usage: 0.00, npu graph: False, gen throughput (token/s): 8.10, #queue-req: 0,
[2026-01-17 08:43:25] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [02:24<00:00,  3.53s/it]
100%|██████████| 200/200 [02:24<00:00,  1.39it/s]
.
----------------------------------------------------------------------
Ran 1 test in 509.358s

OK
Accuracy: 0.950
Invalid: 0.000
Latency: 147.776 s
Output throughput: 217.856 token/s
metrics={'accuracy': np.float64(0.95), 'invalid': np.float64(0.0), 'latency': 147.77630304999184, 'output_throughput': 217.8563094050943}
.
.
End (18/62):
filename='ascend/llm_models/test_qwen3_next_models.py', elapsed=520, estimated_time=400
.
.

.
.
Begin (19/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_qwen3_30B_models.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:43:46] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.7, max_running_requests=32, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=773417889, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=8, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:43:46] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:43:55 TP0] Init torch distributed begin.
[2026-01-17 08:43:55 TP1] Init torch distributed begin.
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2026-01-17 08:44:00 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:44:00 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:44:01 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:44:01 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:44:02 TP0] Load weight begin. avail mem=60.84 GB
[2026-01-17 08:44:02 TP1] Load weight begin. avail mem=61.12 GB

Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   6% Completed | 1/16 [00:05<01:15,  5.06s/it]

Loading safetensors checkpoint shards:  12% Completed | 2/16 [00:10<01:12,  5.18s/it]

Loading safetensors checkpoint shards:  19% Completed | 3/16 [00:15<01:06,  5.11s/it]

Loading safetensors checkpoint shards:  25% Completed | 4/16 [00:20<01:01,  5.09s/it]

Loading safetensors checkpoint shards:  31% Completed | 5/16 [00:25<00:55,  5.04s/it]

Loading safetensors checkpoint shards:  38% Completed | 6/16 [00:26<00:37,  3.76s/it]

Loading safetensors checkpoint shards:  44% Completed | 7/16 [00:31<00:36,  4.07s/it]

Loading safetensors checkpoint shards:  50% Completed | 8/16 [00:37<00:36,  4.59s/it]

Loading safetensors checkpoint shards:  56% Completed | 9/16 [00:42<00:34,  4.92s/it]

Loading safetensors checkpoint shards:  62% Completed | 10/16 [00:47<00:30,  5.02s/it]

Loading safetensors checkpoint shards:  69% Completed | 11/16 [00:53<00:25,  5.04s/it]

Loading safetensors checkpoint shards:  75% Completed | 12/16 [00:58<00:20,  5.07s/it]

Loading safetensors checkpoint shards:  81% Completed | 13/16 [01:03<00:15,  5.09s/it]

Loading safetensors checkpoint shards:  88% Completed | 14/16 [01:08<00:10,  5.23s/it]

Loading safetensors checkpoint shards:  94% Completed | 15/16 [01:14<00:05,  5.47s/it]

Loading safetensors checkpoint shards: 100% Completed | 16/16 [01:20<00:00,  5.52s/it]

Loading safetensors checkpoint shards: 100% Completed | 16/16 [01:20<00:00,  5.03s/it]

[2026-01-17 08:45:35 TP0] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=32.10 GB, mem usage=28.74 GB.
[2026-01-17 08:45:36 TP1] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=32.38 GB, mem usage=28.74 GB.
[2026-01-17 08:45:36 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:45:36 TP0] The available memory for KV cache is 13.84 GB.
[2026-01-17 08:45:36 TP1] The available memory for KV cache is 13.84 GB.
[2026-01-17 08:45:36 TP0] KV Cache is allocated. #tokens: 302336, K size: 6.92 GB, V size: 6.92 GB
[2026-01-17 08:45:36 TP0] Memory pool end. avail mem=18.22 GB
[2026-01-17 08:45:36 TP1] KV Cache is allocated. #tokens: 302336, K size: 6.92 GB, V size: 6.92 GB
[2026-01-17 08:45:36 TP1] Memory pool end. avail mem=18.50 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:45:37 TP0] max_total_num_tokens=302336, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=32, context_len=262144, available_gpu_mem=18.22 GB
[2026-01-17 08:45:38] INFO:     Started server process [74327]
[2026-01-17 08:45:38] INFO:     Waiting for application startup.
[2026-01-17 08:45:38] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 08:45:38] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 08:45:38] INFO:     Application startup complete.
[2026-01-17 08:45:38] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:45:39] INFO:     127.0.0.1:37766 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:45:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank1]:[W117 08:45:39.214538022 compiler_depend.ts:3136] Warning: The indexFromRank 1is not equal indexFromCurDevice 9 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[rank0]:[W117 08:45:39.220707280 compiler_depend.ts:3136] Warning: The indexFromRank 0is not equal indexFromCurDevice 8 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
..[2026-01-17 08:45:46] INFO:     127.0.0.1:37794 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:45:50] INFO:     127.0.0.1:37782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:45:50] The server is fired up and ready to roll!
[2026-01-17 08:45:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:45:57] INFO:     127.0.0.1:55854 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:45:57] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:45:57] INFO:     127.0.0.1:38464 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:45:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:45:57] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507 --trust-remote-code --mem-fraction-static 0.7 --max-running-requests 32 --attention-backend ascend --disable-cuda-graph --cuda-graph-max-bs 32 --tp-size 2 --base-gpu-id 8 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestAscendTp4Bf16.test_a_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:45:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:45:57 TP0] Prefill batch, #new-seq: 31, #new-token: 4224, #cached-token: 23808, token usage: 0.00, #running-req: 1, #queue-req: 4,
[2026-01-17 08:46:04 TP0] Decode batch, #running-req: 32, #token: 5248, token usage: 0.02, npu graph: False, gen throughput (token/s): 7.74, #queue-req: 96,
[2026-01-17 08:46:05] INFO:     127.0.0.1:38488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:05] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:07<25:58,  7.83s/it]
  1%|          | 2/200 [00:07<16:16,  4.93s/it][2026-01-17 08:46:05 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-17 08:46:06] INFO:     127.0.0.1:38478 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:08<11:52,  3.62s/it][2026-01-17 08:46:06 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:07 TP0] Decode batch, #running-req: 32, #token: 8448, token usage: 0.03, npu graph: False, gen throughput (token/s): 406.77, #queue-req: 96,
[2026-01-17 08:46:07] INFO:     127.0.0.1:38612 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:10<09:13,  2.82s/it][2026-01-17 08:46:08] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:08 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:08 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:08] INFO:     127.0.0.1:38672 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:10<04:56,  1.53s/it][2026-01-17 08:46:08 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:08] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:10<03:57,  1.23s/it][2026-01-17 08:46:08 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:10] INFO:     127.0.0.1:38728 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:12<04:11,  1.31s/it][2026-01-17 08:46:10] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:10] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:12<02:39,  1.19it/s][2026-01-17 08:46:10 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:10] INFO:     127.0.0.1:38622 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:13<02:08,  1.47it/s][2026-01-17 08:46:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:10] INFO:     127.0.0.1:38504 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:13<01:43,  1.82it/s][2026-01-17 08:46:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:11 TP0] Decode batch, #running-req: 32, #token: 7552, token usage: 0.02, npu graph: False, gen throughput (token/s): 338.11, #queue-req: 96,
[2026-01-17 08:46:11] INFO:     127.0.0.1:38516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:11] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:13<01:52,  1.66it/s]
  7%|▋         | 14/200 [00:13<01:33,  1.98it/s][2026-01-17 08:46:11 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-17 08:46:12] INFO:     127.0.0.1:38778 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:14<01:28,  2.09it/s][2026-01-17 08:46:12 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:12] INFO:     127.0.0.1:38562 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:14<01:36,  1.91it/s][2026-01-17 08:46:12 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:13] INFO:     127.0.0.1:38684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:13] INFO:     127.0.0.1:38758 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:15<01:43,  1.77it/s]
  9%|▉         | 18/200 [00:15<01:25,  2.14it/s][2026-01-17 08:46:13 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-17 08:46:14] INFO:     127.0.0.1:38578 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:16<01:49,  1.65it/s][2026-01-17 08:46:14 TP0] Decode batch, #running-req: 32, #token: 8320, token usage: 0.03, npu graph: False, gen throughput (token/s): 385.21, #queue-req: 96,
[2026-01-17 08:46:14 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:14] INFO:     127.0.0.1:38532 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:16<01:28,  2.03it/s][2026-01-17 08:46:14 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:15] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:17<01:23,  2.15it/s][2026-01-17 08:46:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:15] INFO:     127.0.0.1:38580 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:17<01:14,  2.38it/s][2026-01-17 08:46:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:16] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:16] INFO:     127.0.0.1:38788 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:18<01:27,  2.02it/s]
 12%|█▏        | 24/200 [00:18<01:15,  2.34it/s][2026-01-17 08:46:16] INFO:     127.0.0.1:38616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:16 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-17 08:46:16 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:16] INFO:     127.0.0.1:38706 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:18<00:55,  3.12it/s][2026-01-17 08:46:16 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:17] INFO:     127.0.0.1:38856 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:19<01:14,  2.32it/s][2026-01-17 08:46:17 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:17] INFO:     127.0.0.1:38692 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:19<01:11,  2.39it/s][2026-01-17 08:46:17 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:18 TP0] Decode batch, #running-req: 32, #token: 8576, token usage: 0.03, npu graph: False, gen throughput (token/s): 340.74, #queue-req: 96,
[2026-01-17 08:46:18] INFO:     127.0.0.1:38792 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:20<01:29,  1.91it/s][2026-01-17 08:46:18 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:19] INFO:     127.0.0.1:38884 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:21<01:42,  1.65it/s][2026-01-17 08:46:19 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:19] INFO:     127.0.0.1:38782 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:21<01:34,  1.79it/s][2026-01-17 08:46:19] INFO:     127.0.0.1:38826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:19 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:19 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:19] INFO:     127.0.0.1:38810 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:22<01:03,  2.63it/s][2026-01-17 08:46:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:20] INFO:     127.0.0.1:38490 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:22<00:55,  3.02it/s][2026-01-17 08:46:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:20] INFO:     127.0.0.1:38904 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:23<01:08,  2.39it/s][2026-01-17 08:46:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:20] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:23<00:56,  2.89it/s][2026-01-17 08:46:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:21] INFO:     127.0.0.1:38594 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:23<00:55,  2.95it/s][2026-01-17 08:46:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:21] INFO:     127.0.0.1:39010 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:23<00:47,  3.44it/s][2026-01-17 08:46:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:22 TP0] Decode batch, #running-req: 32, #token: 8576, token usage: 0.03, npu graph: False, gen throughput (token/s): 331.97, #queue-req: 96,
[2026-01-17 08:46:22] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:24<01:25,  1.88it/s][2026-01-17 08:46:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:23] INFO:     127.0.0.1:38636 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:25<01:21,  1.96it/s][2026-01-17 08:46:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:23] INFO:     127.0.0.1:38528 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:25<01:25,  1.86it/s][2026-01-17 08:46:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:24] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:26<01:17,  2.05it/s][2026-01-17 08:46:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:24] INFO:     127.0.0.1:38938 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:26<01:09,  2.27it/s][2026-01-17 08:46:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:24] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:26<01:08,  2.27it/s][2026-01-17 08:46:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:25 TP0] Decode batch, #running-req: 32, #token: 8704, token usage: 0.03, npu graph: False, gen throughput (token/s): 363.41, #queue-req: 96,
[2026-01-17 08:46:25] INFO:     127.0.0.1:39054 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:27<01:28,  1.76it/s][2026-01-17 08:46:25] INFO:     127.0.0.1:38592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:25] INFO:     127.0.0.1:38966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:25] INFO:     127.0.0.1:38980 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:28<00:52,  2.91it/s]
 24%|██▍       | 48/200 [00:28<00:30,  5.02it/s][2026-01-17 08:46:25 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.03, #running-req: 29, #queue-req: 96,
[2026-01-17 08:46:26] INFO:     127.0.0.1:39056 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:28<00:49,  3.02it/s][2026-01-17 08:46:26 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:27] INFO:     127.0.0.1:38804 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:29<00:51,  2.89it/s][2026-01-17 08:46:27 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:27] INFO:     127.0.0.1:39012 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:29<01:01,  2.43it/s][2026-01-17 08:46:27 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:28] INFO:     127.0.0.1:38890 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:30<01:01,  2.42it/s][2026-01-17 08:46:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:28] INFO:     127.0.0.1:38918 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:30<00:56,  2.59it/s][2026-01-17 08:46:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:29] INFO:     127.0.0.1:39024 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:31<01:09,  2.09it/s][2026-01-17 08:46:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:29 TP0] Decode batch, #running-req: 32, #token: 8192, token usage: 0.03, npu graph: False, gen throughput (token/s): 330.99, #queue-req: 96,
[2026-01-17 08:46:29] INFO:     127.0.0.1:38902 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:32<01:18,  1.85it/s][2026-01-17 08:46:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:30] INFO:     127.0.0.1:38842 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:32<01:09,  2.07it/s][2026-01-17 08:46:30 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:30] INFO:     127.0.0.1:38950 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:32<01:11,  2.00it/s][2026-01-17 08:46:30 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:31] INFO:     127.0.0.1:39026 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:33<01:20,  1.77it/s][2026-01-17 08:46:31] INFO:     127.0.0.1:39038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:31] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:33<00:48,  2.91it/s][2026-01-17 08:46:31 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-17 08:46:31] INFO:     127.0.0.1:39112 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:34<00:41,  3.34it/s][2026-01-17 08:46:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:32] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:34<00:59,  2.33it/s][2026-01-17 08:46:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:32] INFO:     127.0.0.1:38868 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:34<00:48,  2.81it/s][2026-01-17 08:46:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:33] INFO:     127.0.0.1:39172 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:35<00:46,  2.91it/s][2026-01-17 08:46:33 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:33 TP0] Decode batch, #running-req: 32, #token: 7808, token usage: 0.03, npu graph: False, gen throughput (token/s): 323.79, #queue-req: 96,
[2026-01-17 08:46:33] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:35<00:55,  2.45it/s][2026-01-17 08:46:33 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:33] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:36<00:45,  2.97it/s][2026-01-17 08:46:33 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:34] INFO:     127.0.0.1:39074 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:36<00:50,  2.62it/s][2026-01-17 08:46:34] INFO:     127.0.0.1:39216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:34 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:34 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:34] INFO:     127.0.0.1:39228 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:37<00:46,  2.84it/s][2026-01-17 08:46:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:35] INFO:     127.0.0.1:39188 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:37<00:45,  2.87it/s][2026-01-17 08:46:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:35] INFO:     127.0.0.1:39160 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:37<00:46,  2.79it/s][2026-01-17 08:46:35 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:36] INFO:     127.0.0.1:38932 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:38<00:58,  2.20it/s][2026-01-17 08:46:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-17 08:46:36] INFO:     127.0.0.1:39152 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:38<00:48,  2.64it/s][2026-01-17 08:46:36 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 95,
[2026-01-17 08:46:36] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:38<00:40,  3.14it/s][2026-01-17 08:46:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 94,
[2026-01-17 08:46:37] INFO:     127.0.0.1:38990 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:39<00:46,  2.72it/s][2026-01-17 08:46:37 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 93,
[2026-01-17 08:46:37] INFO:     127.0.0.1:39058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:37] INFO:     127.0.0.1:39088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:37] INFO:     127.0.0.1:39320 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [00:39<00:38,  3.20it/s]
 39%|███▉      | 78/200 [00:39<00:16,  7.33it/s]
 39%|███▉      | 78/200 [00:39<00:16,  7.33it/s][2026-01-17 08:46:37 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.02, #running-req: 29, #queue-req: 90,
[2026-01-17 08:46:37 TP0] Decode batch, #running-req: 29, #token: 7168, token usage: 0.02, npu graph: False, gen throughput (token/s): 305.38, #queue-req: 90,
[2026-01-17 08:46:37] INFO:     127.0.0.1:39302 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:39<00:17,  7.02it/s][2026-01-17 08:46:37 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 89,
[2026-01-17 08:46:38] INFO:     127.0.0.1:39282 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:40<00:25,  4.66it/s][2026-01-17 08:46:38 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 88,
[2026-01-17 08:46:38] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:40<00:24,  4.87it/s][2026-01-17 08:46:38 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 87,
[2026-01-17 08:46:38] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:40<00:35,  3.36it/s][2026-01-17 08:46:38 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 86,
[2026-01-17 08:46:39] INFO:     127.0.0.1:39266 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:41<00:38,  3.08it/s][2026-01-17 08:46:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 85,
[2026-01-17 08:46:40] INFO:     127.0.0.1:39336 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:42<01:04,  1.81it/s][2026-01-17 08:46:40 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 84,
[2026-01-17 08:46:40] INFO:     127.0.0.1:39314 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:42<00:51,  2.25it/s][2026-01-17 08:46:40 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 83,
[2026-01-17 08:46:40] INFO:     127.0.0.1:39286 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:43<00:46,  2.44it/s][2026-01-17 08:46:40 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 82,
[2026-01-17 08:46:41] INFO:     127.0.0.1:39394 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:43<00:43,  2.60it/s][2026-01-17 08:46:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 81,
[2026-01-17 08:46:41 TP0] Decode batch, #running-req: 32, #token: 7552, token usage: 0.02, npu graph: False, gen throughput (token/s): 310.11, #queue-req: 81,
[2026-01-17 08:46:42] INFO:     127.0.0.1:39392 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:44<00:59,  1.87it/s][2026-01-17 08:46:42 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 80,
[2026-01-17 08:46:42] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:44<00:58,  1.90it/s][2026-01-17 08:46:42 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 79,
[2026-01-17 08:46:44] INFO:     127.0.0.1:39542 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:47<01:54,  1.04s/it][2026-01-17 08:46:44] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:44 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 78,
[2026-01-17 08:46:44] INFO:     127.0.0.1:39484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:45 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 76,
[2026-01-17 08:46:45 TP0] Decode batch, #running-req: 30, #token: 8448, token usage: 0.03, npu graph: False, gen throughput (token/s): 364.92, #queue-req: 76,
[2026-01-17 08:46:45] INFO:     127.0.0.1:39382 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:47<00:58,  1.81it/s][2026-01-17 08:46:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 75,
[2026-01-17 08:46:45] INFO:     127.0.0.1:39252 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:48<00:57,  1.86it/s][2026-01-17 08:46:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 74,
[2026-01-17 08:46:45] INFO:     127.0.0.1:39362 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [00:48<00:47,  2.22it/s][2026-01-17 08:46:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 73,
[2026-01-17 08:46:46] INFO:     127.0.0.1:39346 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:48<00:43,  2.39it/s][2026-01-17 08:46:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 72,
[2026-01-17 08:46:46] INFO:     127.0.0.1:39218 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:49<00:46,  2.22it/s][2026-01-17 08:46:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 71,
[2026-01-17 08:46:47] INFO:     127.0.0.1:39426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:47] INFO:     127.0.0.1:39534 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:49<00:48,  2.09it/s]
 50%|████▉     | 99/200 [00:49<00:39,  2.57it/s][2026-01-17 08:46:47 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 69,
[2026-01-17 08:46:47] INFO:     127.0.0.1:39370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:47] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:50<00:40,  2.48it/s]
 50%|█████     | 101/200 [00:50<00:32,  3.02it/s][2026-01-17 08:46:47] INFO:     127.0.0.1:39386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:47 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 30, #queue-req: 67,
[2026-01-17 08:46:48 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 66,
[2026-01-17 08:46:48 TP0] Decode batch, #running-req: 32, #token: 7552, token usage: 0.02, npu graph: False, gen throughput (token/s): 332.95, #queue-req: 66,
[2026-01-17 08:46:48] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:51<00:40,  2.39it/s][2026-01-17 08:46:49 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 65,
[2026-01-17 08:46:49] INFO:     127.0.0.1:39486 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:51<00:41,  2.33it/s][2026-01-17 08:46:49 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 64,
[2026-01-17 08:46:50] INFO:     127.0.0.1:39476 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:52<00:47,  2.00it/s][2026-01-17 08:46:50 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 63,
[2026-01-17 08:46:50] INFO:     127.0.0.1:39410 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:52<00:44,  2.11it/s][2026-01-17 08:46:50 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 62,
[2026-01-17 08:46:50] INFO:     127.0.0.1:39446 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:52<00:36,  2.55it/s][2026-01-17 08:46:50 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 61,
[2026-01-17 08:46:51] INFO:     127.0.0.1:39560 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:53<00:39,  2.35it/s][2026-01-17 08:46:51 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 60,
[2026-01-17 08:46:51] INFO:     127.0.0.1:39454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:51] INFO:     127.0.0.1:39620 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:54<00:44,  2.05it/s]
 55%|█████▌    | 110/200 [00:54<00:37,  2.41it/s][2026-01-17 08:46:51] INFO:     127.0.0.1:39602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:51 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 30, #queue-req: 58,
[2026-01-17 08:46:52 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 57,
[2026-01-17 08:46:52] INFO:     127.0.0.1:39470 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:54<00:32,  2.75it/s][2026-01-17 08:46:52 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 56,
[2026-01-17 08:46:52 TP0] Decode batch, #running-req: 32, #token: 8064, token usage: 0.03, npu graph: False, gen throughput (token/s): 334.15, #queue-req: 56,
[2026-01-17 08:46:53] INFO:     127.0.0.1:39636 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:55<00:37,  2.32it/s][2026-01-17 08:46:53 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 55,
[2026-01-17 08:46:53] INFO:     127.0.0.1:39570 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:55<00:37,  2.28it/s][2026-01-17 08:46:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 54,
[2026-01-17 08:46:53] INFO:     127.0.0.1:39490 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:56<00:34,  2.48it/s][2026-01-17 08:46:53] INFO:     127.0.0.1:39430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 53,
[2026-01-17 08:46:53] INFO:     127.0.0.1:39616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:54 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 51,
[2026-01-17 08:46:54] INFO:     127.0.0.1:39450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:54] INFO:     127.0.0.1:39558 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:56<00:25,  3.19it/s]
 60%|█████▉    | 119/200 [00:56<00:19,  4.11it/s][2026-01-17 08:46:54 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 49,
[2026-01-17 08:46:55] INFO:     127.0.0.1:39626 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:57<00:21,  3.65it/s][2026-01-17 08:46:55] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:55 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 48,
[2026-01-17 08:46:55 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 47,
[2026-01-17 08:46:56] INFO:     127.0.0.1:39658 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:58<00:26,  2.94it/s][2026-01-17 08:46:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 46,
[2026-01-17 08:46:56 TP0] Decode batch, #running-req: 32, #token: 8576, token usage: 0.03, npu graph: False, gen throughput (token/s): 348.44, #queue-req: 46,
[2026-01-17 08:46:56] INFO:     127.0.0.1:39588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:56] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:59<00:32,  2.39it/s]
 62%|██████▏   | 124/200 [00:59<00:31,  2.45it/s][2026-01-17 08:46:56 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 44,
[2026-01-17 08:46:56] INFO:     127.0.0.1:59840 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:59<00:26,  2.79it/s][2026-01-17 08:46:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 43,
[2026-01-17 08:46:57] INFO:     127.0.0.1:39416 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:59<00:29,  2.51it/s][2026-01-17 08:46:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 42,
[2026-01-17 08:46:57] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [01:00<00:28,  2.55it/s][2026-01-17 08:46:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 41,
[2026-01-17 08:46:58] INFO:     127.0.0.1:39298 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [01:00<00:29,  2.46it/s][2026-01-17 08:46:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 40,
[2026-01-17 08:46:58] INFO:     127.0.0.1:39496 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [01:00<00:27,  2.60it/s][2026-01-17 08:46:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 39,
[2026-01-17 08:46:58] INFO:     127.0.0.1:39590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:46:58] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [01:00<00:22,  3.11it/s]
 66%|██████▌   | 131/200 [01:00<00:14,  4.65it/s][2026-01-17 08:46:58 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 30, #queue-req: 37,
[2026-01-17 08:46:59] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [01:01<00:20,  3.27it/s][2026-01-17 08:46:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 36,
[2026-01-17 08:46:59] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [01:01<00:18,  3.67it/s][2026-01-17 08:46:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 31, #queue-req: 35,
[2026-01-17 08:47:00 TP0] Decode batch, #running-req: 32, #token: 8064, token usage: 0.03, npu graph: False, gen throughput (token/s): 337.80, #queue-req: 35,
[2026-01-17 08:47:00] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:00] INFO:     127.0.0.1:59990 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [01:02<00:34,  1.90it/s]
 68%|██████▊   | 135/200 [01:02<00:36,  1.78it/s][2026-01-17 08:47:00 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 33,
[2026-01-17 08:47:02] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [01:04<00:46,  1.36it/s][2026-01-17 08:47:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 32,
[2026-01-17 08:47:03 TP0] Decode batch, #running-req: 32, #token: 9472, token usage: 0.03, npu graph: False, gen throughput (token/s): 400.75, #queue-req: 32,
[2026-01-17 08:47:03] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [01:06<01:02,  1.01it/s][2026-01-17 08:47:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 31,
[2026-01-17 08:47:04] INFO:     127.0.0.1:59910 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [01:06<00:53,  1.16it/s][2026-01-17 08:47:04 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 30,
[2026-01-17 08:47:04] INFO:     127.0.0.1:60018 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [01:06<00:40,  1.49it/s][2026-01-17 08:47:04 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 29,
[2026-01-17 08:47:04] INFO:     127.0.0.1:39510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:04] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [01:07<00:36,  1.63it/s]
 70%|███████   | 141/200 [01:07<00:26,  2.26it/s][2026-01-17 08:47:05 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 27,
[2026-01-17 08:47:05] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [01:07<00:29,  1.99it/s][2026-01-17 08:47:05 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 26,
[2026-01-17 08:47:06] INFO:     127.0.0.1:50982 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [01:08<00:33,  1.70it/s][2026-01-17 08:47:06 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 25,
[2026-01-17 08:47:06 TP0] Decode batch, #running-req: 32, #token: 8448, token usage: 0.03, npu graph: False, gen throughput (token/s): 357.93, #queue-req: 25,
[2026-01-17 08:47:06] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:06] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [01:09<00:30,  1.84it/s]
 72%|███████▎  | 145/200 [01:09<00:21,  2.51it/s][2026-01-17 08:47:06] INFO:     127.0.0.1:59974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:06 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 23,
[2026-01-17 08:47:07 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 22,
[2026-01-17 08:47:07] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [01:09<00:18,  2.81it/s][2026-01-17 08:47:07 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 21,
[2026-01-17 08:47:07] INFO:     127.0.0.1:39586 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [01:10<00:19,  2.72it/s][2026-01-17 08:47:07 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 20,
[2026-01-17 08:47:09] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [01:11<00:30,  1.65it/s][2026-01-17 08:47:09] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:09 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 19,
[2026-01-17 08:47:09] INFO:     127.0.0.1:51092 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [01:11<00:19,  2.53it/s][2026-01-17 08:47:09 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 17,
[2026-01-17 08:47:10] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [01:12<00:21,  2.28it/s][2026-01-17 08:47:10 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 16,
[2026-01-17 08:47:10] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [01:12<00:22,  2.12it/s][2026-01-17 08:47:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 15,
[2026-01-17 08:47:10 TP0] Decode batch, #running-req: 31, #token: 9344, token usage: 0.03, npu graph: False, gen throughput (token/s): 328.54, #queue-req: 15,
[2026-01-17 08:47:11] INFO:     127.0.0.1:39656 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [01:13<00:21,  2.11it/s][2026-01-17 08:47:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 14,
[2026-01-17 08:47:11] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:11] INFO:     127.0.0.1:51036 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [01:13<00:22,  2.01it/s]
 78%|███████▊  | 156/200 [01:13<00:17,  2.49it/s][2026-01-17 08:47:11 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 12,
[2026-01-17 08:47:11] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [01:14<00:14,  2.88it/s][2026-01-17 08:47:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 11,
[2026-01-17 08:47:12] INFO:     127.0.0.1:39644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:12] INFO:     127.0.0.1:51028 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [01:14<00:16,  2.53it/s]
 80%|███████▉  | 159/200 [01:14<00:14,  2.92it/s][2026-01-17 08:47:12 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 9,
[2026-01-17 08:47:12] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:12] INFO:     127.0.0.1:51042 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [01:14<00:14,  2.81it/s]
 80%|████████  | 161/200 [01:14<00:11,  3.40it/s][2026-01-17 08:47:12 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 30, #queue-req: 7,
[2026-01-17 08:47:13] INFO:     127.0.0.1:59992 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [01:15<00:11,  3.18it/s][2026-01-17 08:47:13 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 6,
[2026-01-17 08:47:14 TP0] Decode batch, #running-req: 32, #token: 8448, token usage: 0.03, npu graph: False, gen throughput (token/s): 345.64, #queue-req: 6,
[2026-01-17 08:47:14] INFO:     127.0.0.1:59882 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [01:17<00:24,  1.50it/s][2026-01-17 08:47:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 5,
[2026-01-17 08:47:15] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [01:17<00:24,  1.48it/s][2026-01-17 08:47:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 4,
[2026-01-17 08:47:15] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [01:18<00:18,  1.85it/s][2026-01-17 08:47:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 31, #queue-req: 3,
[2026-01-17 08:47:16] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:16] INFO:     127.0.0.1:51166 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:16] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [01:18<00:20,  1.64it/s]
 84%|████████▍ | 168/200 [01:18<00:12,  2.55it/s]
 84%|████████▍ | 168/200 [01:18<00:12,  2.55it/s][2026-01-17 08:47:16 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.03, #running-req: 29, #queue-req: 0,
[2026-01-17 08:47:16] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [01:18<00:10,  2.85it/s][2026-01-17 08:47:16] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [01:19<00:09,  3.07it/s][2026-01-17 08:47:17] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [01:19<00:09,  2.94it/s][2026-01-17 08:47:17 TP0] Decode batch, #running-req: 29, #token: 7680, token usage: 0.03, npu graph: False, gen throughput (token/s): 356.54, #queue-req: 0,
[2026-01-17 08:47:18] INFO:     127.0.0.1:51082 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [01:21<00:17,  1.64it/s][2026-01-17 08:47:19] INFO:     127.0.0.1:51130 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [01:21<00:13,  1.96it/s][2026-01-17 08:47:19] INFO:     127.0.0.1:50702 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [01:21<00:10,  2.41it/s][2026-01-17 08:47:19] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [01:21<00:08,  2.92it/s][2026-01-17 08:47:20] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [01:22<00:11,  2.13it/s][2026-01-17 08:47:20] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [01:22<00:08,  2.64it/s][2026-01-17 08:47:20] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:20] INFO:     127.0.0.1:51116 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [01:23<00:10,  2.20it/s]
 90%|████████▉ | 179/200 [01:23<00:08,  2.55it/s][2026-01-17 08:47:21 TP0] Decode batch, #running-req: 21, #token: 6912, token usage: 0.02, npu graph: False, gen throughput (token/s): 329.58, #queue-req: 0,
[2026-01-17 08:47:21] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [01:23<00:08,  2.32it/s][2026-01-17 08:47:21] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [01:24<00:08,  2.34it/s][2026-01-17 08:47:22] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [01:24<00:06,  2.65it/s][2026-01-17 08:47:22] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [01:24<00:07,  2.32it/s][2026-01-17 08:47:23] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [01:25<00:07,  2.12it/s][2026-01-17 08:47:23] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [01:25<00:07,  2.08it/s][2026-01-17 08:47:24] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [01:26<00:06,  2.30it/s][2026-01-17 08:47:24 TP0] Decode batch, #running-req: 14, #token: 5248, token usage: 0.02, npu graph: False, gen throughput (token/s): 218.34, #queue-req: 0,
[2026-01-17 08:47:25] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [01:27<00:08,  1.45it/s][2026-01-17 08:47:26] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [01:28<00:09,  1.25it/s][2026-01-17 08:47:26] INFO:     127.0.0.1:50738 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [01:29<00:07,  1.41it/s][2026-01-17 08:47:27] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:47:27 TP0] Decode batch, #running-req: 10, #token: 4352, token usage: 0.01, npu graph: False, gen throughput (token/s): 156.05, #queue-req: 0,
[2026-01-17 08:47:27] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [01:29<00:05,  1.75it/s][2026-01-17 08:47:28] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [01:30<00:04,  1.82it/s][2026-01-17 08:47:29] INFO:     127.0.0.1:50818 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [01:31<00:05,  1.38it/s][2026-01-17 08:47:30 TP0] Decode batch, #running-req: 7, #token: 3840, token usage: 0.01, npu graph: False, gen throughput (token/s): 96.57, #queue-req: 0,
[2026-01-17 08:47:32] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [01:35<00:08,  1.46s/it][2026-01-17 08:47:33] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [01:36<00:06,  1.30s/it][2026-01-17 08:47:34 TP0] Decode batch, #running-req: 5, #token: 3072, token usage: 0.01, npu graph: False, gen throughput (token/s): 81.04, #queue-req: 0,
[2026-01-17 08:47:35] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [01:37<00:05,  1.39s/it][2026-01-17 08:47:36] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [01:39<00:04,  1.41s/it][2026-01-17 08:47:37 TP0] Decode batch, #running-req: 3, #token: 2176, token usage: 0.01, npu graph: False, gen throughput (token/s): 53.08, #queue-req: 0,
[2026-01-17 08:47:39] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [01:41<00:03,  1.64s/it][2026-01-17 08:47:40] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [01:42<00:01,  1.47s/it][2026-01-17 08:47:40 TP0] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, npu graph: False, gen throughput (token/s): 29.80, #queue-req: 0,
[2026-01-17 08:47:43 TP0] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 12.54, #queue-req: 0,
[2026-01-17 08:47:46 TP0] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 12.57, #queue-req: 0,
[2026-01-17 08:47:50 TP0] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 12.67, #queue-req: 0,
[2026-01-17 08:47:53 TP0] Decode batch, #running-req: 1, #token: 1536, token usage: 0.01, npu graph: False, gen throughput (token/s): 13.00, #queue-req: 0,
[2026-01-17 08:47:53] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [01:55<00:00,  4.96s/it]
100%|██████████| 200/200 [01:55<00:00,  1.73it/s]
.
----------------------------------------------------------------------
Ran 1 test in 257.060s

OK
Accuracy: 0.960
Invalid: 0.000
Latency: 115.690 s
Output throughput: 256.712 token/s
.
.
End (19/62):
filename='ascend/llm_models/test_qwen3_30B_models.py', elapsed=267, estimated_time=400
.
.

.
.
Begin (20/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_qwen3_0_6B.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:48:13] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.837, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=256, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=737693092, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:48:13] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:48:23] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 08:48:24] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:48:24] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 08:48:25] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:48:25] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.94s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.94s/it]

[2026-01-17 08:48:27] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=59.56 GB, mem usage=1.25 GB.
[2026-01-17 08:48:27] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:48:27] The available memory for KV cache is 49.65 GB.
[2026-01-17 08:48:28] KV Cache is allocated. #tokens: 464768, K size: 24.83 GB, V size: 24.83 GB
[2026-01-17 08:48:28] Memory pool end. avail mem=9.27 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:48:29] max_total_num_tokens=464768, chunked_prefill_size=256, max_prefill_tokens=16384, max_running_requests=4096, context_len=40960, available_gpu_mem=9.25 GB
[2026-01-17 08:48:30] INFO:     Started server process [78727]
[2026-01-17 08:48:30] INFO:     Waiting for application startup.
[2026-01-17 08:48:30] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-17 08:48:30] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-17 08:48:30] INFO:     Application startup complete.
[2026-01-17 08:48:30] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:48:31] INFO:     127.0.0.1:60062 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:48:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:48:33] INFO:     127.0.0.1:60080 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:48:43] INFO:     127.0.0.1:43814 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:48:49] INFO:     127.0.0.1:60074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:48:49] The server is fired up and ready to roll!
[2026-01-17 08:48:53] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:48:54] INFO:     127.0.0.1:49740 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:48:54] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:48:54] INFO:     127.0.0.1:49748 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:48:54] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:48:57] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:48:57] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:48:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:48:58] INFO:     127.0.0.1:49752 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B --chunked-prefill-size 256 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestQwenPPTieWeightsAccuracy.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:48:58] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:48:58] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 1, #queue-req: 9,
[2026-01-17 08:48:58] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 3, #queue-req: 19,
[2026-01-17 08:48:58] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 4, #queue-req: 30,
[2026-01-17 08:48:58] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 6, #queue-req: 40,
[2026-01-17 08:48:58] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 8, #queue-req: 50,
[2026-01-17 08:48:58] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 9, #queue-req: 49,
[2026-01-17 08:48:58] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 11, #queue-req: 47,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 13, #queue-req: 45,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 15, #queue-req: 43,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 17, #queue-req: 50,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 19, #queue-req: 59,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 21, #queue-req: 68,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 23, #queue-req: 76,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 25, #queue-req: 84,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 27, #queue-req: 93,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 29, #queue-req: 97,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 31, #queue-req: 95,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 33, #queue-req: 93,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 35, #queue-req: 91,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 37, #queue-req: 89,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 39, #queue-req: 87,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.01, #running-req: 41, #queue-req: 86,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 42, #queue-req: 84,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 44, #queue-req: 82,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 46, #queue-req: 80,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 48, #queue-req: 78,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 50, #queue-req: 76,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 52, #queue-req: 74,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 54, #queue-req: 73,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 55, #queue-req: 71,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 57, #queue-req: 69,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 59, #queue-req: 67,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 60, #queue-req: 66,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 62, #queue-req: 64,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 64, #queue-req: 62,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 66, #queue-req: 60,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 68, #queue-req: 58,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 70, #queue-req: 56,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 72, #queue-req: 54,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 74, #queue-req: 52,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 76, #queue-req: 50,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 78, #queue-req: 48,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 80, #queue-req: 46,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 82, #queue-req: 44,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 84, #queue-req: 42,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 86, #queue-req: 40,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 88, #queue-req: 38,
[2026-01-17 08:48:59] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 90, #queue-req: 36,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 92, #queue-req: 34,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 94, #queue-req: 32,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 96, #queue-req: 30,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 98, #queue-req: 28,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 100, #queue-req: 26,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 102, #queue-req: 24,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 104, #queue-req: 22,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 106, #queue-req: 20,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.03, #running-req: 108, #queue-req: 19,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 109, #queue-req: 17,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 111, #queue-req: 15,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 113, #queue-req: 13,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 115, #queue-req: 11,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 117, #queue-req: 9,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 119, #queue-req: 7,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 121, #queue-req: 5,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 123, #queue-req: 3,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 125, #queue-req: 1,
[2026-01-17 08:49:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:00] INFO:     127.0.0.1:46830 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<07:13,  2.18s/it][2026-01-17 08:49:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:01] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:01] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] INFO:     127.0.0.1:47326 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:02<02:08,  1.53it/s]
  2%|▏         | 4/200 [00:02<00:59,  3.30it/s][2026-01-17 08:49:01] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 08:49:01] INFO:     127.0.0.1:47428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:01] Decode batch, #running-req: 127, #token: 21120, token usage: 0.05, npu graph: False, gen throughput (token/s): 102.56, #queue-req: 0,
[2026-01-17 08:49:01] INFO:     127.0.0.1:46738 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:02<00:44,  4.39it/s][2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:01] INFO:     127.0.0.1:46910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:01] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:02<00:35,  5.46it/s][2026-01-17 08:49:01] INFO:     127.0.0.1:47444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:01] INFO:     127.0.0.1:47246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 08:49:01] INFO:     127.0.0.1:47172 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:02<00:23,  7.95it/s][2026-01-17 08:49:01] INFO:     127.0.0.1:47136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:01] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 08:49:01] INFO:     127.0.0.1:46660 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:03<00:17, 10.36it/s][2026-01-17 08:49:01] INFO:     127.0.0.1:46694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:01] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:01] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 08:49:01] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 130, #queue-req: 0,
[2026-01-17 08:49:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 131, #queue-req: 0,
[2026-01-17 08:49:02] INFO:     127.0.0.1:47070 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:03<00:12, 14.59it/s][2026-01-17 08:49:02] INFO:     127.0.0.1:46948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:02] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 08:49:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 08:49:02] INFO:     127.0.0.1:46990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:03<00:10, 16.20it/s]
 12%|█▏        | 23/200 [00:03<00:09, 19.29it/s][2026-01-17 08:49:02] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:49:02] INFO:     127.0.0.1:46826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:46846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:47030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:47510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 08:49:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 129, #queue-req: 4,
[2026-01-17 08:49:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 131, #queue-req: 2,
[2026-01-17 08:49:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 133, #queue-req: 0,
[2026-01-17 08:49:02] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:03<00:06, 26.07it/s]
 16%|█▌        | 32/200 [00:03<00:05, 32.08it/s][2026-01-17 08:49:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:49:02] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:02] INFO:     127.0.0.1:46744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:47312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:49:02] INFO:     127.0.0.1:47110 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:03<00:05, 27.38it/s][2026-01-17 08:49:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:02] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:47482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 125, #queue-req: 2,
[2026-01-17 08:49:02] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:03<00:05, 28.45it/s]
 20%|██        | 41/200 [00:03<00:05, 31.25it/s][2026-01-17 08:49:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 2,
[2026-01-17 08:49:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:49:02] INFO:     127.0.0.1:47160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:02] INFO:     127.0.0.1:47256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 08:49:03] Decode batch, #running-req: 128, #token: 28416, token usage: 0.06, npu graph: False, gen throughput (token/s): 2970.89, #queue-req: 0,
[2026-01-17 08:49:03] INFO:     127.0.0.1:47354 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:04<00:06, 24.45it/s][2026-01-17 08:49:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:03] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 124, #queue-req: 2,
[2026-01-17 08:49:03] INFO:     127.0.0.1:47022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,

 25%|██▌       | 50/200 [00:04<00:05, 28.87it/s]
 26%|██▌       | 51/200 [00:04<00:04, 34.93it/s][2026-01-17 08:49:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:49:03] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:03] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:49:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 08:49:03] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:04<00:05, 28.04it/s][2026-01-17 08:49:03] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:03] INFO:     127.0.0.1:47148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 08:49:03] INFO:     127.0.0.1:47622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 130, #queue-req: 2,
[2026-01-17 08:49:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 131, #queue-req: 0,
[2026-01-17 08:49:03] INFO:     127.0.0.1:46934 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:04<00:04, 30.11it/s][2026-01-17 08:49:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:03] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 08:49:03] INFO:     127.0.0.1:46748 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:04<00:04, 28.10it/s][2026-01-17 08:49:03] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 125, #queue-req: 1,
[2026-01-17 08:49:03] INFO:     127.0.0.1:46966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] INFO:     127.0.0.1:47696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:03] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 6,
[2026-01-17 08:49:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 4,
[2026-01-17 08:49:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 130, #queue-req: 2,
[2026-01-17 08:49:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 132, #queue-req: 0,
[2026-01-17 08:49:03] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:05<00:04, 29.64it/s][2026-01-17 08:49:03] INFO:     127.0.0.1:46478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47264 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:05<00:03, 35.25it/s][2026-01-17 08:49:04] INFO:     127.0.0.1:46926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:05<00:02, 41.99it/s][2026-01-17 08:49:04] INFO:     127.0.0.1:46820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47758 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:05<00:03, 34.57it/s]
 46%|████▋     | 93/200 [00:05<00:03, 32.11it/s]
 46%|████▋     | 93/200 [00:05<00:03, 32.11it/s][2026-01-17 08:49:04] INFO:     127.0.0.1:46514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] Decode batch, #running-req: 106, #token: 23424, token usage: 0.05, npu graph: False, gen throughput (token/s): 3419.77, #queue-req: 0,
[2026-01-17 08:49:04] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47362 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:05<00:02, 35.62it/s][2026-01-17 08:49:04] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:05<00:02, 40.40it/s][2026-01-17 08:49:04] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:05<00:01, 44.65it/s][2026-01-17 08:49:04] INFO:     127.0.0.1:47730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:06<00:01, 43.53it/s][2026-01-17 08:49:04] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:48068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:04] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [00:06<00:01, 49.46it/s][2026-01-17 08:49:04] INFO:     127.0.0.1:48010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:06<00:01, 42.40it/s][2026-01-17 08:49:05] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:46866 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:06<00:01, 44.06it/s][2026-01-17 08:49:05] INFO:     127.0.0.1:48266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:46976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48166 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48128 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:06<00:01, 47.57it/s][2026-01-17 08:49:05] Decode batch, #running-req: 57, #token: 15872, token usage: 0.03, npu graph: False, gen throughput (token/s): 3266.97, #queue-req: 0,
[2026-01-17 08:49:05] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:46610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:06<00:01, 46.09it/s]
 75%|███████▌  | 150/200 [00:06<00:01, 47.27it/s][2026-01-17 08:49:05] INFO:     127.0.0.1:47164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48122 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:07<00:01, 33.67it/s][2026-01-17 08:49:05] INFO:     127.0.0.1:47986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:07<00:01, 32.53it/s]
 80%|████████  | 160/200 [00:07<00:01, 33.58it/s][2026-01-17 08:49:05] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:46822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:05] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:48226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:47468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:47918 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:07<00:00, 37.52it/s]
 84%|████████▎ | 167/200 [00:07<00:00, 42.77it/s][2026-01-17 08:49:06] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:47122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:07<00:00, 37.13it/s]
 86%|████████▋ | 173/200 [00:07<00:00, 35.61it/s][2026-01-17 08:49:06] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] Decode batch, #running-req: 26, #token: 8320, token usage: 0.02, npu graph: False, gen throughput (token/s): 1770.30, #queue-req: 0,
[2026-01-17 08:49:06] INFO:     127.0.0.1:48092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:46624 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:07<00:00, 32.40it/s][2026-01-17 08:49:06] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:47966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:08<00:00, 19.83it/s]
 91%|█████████ | 182/200 [00:08<00:01, 16.15it/s][2026-01-17 08:49:06] INFO:     127.0.0.1:48144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:06] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:07] INFO:     127.0.0.1:47890 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:08<00:00, 15.21it/s][2026-01-17 08:49:07] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:07] INFO:     127.0.0.1:48014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:07] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:08<00:00, 16.88it/s][2026-01-17 08:49:07] INFO:     127.0.0.1:48210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:07] Decode batch, #running-req: 12, #token: 4736, token usage: 0.01, npu graph: False, gen throughput (token/s): 882.62, #queue-req: 0,
[2026-01-17 08:49:07] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:07] INFO:     127.0.0.1:47714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:07] INFO:     127.0.0.1:47850 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:09<00:00,  9.23it/s]
 96%|█████████▌| 192/200 [00:09<00:01,  7.38it/s][2026-01-17 08:49:08] Decode batch, #running-req: 8, #token: 3840, token usage: 0.01, npu graph: False, gen throughput (token/s): 490.63, #queue-req: 0,
[2026-01-17 08:49:08] Decode batch, #running-req: 8, #token: 4224, token usage: 0.01, npu graph: False, gen throughput (token/s): 373.70, #queue-req: 0,
[2026-01-17 08:49:09] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:09] INFO:     127.0.0.1:47906 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:10<00:01,  3.85it/s][2026-01-17 08:49:09] Decode batch, #running-req: 6, #token: 3584, token usage: 0.01, npu graph: False, gen throughput (token/s): 322.70, #queue-req: 0,
[2026-01-17 08:49:10] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:11<00:01,  2.96it/s][2026-01-17 08:49:10] Decode batch, #running-req: 5, #token: 3328, token usage: 0.01, npu graph: False, gen throughput (token/s): 276.33, #queue-req: 0,
[2026-01-17 08:49:11] Decode batch, #running-req: 5, #token: 3584, token usage: 0.01, npu graph: False, gen throughput (token/s): 236.66, #queue-req: 0,
[2026-01-17 08:49:12] Decode batch, #running-req: 5, #token: 3712, token usage: 0.01, npu graph: False, gen throughput (token/s): 238.06, #queue-req: 0,
[2026-01-17 08:49:13] Decode batch, #running-req: 5, #token: 2688, token usage: 0.01, npu graph: False, gen throughput (token/s): 235.88, #queue-req: 0,
[2026-01-17 08:49:13] INFO:     127.0.0.1:46530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:49:13] INFO:     127.0.0.1:46994 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:14<00:02,  1.48it/s]
 98%|█████████▊| 197/200 [00:14<00:02,  1.15it/s][2026-01-17 08:49:14] Decode batch, #running-req: 3, #token: 2688, token usage: 0.01, npu graph: False, gen throughput (token/s): 146.07, #queue-req: 0,
[2026-01-17 08:49:14] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:15<00:01,  1.12it/s][2026-01-17 08:49:14] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:15<00:00,  1.21it/s][2026-01-17 08:49:14] INFO:     127.0.0.1:48114 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:16<00:00, 12.48it/s]
.
----------------------------------------------------------------------
Ran 1 test in 71.186s

OK
Accuracy: 0.420
Invalid: 0.010
Latency: 19.922 s
Output throughput: 1086.305 token/s
.
.
End (20/62):
filename='ascend/llm_models/test_qwen3_0_6B.py', elapsed=82, estimated_time=400
.
.

.
.
Begin (21/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_deepseek_v3_2_exp_w8a8.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:49:34] WARNING model_config.py:820: modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:34] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8', tokenizer_path='/root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='modelslim', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.9, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=941655941, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='modelslim', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:49:35] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:35] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2026-01-17 08:49:45 TP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-17 08:49:46 TP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:46 TP4] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:49:46 TP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:49:47 TP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:47 TP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:47 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:49:47 TP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:49:47 TP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:47 TP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:49:47 TP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:49:47 TP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:49:47 TP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:47 TP14] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:49:47 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:47 TP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:49:48 TP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP7] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:49:48 TP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP5] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:49:48 TP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP0] Init torch distributed begin.
[2026-01-17 08:49:48 TP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP3] Init torch distributed begin.
[2026-01-17 08:49:48 TP12] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:49:48 TP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP6] Init torch distributed begin.
[2026-01-17 08:49:48 TP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP10] Init torch distributed begin.
[2026-01-17 08:49:48 TP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP2] Init torch distributed begin.
[2026-01-17 08:49:48 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:49:48 TP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:48 TP11] Init torch distributed begin.
[2026-01-17 08:49:49 TP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:49 TP15] Init torch distributed begin.
[2026-01-17 08:49:49 TP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:49 TP13] Init torch distributed begin.
[2026-01-17 08:49:49 TP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:49 TP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:49:49 TP8] Init torch distributed begin.
[2026-01-17 08:49:49 TP9] Init torch distributed begin.
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[2026-01-17 08:50:01 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:50:01 TP15] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:50:01 TP14] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:50:01 TP11] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 08:50:01 TP13] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 08:50:01 TP12] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:50:01 TP9] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 08:50:01 TP10] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:50:01 TP5] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 08:50:01 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:50:01 TP1] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 08:50:01 TP8] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:50:01 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:50:01 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:50:01 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:50:01 TP2] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 08:50:02 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP12] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP15] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP10] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP11] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP13] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP14] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP8] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:02 TP9] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:50:03 TP1] Load weight begin. avail mem=61.09 GB
[2026-01-17 08:50:03 TP5] Load weight begin. avail mem=61.10 GB
[2026-01-17 08:50:03 TP9] Load weight begin. avail mem=61.10 GB
[2026-01-17 08:50:03 TP7] Load weight begin. avail mem=61.08 GB
[2026-01-17 08:50:03 TP12] Load weight begin. avail mem=60.82 GB
[2026-01-17 08:50:03 TP4] Load weight begin. avail mem=60.82 GB
[2026-01-17 08:50:03 TP6] Load weight begin. avail mem=60.83 GB
[2026-01-17 08:50:03 TP10] Load weight begin. avail mem=60.83 GB
[2026-01-17 08:50:03 TP3] Load weight begin. avail mem=61.09 GB
[2026-01-17 08:50:03 TP11] Load weight begin. avail mem=61.08 GB
[2026-01-17 08:50:03 TP2] Load weight begin. avail mem=60.82 GB
[2026-01-17 08:50:03 TP15] Load weight begin. avail mem=61.08 GB
[2026-01-17 08:50:03 TP13] Load weight begin. avail mem=61.10 GB
[2026-01-17 08:50:03 TP14] Load weight begin. avail mem=60.83 GB
[2026-01-17 08:50:03 TP0] Load weight begin. avail mem=60.76 GB
[2026-01-17 08:50:03 TP8] Load weight begin. avail mem=60.82 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
[2026-01-17 08:50:04 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 or AMD-platform with capability >= gfx942(MI30x) can use shared experts fusion optimization. Shared experts fusion optimization is disabled.

Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:36,  4.39it/s]

Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:41,  3.85it/s]

Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:38,  4.15it/s]

Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:01<00:42,  3.75it/s]

Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:01<00:49,  3.21it/s]

Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:01<00:52,  2.97it/s]

Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:02<00:54,  2.85it/s]

Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:02<00:54,  2.83it/s]

Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:02<00:48,  3.20it/s]

Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:03<00:44,  3.45it/s]

Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:03<00:45,  3.35it/s]

Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:03<00:49,  3.05it/s]

Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:04<00:52,  2.83it/s]

Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:04<00:51,  2.88it/s]

Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:04<00:57,  2.58it/s]

Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:05<00:59,  2.48it/s]

Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:05<01:01,  2.37it/s]

Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:06<01:02,  2.32it/s]

Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:06<01:00,  2.40it/s]

Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:07<00:58,  2.46it/s]

Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:08<01:26,  1.64it/s]

Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:08<01:10,  1.99it/s]

Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:08<01:03,  2.21it/s]

Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:09<01:00,  2.31it/s]

Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:09<00:57,  2.40it/s]

Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:09<00:40,  3.35it/s]

Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:10<00:41,  3.25it/s]

Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:10<00:41,  3.26it/s]

Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:10<00:40,  3.28it/s]

Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:11<00:41,  3.16it/s]

Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:11<00:43,  3.02it/s]

Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:11<00:46,  2.79it/s]

Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:12<00:46,  2.76it/s]

Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:13<01:16,  1.66it/s]

Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:14<01:29,  1.42it/s]

Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:14<01:15,  1.66it/s]

Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:15<01:17,  1.61it/s]

Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:16<01:34,  1.31it/s]

Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:18<02:11,  1.07s/it]

Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:18<01:45,  1.16it/s]

Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:20<02:02,  1.01s/it]

Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:22<02:52,  1.44s/it]

Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:23<02:25,  1.23s/it]

Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:25<02:51,  1.45s/it]

Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:27<03:23,  1.74s/it]

Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:28<02:49,  1.46s/it]

Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:29<02:00,  1.05s/it]

Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:30<02:04,  1.10s/it]

Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:32<02:29,  1.34s/it]

Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:33<01:57,  1.06s/it]

Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:36<02:56,  1.60s/it]

Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:37<02:48,  1.55s/it]

Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:39<02:48,  1.56s/it]

Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:40<02:46,  1.55s/it]

Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:42<02:38,  1.49s/it]

Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:44<02:55,  1.68s/it]

Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:45<02:59,  1.73s/it]

Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:47<02:51,  1.67s/it]

Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:49<03:03,  1.80s/it]

Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:51<02:59,  1.78s/it]

Loading safetensors checkpoint shards:  39% Completed | 63/163 [00:51<02:21,  1.42s/it]

Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:52<02:00,  1.22s/it]

Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:54<02:02,  1.25s/it]

Loading safetensors checkpoint shards:  40% Completed | 66/163 [00:54<01:39,  1.03s/it]

Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:55<01:36,  1.00s/it]

Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:56<01:45,  1.11s/it]

Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:59<02:17,  1.46s/it]

Loading safetensors checkpoint shards:  43% Completed | 70/163 [01:00<02:13,  1.44s/it]

Loading safetensors checkpoint shards:  44% Completed | 71/163 [01:01<02:14,  1.46s/it]

Loading safetensors checkpoint shards:  44% Completed | 72/163 [01:02<01:46,  1.18s/it]

Loading safetensors checkpoint shards:  45% Completed | 73/163 [01:03<01:31,  1.01s/it]

Loading safetensors checkpoint shards:  45% Completed | 74/163 [01:04<01:28,  1.00it/s]

Loading safetensors checkpoint shards:  46% Completed | 75/163 [01:04<01:15,  1.16it/s]

Loading safetensors checkpoint shards:  47% Completed | 76/163 [01:05<01:20,  1.08it/s]

Loading safetensors checkpoint shards:  47% Completed | 77/163 [01:06<01:11,  1.20it/s]

Loading safetensors checkpoint shards:  48% Completed | 78/163 [01:07<01:22,  1.03it/s]

Loading safetensors checkpoint shards:  48% Completed | 79/163 [01:08<01:22,  1.02it/s]

Loading safetensors checkpoint shards:  49% Completed | 80/163 [01:10<01:32,  1.11s/it]

Loading safetensors checkpoint shards:  50% Completed | 81/163 [01:10<01:22,  1.01s/it]

Loading safetensors checkpoint shards:  50% Completed | 82/163 [01:12<01:47,  1.33s/it]

Loading safetensors checkpoint shards:  51% Completed | 83/163 [01:14<01:55,  1.44s/it]

Loading safetensors checkpoint shards:  52% Completed | 84/163 [01:16<01:59,  1.51s/it]

Loading safetensors checkpoint shards:  52% Completed | 85/163 [01:17<01:58,  1.52s/it]

Loading safetensors checkpoint shards:  53% Completed | 86/163 [01:19<02:02,  1.60s/it]

Loading safetensors checkpoint shards:  53% Completed | 87/163 [01:20<01:56,  1.53s/it]

Loading safetensors checkpoint shards:  54% Completed | 88/163 [01:22<01:51,  1.49s/it]

Loading safetensors checkpoint shards:  55% Completed | 89/163 [01:23<01:46,  1.43s/it]

Loading safetensors checkpoint shards:  55% Completed | 90/163 [01:24<01:35,  1.31s/it]

Loading safetensors checkpoint shards:  56% Completed | 91/163 [01:25<01:21,  1.14s/it]

Loading safetensors checkpoint shards:  56% Completed | 92/163 [01:25<01:04,  1.10it/s]

Loading safetensors checkpoint shards:  57% Completed | 93/163 [01:25<00:47,  1.46it/s]

Loading safetensors checkpoint shards:  58% Completed | 94/163 [01:26<00:48,  1.42it/s]

Loading safetensors checkpoint shards:  58% Completed | 95/163 [01:27<00:44,  1.53it/s]

Loading safetensors checkpoint shards:  59% Completed | 96/163 [01:27<00:38,  1.72it/s]

Loading safetensors checkpoint shards:  60% Completed | 97/163 [01:28<00:35,  1.84it/s]

Loading safetensors checkpoint shards:  60% Completed | 98/163 [01:28<00:39,  1.65it/s]

Loading safetensors checkpoint shards:  61% Completed | 99/163 [01:29<00:34,  1.87it/s]

Loading safetensors checkpoint shards:  61% Completed | 100/163 [01:29<00:29,  2.15it/s]

Loading safetensors checkpoint shards:  62% Completed | 101/163 [01:30<00:39,  1.55it/s]

Loading safetensors checkpoint shards:  63% Completed | 102/163 [01:30<00:34,  1.79it/s]

Loading safetensors checkpoint shards:  63% Completed | 103/163 [01:31<00:28,  2.11it/s]

Loading safetensors checkpoint shards:  64% Completed | 104/163 [01:32<00:42,  1.39it/s]

Loading safetensors checkpoint shards:  64% Completed | 105/163 [01:32<00:36,  1.61it/s]

Loading safetensors checkpoint shards:  65% Completed | 106/163 [01:33<00:29,  1.95it/s]

Loading safetensors checkpoint shards:  66% Completed | 107/163 [01:33<00:22,  2.48it/s]

Loading safetensors checkpoint shards:  66% Completed | 108/163 [01:33<00:23,  2.35it/s]

Loading safetensors checkpoint shards:  67% Completed | 109/163 [01:35<00:37,  1.45it/s]

Loading safetensors checkpoint shards:  67% Completed | 110/163 [01:35<00:32,  1.61it/s]

Loading safetensors checkpoint shards:  68% Completed | 111/163 [01:35<00:26,  1.93it/s]

Loading safetensors checkpoint shards:  69% Completed | 112/163 [01:35<00:20,  2.45it/s]

Loading safetensors checkpoint shards:  69% Completed | 113/163 [01:36<00:22,  2.21it/s]

Loading safetensors checkpoint shards:  70% Completed | 114/163 [01:38<00:40,  1.22it/s]

Loading safetensors checkpoint shards:  71% Completed | 115/163 [01:38<00:30,  1.59it/s]

Loading safetensors checkpoint shards:  71% Completed | 116/163 [01:39<00:31,  1.49it/s]

Loading safetensors checkpoint shards:  72% Completed | 117/163 [01:39<00:24,  1.89it/s]

Loading safetensors checkpoint shards:  72% Completed | 118/163 [01:39<00:18,  2.39it/s]

Loading safetensors checkpoint shards:  73% Completed | 119/163 [01:39<00:14,  3.00it/s]

Loading safetensors checkpoint shards:  74% Completed | 120/163 [01:39<00:11,  3.77it/s]

Loading safetensors checkpoint shards:  74% Completed | 121/163 [01:39<00:09,  4.29it/s]

Loading safetensors checkpoint shards:  75% Completed | 122/163 [01:40<00:08,  4.62it/s]

Loading safetensors checkpoint shards:  75% Completed | 123/163 [01:40<00:11,  3.58it/s]

Loading safetensors checkpoint shards:  76% Completed | 124/163 [01:41<00:13,  2.91it/s]

Loading safetensors checkpoint shards:  77% Completed | 125/163 [01:41<00:11,  3.33it/s]

Loading safetensors checkpoint shards:  77% Completed | 126/163 [01:41<00:09,  3.88it/s]

Loading safetensors checkpoint shards:  78% Completed | 127/163 [01:41<00:11,  3.07it/s]

Loading safetensors checkpoint shards:  79% Completed | 128/163 [01:42<00:09,  3.66it/s]

Loading safetensors checkpoint shards:  80% Completed | 130/163 [01:42<00:06,  5.27it/s]

Loading safetensors checkpoint shards:  80% Completed | 131/163 [01:42<00:05,  5.34it/s]

Loading safetensors checkpoint shards:  81% Completed | 132/163 [01:42<00:08,  3.63it/s]

Loading safetensors checkpoint shards:  82% Completed | 133/163 [01:44<00:19,  1.51it/s]

Loading safetensors checkpoint shards:  82% Completed | 134/163 [01:44<00:15,  1.85it/s]

Loading safetensors checkpoint shards:  83% Completed | 135/163 [01:45<00:16,  1.68it/s]

Loading safetensors checkpoint shards:  83% Completed | 136/163 [01:45<00:12,  2.10it/s]

Loading safetensors checkpoint shards:  84% Completed | 137/163 [01:45<00:09,  2.64it/s]

Loading safetensors checkpoint shards:  85% Completed | 138/163 [01:46<00:11,  2.22it/s]

Loading safetensors checkpoint shards:  85% Completed | 139/163 [01:49<00:28,  1.19s/it]

Loading safetensors checkpoint shards:  86% Completed | 140/163 [01:51<00:35,  1.55s/it]

Loading safetensors checkpoint shards:  87% Completed | 141/163 [01:53<00:35,  1.60s/it]

Loading safetensors checkpoint shards:  87% Completed | 142/163 [01:53<00:25,  1.22s/it]

Loading safetensors checkpoint shards:  88% Completed | 143/163 [01:54<00:18,  1.05it/s]

Loading safetensors checkpoint shards:  88% Completed | 144/163 [01:54<00:14,  1.34it/s]

Loading safetensors checkpoint shards:  89% Completed | 145/163 [01:54<00:10,  1.71it/s]

Loading safetensors checkpoint shards:  90% Completed | 146/163 [01:55<00:08,  1.94it/s]

Loading safetensors checkpoint shards:  90% Completed | 147/163 [01:55<00:07,  2.13it/s]

Loading safetensors checkpoint shards:  91% Completed | 148/163 [01:56<00:10,  1.45it/s]

Loading safetensors checkpoint shards:  91% Completed | 149/163 [01:56<00:07,  1.78it/s]

Loading safetensors checkpoint shards:  93% Completed | 151/163 [01:57<00:04,  2.58it/s]

Loading safetensors checkpoint shards:  93% Completed | 152/163 [01:57<00:04,  2.69it/s]

Loading safetensors checkpoint shards:  94% Completed | 153/163 [01:57<00:03,  2.74it/s]

Loading safetensors checkpoint shards:  94% Completed | 154/163 [01:58<00:03,  2.85it/s]

Loading safetensors checkpoint shards:  95% Completed | 155/163 [01:58<00:02,  2.75it/s]

Loading safetensors checkpoint shards:  96% Completed | 156/163 [01:58<00:02,  2.85it/s]

Loading safetensors checkpoint shards:  96% Completed | 157/163 [01:59<00:02,  2.90it/s]

Loading safetensors checkpoint shards:  97% Completed | 158/163 [01:59<00:01,  2.87it/s]

Loading safetensors checkpoint shards:  98% Completed | 159/163 [01:59<00:01,  3.04it/s]

Loading safetensors checkpoint shards:  98% Completed | 160/163 [02:00<00:00,  3.23it/s]

Loading safetensors checkpoint shards:  99% Completed | 161/163 [02:00<00:00,  3.36it/s]

Loading safetensors checkpoint shards:  99% Completed | 162/163 [02:00<00:00,  3.05it/s]

Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:01<00:00,  3.33it/s]

Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:01<00:00,  1.35it/s]

[2026-01-17 08:52:41 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.67 GB, mem usage=44.16 GB.
[2026-01-17 08:52:41 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.60 GB, mem usage=44.16 GB.
[2026-01-17 08:52:41 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.93 GB, mem usage=44.16 GB.
[2026-01-17 08:52:41 TP11] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.92 GB, mem usage=44.16 GB.
[2026-01-17 08:52:41 TP8] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.65 GB, mem usage=44.16 GB.
[2026-01-17 08:52:42 TP12] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.66 GB, mem usage=44.16 GB.
[2026-01-17 08:52:42 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.66 GB, mem usage=44.16 GB.
[2026-01-17 08:52:42 TP9] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.93 GB, mem usage=44.16 GB.
[2026-01-17 08:52:42 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.65 GB, mem usage=44.16 GB.
[2026-01-17 08:52:42 TP13] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.93 GB, mem usage=44.16 GB.
[2026-01-17 08:52:42 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.93 GB, mem usage=44.16 GB.
[2026-01-17 08:52:42 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.93 GB, mem usage=44.16 GB.
[2026-01-17 08:52:42 TP10] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.67 GB, mem usage=44.16 GB.
[2026-01-17 08:52:42 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.92 GB, mem usage=44.16 GB.
[2026-01-17 08:52:43 TP14] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.67 GB, mem usage=44.16 GB.
[2026-01-17 08:52:43 TP15] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.92 GB, mem usage=44.16 GB.
[2026-01-17 08:52:43 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 08:52:43 TP0] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP15] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP14] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP13] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP12] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP10] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP11] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP9] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP8] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP7] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP6] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP5] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP4] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP3] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP2] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP1] The available memory for KV cache is 10.50 GB.
[2026-01-17 08:52:43 TP0] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP0] Memory pool end. avail mem=3.80 GB
[2026-01-17 08:52:43 TP1] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP1] Memory pool end. avail mem=4.13 GB
[2026-01-17 08:52:43 TP2] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP3] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP15] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP2] Memory pool end. avail mem=3.85 GB
[2026-01-17 08:52:43 TP3] Memory pool end. avail mem=4.13 GB
[2026-01-17 08:52:43 TP15] Memory pool end. avail mem=4.12 GB
[2026-01-17 08:52:43 TP11] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP11] Memory pool end. avail mem=4.12 GB
[2026-01-17 08:52:43 TP14] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP5] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP4] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP14] Memory pool end. avail mem=3.86 GB
[2026-01-17 08:52:43 TP10] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP4] Memory pool end. avail mem=3.85 GB
[2026-01-17 08:52:43 TP5] Memory pool end. avail mem=4.13 GB
[2026-01-17 08:52:43 TP10] Memory pool end. avail mem=3.86 GB
[2026-01-17 08:52:43 TP7] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP6] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP7] Memory pool end. avail mem=4.12 GB
[2026-01-17 08:52:43 TP6] Memory pool end. avail mem=3.86 GB
[2026-01-17 08:52:43 TP9] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP9] Memory pool end. avail mem=4.13 GB
[2026-01-17 08:52:43 TP8] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP13] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP8] Memory pool end. avail mem=3.85 GB
[2026-01-17 08:52:43 TP13] Memory pool end. avail mem=4.13 GB
[2026-01-17 08:52:43 TP12] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 08:52:43 TP12] Memory pool end. avail mem=3.85 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 08:52:44 TP0] max_total_num_tokens=143872, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=163840, available_gpu_mem=3.81 GB
[2026-01-17 08:52:45] INFO:     Started server process [81621]
[2026-01-17 08:52:45] INFO:     Waiting for application startup.
[2026-01-17 08:52:45] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.95}
[2026-01-17 08:52:45] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.95}
[2026-01-17 08:52:45] INFO:     Application startup complete.
[2026-01-17 08:52:45] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 08:52:45] INFO:     127.0.0.1:37560 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 08:52:46] INFO:     127.0.0.1:37572 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 08:52:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
................[2026-01-17 08:52:55] INFO:     127.0.0.1:48768 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[rank4]:[W117 08:52:56.744819611 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank9]:[W117 08:52:56.744832822 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank14]:[W117 08:52:56.744867503 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank1]:[W117 08:52:56.744895124 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank10]:[W117 08:52:56.744891604 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank7]:[W117 08:52:56.744895784 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank2]:[W117 08:52:56.744981878 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank11]:[W117 08:52:56.744981858 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank15]:[W117 08:52:56.744988068 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank6]:[W117 08:52:56.745005019 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank8]:[W117 08:52:56.745012479 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank13]:[W117 08:52:56.745072851 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank5]:[W117 08:52:56.745363633 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank0]:[W117 08:52:56.745529419 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank12]:[W117 08:52:56.745635083 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank3]:[W117 08:52:56.745636514 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[2026-01-17 08:53:02] INFO:     127.0.0.1:37586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:02] The server is fired up and ready to roll!
[2026-01-17 08:53:05 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:53:06] INFO:     127.0.0.1:60290 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 08:53:06] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 08:53:06] INFO:     127.0.0.1:60294 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 08:53:06 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:53:07] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8 --trust-remote-code --mem-fraction-static 0.9 --attention-backend ascend --disable-cuda-graph --tp-size 16 --quantization modelslim --disable-radix-cache --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 08:53:07 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 08:53:07 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 1, #queue-req: 69,
[2026-01-17 08:53:10 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 11, #queue-req: 106,
[2026-01-17 08:53:11 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.12, #running-req: 22, #queue-req: 95,
[2026-01-17 08:53:13 TP0] Prefill batch, #new-seq: 12, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 32, #queue-req: 84,
[2026-01-17 08:53:14 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 43, #queue-req: 74,
[2026-01-17 08:53:15] INFO:     127.0.0.1:60678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:15] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:15 TP0] Prefill batch, #new-seq: 12, #new-token: 8192, #cached-token: 0, token usage: 0.28, #running-req: 53, #queue-req: 63,

  0%|          | 1/200 [00:08<29:04,  8.77s/it]
  1%|          | 2/200 [00:08<18:13,  5.52s/it][2026-01-17 08:53:17 TP0] Prefill batch, #new-seq: 12, #new-token: 8192, #cached-token: 0, token usage: 0.34, #running-req: 64, #queue-req: 54,
[2026-01-17 08:53:18 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.39, #running-req: 75, #queue-req: 44,
[2026-01-17 08:53:20] INFO:     127.0.0.1:60972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:20 TP0] Prefill batch, #new-seq: 12, #new-token: 8192, #cached-token: 0, token usage: 0.44, #running-req: 85, #queue-req: 33,

  2%|▏         | 3/200 [00:12<16:34,  5.05s/it][2026-01-17 08:53:21 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.50, #running-req: 96, #queue-req: 24,
[2026-01-17 08:53:22] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:22] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:22 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.55, #running-req: 107, #queue-req: 13,

  2%|▏         | 4/200 [00:15<14:05,  4.31s/it]
  2%|▎         | 5/200 [00:15<09:26,  2.90s/it][2026-01-17 08:53:24 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.60, #running-req: 117, #queue-req: 5,
[2026-01-17 08:53:25 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.66, #running-req: 128, #queue-req: 3,
[2026-01-17 08:53:26 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.67, #running-req: 125, #queue-req: 2,
[2026-01-17 08:53:26] INFO:     127.0.0.1:60686 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:26] INFO:     127.0.0.1:60838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:26] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:26] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:26] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:19<10:06,  3.13s/it]
  5%|▌         | 10/200 [00:19<03:37,  1.14s/it]
  5%|▌         | 10/200 [00:19<03:37,  1.14s/it]
  5%|▌         | 10/200 [00:19<03:37,  1.14s/it]
  5%|▌         | 10/200 [00:19<03:37,  1.14s/it][2026-01-17 08:53:26 TP0] Prefill batch, #new-seq: 5, #new-token: 3840, #cached-token: 0, token usage: 0.65, #running-req: 121, #queue-req: 2,
[2026-01-17 08:53:29] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:29] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:29] INFO:     127.0.0.1:60870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:29] INFO:     127.0.0.1:60876 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:22<04:11,  1.33s/it]
  7%|▋         | 14/200 [00:22<03:05,  1.01it/s]
  7%|▋         | 14/200 [00:22<03:05,  1.01it/s]
  7%|▋         | 14/200 [00:22<03:05,  1.01it/s][2026-01-17 08:53:29 TP0] Prefill batch, #new-seq: 4, #new-token: 3072, #cached-token: 0, token usage: 0.66, #running-req: 122, #queue-req: 2,
[2026-01-17 08:53:30] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:23<03:02,  1.01it/s][2026-01-17 08:53:30 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.67, #running-req: 125, #queue-req: 1,
[2026-01-17 08:53:31] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:24<03:07,  1.02s/it][2026-01-17 08:53:31 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.68, #running-req: 126, #queue-req: 1,
[2026-01-17 08:53:39 TP0] Decode batch, #running-req: 127, #token: 102400, token usage: 0.71, npu graph: False, gen throughput (token/s): 16.95, #queue-req: 1,
[2026-01-17 08:53:42] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:42] INFO:     127.0.0.1:33112 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:35<08:28,  2.78s/it]
  9%|▉         | 18/200 [00:35<11:09,  3.68s/it][2026-01-17 08:53:42 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.72, #running-req: 125, #queue-req: 2,
[2026-01-17 08:53:42] INFO:     127.0.0.1:33146 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:35<09:17,  3.08s/it][2026-01-17 08:53:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.72, #running-req: 125, #queue-req: 2,
[2026-01-17 08:53:43] INFO:     127.0.0.1:60596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:43] INFO:     127.0.0.1:33098 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:43] INFO:     127.0.0.1:39232 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:36<07:36,  2.53s/it]
 11%|█         | 22/200 [00:36<02:58,  1.01s/it]
 11%|█         | 22/200 [00:36<02:58,  1.01s/it][2026-01-17 08:53:43 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.71, #running-req: 123, #queue-req: 3,
[2026-01-17 08:53:44] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:37<02:48,  1.05it/s][2026-01-17 08:53:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.72, #running-req: 124, #queue-req: 3,
[2026-01-17 08:53:45] INFO:     127.0.0.1:33088 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:38<03:05,  1.05s/it][2026-01-17 08:53:46] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:46] INFO:     127.0.0.1:60558 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:38<02:34,  1.13it/s][2026-01-17 08:53:46 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.72, #running-req: 124, #queue-req: 2,

 13%|█▎        | 26/200 [00:38<01:44,  1.67it/s][2026-01-17 08:53:47] INFO:     127.0.0.1:39216 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:40<02:07,  1.36it/s][2026-01-17 08:53:47 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.73, #running-req: 123, #queue-req: 4,
[2026-01-17 08:53:47] INFO:     127.0.0.1:39238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:47] INFO:     127.0.0.1:38560 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:40<02:00,  1.43it/s]
 14%|█▍        | 29/200 [00:40<01:31,  1.87it/s][2026-01-17 08:53:48 TP0] Prefill batch, #new-seq: 2, #new-token: 1664, #cached-token: 0, token usage: 0.73, #running-req: 122, #queue-req: 4,
[2026-01-17 08:53:48] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:41<01:36,  1.77it/s][2026-01-17 08:53:49] INFO:     127.0.0.1:60320 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:42<01:46,  1.59it/s][2026-01-17 08:53:49 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.74, #running-req: 122, #queue-req: 5,
[2026-01-17 08:53:50] INFO:     127.0.0.1:38576 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:43<02:05,  1.33it/s][2026-01-17 08:53:50 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.74, #running-req: 122, #queue-req: 5,
[2026-01-17 08:53:52] INFO:     127.0.0.1:33128 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:45<02:45,  1.01it/s][2026-01-17 08:53:52] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:45<02:16,  1.22it/s][2026-01-17 08:53:52 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.74, #running-req: 122, #queue-req: 5,
[2026-01-17 08:53:53] INFO:     127.0.0.1:60576 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:45<02:03,  1.34it/s][2026-01-17 08:53:53 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.74, #running-req: 121, #queue-req: 6,
[2026-01-17 08:53:54] INFO:     127.0.0.1:39206 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:47<02:45,  1.01s/it][2026-01-17 08:53:55 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.74, #running-req: 121, #queue-req: 5,
[2026-01-17 08:53:55] INFO:     127.0.0.1:60370 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:48<02:39,  1.02it/s][2026-01-17 08:53:55 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.75, #running-req: 122, #queue-req: 5,
[2026-01-17 08:53:56 TP0] Decode batch, #running-req: 122, #token: 108032, token usage: 0.75, npu graph: False, gen throughput (token/s): 291.76, #queue-req: 5,
[2026-01-17 08:53:56] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:49<02:22,  1.14it/s][2026-01-17 08:53:56 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.75, #running-req: 122, #queue-req: 5,
[2026-01-17 08:53:56] INFO:     127.0.0.1:38582 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:49<02:16,  1.18it/s][2026-01-17 08:53:57 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.75, #running-req: 122, #queue-req: 5,
[2026-01-17 08:53:59] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:53:59] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:52<03:31,  1.32s/it]
 20%|██        | 41/200 [00:52<03:23,  1.28s/it][2026-01-17 08:53:59 TP0] Prefill batch, #new-seq: 3, #new-token: 2304, #cached-token: 0, token usage: 0.74, #running-req: 121, #queue-req: 4,
[2026-01-17 08:54:00] INFO:     127.0.0.1:38580 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:53<03:25,  1.30s/it][2026-01-17 08:54:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.75, #running-req: 123, #queue-req: 4,
[2026-01-17 08:54:02] INFO:     127.0.0.1:39190 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:55<03:36,  1.38s/it][2026-01-17 08:54:02] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:02] INFO:     127.0.0.1:33086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:02 TP0] Prefill batch, #new-seq: 3, #new-token: 2304, #cached-token: 0, token usage: 0.74, #running-req: 123, #queue-req: 2,

 22%|██▏       | 44/200 [00:55<02:48,  1.08s/it]
 22%|██▎       | 45/200 [00:55<01:43,  1.50it/s][2026-01-17 08:54:03] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:56<01:37,  1.58it/s][2026-01-17 08:54:03 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.75, #running-req: 123, #queue-req: 3,
[2026-01-17 08:54:05] INFO:     127.0.0.1:60590 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:58<02:25,  1.05it/s][2026-01-17 08:54:05 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.76, #running-req: 124, #queue-req: 3,
[2026-01-17 08:54:05] INFO:     127.0.0.1:60348 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:58<02:16,  1.11it/s][2026-01-17 08:54:06 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.76, #running-req: 124, #queue-req: 3,
[2026-01-17 08:54:07] INFO:     127.0.0.1:39266 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [01:00<03:01,  1.20s/it][2026-01-17 08:54:08] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [01:01<02:24,  1.04it/s][2026-01-17 08:54:08 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.75, #running-req: 124, #queue-req: 2,
[2026-01-17 08:54:08 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.76, #running-req: 125, #queue-req: 2,
[2026-01-17 08:54:11] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [01:04<04:15,  1.72s/it][2026-01-17 08:54:12 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.77, #running-req: 125, #queue-req: 1,
[2026-01-17 08:54:13] INFO:     127.0.0.1:39276 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [01:06<03:59,  1.62s/it][2026-01-17 08:54:13 TP0] Decode batch, #running-req: 127, #token: 109952, token usage: 0.76, npu graph: False, gen throughput (token/s): 288.55, #queue-req: 1,
[2026-01-17 08:54:13] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [01:06<03:01,  1.23s/it][2026-01-17 08:54:13 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.76, #running-req: 126, #queue-req: 0,
[2026-01-17 08:54:13] INFO:     127.0.0.1:38932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:13 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.77, #running-req: 128, #queue-req: 0,

 27%|██▋       | 54/200 [01:06<02:28,  1.01s/it][2026-01-17 08:54:15] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [01:08<02:44,  1.14s/it][2026-01-17 08:54:15 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.77, #running-req: 126, #queue-req: 1,
[2026-01-17 08:54:16] INFO:     127.0.0.1:39242 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [01:09<03:03,  1.28s/it][2026-01-17 08:54:17 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.77, #running-req: 126, #queue-req: 1,
[2026-01-17 08:54:18] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [01:11<03:16,  1.37s/it][2026-01-17 08:54:18] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [01:11<02:29,  1.05s/it][2026-01-17 08:54:18 TP0] Prefill batch, #new-seq: 2, #new-token: 1664, #cached-token: 0, token usage: 0.77, #running-req: 126, #queue-req: 0,
[2026-01-17 08:54:19 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.78, #running-req: 128, #queue-req: 0,
[2026-01-17 08:54:20] INFO:     127.0.0.1:38592 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [01:13<03:04,  1.31s/it][2026-01-17 08:54:21] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:21] INFO:     127.0.0.1:46792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.77, #running-req: 127, #queue-req: 0,

 30%|███       | 60/200 [01:13<02:19,  1.00it/s]
 30%|███       | 61/200 [01:13<01:23,  1.66it/s][2026-01-17 08:54:21] INFO:     127.0.0.1:39320 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [01:14<01:18,  1.75it/s][2026-01-17 08:54:21 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.77, #running-req: 128, #queue-req: 0,
[2026-01-17 08:54:22 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.78, #running-req: 127, #queue-req: 0,
[2026-01-17 08:54:22] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:22] INFO:     127.0.0.1:46832 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [01:15<01:41,  1.35it/s]
 32%|███▏      | 64/200 [01:15<01:33,  1.46it/s][2026-01-17 08:54:23 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.77, #running-req: 126, #queue-req: 0,
[2026-01-17 08:54:24] INFO:     127.0.0.1:60330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:24] INFO:     127.0.0.1:50640 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [01:17<02:07,  1.06it/s]
 33%|███▎      | 66/200 [01:17<02:04,  1.08it/s][2026-01-17 08:54:24 TP0] Prefill batch, #new-seq: 2, #new-token: 1664, #cached-token: 0, token usage: 0.77, #running-req: 126, #queue-req: 0,
[2026-01-17 08:54:26] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [01:19<02:30,  1.13s/it][2026-01-17 08:54:26 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.78, #running-req: 127, #queue-req: 0,
[2026-01-17 08:54:29] INFO:     127.0.0.1:38926 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [01:22<03:36,  1.64s/it][2026-01-17 08:54:29 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.79, #running-req: 127, #queue-req: 0,
[2026-01-17 08:54:31] INFO:     127.0.0.1:60552 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [01:24<03:38,  1.67s/it][2026-01-17 08:54:31 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.79, #running-req: 127, #queue-req: 0,
[2026-01-17 08:54:32] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:32] INFO:     127.0.0.1:46820 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [01:25<03:29,  1.62s/it]
 36%|███▌      | 71/200 [01:25<02:39,  1.24s/it][2026-01-17 08:54:33 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.79, #running-req: 126, #queue-req: 0,
[2026-01-17 08:54:34] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [01:27<02:50,  1.33s/it][2026-01-17 08:54:34 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.79, #running-req: 127, #queue-req: 0,
[2026-01-17 08:54:36 TP0] Decode batch, #running-req: 128, #token: 114560, token usage: 0.80, npu graph: False, gen throughput (token/s): 224.66, #queue-req: 0,
[2026-01-17 08:54:36] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [01:29<02:58,  1.40s/it][2026-01-17 08:54:36] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:36] INFO:     127.0.0.1:34348 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [01:29<02:30,  1.19s/it]
 38%|███▊      | 75/200 [01:29<01:40,  1.24it/s][2026-01-17 08:54:37] INFO:     127.0.0.1:46872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:37] INFO:     127.0.0.1:34354 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [01:30<01:42,  1.21it/s]
 38%|███▊      | 77/200 [01:30<01:23,  1.48it/s][2026-01-17 08:54:37] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [01:30<01:11,  1.70it/s][2026-01-17 08:54:39] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [01:31<01:26,  1.40it/s][2026-01-17 08:54:40] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [01:33<01:39,  1.20it/s][2026-01-17 08:54:41] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [01:34<01:42,  1.16it/s][2026-01-17 08:54:41] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [01:34<01:28,  1.33it/s][2026-01-17 08:54:42] INFO:     127.0.0.1:39254 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [01:35<01:34,  1.24it/s][2026-01-17 08:54:44] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [01:37<02:16,  1.17s/it][2026-01-17 08:54:45] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [01:38<01:52,  1.02it/s][2026-01-17 08:54:46] INFO:     127.0.0.1:46856 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [01:38<01:50,  1.03it/s][2026-01-17 08:54:47] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:47] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [01:39<01:49,  1.04it/s]
 44%|████▍     | 88/200 [01:39<01:23,  1.35it/s][2026-01-17 08:54:47] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [01:40<01:15,  1.48it/s][2026-01-17 08:54:48] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [01:41<01:22,  1.33it/s][2026-01-17 08:54:48] INFO:     127.0.0.1:50650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:54:48] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [01:41<01:14,  1.47it/s]
 46%|████▌     | 92/200 [01:41<00:52,  2.04it/s][2026-01-17 08:54:49] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [01:42<01:05,  1.64it/s][2026-01-17 08:54:50] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [01:43<01:14,  1.42it/s][2026-01-17 08:54:51] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [01:44<01:08,  1.53it/s][2026-01-17 08:54:53 TP0] Decode batch, #running-req: 105, #token: 103680, token usage: 0.72, npu graph: False, gen throughput (token/s): 277.15, #queue-req: 0,
[2026-01-17 08:54:53] INFO:     127.0.0.1:39240 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [01:46<02:01,  1.17s/it][2026-01-17 08:54:54] INFO:     127.0.0.1:34344 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [01:47<01:54,  1.11s/it][2026-01-17 08:54:56] INFO:     127.0.0.1:44934 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [01:49<02:17,  1.34s/it][2026-01-17 08:54:58] INFO:     127.0.0.1:46840 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [01:51<02:18,  1.37s/it][2026-01-17 08:54:58] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [01:51<01:50,  1.11s/it][2026-01-17 08:54:59] INFO:     127.0.0.1:50682 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [01:52<01:45,  1.07s/it][2026-01-17 08:55:00] INFO:     127.0.0.1:46782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:55:00] INFO:     127.0.0.1:44932 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [01:53<01:27,  1.12it/s]
 52%|█████▏    | 103/200 [01:53<00:57,  1.68it/s][2026-01-17 08:55:00] INFO:     127.0.0.1:44944 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [01:53<00:54,  1.77it/s][2026-01-17 08:55:01] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [01:54<00:51,  1.83it/s][2026-01-17 08:55:04] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [01:57<01:51,  1.19s/it][2026-01-17 08:55:05] INFO:     127.0.0.1:52866 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [01:58<01:57,  1.27s/it][2026-01-17 08:55:10] INFO:     127.0.0.1:39278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:55:10] INFO:     127.0.0.1:34358 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [02:03<03:27,  2.26s/it]
 55%|█████▍    | 109/200 [02:03<03:30,  2.31s/it][2026-01-17 08:55:10] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [02:03<02:47,  1.86s/it][2026-01-17 08:55:11] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [02:04<02:24,  1.62s/it][2026-01-17 08:55:12 TP0] Decode batch, #running-req: 90, #token: 90368, token usage: 0.63, npu graph: False, gen throughput (token/s): 202.57, #queue-req: 0,
[2026-01-17 08:55:17] INFO:     127.0.0.1:39336 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [02:10<04:11,  2.86s/it][2026-01-17 08:55:20] INFO:     127.0.0.1:39218 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [02:13<03:57,  2.74s/it][2026-01-17 08:55:21] INFO:     127.0.0.1:44948 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [02:14<03:12,  2.23s/it][2026-01-17 08:55:21] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [02:14<02:29,  1.76s/it][2026-01-17 08:55:31] INFO:     127.0.0.1:46804 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [02:24<05:32,  3.96s/it][2026-01-17 08:55:31 TP0] Decode batch, #running-req: 85, #token: 87424, token usage: 0.61, npu graph: False, gen throughput (token/s): 178.22, #queue-req: 0,
[2026-01-17 08:55:33] INFO:     127.0.0.1:52852 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [02:26<04:39,  3.37s/it][2026-01-17 08:55:41] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [02:34<06:26,  4.72s/it][2026-01-17 08:55:50] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [02:43<08:09,  6.04s/it][2026-01-17 08:55:51 TP0] Decode batch, #running-req: 81, #token: 91136, token usage: 0.63, npu graph: False, gen throughput (token/s): 168.90, #queue-req: 0,
[2026-01-17 08:56:10 TP0] Decode batch, #running-req: 81, #token: 93184, token usage: 0.65, npu graph: False, gen throughput (token/s): 167.82, #queue-req: 0,
[2026-01-17 08:56:11] INFO:     127.0.0.1:34332 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [03:04<14:04, 10.56s/it][2026-01-17 08:56:29 TP0] Decode batch, #running-req: 80, #token: 92928, token usage: 0.65, npu graph: False, gen throughput (token/s): 165.72, #queue-req: 0,
[2026-01-17 08:56:49 TP0] Decode batch, #running-req: 80, #token: 98304, token usage: 0.68, npu graph: False, gen throughput (token/s): 166.25, #queue-req: 0,
[2026-01-17 08:57:08 TP0] Decode batch, #running-req: 80, #token: 102400, token usage: 0.71, npu graph: False, gen throughput (token/s): 163.51, #queue-req: 0,
[2026-01-17 08:57:28] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60652 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28 TP0] Decode batch, #running-req: 80, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 161.16, #queue-req: 0,
[2026-01-17 08:57:28] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 08:57:28] INFO:     127.0.0.1:33244 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [04:21<40:04, 30.44s/it]
 77%|███████▋  | 154/200 [04:21<22:58, 29.98s/it]
 96%|█████████▌| 192/200 [04:21<03:59, 29.98s/it]
100%|██████████| 200/200 [04:21<00:00,  1.31s/it]
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/llm_models/gsm8k_ascend_mixin.py", line 66, in test_gsm8k
    self.assertGreaterEqual(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1277, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: np.float64(0.515) not greater than or equal to 0.565 : Accuracy of /root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8 is 0.515, is lower than 0.565
E
======================================================================
ERROR: test_gsm8k (__main__.TestMistral7B.test_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: np.float64(0.515) not greater than or equal to 0.565 : Accuracy of /root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8 is 0.515, is lower than 0.565

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 484.517s

FAILED (errors=1)
Accuracy: 0.515
Invalid: 0.090
Latency: 261.959 s
Output throughput: 192.580 token/s
.
.
End (21/62):
filename='ascend/llm_models/test_ascend_deepseek_v3_2_exp_w8a8.py', elapsed=496, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_ascend_deepseek_v3_2_exp_w8a8.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/llm_models/test_ascend_deepseek_v3_2_exp_w8a8.py

.
.
Begin (21/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_deepseek_v3_2_exp_w8a8.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:58:50] WARNING model_config.py:820: modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:58:51] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8', tokenizer_path='/root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='modelslim', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.9, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=34938308, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='modelslim', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 08:58:51] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:58:51] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2026-01-17 08:59:02 TP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-17 08:59:02 TP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:59:03 TP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:03 TP5] Init torch distributed begin.
[2026-01-17 08:59:03 TP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 08:59:03 TP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:03 TP4] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:59:03 TP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:03 TP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:03 TP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:03 TP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:03 TP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:03 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:03 TP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:03 TP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:59:03 TP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:03 TP2] Init torch distributed begin.
[2026-01-17 08:59:04 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:59:04 TP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:59:04 TP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP3] Init torch distributed begin.
[2026-01-17 08:59:04 TP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP12] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:59:04 TP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP8] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 08:59:04 TP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP10] Init torch distributed begin.
[2026-01-17 08:59:04 TP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP6] Init torch distributed begin.
[2026-01-17 08:59:04 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP1] Init torch distributed begin.
[2026-01-17 08:59:04 TP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP7] Init torch distributed begin.
[2026-01-17 08:59:04 TP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP15] Init torch distributed begin.
[2026-01-17 08:59:04 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP0] Init torch distributed begin.
[2026-01-17 08:59:04 TP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:04 TP13] Init torch distributed begin.
[2026-01-17 08:59:05 TP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:05 TP14] Init torch distributed begin.
[2026-01-17 08:59:05 TP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:05 TP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 08:59:05 TP9] Init torch distributed begin.
[2026-01-17 08:59:05 TP11] Init torch distributed begin.
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[2026-01-17 08:59:07 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP15] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP14] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 08:59:07 TP13] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP12] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP9] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP11] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP10] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP8] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP5] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 08:59:07 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:07 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 08:59:08 TP12] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP14] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP13] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP10] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP15] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP9] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP11] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP8] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 08:59:08 TP11] Load weight begin. avail mem=61.08 GB
[2026-01-17 08:59:08 TP3] Load weight begin. avail mem=61.09 GB
[2026-01-17 08:59:08 TP14] Load weight begin. avail mem=60.83 GB
[2026-01-17 08:59:08 TP1] Load weight begin. avail mem=61.10 GB
[2026-01-17 08:59:08 TP9] Load weight begin. avail mem=61.10 GB
[2026-01-17 08:59:08 TP7] Load weight begin. avail mem=61.08 GB
[2026-01-17 08:59:08 TP0] Load weight begin. avail mem=60.76 GB
[2026-01-17 08:59:08 TP4] Load weight begin. avail mem=60.82 GB
[2026-01-17 08:59:08 TP13] Load weight begin. avail mem=61.09 GB
[2026-01-17 08:59:08 TP6] Load weight begin. avail mem=60.83 GB
[2026-01-17 08:59:08 TP8] Load weight begin. avail mem=60.82 GB
[2026-01-17 08:59:08 TP12] Load weight begin. avail mem=60.82 GB
[2026-01-17 08:59:08 TP5] Load weight begin. avail mem=61.09 GB
[2026-01-17 08:59:08 TP10] Load weight begin. avail mem=60.83 GB
[2026-01-17 08:59:08 TP2] Load weight begin. avail mem=60.82 GB
[2026-01-17 08:59:08 TP15] Load weight begin. avail mem=61.08 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
[2026-01-17 08:59:09 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 or AMD-platform with capability >= gfx942(MI30x) can use shared experts fusion optimization. Shared experts fusion optimization is disabled.

Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:43,  3.75it/s]

Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<01:25,  1.89it/s]

Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:01<01:09,  2.30it/s]

Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:01<01:08,  2.33it/s]

Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:02<01:39,  1.58it/s]

Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:04<02:23,  1.09it/s]

Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:05<02:19,  1.12it/s]

Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:05<01:58,  1.31it/s]

Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:05<01:37,  1.58it/s]

Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:06<01:26,  1.77it/s]

Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:06<01:18,  1.93it/s]

Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:07<01:14,  2.04it/s]

Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:07<01:10,  2.11it/s]

Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:07<01:06,  2.25it/s]

Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:08<01:05,  2.25it/s]

Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:08<01:03,  2.33it/s]

Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:09<01:04,  2.27it/s]

Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:09<01:01,  2.34it/s]

Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:09<00:55,  2.59it/s]

Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:10<01:11,  2.00it/s]

Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:10<01:02,  2.29it/s]

Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:11<00:52,  2.68it/s]

Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:11<00:45,  3.07it/s]

Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:11<00:42,  3.31it/s]

Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:12<00:43,  3.15it/s]

Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:12<00:32,  4.24it/s]

Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:12<00:37,  3.61it/s]

Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:13<00:38,  3.45it/s]

Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:13<00:40,  3.31it/s]

Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:13<00:43,  3.05it/s]

Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:14<00:44,  2.97it/s]

Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:14<00:46,  2.81it/s]

Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:14<00:42,  3.00it/s]

Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:15<00:40,  3.14it/s]

Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:15<00:42,  2.97it/s]

Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:15<00:40,  3.13it/s]

Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:16<00:39,  3.19it/s]

Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:16<00:39,  3.13it/s]

Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:16<00:42,  2.88it/s]

Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:17<00:39,  3.06it/s]

Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:17<00:41,  2.94it/s]

Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:17<00:45,  2.64it/s]

Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:18<00:40,  2.93it/s]

Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:18<00:37,  3.15it/s]

Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:18<00:35,  3.33it/s]

Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:18<00:35,  3.31it/s]

Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:19<00:24,  4.57it/s]

Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:19<00:24,  4.60it/s]

Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:20<00:45,  2.44it/s]

Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:20<00:41,  2.68it/s]

Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:21<00:43,  2.54it/s]

Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:21<00:39,  2.79it/s]

Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:21<00:35,  3.05it/s]

Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:21<00:33,  3.23it/s]

Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:22<00:30,  3.42it/s]

Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:22<00:28,  3.64it/s]

Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:22<00:31,  3.28it/s]

Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:23<00:30,  3.40it/s]

Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:23<00:30,  3.31it/s]

Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:23<00:34,  2.91it/s]

Loading safetensors checkpoint shards:  39% Completed | 63/163 [00:24<00:31,  3.18it/s]

Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:24<00:28,  3.47it/s]

Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:24<00:28,  3.43it/s]

Loading safetensors checkpoint shards:  40% Completed | 66/163 [00:24<00:29,  3.31it/s]

Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:25<00:26,  3.60it/s]

Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:25<00:28,  3.37it/s]

Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:25<00:26,  3.57it/s]

Loading safetensors checkpoint shards:  43% Completed | 70/163 [00:25<00:25,  3.69it/s]

Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:26<00:27,  3.38it/s]

Loading safetensors checkpoint shards:  44% Completed | 72/163 [00:26<00:28,  3.15it/s]

Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:26<00:27,  3.30it/s]

Loading safetensors checkpoint shards:  45% Completed | 74/163 [00:27<00:27,  3.24it/s]

Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:27<00:28,  3.10it/s]

Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:27<00:29,  2.97it/s]

Loading safetensors checkpoint shards:  47% Completed | 77/163 [00:28<00:29,  2.93it/s]

Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:28<00:27,  3.14it/s]

Loading safetensors checkpoint shards:  48% Completed | 79/163 [00:28<00:25,  3.27it/s]

Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:29<00:25,  3.23it/s]

Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:29<00:25,  3.21it/s]

Loading safetensors checkpoint shards:  50% Completed | 82/163 [00:29<00:23,  3.43it/s]

Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:30<00:25,  3.17it/s]

Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:30<00:24,  3.25it/s]

Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:30<00:26,  2.91it/s]

Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:31<00:41,  1.88it/s]

Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:32<00:37,  2.03it/s]

Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:32<00:33,  2.25it/s]

Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:32<00:28,  2.56it/s]

Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:33<00:29,  2.52it/s]

Loading safetensors checkpoint shards:  56% Completed | 91/163 [00:33<00:25,  2.86it/s]

Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:33<00:24,  2.86it/s]

Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:34<00:22,  3.06it/s]

Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:34<00:21,  3.25it/s]

Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:34<00:19,  3.50it/s]

Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:35<00:22,  3.02it/s]

Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:35<00:20,  3.22it/s]

Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:35<00:18,  3.43it/s]

Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:35<00:17,  3.65it/s]

Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:36<00:16,  3.74it/s]

Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:36<00:14,  4.14it/s]

Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:36<00:16,  3.67it/s]

Loading safetensors checkpoint shards:  64% Completed | 104/163 [00:37<00:17,  3.44it/s]

Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:37<00:16,  3.61it/s]

Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:37<00:18,  3.08it/s]

Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:37<00:14,  3.80it/s]

Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:38<00:16,  3.33it/s]

Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:38<00:16,  3.23it/s]

Loading safetensors checkpoint shards:  67% Completed | 110/163 [00:39<00:19,  2.66it/s]

Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:39<00:20,  2.54it/s]

Loading safetensors checkpoint shards:  69% Completed | 112/163 [00:40<00:20,  2.48it/s]

Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:40<00:20,  2.46it/s]

Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:40<00:19,  2.57it/s]

Loading safetensors checkpoint shards:  71% Completed | 115/163 [00:41<00:18,  2.63it/s]

Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:41<00:19,  2.41it/s]

Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:41<00:17,  2.58it/s]

Loading safetensors checkpoint shards:  72% Completed | 118/163 [00:42<00:17,  2.63it/s]

Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:42<00:15,  2.82it/s]

Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:42<00:14,  2.94it/s]

Loading safetensors checkpoint shards:  74% Completed | 121/163 [00:43<00:14,  2.95it/s]

Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:43<00:13,  2.96it/s]

Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:44<00:14,  2.83it/s]

Loading safetensors checkpoint shards:  76% Completed | 124/163 [00:44<00:13,  2.93it/s]

Loading safetensors checkpoint shards:  77% Completed | 125/163 [00:44<00:13,  2.79it/s]

Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:45<00:13,  2.81it/s]

Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:45<00:12,  2.91it/s]

Loading safetensors checkpoint shards:  79% Completed | 128/163 [00:45<00:12,  2.79it/s]

Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:46<00:08,  3.74it/s]

Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:46<00:09,  3.20it/s]

Loading safetensors checkpoint shards:  81% Completed | 132/163 [00:47<00:17,  1.79it/s]

Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:48<00:15,  1.90it/s]

Loading safetensors checkpoint shards:  82% Completed | 134/163 [00:48<00:13,  2.14it/s]

Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:48<00:12,  2.22it/s]

Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:49<00:11,  2.39it/s]

Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:49<00:09,  2.64it/s]

Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:49<00:08,  2.82it/s]

Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:50<00:08,  2.93it/s]

Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:50<00:07,  3.22it/s]

Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:50<00:06,  3.46it/s]

Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:50<00:05,  3.50it/s]

Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:51<00:06,  3.19it/s]

Loading safetensors checkpoint shards:  88% Completed | 144/163 [00:51<00:05,  3.53it/s]

Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:51<00:05,  3.21it/s]

Loading safetensors checkpoint shards:  90% Completed | 146/163 [00:52<00:05,  2.95it/s]

Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:52<00:05,  2.84it/s]

Loading safetensors checkpoint shards:  91% Completed | 148/163 [00:53<00:05,  2.90it/s]

Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:53<00:04,  2.96it/s]

Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:53<00:03,  3.65it/s]

Loading safetensors checkpoint shards:  93% Completed | 152/163 [00:54<00:03,  3.49it/s]

Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:54<00:02,  3.41it/s]

Loading safetensors checkpoint shards:  94% Completed | 154/163 [00:54<00:02,  3.42it/s]

Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:55<00:02,  3.37it/s]

Loading safetensors checkpoint shards:  96% Completed | 156/163 [00:55<00:02,  3.32it/s]

Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:55<00:01,  3.24it/s]

Loading safetensors checkpoint shards:  97% Completed | 158/163 [00:56<00:01,  2.99it/s]

Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:56<00:01,  3.16it/s]

Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:56<00:00,  3.23it/s]

Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:56<00:00,  3.26it/s]

Loading safetensors checkpoint shards:  99% Completed | 162/163 [00:57<00:00,  2.87it/s]

Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:57<00:00,  3.03it/s]

Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:57<00:00,  2.83it/s]

[2026-01-17 09:00:32 TP8] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.65 GB, mem usage=44.16 GB.
[2026-01-17 09:00:33 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.93 GB, mem usage=44.16 GB.
[2026-01-17 09:00:34 TP13] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.93 GB, mem usage=44.16 GB.
[2026-01-17 09:00:34 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.65 GB, mem usage=44.16 GB.
[2026-01-17 09:00:34 TP15] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.92 GB, mem usage=44.16 GB.
[2026-01-17 09:00:34 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.66 GB, mem usage=44.16 GB.
[2026-01-17 09:00:35 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.93 GB, mem usage=44.16 GB.
[2026-01-17 09:00:35 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.67 GB, mem usage=44.16 GB.
[2026-01-17 09:00:36 TP9] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.93 GB, mem usage=44.16 GB.
[2026-01-17 09:00:36 TP12] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.66 GB, mem usage=44.16 GB.
[2026-01-17 09:00:36 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.60 GB, mem usage=44.16 GB.
[2026-01-17 09:00:39 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.92 GB, mem usage=44.16 GB.
[2026-01-17 09:00:39 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.93 GB, mem usage=44.16 GB.
[2026-01-17 09:00:39 TP10] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.67 GB, mem usage=44.16 GB.
[2026-01-17 09:00:39 TP14] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.67 GB, mem usage=44.16 GB.
[2026-01-17 09:00:40 TP11] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=16.92 GB, mem usage=44.16 GB.
[2026-01-17 09:00:40 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 09:00:40 TP0] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP14] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP15] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP13] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP12] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP11] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP10] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP9] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP7] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP8] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP6] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP5] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP4] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP1] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP3] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP2] The available memory for KV cache is 10.50 GB.
[2026-01-17 09:00:40 TP5] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP9] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP5] Memory pool end. avail mem=4.13 GB
[2026-01-17 09:00:40 TP4] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP0] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP11] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP9] Memory pool end. avail mem=4.13 GB
[2026-01-17 09:00:40 TP0] Memory pool end. avail mem=3.79 GB
[2026-01-17 09:00:40 TP4] Memory pool end. avail mem=3.85 GB
[2026-01-17 09:00:40 TP11] Memory pool end. avail mem=4.12 GB
[2026-01-17 09:00:40 TP13] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP8] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP12] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP8] Memory pool end. avail mem=3.85 GB
[2026-01-17 09:00:40 TP13] Memory pool end. avail mem=4.13 GB
[2026-01-17 09:00:40 TP12] Memory pool end. avail mem=3.85 GB
[2026-01-17 09:00:40 TP10] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP10] Memory pool end. avail mem=3.86 GB
[2026-01-17 09:00:40 TP1] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP7] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP1] Memory pool end. avail mem=4.13 GB
[2026-01-17 09:00:40 TP7] Memory pool end. avail mem=4.12 GB
[2026-01-17 09:00:40 TP15] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP6] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP14] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP2] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP15] Memory pool end. avail mem=4.12 GB
[2026-01-17 09:00:40 TP14] Memory pool end. avail mem=3.86 GB
[2026-01-17 09:00:40 TP6] Memory pool end. avail mem=3.86 GB
[2026-01-17 09:00:40 TP2] Memory pool end. avail mem=3.85 GB
[2026-01-17 09:00:40 TP3] KV Cache is allocated. #tokens: 143872, KV size: 11.52 GB
[2026-01-17 09:00:40 TP3] Memory pool end. avail mem=4.13 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 09:00:41 TP0] max_total_num_tokens=143872, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=163840, available_gpu_mem=3.81 GB
[2026-01-17 09:00:42] INFO:     Started server process [110098]
[2026-01-17 09:00:42] INFO:     Waiting for application startup.
[2026-01-17 09:00:42] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.95}
[2026-01-17 09:00:42] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.95}
[2026-01-17 09:00:42] INFO:     Application startup complete.
[2026-01-17 09:00:42] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 09:00:43] INFO:     127.0.0.1:49888 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 09:00:43 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
................[rank7]:[W117 09:00:47.232858709 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank5]:[W117 09:00:47.234484411 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank12]:[W117 09:00:47.234523603 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank13]:[W117 09:00:47.234705050 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank4]:[W117 09:00:47.234780223 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank9]:[W117 09:00:47.234869456 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank3]:[W117 09:00:47.234878027 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank8]:[W117 09:00:47.235482700 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank6]:[W117 09:00:47.235530462 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank1]:[W117 09:00:47.235845344 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank10]:[W117 09:00:47.236015291 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank11]:[W117 09:00:47.236421556 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank15]:[W117 09:00:47.236438107 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank0]:[W117 09:00:47.236884324 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank14]:[W117 09:00:47.237070561 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank2]:[W117 09:00:47.237217787 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[2026-01-17 09:00:50] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:00:50] The server is fired up and ready to roll!
[2026-01-17 09:00:51 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:00:52] INFO:     127.0.0.1:44132 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 09:00:52] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 09:00:52] INFO:     127.0.0.1:44140 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 09:00:52 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:00:53] INFO:     127.0.0.1:44144 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8 --trust-remote-code --mem-fraction-static 0.9 --attention-backend ascend --disable-cuda-graph --tp-size 16 --quantization modelslim --disable-radix-cache --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 09:00:53 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:00:53 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 1, #queue-req: 79,
[2026-01-17 09:00:54 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 11, #queue-req: 106,
[2026-01-17 09:00:56 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.12, #running-req: 22, #queue-req: 95,
[2026-01-17 09:00:57 TP0] Prefill batch, #new-seq: 12, #new-token: 8192, #cached-token: 0, token usage: 0.18, #running-req: 32, #queue-req: 84,
[2026-01-17 09:00:58 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.23, #running-req: 43, #queue-req: 74,
[2026-01-17 09:01:00 TP0] Prefill batch, #new-seq: 12, #new-token: 8192, #cached-token: 0, token usage: 0.29, #running-req: 53, #queue-req: 63,
[2026-01-17 09:01:01 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.35, #running-req: 64, #queue-req: 53,
[2026-01-17 09:01:03] INFO:     127.0.0.1:44614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:03 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.40, #running-req: 75, #queue-req: 42,

  0%|          | 1/200 [00:09<32:58,  9.94s/it][2026-01-17 09:01:04 TP0] Prefill batch, #new-seq: 12, #new-token: 8192, #cached-token: 0, token usage: 0.46, #running-req: 85, #queue-req: 32,
[2026-01-17 09:01:05 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.51, #running-req: 96, #queue-req: 22,
[2026-01-17 09:01:07 TP0] Prefill batch, #new-seq: 11, #new-token: 8192, #cached-token: 0, token usage: 0.57, #running-req: 107, #queue-req: 11,
[2026-01-17 09:01:08] INFO:     127.0.0.1:44992 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:15<24:09,  7.32s/it][2026-01-17 09:01:08 TP0] Prefill batch, #new-seq: 10, #new-token: 7424, #cached-token: 0, token usage: 0.62, #running-req: 117, #queue-req: 2,
[2026-01-17 09:01:10 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.67, #running-req: 125, #queue-req: 2,
[2026-01-17 09:01:10] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:10] INFO:     127.0.0.1:44640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:10] INFO:     127.0.0.1:44898 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:17<15:58,  4.86s/it]
  2%|▎         | 5/200 [00:17<05:03,  1.56s/it]
  2%|▎         | 5/200 [00:17<05:03,  1.56s/it][2026-01-17 09:01:10 TP0] Prefill batch, #new-seq: 3, #new-token: 2304, #cached-token: 0, token usage: 0.66, #running-req: 123, #queue-req: 2,
[2026-01-17 09:01:11] INFO:     127.0.0.1:44282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:11] INFO:     127.0.0.1:44652 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:11] INFO:     127.0.0.1:44892 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:18<04:32,  1.40s/it]
  4%|▍         | 8/200 [00:18<02:14,  1.43it/s]
  4%|▍         | 8/200 [00:18<02:14,  1.43it/s][2026-01-17 09:01:11 TP0] Prefill batch, #new-seq: 3, #new-token: 2304, #cached-token: 0, token usage: 0.66, #running-req: 123, #queue-req: 2,
[2026-01-17 09:01:20] INFO:     127.0.0.1:44936 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:27<06:46,  2.13s/it][2026-01-17 09:01:20 TP0] Decode batch, #running-req: 126, #token: 100736, token usage: 0.70, npu graph: False, gen throughput (token/s): 28.64, #queue-req: 2,
[2026-01-17 09:01:20 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.70, #running-req: 125, #queue-req: 1,
[2026-01-17 09:01:23] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:30<07:27,  2.35s/it][2026-01-17 09:01:24] INFO:     127.0.0.1:44820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:24] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:31<06:35,  2.09s/it]
  6%|▌         | 12/200 [00:31<04:44,  1.51s/it][2026-01-17 09:01:25 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.72, #running-req: 124, #queue-req: 3,
[2026-01-17 09:01:26] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:26] INFO:     127.0.0.1:44618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:26] INFO:     127.0.0.1:45106 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:33<04:41,  1.50s/it]
  8%|▊         | 15/200 [00:33<02:44,  1.13it/s]
  8%|▊         | 15/200 [00:33<02:44,  1.13it/s][2026-01-17 09:01:26 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.72, #running-req: 122, #queue-req: 4,
[2026-01-17 09:01:28] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:35<03:23,  1.11s/it][2026-01-17 09:01:28] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:28 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.73, #running-req: 123, #queue-req: 4,

  8%|▊         | 17/200 [00:35<02:51,  1.07it/s][2026-01-17 09:01:28] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:35<02:23,  1.27it/s][2026-01-17 09:01:29 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.73, #running-req: 122, #queue-req: 4,
[2026-01-17 09:01:29] INFO:     127.0.0.1:44868 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:36<02:17,  1.31it/s][2026-01-17 09:01:30] INFO:     127.0.0.1:45090 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:36<02:07,  1.42it/s][2026-01-17 09:01:30 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.74, #running-req: 122, #queue-req: 5,
[2026-01-17 09:01:32] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:32] INFO:     127.0.0.1:44836 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:38<03:05,  1.04s/it]
 11%|█         | 22/200 [00:38<02:59,  1.01s/it][2026-01-17 09:01:32] INFO:     127.0.0.1:44402 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:39<02:28,  1.19it/s][2026-01-17 09:01:32 TP0] Prefill batch, #new-seq: 2, #new-token: 1664, #cached-token: 0, token usage: 0.73, #running-req: 121, #queue-req: 5,
[2026-01-17 09:01:32 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.75, #running-req: 122, #queue-req: 5,
[2026-01-17 09:01:34] INFO:     127.0.0.1:44190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:34] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:41<03:27,  1.18s/it]
 12%|█▎        | 25/200 [00:41<03:19,  1.14s/it][2026-01-17 09:01:34 TP0] Decode batch, #running-req: 123, #token: 105984, token usage: 0.74, npu graph: False, gen throughput (token/s): 349.70, #queue-req: 5,
[2026-01-17 09:01:34] INFO:     127.0.0.1:44400 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:41<02:43,  1.06it/s][2026-01-17 09:01:34 TP0] Prefill batch, #new-seq: 3, #new-token: 2432, #cached-token: 0, token usage: 0.74, #running-req: 121, #queue-req: 4,
[2026-01-17 09:01:37] INFO:     127.0.0.1:44852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:37] INFO:     127.0.0.1:45086 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:43<03:44,  1.30s/it]
 14%|█▍        | 28/200 [00:43<03:35,  1.25s/it][2026-01-17 09:01:37] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:44<02:56,  1.03s/it][2026-01-17 09:01:37 TP0] Prefill batch, #new-seq: 3, #new-token: 2304, #cached-token: 0, token usage: 0.74, #running-req: 121, #queue-req: 4,
[2026-01-17 09:01:38 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.75, #running-req: 123, #queue-req: 4,
[2026-01-17 09:01:39] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:46<03:33,  1.26s/it][2026-01-17 09:01:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.75, #running-req: 123, #queue-req: 4,
[2026-01-17 09:01:40] INFO:     127.0.0.1:45096 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:47<03:15,  1.16s/it][2026-01-17 09:01:40 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.75, #running-req: 123, #queue-req: 4,
[2026-01-17 09:01:42] INFO:     127.0.0.1:45072 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:49<04:04,  1.45s/it][2026-01-17 09:01:42 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.76, #running-req: 123, #queue-req: 3,
[2026-01-17 09:01:43] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:50<03:31,  1.26s/it][2026-01-17 09:01:43 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.76, #running-req: 124, #queue-req: 3,
[2026-01-17 09:01:44] INFO:     127.0.0.1:44404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:44] INFO:     127.0.0.1:45084 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:50<03:05,  1.12s/it]
 18%|█▊        | 35/200 [00:50<02:09,  1.27it/s][2026-01-17 09:01:44 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.75, #running-req: 123, #queue-req: 3,
[2026-01-17 09:01:44] INFO:     127.0.0.1:44798 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:51<02:08,  1.28it/s][2026-01-17 09:01:45 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.76, #running-req: 124, #queue-req: 3,
[2026-01-17 09:01:45] INFO:     127.0.0.1:44172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:45] INFO:     127.0.0.1:45210 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:52<02:06,  1.28it/s]
 19%|█▉        | 38/200 [00:52<01:38,  1.64it/s][2026-01-17 09:01:45 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.75, #running-req: 123, #queue-req: 3,
[2026-01-17 09:01:46] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:53<02:07,  1.26it/s][2026-01-17 09:01:47] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:54<01:47,  1.49it/s][2026-01-17 09:01:47 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.75, #running-req: 124, #queue-req: 2,
[2026-01-17 09:01:47] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:54<01:39,  1.61it/s][2026-01-17 09:01:48 TP0] Prefill batch, #new-seq: 2, #new-token: 1664, #cached-token: 0, token usage: 0.76, #running-req: 124, #queue-req: 2,
[2026-01-17 09:01:48] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:55<01:45,  1.50it/s][2026-01-17 09:01:48 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.76, #running-req: 125, #queue-req: 2,
[2026-01-17 09:01:49] INFO:     127.0.0.1:45114 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:56<01:50,  1.42it/s][2026-01-17 09:01:49 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.76, #running-req: 125, #queue-req: 2,
[2026-01-17 09:01:50] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:56<01:52,  1.39it/s][2026-01-17 09:01:50 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.76, #running-req: 125, #queue-req: 2,
[2026-01-17 09:01:51] INFO:     127.0.0.1:44346 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:58<02:44,  1.06s/it][2026-01-17 09:01:52 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.76, #running-req: 125, #queue-req: 1,
[2026-01-17 09:01:53] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [01:00<03:13,  1.25s/it][2026-01-17 09:01:54 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.77, #running-req: 126, #queue-req: 1,
[2026-01-17 09:01:55 TP0] Decode batch, #running-req: 127, #token: 110336, token usage: 0.77, npu graph: False, gen throughput (token/s): 245.05, #queue-req: 1,
[2026-01-17 09:01:55] INFO:     127.0.0.1:44180 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [01:01<03:15,  1.28s/it][2026-01-17 09:01:55] INFO:     127.0.0.1:55044 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [01:02<02:29,  1.02it/s][2026-01-17 09:01:55 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.76, #running-req: 126, #queue-req: 0,
[2026-01-17 09:01:56] INFO:     127.0.0.1:44848 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [01:03<02:42,  1.07s/it][2026-01-17 09:01:56] INFO:     127.0.0.1:44196 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [01:03<02:05,  1.19it/s][2026-01-17 09:01:56 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.76, #running-req: 126, #queue-req: 0,
[2026-01-17 09:01:57 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.77, #running-req: 127, #queue-req: 0,
[2026-01-17 09:01:59] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:01:59] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [01:05<03:03,  1.23s/it]
 26%|██▌       | 52/200 [01:05<02:52,  1.16s/it][2026-01-17 09:01:59] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [01:06<02:19,  1.06it/s][2026-01-17 09:01:59 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.76, #running-req: 126, #queue-req: 0,
[2026-01-17 09:01:59 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.77, #running-req: 128, #queue-req: 0,
[2026-01-17 09:02:02] INFO:     127.0.0.1:55060 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [01:09<03:50,  1.58s/it][2026-01-17 09:02:03 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.77, #running-req: 127, #queue-req: 0,
[2026-01-17 09:02:03] INFO:     127.0.0.1:53340 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [01:10<03:17,  1.36s/it][2026-01-17 09:02:03 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.77, #running-req: 127, #queue-req: 0,
[2026-01-17 09:02:04] INFO:     127.0.0.1:44382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:04] INFO:     127.0.0.1:39484 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [01:11<02:51,  1.19s/it]
 28%|██▊       | 57/200 [01:11<01:59,  1.20it/s][2026-01-17 09:02:04 TP0] Prefill batch, #new-seq: 2, #new-token: 1664, #cached-token: 0, token usage: 0.77, #running-req: 126, #queue-req: 0,
[2026-01-17 09:02:05] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [01:12<02:28,  1.05s/it][2026-01-17 09:02:06 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.78, #running-req: 127, #queue-req: 0,
[2026-01-17 09:02:07] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [01:14<02:53,  1.23s/it][2026-01-17 09:02:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.78, #running-req: 127, #queue-req: 0,
[2026-01-17 09:02:08] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:08] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [01:15<02:42,  1.16s/it]
 30%|███       | 61/200 [01:15<02:00,  1.15it/s][2026-01-17 09:02:09 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.77, #running-req: 126, #queue-req: 0,
[2026-01-17 09:02:11] INFO:     127.0.0.1:44418 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [01:18<03:08,  1.36s/it][2026-01-17 09:02:12 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.79, #running-req: 127, #queue-req: 0,
[2026-01-17 09:02:12] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:12] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [01:19<02:49,  1.24s/it]
 32%|███▏      | 64/200 [01:19<02:02,  1.11it/s][2026-01-17 09:02:12 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.78, #running-req: 126, #queue-req: 0,
[2026-01-17 09:02:13] INFO:     127.0.0.1:44370 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [01:20<01:58,  1.14it/s][2026-01-17 09:02:13 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.78, #running-req: 127, #queue-req: 0,
[2026-01-17 09:02:14] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [01:20<01:54,  1.17it/s][2026-01-17 09:02:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.78, #running-req: 127, #queue-req: 0,
[2026-01-17 09:02:16] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:16] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [01:23<02:38,  1.19s/it]
 34%|███▍      | 68/200 [01:23<02:31,  1.14s/it][2026-01-17 09:02:16] INFO:     127.0.0.1:44768 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [01:23<02:03,  1.06it/s][2026-01-17 09:02:16 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.78, #running-req: 126, #queue-req: 0,
[2026-01-17 09:02:17] INFO:     127.0.0.1:39562 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [01:23<01:46,  1.22it/s][2026-01-17 09:02:17 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.78, #running-req: 128, #queue-req: 0,
[2026-01-17 09:02:17 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.79, #running-req: 129, #queue-req: 0,
[2026-01-17 09:02:18] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:18] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:18] INFO:     127.0.0.1:39494 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [01:25<02:13,  1.04s/it]
 36%|███▋      | 73/200 [01:25<01:35,  1.33it/s]
 36%|███▋      | 73/200 [01:25<01:35,  1.33it/s][2026-01-17 09:02:18 TP0] Prefill batch, #new-seq: 2, #new-token: 1536, #cached-token: 0, token usage: 0.78, #running-req: 125, #queue-req: 0,
[2026-01-17 09:02:19 TP0] Decode batch, #running-req: 125, #token: 111616, token usage: 0.78, npu graph: False, gen throughput (token/s): 209.04, #queue-req: 0,
[2026-01-17 09:02:19] INFO:     127.0.0.1:53318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:19] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [01:26<01:34,  1.34it/s]
 38%|███▊      | 75/200 [01:26<01:17,  1.62it/s][2026-01-17 09:02:20] INFO:     127.0.0.1:44786 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [01:27<01:29,  1.38it/s][2026-01-17 09:02:21] INFO:     127.0.0.1:39540 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [01:28<01:32,  1.34it/s][2026-01-17 09:02:23] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [01:30<02:20,  1.15s/it][2026-01-17 09:02:24] INFO:     127.0.0.1:40652 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [01:31<01:54,  1.05it/s][2026-01-17 09:02:24] INFO:     127.0.0.1:55018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:24] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [01:31<01:42,  1.17it/s]
 40%|████      | 81/200 [01:31<01:12,  1.63it/s][2026-01-17 09:02:25] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [01:32<01:08,  1.72it/s][2026-01-17 09:02:27] INFO:     127.0.0.1:40668 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [01:34<01:47,  1.08it/s][2026-01-17 09:02:27] INFO:     127.0.0.1:40712 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [01:34<01:33,  1.24it/s][2026-01-17 09:02:28] INFO:     127.0.0.1:47074 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [01:35<01:38,  1.17it/s][2026-01-17 09:02:31] INFO:     127.0.0.1:48344 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [01:38<02:44,  1.44s/it][2026-01-17 09:02:36 TP0] Decode batch, #running-req: 114, #token: 107648, token usage: 0.75, npu graph: False, gen throughput (token/s): 273.42, #queue-req: 0,
[2026-01-17 09:02:36] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:36] INFO:     127.0.0.1:40642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:36] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [01:43<04:44,  2.52s/it]
 44%|████▍     | 89/200 [01:43<03:58,  2.15s/it]
 44%|████▍     | 89/200 [01:43<03:58,  2.15s/it][2026-01-17 09:02:38] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:38] INFO:     127.0.0.1:47038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:38] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [01:45<03:44,  2.04s/it]
 46%|████▌     | 92/200 [01:45<02:02,  1.14s/it]
 46%|████▌     | 92/200 [01:45<02:02,  1.14s/it][2026-01-17 09:02:38] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:38] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [01:45<01:50,  1.03s/it]
 47%|████▋     | 94/200 [01:45<01:23,  1.28it/s][2026-01-17 09:02:39] INFO:     127.0.0.1:48342 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [01:46<01:17,  1.36it/s][2026-01-17 09:02:39] INFO:     127.0.0.1:39510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:39] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [01:46<01:12,  1.44it/s]
 48%|████▊     | 97/200 [01:46<00:54,  1.88it/s][2026-01-17 09:02:43] INFO:     127.0.0.1:48406 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [01:49<01:50,  1.09s/it][2026-01-17 09:02:43] INFO:     127.0.0.1:40638 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [01:50<01:35,  1.05it/s][2026-01-17 09:02:44] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [01:50<01:22,  1.21it/s][2026-01-17 09:02:44] INFO:     127.0.0.1:48410 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [01:51<01:24,  1.17it/s][2026-01-17 09:02:45] INFO:     127.0.0.1:40694 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [01:52<01:13,  1.33it/s][2026-01-17 09:02:47] INFO:     127.0.0.1:53372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:47] INFO:     127.0.0.1:48398 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [01:54<01:49,  1.13s/it]
 52%|█████▏    | 104/200 [01:54<01:44,  1.09s/it][2026-01-17 09:02:52] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:02:52] INFO:     127.0.0.1:47042 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [01:59<03:07,  1.98s/it]
 53%|█████▎    | 106/200 [01:59<03:21,  2.14s/it][2026-01-17 09:02:52] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [01:59<02:44,  1.77s/it][2026-01-17 09:02:56 TP0] Decode batch, #running-req: 93, #token: 93824, token usage: 0.65, npu graph: False, gen throughput (token/s): 198.30, #queue-req: 0,
[2026-01-17 09:02:57] INFO:     127.0.0.1:40696 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [02:04<03:57,  2.58s/it][2026-01-17 09:02:58] INFO:     127.0.0.1:47022 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [02:05<03:06,  2.05s/it][2026-01-17 09:02:58] INFO:     127.0.0.1:40684 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [02:05<02:26,  1.63s/it][2026-01-17 09:03:00] INFO:     127.0.0.1:55006 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [02:07<02:20,  1.58s/it][2026-01-17 09:03:02] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [02:09<02:39,  1.81s/it][2026-01-17 09:03:03] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [02:10<02:16,  1.57s/it][2026-01-17 09:03:06] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [02:12<02:35,  1.80s/it][2026-01-17 09:03:08] INFO:     127.0.0.1:35214 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [02:15<02:50,  2.01s/it][2026-01-17 09:03:14] INFO:     127.0.0.1:53396 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [02:20<04:15,  3.04s/it][2026-01-17 09:03:14] INFO:     127.0.0.1:53302 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [02:21<03:09,  2.29s/it][2026-01-17 09:03:16] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [02:22<02:49,  2.06s/it][2026-01-17 09:03:16 TP0] Decode batch, #running-req: 83, #token: 85248, token usage: 0.59, npu graph: False, gen throughput (token/s): 175.56, #queue-req: 0,
[2026-01-17 09:03:24] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [02:31<05:11,  3.85s/it][2026-01-17 09:03:27] INFO:     127.0.0.1:39476 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [02:33<04:46,  3.58s/it][2026-01-17 09:03:28] INFO:     127.0.0.1:53404 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [02:34<03:41,  2.80s/it][2026-01-17 09:03:29] INFO:     127.0.0.1:48372 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [02:36<03:06,  2.39s/it][2026-01-17 09:03:36 TP0] Decode batch, #running-req: 78, #token: 87296, token usage: 0.61, npu graph: False, gen throughput (token/s): 165.20, #queue-req: 0,
[2026-01-17 09:03:40] INFO:     127.0.0.1:48310 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [02:47<06:21,  4.96s/it][2026-01-17 09:03:55 TP0] Decode batch, #running-req: 77, #token: 88576, token usage: 0.62, npu graph: False, gen throughput (token/s): 160.51, #queue-req: 0,
[2026-01-17 09:04:01] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [03:07<12:10,  9.62s/it][2026-01-17 09:04:15 TP0] Decode batch, #running-req: 76, #token: 88576, token usage: 0.62, npu graph: False, gen throughput (token/s): 154.84, #queue-req: 0,
[2026-01-17 09:04:33 TP0] Decode batch, #running-req: 76, #token: 92800, token usage: 0.65, npu graph: False, gen throughput (token/s): 162.22, #queue-req: 0,
[2026-01-17 09:04:53 TP0] Decode batch, #running-req: 76, #token: 97280, token usage: 0.68, npu graph: False, gen throughput (token/s): 156.88, #queue-req: 0,
[2026-01-17 09:05:12] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44306 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44324 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12 TP0] Decode batch, #running-req: 76, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 156.02, #queue-req: 0,
[2026-01-17 09:05:12] INFO:     127.0.0.1:44426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:44986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:05:12] INFO:     127.0.0.1:45206 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [04:19<35:18, 28.25s/it]
100%|██████████| 200/200 [04:19<00:00,  1.30s/it]
.
----------------------------------------------------------------------
Ran 1 test in 392.966s

OK
Accuracy: 0.580
Invalid: 0.090
Latency: 259.966 s
Output throughput: 191.325 token/s
.
.
End (21/62):
filename='ascend/llm_models/test_ascend_deepseek_v3_2_exp_w8a8.py', elapsed=404, estimated_time=400
.
.


✓ PASSED on retry (attempt 2): ascend/llm_models/test_ascend_deepseek_v3_2_exp_w8a8.py

.
.
Begin (22/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_qwen3_235b_a22b_w8a8.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:05:34] WARNING model_config.py:820: modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:34] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/vllm-ascend/Qwen3-235B-A22B-W8A8', tokenizer_path='/root/.cache/modelscope/hub/models/vllm-ascend/Qwen3-235B-A22B-W8A8', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='modelslim', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=519400009, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/vllm-ascend/Qwen3-235B-A22B-W8A8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='modelslim', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 09:05:34] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:35] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:05:44 TP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:44 TP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:05:44 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:44 TP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:05:45 TP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:05:45 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:05:45 TP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:05:45 TP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:05:45 TP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:45 TP6] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:05:45 TP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:45 TP7] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:05:45 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:45 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:05:45 TP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:45 TP2] Init torch distributed begin.
[2026-01-17 09:05:45 TP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:45 TP5] Init torch distributed begin.
[2026-01-17 09:05:45 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:45 TP1] Init torch distributed begin.
[2026-01-17 09:05:46 TP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:46 TP4] Init torch distributed begin.
[2026-01-17 09:05:46 TP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:05:46 TP3] Init torch distributed begin.
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2026-01-17 09:05:47 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:05:47 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:05:47 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:05:47 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:05:47 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:05:47 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:05:47 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:05:47 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:05:47 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:05:48 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:05:48 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:05:48 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:05:48 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:05:48 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:05:48 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:05:48 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:05:48 TP7] Load weight begin. avail mem=61.10 GB
[2026-01-17 09:05:48 TP2] Load weight begin. avail mem=60.84 GB
[2026-01-17 09:05:48 TP1] Load weight begin. avail mem=61.11 GB
[2026-01-17 09:05:48 TP5] Load weight begin. avail mem=61.11 GB
[2026-01-17 09:05:48 TP4] Load weight begin. avail mem=60.84 GB
[2026-01-17 09:05:48 TP6] Load weight begin. avail mem=60.85 GB
[2026-01-17 09:05:48 TP0] Load weight begin. avail mem=60.78 GB
[2026-01-17 09:05:48 TP3] Load weight begin. avail mem=61.11 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)

Loading safetensors checkpoint shards:   0% Completed | 0/56 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   2% Completed | 1/56 [00:06<05:46,  6.31s/it]

Loading safetensors checkpoint shards:   4% Completed | 2/56 [00:12<05:50,  6.49s/it]

Loading safetensors checkpoint shards:   5% Completed | 3/56 [00:19<05:50,  6.61s/it]

Loading safetensors checkpoint shards:   7% Completed | 4/56 [00:26<05:41,  6.56s/it]

Loading safetensors checkpoint shards:   9% Completed | 5/56 [00:32<05:31,  6.51s/it]

Loading safetensors checkpoint shards:  11% Completed | 6/56 [00:38<05:22,  6.46s/it]

Loading safetensors checkpoint shards:  12% Completed | 7/56 [00:45<05:16,  6.45s/it]

Loading safetensors checkpoint shards:  14% Completed | 8/56 [00:52<05:14,  6.55s/it]

Loading safetensors checkpoint shards:  16% Completed | 9/56 [00:58<05:04,  6.48s/it]

Loading safetensors checkpoint shards:  18% Completed | 10/56 [01:04<04:56,  6.44s/it]

Loading safetensors checkpoint shards:  20% Completed | 11/56 [01:11<04:55,  6.56s/it]

Loading safetensors checkpoint shards:  21% Completed | 12/56 [01:18<04:50,  6.59s/it]

Loading safetensors checkpoint shards:  23% Completed | 13/56 [01:24<04:41,  6.54s/it]

Loading safetensors checkpoint shards:  25% Completed | 14/56 [01:30<04:25,  6.31s/it]

Loading safetensors checkpoint shards:  27% Completed | 15/56 [01:36<04:19,  6.33s/it]

Loading safetensors checkpoint shards:  29% Completed | 16/56 [01:43<04:13,  6.33s/it]

Loading safetensors checkpoint shards:  30% Completed | 17/56 [01:49<04:06,  6.32s/it]

Loading safetensors checkpoint shards:  32% Completed | 18/56 [01:55<04:00,  6.33s/it]

Loading safetensors checkpoint shards:  34% Completed | 19/56 [02:02<03:53,  6.31s/it]

Loading safetensors checkpoint shards:  36% Completed | 20/56 [02:03<02:55,  4.86s/it]

Loading safetensors checkpoint shards:  38% Completed | 21/56 [02:09<03:01,  5.18s/it]

Loading safetensors checkpoint shards:  39% Completed | 22/56 [02:15<03:05,  5.45s/it]

Loading safetensors checkpoint shards:  41% Completed | 23/56 [02:22<03:09,  5.75s/it]

Loading safetensors checkpoint shards:  43% Completed | 24/56 [02:28<03:06,  5.83s/it]

Loading safetensors checkpoint shards:  45% Completed | 25/56 [02:34<03:02,  5.88s/it]

Loading safetensors checkpoint shards:  46% Completed | 26/56 [02:40<02:59,  6.00s/it]

Loading safetensors checkpoint shards:  48% Completed | 27/56 [02:45<02:49,  5.86s/it]

Loading safetensors checkpoint shards:  50% Completed | 28/56 [02:52<02:50,  6.10s/it]

Loading safetensors checkpoint shards:  52% Completed | 29/56 [02:59<02:49,  6.27s/it]

Loading safetensors checkpoint shards:  54% Completed | 30/56 [03:06<02:47,  6.44s/it]

Loading safetensors checkpoint shards:  55% Completed | 31/56 [03:12<02:41,  6.46s/it]

Loading safetensors checkpoint shards:  57% Completed | 32/56 [03:19<02:37,  6.56s/it]

Loading safetensors checkpoint shards:  59% Completed | 33/56 [03:26<02:32,  6.64s/it]

Loading safetensors checkpoint shards:  61% Completed | 34/56 [03:32<02:27,  6.69s/it]

Loading safetensors checkpoint shards:  62% Completed | 35/56 [03:39<02:19,  6.63s/it]

Loading safetensors checkpoint shards:  64% Completed | 36/56 [03:45<02:11,  6.56s/it]

Loading safetensors checkpoint shards:  66% Completed | 37/56 [03:52<02:03,  6.52s/it]

Loading safetensors checkpoint shards:  68% Completed | 38/56 [03:58<01:57,  6.51s/it]

Loading safetensors checkpoint shards:  70% Completed | 39/56 [04:05<01:51,  6.57s/it]

Loading safetensors checkpoint shards:  71% Completed | 40/56 [04:12<01:45,  6.58s/it]

Loading safetensors checkpoint shards:  73% Completed | 41/56 [04:18<01:37,  6.50s/it]

Loading safetensors checkpoint shards:  75% Completed | 42/56 [04:24<01:28,  6.30s/it]

Loading safetensors checkpoint shards:  77% Completed | 43/56 [04:30<01:22,  6.36s/it]

Loading safetensors checkpoint shards:  79% Completed | 44/56 [04:37<01:16,  6.37s/it]

Loading safetensors checkpoint shards:  80% Completed | 45/56 [04:43<01:10,  6.40s/it]

Loading safetensors checkpoint shards:  82% Completed | 46/56 [04:49<01:03,  6.36s/it]

Loading safetensors checkpoint shards:  84% Completed | 47/56 [04:56<00:57,  6.37s/it]

Loading safetensors checkpoint shards:  86% Completed | 48/56 [05:02<00:50,  6.31s/it]

Loading safetensors checkpoint shards:  88% Completed | 49/56 [05:08<00:43,  6.26s/it]

Loading safetensors checkpoint shards:  89% Completed | 50/56 [05:14<00:37,  6.21s/it]

Loading safetensors checkpoint shards:  91% Completed | 51/56 [05:21<00:31,  6.26s/it]

Loading safetensors checkpoint shards:  93% Completed | 52/56 [05:27<00:25,  6.32s/it]

Loading safetensors checkpoint shards:  95% Completed | 53/56 [05:33<00:19,  6.37s/it]

Loading safetensors checkpoint shards:  96% Completed | 54/56 [05:40<00:12,  6.40s/it]

Loading safetensors checkpoint shards:  98% Completed | 55/56 [05:47<00:06,  6.51s/it]

Loading safetensors checkpoint shards: 100% Completed | 56/56 [05:53<00:00,  6.47s/it]

Loading safetensors checkpoint shards: 100% Completed | 56/56 [05:53<00:00,  6.31s/it]

[2026-01-17 09:12:12 TP6] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.92 GB, mem usage=29.93 GB.
[2026-01-17 09:12:14 TP4] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.91 GB, mem usage=29.93 GB.
[2026-01-17 09:12:14 TP2] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.90 GB, mem usage=29.93 GB.
[2026-01-17 09:12:14 TP1] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.18 GB, mem usage=29.93 GB.
[2026-01-17 09:12:15 TP7] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.17 GB, mem usage=29.93 GB.
[2026-01-17 09:12:15 TP5] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.18 GB, mem usage=29.93 GB.
[2026-01-17 09:12:15 TP0] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.85 GB, mem usage=29.93 GB.
[2026-01-17 09:12:15 TP3] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.18 GB, mem usage=29.93 GB.
[2026-01-17 09:12:15 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 09:12:15 TP0] The available memory for KV cache is 18.69 GB.
[2026-01-17 09:12:15 TP7] The available memory for KV cache is 18.69 GB.
[2026-01-17 09:12:15 TP6] The available memory for KV cache is 18.69 GB.
[2026-01-17 09:12:15 TP3] The available memory for KV cache is 18.69 GB.
[2026-01-17 09:12:15 TP5] The available memory for KV cache is 18.69 GB.
[2026-01-17 09:12:15 TP4] The available memory for KV cache is 18.69 GB.
[2026-01-17 09:12:15 TP2] The available memory for KV cache is 18.69 GB.
[2026-01-17 09:12:15 TP1] The available memory for KV cache is 18.69 GB.
[2026-01-17 09:12:16 TP0] KV Cache is allocated. #tokens: 416896, K size: 9.35 GB, V size: 9.35 GB
[2026-01-17 09:12:16 TP2] KV Cache is allocated. #tokens: 416896, K size: 9.35 GB, V size: 9.35 GB
[2026-01-17 09:12:16 TP0] Memory pool end. avail mem=11.53 GB
[2026-01-17 09:12:16 TP2] Memory pool end. avail mem=11.59 GB
[2026-01-17 09:12:16 TP6] KV Cache is allocated. #tokens: 416896, K size: 9.35 GB, V size: 9.35 GB
[2026-01-17 09:12:16 TP1] KV Cache is allocated. #tokens: 416896, K size: 9.35 GB, V size: 9.35 GB
[2026-01-17 09:12:16 TP3] KV Cache is allocated. #tokens: 416896, K size: 9.35 GB, V size: 9.35 GB
[2026-01-17 09:12:16 TP6] Memory pool end. avail mem=11.60 GB
[2026-01-17 09:12:16 TP1] Memory pool end. avail mem=11.87 GB
[2026-01-17 09:12:16 TP4] KV Cache is allocated. #tokens: 416896, K size: 9.35 GB, V size: 9.35 GB
[2026-01-17 09:12:16 TP3] Memory pool end. avail mem=11.87 GB
[2026-01-17 09:12:16 TP7] KV Cache is allocated. #tokens: 416896, K size: 9.35 GB, V size: 9.35 GB
[2026-01-17 09:12:16 TP4] Memory pool end. avail mem=11.59 GB
[2026-01-17 09:12:16 TP5] KV Cache is allocated. #tokens: 416896, K size: 9.35 GB, V size: 9.35 GB
[2026-01-17 09:12:16 TP7] Memory pool end. avail mem=11.86 GB
[2026-01-17 09:12:16 TP5] Memory pool end. avail mem=11.87 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 09:12:17 TP0] max_total_num_tokens=416896, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=40960, available_gpu_mem=11.53 GB
[2026-01-17 09:12:18] INFO:     Started server process [132682]
[2026-01-17 09:12:18] INFO:     Waiting for application startup.
[2026-01-17 09:12:18] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-17 09:12:18] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-17 09:12:18] INFO:     Application startup complete.
[2026-01-17 09:12:18] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 09:12:19] INFO:     127.0.0.1:42080 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 09:12:19 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
........[2026-01-17 09:12:25] INFO:     127.0.0.1:42092 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[rank0]:[W117 09:12:27.963726890 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank7]:[W117 09:12:27.251628218 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank6]:[W117 09:12:27.255081772 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank1]:[W117 09:12:27.309639915 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank3]:[W117 09:12:27.432034933 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank5]:[W117 09:12:28.318222666 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank4]:[W117 09:12:28.318233966 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank2]:[W117 09:12:28.318626941 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
........[2026-01-17 09:12:35] INFO:     127.0.0.1:46066 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 09:12:36] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:12:36] The server is fired up and ready to roll!
[2026-01-17 09:12:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:12:46] INFO:     127.0.0.1:55082 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 09:12:46] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 09:12:46] INFO:     127.0.0.1:55088 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 09:12:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:12:46] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/vllm-ascend/Qwen3-235B-A22B-W8A8 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 8 --quantization modelslim --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 09:12:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:12:46 TP0] Prefill batch, #new-seq: 45, #new-token: 6144, #cached-token: 34560, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 09:12:47 TP0] Prefill batch, #new-seq: 62, #new-token: 8192, #cached-token: 47616, token usage: 0.02, #running-req: 46, #queue-req: 20,
[2026-01-17 09:12:47 TP0] Prefill batch, #new-seq: 20, #new-token: 2688, #cached-token: 15360, token usage: 0.04, #running-req: 108, #queue-req: 0,
[2026-01-17 09:12:56 TP0] Decode batch, #running-req: 128, #token: 21376, token usage: 0.05, npu graph: False, gen throughput (token/s): 9.22, #queue-req: 0,
[2026-01-17 09:12:59] INFO:     127.0.0.1:55694 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:12<42:28, 12.81s/it][2026-01-17 09:12:59] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:12:59] INFO:     127.0.0.1:55126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:12:59] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:13<17:46,  5.39s/it]
  2%|▏         | 4/200 [00:13<03:00,  1.09it/s]
  2%|▏         | 4/200 [00:13<03:00,  1.09it/s][2026-01-17 09:12:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:12:59] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:12:59 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 09:13:00 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 131, #queue-req: 0,
[2026-01-17 09:13:01] INFO:     127.0.0.1:55586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:01] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:14<02:54,  1.11it/s]
  4%|▎         | 7/200 [00:14<02:27,  1.31it/s][2026-01-17 09:13:01 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:13:02] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:15<02:26,  1.31it/s][2026-01-17 09:13:02] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:02] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:15<02:04,  1.53it/s]
  5%|▌         | 10/200 [00:15<01:26,  2.20it/s][2026-01-17 09:13:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:02] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:15<01:15,  2.49it/s][2026-01-17 09:13:02 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:13:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 130, #queue-req: 0,
[2026-01-17 09:13:03] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:03] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:03] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:16<01:31,  2.06it/s]
  7%|▋         | 14/200 [00:16<01:06,  2.78it/s]
  7%|▋         | 14/200 [00:16<01:06,  2.78it/s][2026-01-17 09:13:03 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-17 09:13:04] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:18<01:42,  1.80it/s][2026-01-17 09:13:04 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:05] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:18<01:34,  1.94it/s][2026-01-17 09:13:05 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:05 TP0] Decode batch, #running-req: 128, #token: 30592, token usage: 0.07, npu graph: False, gen throughput (token/s): 542.83, #queue-req: 0,
[2026-01-17 09:13:05] INFO:     127.0.0.1:55544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:05] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:19<01:42,  1.78it/s]
  9%|▉         | 18/200 [00:19<01:28,  2.07it/s][2026-01-17 09:13:06] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:19<01:15,  2.39it/s][2026-01-17 09:13:06 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:13:06] INFO:     127.0.0.1:55240 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:19<01:05,  2.75it/s][2026-01-17 09:13:06 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:13:06 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.07, #running-req: 129, #queue-req: 0,
[2026-01-17 09:13:06] INFO:     127.0.0.1:55594 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:20<01:22,  2.17it/s][2026-01-17 09:13:07 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:07] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:20<01:18,  2.26it/s][2026-01-17 09:13:07 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:07] INFO:     127.0.0.1:56078 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:21<01:14,  2.38it/s][2026-01-17 09:13:07 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:08] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:08] INFO:     127.0.0.1:55780 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:21<01:34,  1.86it/s]
 12%|█▎        | 25/200 [00:21<01:24,  2.06it/s][2026-01-17 09:13:08 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:13:09] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:22<01:37,  1.79it/s][2026-01-17 09:13:09] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:09] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:09] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:22<01:19,  2.16it/s]
 14%|█▍        | 29/200 [00:22<00:32,  5.27it/s]
 14%|█▍        | 29/200 [00:22<00:32,  5.27it/s][2026-01-17 09:13:09 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:09] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:09] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:23<00:32,  5.25it/s]
 16%|█▌        | 31/200 [00:23<00:26,  6.32it/s][2026-01-17 09:13:09 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:13:09 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 131, #queue-req: 0,
[2026-01-17 09:13:11] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:24<01:06,  2.53it/s][2026-01-17 09:13:11] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:24<00:58,  2.86it/s][2026-01-17 09:13:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:11] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:24<00:52,  3.18it/s][2026-01-17 09:13:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:13:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 129, #queue-req: 0,
[2026-01-17 09:13:12] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:12] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:25<01:18,  2.09it/s]
 18%|█▊        | 36/200 [00:25<01:18,  2.09it/s][2026-01-17 09:13:12 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:13:12] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:12] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:26<01:13,  2.21it/s]
 19%|█▉        | 38/200 [00:26<00:56,  2.88it/s][2026-01-17 09:13:12 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:13:13] INFO:     127.0.0.1:55152 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:26<01:12,  2.23it/s][2026-01-17 09:13:13] INFO:     127.0.0.1:55618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:13] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:27<01:02,  2.56it/s]
 20%|██        | 41/200 [00:27<00:43,  3.64it/s][2026-01-17 09:13:13 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:14] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:27<00:41,  3.81it/s][2026-01-17 09:13:14 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:13:14 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 130, #queue-req: 0,
[2026-01-17 09:13:14] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:14] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:14] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:28<01:04,  2.42it/s]
 22%|██▎       | 45/200 [00:28<00:56,  2.76it/s]
 22%|██▎       | 45/200 [00:28<00:56,  2.76it/s][2026-01-17 09:13:15 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-17 09:13:16] INFO:     127.0.0.1:55684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:16] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:16] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:16] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:29<01:21,  1.89it/s]
 24%|██▍       | 49/200 [00:29<01:03,  2.36it/s]
 24%|██▍       | 49/200 [00:29<01:03,  2.36it/s]
 24%|██▍       | 49/200 [00:29<01:03,  2.36it/s][2026-01-17 09:13:16] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:29<00:59,  2.53it/s][2026-01-17 09:13:16 TP0] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.07, #running-req: 124, #queue-req: 0,
[2026-01-17 09:13:16] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:29<00:54,  2.73it/s][2026-01-17 09:13:16 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:13:16 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 129, #queue-req: 0,
[2026-01-17 09:13:17] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:17] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:31<01:14,  2.00it/s]
 26%|██▋       | 53/200 [00:31<01:15,  1.94it/s][2026-01-17 09:13:17] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:17] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:17] INFO:     127.0.0.1:55842 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:31<01:06,  2.20it/s]
 28%|██▊       | 56/200 [00:31<00:30,  4.66it/s]
 28%|██▊       | 56/200 [00:31<00:30,  4.66it/s][2026-01-17 09:13:17 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:13:18] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:18] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:31<00:30,  4.65it/s][2026-01-17 09:13:18 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:13:18 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 131, #queue-req: 0,
[2026-01-17 09:13:18 TP0] Decode batch, #running-req: 128, #token: 28288, token usage: 0.07, npu graph: False, gen throughput (token/s): 387.26, #queue-req: 0,
[2026-01-17 09:13:19] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:33<00:55,  2.54it/s][2026-01-17 09:13:19] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:19] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:19] INFO:     127.0.0.1:56116 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:33<00:50,  2.79it/s]
 31%|███       | 62/200 [00:33<00:25,  5.46it/s]
 31%|███       | 62/200 [00:33<00:25,  5.46it/s][2026-01-17 09:13:19 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:20 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:13:20] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:34<00:46,  2.96it/s][2026-01-17 09:13:21] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:21] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:34<00:42,  3.20it/s]
 32%|███▎      | 65/200 [00:34<00:32,  4.19it/s][2026-01-17 09:13:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:21] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:21] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:34<00:31,  4.26it/s]
 34%|███▎      | 67/200 [00:34<00:25,  5.29it/s][2026-01-17 09:13:21 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:13:21 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.07, #running-req: 130, #queue-req: 0,
[2026-01-17 09:13:22] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:22] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:35<00:42,  3.14it/s]
 34%|███▍      | 69/200 [00:35<00:45,  2.85it/s][2026-01-17 09:13:22 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:13:22] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:35<00:46,  2.77it/s][2026-01-17 09:13:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:13:22] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:22] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:22] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:36<00:47,  2.74it/s]
 36%|███▋      | 73/200 [00:36<00:27,  4.60it/s]
 36%|███▋      | 73/200 [00:36<00:27,  4.60it/s][2026-01-17 09:13:23 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-17 09:13:23] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:37<00:40,  3.14it/s][2026-01-17 09:13:23] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:37<00:36,  3.43it/s][2026-01-17 09:13:24] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [00:37<00:44,  2.79it/s][2026-01-17 09:13:24] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:24] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:38<00:44,  2.75it/s]
 39%|███▉      | 78/200 [00:38<00:35,  3.44it/s][2026-01-17 09:13:25] INFO:     127.0.0.1:55874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:25] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:38<00:32,  3.72it/s]
 40%|████      | 80/200 [00:38<00:24,  4.98it/s][2026-01-17 09:13:25] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:38<00:28,  4.13it/s][2026-01-17 09:13:25] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:25] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:39<00:27,  4.35it/s]
 42%|████▏     | 83/200 [00:39<00:20,  5.71it/s][2026-01-17 09:13:25] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:25] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:39<00:20,  5.59it/s]
 42%|████▎     | 85/200 [00:39<00:16,  6.82it/s][2026-01-17 09:13:26] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:39<00:17,  6.38it/s][2026-01-17 09:13:26] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:39<00:24,  4.68it/s][2026-01-17 09:13:26] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:40<00:23,  4.81it/s][2026-01-17 09:13:26] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:40<00:22,  4.92it/s][2026-01-17 09:13:27] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:40<00:27,  3.98it/s][2026-01-17 09:13:27] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:27] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:27] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:40<00:25,  4.29it/s]
 46%|████▋     | 93/200 [00:40<00:12,  8.50it/s]
 46%|████▋     | 93/200 [00:40<00:12,  8.50it/s][2026-01-17 09:13:27] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:27] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:40<00:14,  7.44it/s]
 48%|████▊     | 95/200 [00:40<00:12,  8.09it/s][2026-01-17 09:13:28] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:41<00:25,  4.01it/s][2026-01-17 09:13:28 TP0] Decode batch, #running-req: 105, #token: 26112, token usage: 0.06, npu graph: False, gen throughput (token/s): 503.47, #queue-req: 0,
[2026-01-17 09:13:28] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:28] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:41<00:24,  4.16it/s]
 49%|████▉     | 98/200 [00:41<00:19,  5.32it/s][2026-01-17 09:13:29] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:42<00:35,  2.82it/s][2026-01-17 09:13:29] INFO:     127.0.0.1:43476 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:43<00:31,  3.19it/s][2026-01-17 09:13:30] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [00:43<00:32,  3.06it/s][2026-01-17 09:13:30] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:43<00:28,  3.47it/s][2026-01-17 09:13:30] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:43<00:30,  3.17it/s][2026-01-17 09:13:30] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:44<00:26,  3.62it/s][2026-01-17 09:13:30] INFO:     127.0.0.1:56632 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:44<00:23,  3.99it/s][2026-01-17 09:13:31] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:44<00:21,  4.28it/s][2026-01-17 09:13:31] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:31] INFO:     127.0.0.1:43556 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:44<00:20,  4.50it/s]
 54%|█████▍    | 108/200 [00:44<00:15,  6.05it/s][2026-01-17 09:13:32] INFO:     127.0.0.1:43544 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:45<00:28,  3.18it/s][2026-01-17 09:13:32] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:45<00:30,  2.98it/s][2026-01-17 09:13:32] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:46<00:26,  3.41it/s][2026-01-17 09:13:32] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:32] INFO:     127.0.0.1:43498 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:46<00:23,  3.78it/s]
 56%|█████▋    | 113/200 [00:46<00:16,  5.28it/s][2026-01-17 09:13:33] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:33] INFO:     127.0.0.1:43528 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:46<00:20,  4.24it/s]
 57%|█████▊    | 115/200 [00:46<00:18,  4.59it/s][2026-01-17 09:13:33] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:47<00:21,  3.94it/s][2026-01-17 09:13:33] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:33] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:33] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:33] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:33] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:47<00:19,  4.17it/s]
 60%|██████    | 121/200 [00:47<00:05, 14.96it/s]
 60%|██████    | 121/200 [00:47<00:05, 14.96it/s]
 60%|██████    | 121/200 [00:47<00:05, 14.96it/s]
 60%|██████    | 121/200 [00:47<00:05, 14.96it/s][2026-01-17 09:13:34] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:34] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:47<00:07, 10.71it/s][2026-01-17 09:13:34] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:34] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:48<00:10,  7.25it/s][2026-01-17 09:13:35] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:35] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:48<00:11,  6.59it/s][2026-01-17 09:13:35] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:48<00:11,  6.34it/s][2026-01-17 09:13:35] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:35] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:35] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:48<00:11,  6.07it/s]
 66%|██████▌   | 131/200 [00:48<00:07,  9.02it/s]
 66%|██████▌   | 131/200 [00:48<00:07,  9.02it/s][2026-01-17 09:13:35] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:35] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:35] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:49<00:08,  7.70it/s]
 67%|██████▋   | 134/200 [00:49<00:08,  7.97it/s][2026-01-17 09:13:36 TP0] Decode batch, #running-req: 69, #token: 20608, token usage: 0.05, npu graph: False, gen throughput (token/s): 469.74, #queue-req: 0,
[2026-01-17 09:13:36] INFO:     127.0.0.1:55648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:36] INFO:     127.0.0.1:43590 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:49<00:10,  6.21it/s]
 68%|██████▊   | 136/200 [00:49<00:10,  6.00it/s][2026-01-17 09:13:36] INFO:     127.0.0.1:43578 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:49<00:10,  5.86it/s][2026-01-17 09:13:37] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:37] INFO:     127.0.0.1:55682 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:50<00:16,  3.79it/s]
 70%|██████▉   | 139/200 [00:50<00:17,  3.54it/s][2026-01-17 09:13:37] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:37] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:37] INFO:     127.0.0.1:54836 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:51<00:21,  2.85it/s]
 71%|███████   | 142/200 [00:51<00:16,  3.61it/s]
 71%|███████   | 142/200 [00:51<00:16,  3.61it/s][2026-01-17 09:13:38] INFO:     127.0.0.1:43504 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:51<00:18,  3.07it/s][2026-01-17 09:13:38] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:38] INFO:     127.0.0.1:54708 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:51<00:16,  3.34it/s]
 72%|███████▎  | 145/200 [00:51<00:12,  4.42it/s][2026-01-17 09:13:38] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:38] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:52<00:11,  4.52it/s]
 74%|███████▎  | 147/200 [00:52<00:09,  5.66it/s][2026-01-17 09:13:38] INFO:     127.0.0.1:55558 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:52<00:09,  5.57it/s][2026-01-17 09:13:39] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:39] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:53<00:15,  3.21it/s]
 75%|███████▌  | 150/200 [00:53<00:16,  2.96it/s][2026-01-17 09:13:39] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:39] INFO:     127.0.0.1:43640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:39] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:53<00:14,  3.28it/s]
 76%|███████▋  | 153/200 [00:53<00:07,  6.67it/s]
 76%|███████▋  | 153/200 [00:53<00:07,  6.67it/s][2026-01-17 09:13:40] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:40] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:53<00:07,  6.34it/s]
 78%|███████▊  | 155/200 [00:53<00:06,  7.27it/s][2026-01-17 09:13:40] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:54<00:11,  3.86it/s][2026-01-17 09:13:41] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:54<00:13,  3.07it/s][2026-01-17 09:13:41] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:55<00:14,  2.94it/s][2026-01-17 09:13:41] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:41] INFO:     127.0.0.1:43442 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:55<00:12,  3.29it/s]
 80%|████████  | 160/200 [00:55<00:08,  4.60it/s][2026-01-17 09:13:42] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:55<00:08,  4.72it/s][2026-01-17 09:13:42] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:55<00:09,  3.94it/s][2026-01-17 09:13:42] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:56<00:10,  3.44it/s][2026-01-17 09:13:43] INFO:     127.0.0.1:43654 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:56<00:09,  3.79it/s][2026-01-17 09:13:43] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:56<00:08,  4.09it/s][2026-01-17 09:13:43] INFO:     127.0.0.1:43598 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:56<00:07,  4.32it/s][2026-01-17 09:13:43] INFO:     127.0.0.1:43474 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:57<00:07,  4.39it/s][2026-01-17 09:13:43 TP0] Decode batch, #running-req: 34, #token: 11264, token usage: 0.03, npu graph: False, gen throughput (token/s): 255.74, #queue-req: 0,
[2026-01-17 09:13:44] INFO:     127.0.0.1:43616 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:57<00:08,  3.64it/s][2026-01-17 09:13:46] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [01:00<00:29,  1.05it/s][2026-01-17 09:13:47] INFO:     127.0.0.1:43566 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [01:00<00:25,  1.19it/s][2026-01-17 09:13:47] INFO:     127.0.0.1:55260 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [01:01<00:20,  1.41it/s][2026-01-17 09:13:48] INFO:     127.0.0.1:56586 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [01:01<00:17,  1.65it/s][2026-01-17 09:13:48] INFO:     127.0.0.1:43516 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [01:01<00:13,  2.07it/s][2026-01-17 09:13:48] INFO:     127.0.0.1:43614 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [01:01<00:11,  2.18it/s][2026-01-17 09:13:49] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [01:02<00:12,  2.05it/s][2026-01-17 09:13:49] INFO:     127.0.0.1:55446 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [01:02<00:10,  2.19it/s][2026-01-17 09:13:50] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:13:50] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [01:03<00:11,  2.04it/s]
 89%|████████▉ | 178/200 [01:03<00:08,  2.53it/s][2026-01-17 09:13:50] INFO:     127.0.0.1:54726 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [01:03<00:08,  2.57it/s][2026-01-17 09:13:51] INFO:     127.0.0.1:43536 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [01:04<00:08,  2.29it/s][2026-01-17 09:13:52] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [01:05<00:11,  1.71it/s][2026-01-17 09:13:52] INFO:     127.0.0.1:43592 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [01:05<00:08,  2.10it/s][2026-01-17 09:13:53 TP0] Decode batch, #running-req: 18, #token: 7040, token usage: 0.02, npu graph: False, gen throughput (token/s): 109.11, #queue-req: 0,
[2026-01-17 09:13:53] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [01:07<00:12,  1.36it/s][2026-01-17 09:13:53] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [01:07<00:09,  1.72it/s][2026-01-17 09:13:54] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [01:07<00:07,  2.14it/s][2026-01-17 09:13:57] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [01:11<00:20,  1.46s/it][2026-01-17 09:13:58] INFO:     127.0.0.1:43410 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [01:11<00:14,  1.14s/it][2026-01-17 09:13:58] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [01:11<00:10,  1.10it/s][2026-01-17 09:13:59] INFO:     127.0.0.1:43414 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [01:12<00:08,  1.23it/s][2026-01-17 09:13:59] INFO:     127.0.0.1:43634 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [01:13<00:08,  1.24it/s][2026-01-17 09:14:00 TP0] Decode batch, #running-req: 10, #token: 4736, token usage: 0.01, npu graph: False, gen throughput (token/s): 72.71, #queue-req: 0,
[2026-01-17 09:14:01] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [01:15<00:10,  1.14s/it][2026-01-17 09:14:02] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [01:15<00:07,  1.03it/s][2026-01-17 09:14:02] INFO:     127.0.0.1:54828 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [01:16<00:05,  1.25it/s][2026-01-17 09:14:03] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [01:17<00:05,  1.19it/s][2026-01-17 09:14:08 TP0] Decode batch, #running-req: 6, #token: 3328, token usage: 0.01, npu graph: False, gen throughput (token/s): 36.79, #queue-req: 0,
[2026-01-17 09:14:11] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [01:25<00:15,  3.01s/it][2026-01-17 09:14:12] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [01:26<00:09,  2.40s/it][2026-01-17 09:14:14] INFO:     127.0.0.1:43452 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [01:28<00:06,  2.25s/it][2026-01-17 09:14:16 TP0] Decode batch, #running-req: 3, #token: 2176, token usage: 0.01, npu graph: False, gen throughput (token/s): 25.32, #queue-req: 0,
[2026-01-17 09:14:22] INFO:     127.0.0.1:55996 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [01:36<00:08,  4.05s/it][2026-01-17 09:14:23 TP0] Decode batch, #running-req: 2, #token: 1792, token usage: 0.00, npu graph: False, gen throughput (token/s): 15.04, #queue-req: 0,
[2026-01-17 09:14:25] INFO:     127.0.0.1:54740 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [01:39<00:03,  3.70s/it][2026-01-17 09:14:30] INFO:     127.0.0.1:54750 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [01:43<00:00,  3.87s/it]
100%|██████████| 200/200 [01:43<00:00,  1.93it/s]
.
----------------------------------------------------------------------
Ran 1 test in 545.587s

OK
Accuracy: 0.955
Invalid: 0.000
Latency: 103.777 s
Output throughput: 258.043 token/s
.
.
End (22/62):
filename='ascend/llm_models/test_ascend_qwen3_235b_a22b_w8a8.py', elapsed=556, estimated_time=400
.
.

.
.
Begin (23/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_qwen3_coder_480b_a35b_instruct_w8a8_quaRot.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:14:52] WARNING model_config.py:820: modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:14:52] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen3-Coder-480B-A35B-Instruct-w8a8-QuaRot', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen3-Coder-480B-A35B-Instruct-w8a8-QuaRot', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='modelslim', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=197566335, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Qwen3-Coder-480B-A35B-Instruct-w8a8-QuaRot', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='modelslim', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 09:14:52] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:14:53] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:15:04 TP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:04 TP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-17 09:15:04 TP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-17 09:15:04 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-17 09:15:04 TP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:15:04 TP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:15:04 TP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:04 TP3] Init torch distributed begin.
[2026-01-17 09:15:04 TP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:04 TP5] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:15:05 TP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP8] Init torch distributed begin.
[2026-01-17 09:15:05 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:15:05 TP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP9] Init torch distributed begin.
[2026-01-17 09:15:05 TP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:15:05 TP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:05 TP7] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:15:05 TP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:15:06 TP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:06 TP6] Init torch distributed begin.
[2026-01-17 09:15:06 TP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:06 TP12] Init torch distributed begin.
[2026-01-17 09:15:06 TP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:06 TP10] Init torch distributed begin.
[2026-01-17 09:15:06 TP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:06 TP15] Init torch distributed begin.
[2026-01-17 09:15:06 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:06 TP0] Init torch distributed begin.
[2026-01-17 09:15:06 TP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:06 TP2] Init torch distributed begin.
[2026-01-17 09:15:06 TP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:06 TP11] Init torch distributed begin.
[2026-01-17 09:15:06 TP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:06 TP14] Init torch distributed begin.
[2026-01-17 09:15:06 TP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:06 TP4] Init torch distributed begin.
[2026-01-17 09:15:06 TP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:15:06 TP13] Init torch distributed begin.
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[2026-01-17 09:15:08 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP15] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP13] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP14] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP11] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP12] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP9] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP8] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP10] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP1] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 09:15:08 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP3] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 09:15:08 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:08 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:15:09 TP8] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP11] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP14] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP15] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP12] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP10] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP9] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP13] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:15:09 TP15] Load weight begin. avail mem=61.10 GB
[2026-01-17 09:15:09 TP12] Load weight begin. avail mem=60.84 GB
[2026-01-17 09:15:09 TP5] Load weight begin. avail mem=61.11 GB
[2026-01-17 09:15:09 TP8] Load weight begin. avail mem=60.84 GB
[2026-01-17 09:15:09 TP4] Load weight begin. avail mem=60.84 GB
[2026-01-17 09:15:09 TP1] Load weight begin. avail mem=61.11 GB
[2026-01-17 09:15:09 TP10] Load weight begin. avail mem=60.85 GB
[2026-01-17 09:15:09 TP0] Load weight begin. avail mem=60.78 GB
[2026-01-17 09:15:09 TP2] Load weight begin. avail mem=60.84 GB
[2026-01-17 09:15:09 TP6] Load weight begin. avail mem=60.85 GB
[2026-01-17 09:15:09 TP14] Load weight begin. avail mem=60.85 GB
[2026-01-17 09:15:09 TP13] Load weight begin. avail mem=61.11 GB
[2026-01-17 09:15:09 TP11] Load weight begin. avail mem=61.10 GB
[2026-01-17 09:15:09 TP3] Load weight begin. avail mem=61.11 GB
[2026-01-17 09:15:09 TP7] Load weight begin. avail mem=61.10 GB
[2026-01-17 09:15:09 TP9] Load weight begin. avail mem=61.11 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)

Loading safetensors checkpoint shards:   0% Completed | 0/113 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   1% Completed | 1/113 [00:07<14:16,  7.64s/it]

Loading safetensors checkpoint shards:   2% Completed | 2/113 [00:15<14:06,  7.63s/it]

Loading safetensors checkpoint shards:   3% Completed | 3/113 [00:23<14:37,  7.97s/it]

Loading safetensors checkpoint shards:   4% Completed | 4/113 [00:31<14:29,  7.98s/it]

Loading safetensors checkpoint shards:   4% Completed | 5/113 [00:39<14:15,  7.92s/it]

Loading safetensors checkpoint shards:   5% Completed | 6/113 [00:47<13:57,  7.83s/it]

Loading safetensors checkpoint shards:   6% Completed | 7/113 [00:55<13:52,  7.85s/it]

Loading safetensors checkpoint shards:   7% Completed | 8/113 [01:03<14:14,  8.13s/it]

Loading safetensors checkpoint shards:   8% Completed | 9/113 [01:11<14:08,  8.16s/it]

Loading safetensors checkpoint shards:   9% Completed | 10/113 [01:19<13:35,  7.92s/it]

Loading safetensors checkpoint shards:  10% Completed | 11/113 [01:27<13:43,  8.08s/it]

Loading safetensors checkpoint shards:  11% Completed | 12/113 [01:35<13:17,  7.90s/it]

Loading safetensors checkpoint shards:  12% Completed | 13/113 [01:43<13:05,  7.85s/it]

Loading safetensors checkpoint shards:  12% Completed | 14/113 [01:50<12:57,  7.85s/it]

Loading safetensors checkpoint shards:  13% Completed | 15/113 [01:59<13:04,  8.01s/it]

Loading safetensors checkpoint shards:  14% Completed | 16/113 [02:07<13:10,  8.15s/it]

Loading safetensors checkpoint shards:  15% Completed | 17/113 [02:15<12:46,  7.99s/it]

Loading safetensors checkpoint shards:  16% Completed | 18/113 [02:22<12:18,  7.77s/it]

Loading safetensors checkpoint shards:  17% Completed | 19/113 [02:25<09:45,  6.22s/it]

Loading safetensors checkpoint shards:  18% Completed | 20/113 [02:32<10:06,  6.52s/it]

Loading safetensors checkpoint shards:  19% Completed | 21/113 [02:39<10:24,  6.79s/it]

Loading safetensors checkpoint shards:  19% Completed | 22/113 [02:47<10:36,  6.99s/it]

Loading safetensors checkpoint shards:  20% Completed | 23/113 [02:54<10:35,  7.06s/it]

Loading safetensors checkpoint shards:  21% Completed | 24/113 [03:01<10:25,  7.03s/it]

Loading safetensors checkpoint shards:  22% Completed | 25/113 [03:08<10:24,  7.10s/it]

Loading safetensors checkpoint shards:  23% Completed | 26/113 [03:13<09:23,  6.48s/it]

Loading safetensors checkpoint shards:  24% Completed | 27/113 [03:20<09:36,  6.70s/it]

Loading safetensors checkpoint shards:  25% Completed | 28/113 [03:28<09:42,  6.85s/it]

Loading safetensors checkpoint shards:  26% Completed | 29/113 [03:35<09:54,  7.08s/it]

Loading safetensors checkpoint shards:  27% Completed | 30/113 [03:42<09:44,  7.04s/it]

Loading safetensors checkpoint shards:  27% Completed | 31/113 [03:49<09:42,  7.10s/it]

Loading safetensors checkpoint shards:  28% Completed | 32/113 [03:57<09:46,  7.24s/it]

Loading safetensors checkpoint shards:  29% Completed | 33/113 [04:04<09:31,  7.15s/it]

Loading safetensors checkpoint shards:  30% Completed | 34/113 [04:12<09:37,  7.31s/it]

Loading safetensors checkpoint shards:  31% Completed | 35/113 [04:19<09:29,  7.31s/it]

Loading safetensors checkpoint shards:  32% Completed | 36/113 [04:26<09:19,  7.27s/it]

Loading safetensors checkpoint shards:  33% Completed | 37/113 [04:33<09:10,  7.25s/it]

Loading safetensors checkpoint shards:  34% Completed | 38/113 [04:41<09:01,  7.22s/it]

Loading safetensors checkpoint shards:  35% Completed | 39/113 [04:48<09:02,  7.32s/it]

Loading safetensors checkpoint shards:  35% Completed | 40/113 [04:55<08:51,  7.28s/it]

Loading safetensors checkpoint shards:  36% Completed | 41/113 [05:02<08:41,  7.24s/it]

Loading safetensors checkpoint shards:  37% Completed | 42/113 [05:10<08:43,  7.37s/it]

Loading safetensors checkpoint shards:  38% Completed | 43/113 [05:17<08:28,  7.27s/it]

Loading safetensors checkpoint shards:  39% Completed | 44/113 [05:24<08:17,  7.21s/it]

Loading safetensors checkpoint shards:  40% Completed | 45/113 [05:31<08:08,  7.18s/it]

Loading safetensors checkpoint shards:  41% Completed | 46/113 [05:39<08:05,  7.25s/it]

Loading safetensors checkpoint shards:  42% Completed | 47/113 [05:47<08:15,  7.50s/it]

Loading safetensors checkpoint shards:  42% Completed | 48/113 [05:54<07:58,  7.37s/it]

Loading safetensors checkpoint shards:  43% Completed | 49/113 [06:01<07:46,  7.29s/it]

Loading safetensors checkpoint shards:  44% Completed | 50/113 [06:08<07:42,  7.35s/it]

Loading safetensors checkpoint shards:  45% Completed | 51/113 [06:16<07:38,  7.40s/it]

Loading safetensors checkpoint shards:  46% Completed | 52/113 [06:23<07:17,  7.17s/it]

Loading safetensors checkpoint shards:  47% Completed | 53/113 [06:28<06:45,  6.76s/it]

Loading safetensors checkpoint shards:  48% Completed | 54/113 [06:34<06:16,  6.38s/it]

Loading safetensors checkpoint shards:  49% Completed | 55/113 [06:42<06:35,  6.81s/it]

Loading safetensors checkpoint shards:  50% Completed | 56/113 [06:47<06:05,  6.41s/it]

Loading safetensors checkpoint shards:  50% Completed | 57/113 [06:53<05:52,  6.29s/it]

Loading safetensors checkpoint shards:  51% Completed | 58/113 [06:59<05:36,  6.11s/it]

Loading safetensors checkpoint shards:  52% Completed | 59/113 [07:04<05:20,  5.93s/it]

Loading safetensors checkpoint shards:  53% Completed | 60/113 [07:11<05:19,  6.04s/it]

Loading safetensors checkpoint shards:  54% Completed | 61/113 [07:16<05:07,  5.92s/it]

Loading safetensors checkpoint shards:  55% Completed | 62/113 [07:22<05:03,  5.95s/it]

Loading safetensors checkpoint shards:  56% Completed | 63/113 [07:28<04:51,  5.83s/it]

Loading safetensors checkpoint shards:  57% Completed | 64/113 [07:34<04:43,  5.79s/it]

Loading safetensors checkpoint shards:  58% Completed | 65/113 [07:40<04:41,  5.87s/it]

Loading safetensors checkpoint shards:  58% Completed | 66/113 [07:47<04:54,  6.27s/it]

Loading safetensors checkpoint shards:  59% Completed | 67/113 [07:53<04:42,  6.14s/it]

Loading safetensors checkpoint shards:  60% Completed | 68/113 [07:58<04:30,  6.00s/it]

Loading safetensors checkpoint shards:  61% Completed | 69/113 [08:04<04:19,  5.89s/it]

Loading safetensors checkpoint shards:  62% Completed | 70/113 [08:10<04:15,  5.95s/it]

Loading safetensors checkpoint shards:  63% Completed | 71/113 [08:16<04:05,  5.83s/it]

Loading safetensors checkpoint shards:  64% Completed | 72/113 [08:21<03:56,  5.76s/it]

Loading safetensors checkpoint shards:  65% Completed | 73/113 [08:27<03:54,  5.86s/it]

Loading safetensors checkpoint shards:  65% Completed | 74/113 [08:33<03:46,  5.80s/it]

Loading safetensors checkpoint shards:  66% Completed | 75/113 [08:40<03:54,  6.17s/it]

Loading safetensors checkpoint shards:  67% Completed | 76/113 [08:50<04:29,  7.29s/it]

Loading safetensors checkpoint shards:  68% Completed | 77/113 [09:01<05:08,  8.56s/it]

Loading safetensors checkpoint shards:  69% Completed | 78/113 [09:12<05:23,  9.24s/it]

Loading safetensors checkpoint shards:  70% Completed | 79/113 [09:23<05:33,  9.81s/it]

Loading safetensors checkpoint shards:  71% Completed | 80/113 [09:33<05:22,  9.78s/it]

Loading safetensors checkpoint shards:  72% Completed | 81/113 [09:43<05:15,  9.85s/it]

Loading safetensors checkpoint shards:  73% Completed | 82/113 [09:56<05:36, 10.85s/it]

Loading safetensors checkpoint shards:  73% Completed | 83/113 [10:06<05:15, 10.51s/it]

Loading safetensors checkpoint shards:  74% Completed | 84/113 [10:16<05:02, 10.42s/it]

Loading safetensors checkpoint shards:  75% Completed | 85/113 [10:27<04:55, 10.54s/it]

Loading safetensors checkpoint shards:  76% Completed | 86/113 [10:36<04:34, 10.18s/it]

Loading safetensors checkpoint shards:  77% Completed | 87/113 [10:46<04:17,  9.89s/it]

Loading safetensors checkpoint shards:  78% Completed | 88/113 [10:55<04:00,  9.62s/it]

Loading safetensors checkpoint shards:  79% Completed | 89/113 [11:04<03:45,  9.40s/it]

Loading safetensors checkpoint shards:  80% Completed | 90/113 [11:12<03:32,  9.24s/it]

Loading safetensors checkpoint shards:  81% Completed | 91/113 [11:21<03:20,  9.13s/it]

Loading safetensors checkpoint shards:  81% Completed | 92/113 [11:30<03:11,  9.11s/it]

Loading safetensors checkpoint shards:  82% Completed | 93/113 [11:40<03:03,  9.16s/it]

Loading safetensors checkpoint shards:  83% Completed | 94/113 [11:49<02:53,  9.11s/it]

Loading safetensors checkpoint shards:  84% Completed | 95/113 [11:58<02:47,  9.33s/it]

Loading safetensors checkpoint shards:  85% Completed | 96/113 [12:09<02:42,  9.58s/it]

Loading safetensors checkpoint shards:  86% Completed | 97/113 [12:18<02:31,  9.45s/it]

Loading safetensors checkpoint shards:  87% Completed | 98/113 [12:28<02:24,  9.66s/it]

Loading safetensors checkpoint shards:  88% Completed | 99/113 [12:36<02:10,  9.30s/it]

Loading safetensors checkpoint shards:  88% Completed | 100/113 [12:46<02:02,  9.45s/it]

Loading safetensors checkpoint shards:  89% Completed | 101/113 [12:57<01:59,  9.92s/it]

Loading safetensors checkpoint shards:  90% Completed | 102/113 [13:05<01:40,  9.15s/it]

Loading safetensors checkpoint shards:  91% Completed | 103/113 [13:10<01:21,  8.12s/it]

Loading safetensors checkpoint shards:  92% Completed | 104/113 [13:16<01:07,  7.44s/it]

Loading safetensors checkpoint shards:  93% Completed | 105/113 [13:22<00:54,  6.87s/it]

Loading safetensors checkpoint shards:  94% Completed | 106/113 [13:27<00:45,  6.53s/it]

Loading safetensors checkpoint shards:  95% Completed | 107/113 [13:33<00:37,  6.25s/it]

Loading safetensors checkpoint shards:  96% Completed | 108/113 [13:39<00:30,  6.15s/it]

Loading safetensors checkpoint shards:  96% Completed | 109/113 [13:44<00:23,  5.97s/it]

Loading safetensors checkpoint shards:  97% Completed | 110/113 [13:50<00:17,  5.74s/it]

Loading safetensors checkpoint shards:  98% Completed | 111/113 [13:56<00:11,  5.78s/it]

Loading safetensors checkpoint shards:  99% Completed | 112/113 [14:02<00:05,  5.87s/it]

Loading safetensors checkpoint shards: 100% Completed | 113/113 [14:07<00:00,  5.73s/it]

Loading safetensors checkpoint shards: 100% Completed | 113/113 [14:07<00:00,  7.50s/it]

[2026-01-17 09:29:50 TP10] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.88 GB, mem usage=29.98 GB.
[2026-01-17 09:29:50 TP9] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.14 GB, mem usage=29.98 GB.
[2026-01-17 09:29:50 TP11] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.13 GB, mem usage=29.98 GB.
[2026-01-17 09:29:50 TP3] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.14 GB, mem usage=29.98 GB.
[2026-01-17 09:29:50 TP5] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.14 GB, mem usage=29.98 GB.
[2026-01-17 09:29:50 TP1] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.14 GB, mem usage=29.98 GB.
[2026-01-17 09:29:51 TP2] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.86 GB, mem usage=29.98 GB.
[2026-01-17 09:29:51 TP12] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.86 GB, mem usage=29.98 GB.
[2026-01-17 09:29:52 TP7] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.13 GB, mem usage=29.98 GB.
[2026-01-17 09:29:52 TP15] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.13 GB, mem usage=29.98 GB.
[2026-01-17 09:29:52 TP6] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.88 GB, mem usage=29.98 GB.
[2026-01-17 09:29:52 TP8] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.86 GB, mem usage=29.98 GB.
[2026-01-17 09:29:53 TP0] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.80 GB, mem usage=29.98 GB.
[2026-01-17 09:29:53 TP13] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.14 GB, mem usage=29.98 GB.
[2026-01-17 09:29:53 TP14] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.87 GB, mem usage=29.98 GB.
[2026-01-17 09:29:53 TP4] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.86 GB, mem usage=29.98 GB.
[2026-01-17 09:29:53 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 09:29:53 TP0] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP15] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP14] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP13] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP12] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP11] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP10] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP8] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP9] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP7] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP6] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP5] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP4] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP3] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP2] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:53 TP1] The available memory for KV cache is 18.60 GB.
[2026-01-17 09:29:54 TP2] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP14] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP2] Memory pool end. avail mem=10.22 GB
[2026-01-17 09:29:54 TP4] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP0] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP14] Memory pool end. avail mem=10.23 GB
[2026-01-17 09:29:54 TP10] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP4] Memory pool end. avail mem=10.22 GB
[2026-01-17 09:29:54 TP0] Memory pool end. avail mem=10.16 GB
[2026-01-17 09:29:54 TP6] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP10] Memory pool end. avail mem=10.23 GB
[2026-01-17 09:29:54 TP6] Memory pool end. avail mem=10.23 GB
[2026-01-17 09:29:54 TP12] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP1] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP5] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP3] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP8] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP12] Memory pool end. avail mem=10.22 GB
[2026-01-17 09:29:54 TP11] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP1] Memory pool end. avail mem=10.49 GB
[2026-01-17 09:29:54 TP5] Memory pool end. avail mem=10.49 GB
[2026-01-17 09:29:54 TP3] Memory pool end. avail mem=10.49 GB
[2026-01-17 09:29:54 TP8] Memory pool end. avail mem=10.22 GB
[2026-01-17 09:29:54 TP7] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP11] Memory pool end. avail mem=10.48 GB
[2026-01-17 09:29:54 TP9] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP7] Memory pool end. avail mem=10.48 GB
[2026-01-17 09:29:54 TP15] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP9] Memory pool end. avail mem=10.49 GB
[2026-01-17 09:29:54 TP13] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:29:54 TP15] Memory pool end. avail mem=10.48 GB
[2026-01-17 09:29:54 TP13] Memory pool end. avail mem=10.49 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 09:29:55 TP0] max_total_num_tokens=629248, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=262144, available_gpu_mem=10.16 GB
[2026-01-17 09:29:56] INFO:     Started server process [147877]
[2026-01-17 09:29:56] INFO:     Waiting for application startup.
[2026-01-17 09:29:56] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 09:29:56] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 09:29:56] INFO:     Application startup complete.
[2026-01-17 09:29:56] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 09:29:57] INFO:     127.0.0.1:37618 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 09:29:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
................[2026-01-17 09:30:02] INFO:     127.0.0.1:59616 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[rank5]:[W117 09:30:05.146168846 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank9]:[W117 09:30:05.193367460 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank4]:[W117 09:30:05.398028569 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank2]:[W117 09:30:05.398028479 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank7]:[W117 09:30:06.580889030 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank3]:[W117 09:30:06.580889800 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank6]:[W117 09:30:06.608632766 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank11]:[W117 09:30:06.608639366 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank0]:[W117 09:30:06.780168401 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank15]:[W117 09:30:06.987705951 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank1]:[W117 09:30:06.042222527 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank10]:[W117 09:30:06.042222667 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank14]:[W117 09:30:06.042232697 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank13]:[W117 09:30:06.069984674 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank12]:[W117 09:30:06.069984024 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank8]:[W117 09:30:06.069984754 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[2026-01-17 09:30:12] INFO:     127.0.0.1:40622 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 09:30:12] INFO:     127.0.0.1:37624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:12] The server is fired up and ready to roll!
[2026-01-17 09:30:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:30:23] INFO:     127.0.0.1:55386 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 09:30:23] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 09:30:23] INFO:     127.0.0.1:55398 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 09:30:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:30:23] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen3-Coder-480B-A35B-Instruct-w8a8-QuaRot --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 16 --quantization modelslim --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 09:30:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:30:23 TP0] Prefill batch, #new-seq: 32, #new-token: 4352, #cached-token: 24576, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 09:30:23 TP0] Prefill batch, #new-seq: 63, #new-token: 8192, #cached-token: 48384, token usage: 0.01, #running-req: 33, #queue-req: 32,
[2026-01-17 09:30:24 TP0] Prefill batch, #new-seq: 32, #new-token: 4480, #cached-token: 24576, token usage: 0.02, #running-req: 96, #queue-req: 0,
[2026-01-17 09:30:32 TP0] Decode batch, #running-req: 128, #token: 21376, token usage: 0.03, npu graph: False, gen throughput (token/s): 4.29, #queue-req: 0,
[2026-01-17 09:30:34] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:10<35:52, 10.82s/it][2026-01-17 09:30:34] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:34] INFO:     127.0.0.1:55446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:34 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,

  1%|          | 2/200 [00:10<14:58,  4.54s/it]
  2%|▏         | 3/200 [00:10<05:41,  1.74s/it][2026-01-17 09:30:34 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 09:30:35] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:35] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:12<03:57,  1.22s/it][2026-01-17 09:30:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:35] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:12<03:03,  1.05it/s][2026-01-17 09:30:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 09:30:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 09:30:36] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:12<02:41,  1.20it/s][2026-01-17 09:30:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:36] INFO:     127.0.0.1:55420 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:13<02:10,  1.47it/s][2026-01-17 09:30:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:36] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:13<01:47,  1.78it/s][2026-01-17 09:30:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:37] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:37] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:13<01:30,  2.11it/s]
  6%|▌         | 11/200 [00:13<01:00,  3.13it/s][2026-01-17 09:30:37 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:30:37] INFO:     127.0.0.1:56078 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:14<01:09,  2.70it/s][2026-01-17 09:30:37] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:37] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:37] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:14<00:57,  3.24it/s]
  8%|▊         | 15/200 [00:14<00:23,  7.72it/s]
  8%|▊         | 15/200 [00:14<00:23,  7.72it/s][2026-01-17 09:30:37 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:37 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:30:39] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:39] INFO:     127.0.0.1:56466 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:15<00:59,  3.08it/s][2026-01-17 09:30:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:39 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:30:39] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:16<01:05,  2.76it/s][2026-01-17 09:30:39 TP0] Decode batch, #running-req: 128, #token: 30592, token usage: 0.05, npu graph: False, gen throughput (token/s): 654.21, #queue-req: 0,
[2026-01-17 09:30:39] INFO:     127.0.0.1:56290 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:16<00:56,  3.19it/s][2026-01-17 09:30:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:40 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:30:40] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:40] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:17<01:11,  2.52it/s]
 10%|█         | 21/200 [00:17<01:06,  2.67it/s][2026-01-17 09:30:40] INFO:     127.0.0.1:56116 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:17<00:57,  3.12it/s][2026-01-17 09:30:40 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:30:40] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:17<00:49,  3.58it/s][2026-01-17 09:30:40 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:30:40 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 09:30:41] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:18<01:06,  2.63it/s][2026-01-17 09:30:41] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:18<00:55,  3.18it/s][2026-01-17 09:30:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:41] INFO:     127.0.0.1:55858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:41] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:18<00:47,  3.70it/s]
 14%|█▎        | 27/200 [00:18<00:32,  5.39it/s][2026-01-17 09:30:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:30:41 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 09:30:42] INFO:     127.0.0.1:55884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:42] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:42] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:19<00:53,  3.23it/s]
 15%|█▌        | 30/200 [00:19<00:48,  3.51it/s]
 15%|█▌        | 30/200 [00:19<00:48,  3.51it/s][2026-01-17 09:30:42 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.05, #running-req: 125, #queue-req: 0,
[2026-01-17 09:30:43] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:19<00:55,  3.05it/s][2026-01-17 09:30:43 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:43] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:43] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:19<00:52,  3.20it/s]
 16%|█▋        | 33/200 [00:19<00:40,  4.09it/s][2026-01-17 09:30:43 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:30:43] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:20<00:41,  4.01it/s][2026-01-17 09:30:43 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:43] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:20<00:41,  3.98it/s][2026-01-17 09:30:43 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:44] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:20<00:41,  3.97it/s][2026-01-17 09:30:44 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:44] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:20<00:41,  3.96it/s][2026-01-17 09:30:44 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:44] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:44] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:44] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:21<00:41,  3.88it/s]
 20%|██        | 40/200 [00:21<00:23,  6.70it/s]
 20%|██        | 40/200 [00:21<00:23,  6.70it/s][2026-01-17 09:30:44 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.05, #running-req: 125, #queue-req: 0,
[2026-01-17 09:30:45] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:21<00:34,  4.61it/s][2026-01-17 09:30:45] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:21<00:31,  5.07it/s][2026-01-17 09:30:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:45] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:45] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:22<00:28,  5.44it/s][2026-01-17 09:30:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:30:45 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 09:30:45] INFO:     127.0.0.1:55892 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:22<00:33,  4.67it/s][2026-01-17 09:30:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:46] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:22<00:34,  4.41it/s][2026-01-17 09:30:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:46] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:23<00:35,  4.31it/s][2026-01-17 09:30:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:46] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:23<00:36,  4.17it/s][2026-01-17 09:30:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:47] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:23<00:51,  2.92it/s][2026-01-17 09:30:47] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:24<00:42,  3.54it/s][2026-01-17 09:30:47 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:47] INFO:     127.0.0.1:55588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:47] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:24<00:36,  4.10it/s]
 26%|██▌       | 52/200 [00:24<00:24,  5.96it/s][2026-01-17 09:30:47 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:30:47 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 09:30:48] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:24<00:45,  3.23it/s][2026-01-17 09:30:48] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:48] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:48] INFO:     127.0.0.1:56362 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:25<00:38,  3.81it/s][2026-01-17 09:30:48 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:48 TP0] Decode batch, #running-req: 127, #token: 28160, token usage: 0.04, npu graph: False, gen throughput (token/s): 585.42, #queue-req: 0,
[2026-01-17 09:30:48 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 09:30:49] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:49] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:25<00:33,  4.24it/s]
 29%|██▉       | 58/200 [00:25<00:28,  5.06it/s][2026-01-17 09:30:49] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:49] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:25<00:26,  5.37it/s]
 30%|███       | 60/200 [00:25<00:20,  6.74it/s][2026-01-17 09:30:49 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 09:30:49] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:49] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:49] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:49 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.04, #running-req: 128, #queue-req: 0,

 30%|███       | 61/200 [00:25<00:20,  6.88it/s]
 32%|███▏      | 63/200 [00:25<00:11, 11.48it/s]
 32%|███▏      | 63/200 [00:25<00:11, 11.48it/s][2026-01-17 09:30:49 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.04, #running-req: 130, #queue-req: 0,
[2026-01-17 09:30:49] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:49] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:49] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:26<00:18,  7.39it/s]
 33%|███▎      | 66/200 [00:26<00:19,  6.72it/s][2026-01-17 09:30:49] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:26<00:19,  6.90it/s][2026-01-17 09:30:49 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-17 09:30:50] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:50] INFO:     127.0.0.1:55786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:50 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 09:30:50] INFO:     127.0.0.1:43660 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:26<00:18,  6.99it/s]
 35%|███▌      | 70/200 [00:26<00:11, 11.50it/s]
 35%|███▌      | 70/200 [00:26<00:11, 11.50it/s][2026-01-17 09:30:50 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 09:30:50] INFO:     127.0.0.1:56556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:50 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:30:51] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:51] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:27<00:26,  4.86it/s]
 36%|███▋      | 73/200 [00:27<00:32,  3.93it/s][2026-01-17 09:30:51 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:30:51] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:28<00:37,  3.33it/s][2026-01-17 09:30:51] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:51] INFO:     127.0.0.1:43650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:51] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:28<00:33,  3.70it/s]
 38%|███▊      | 77/200 [00:28<00:16,  7.35it/s]
 38%|███▊      | 77/200 [00:28<00:16,  7.35it/s][2026-01-17 09:30:51] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:52] INFO:     127.0.0.1:55630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:52] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:52] INFO:     127.0.0.1:56200 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:28<00:16,  7.20it/s]
 40%|████      | 81/200 [00:28<00:12,  9.79it/s]
 40%|████      | 81/200 [00:28<00:12,  9.79it/s][2026-01-17 09:30:52] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:52] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:28<00:11, 10.54it/s][2026-01-17 09:30:52] INFO:     127.0.0.1:55718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:52] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:29<00:12,  9.51it/s][2026-01-17 09:30:52] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:52] INFO:     127.0.0.1:43622 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:29<00:12,  8.72it/s][2026-01-17 09:30:52] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:52] INFO:     127.0.0.1:56126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:52] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:29<00:13,  8.46it/s]
 45%|████▌     | 90/200 [00:29<00:08, 12.50it/s]
 45%|████▌     | 90/200 [00:29<00:08, 12.50it/s][2026-01-17 09:30:53] INFO:     127.0.0.1:55684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:53] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:29<00:12,  8.95it/s][2026-01-17 09:30:53] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:53] INFO:     127.0.0.1:56436 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:30<00:14,  7.28it/s][2026-01-17 09:30:53] INFO:     127.0.0.1:55768 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [00:30<00:14,  7.28it/s][2026-01-17 09:30:53] INFO:     127.0.0.1:56408 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:30<00:14,  7.30it/s][2026-01-17 09:30:54] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:30<00:14,  7.23it/s][2026-01-17 09:30:54] INFO:     127.0.0.1:43652 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:30<00:14,  7.21it/s][2026-01-17 09:30:54] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:31<00:21,  4.78it/s][2026-01-17 09:30:54] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:31<00:19,  5.25it/s][2026-01-17 09:30:54] INFO:     127.0.0.1:43640 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [00:31<00:17,  5.65it/s][2026-01-17 09:30:55 TP0] Decode batch, #running-req: 99, #token: 24832, token usage: 0.04, npu graph: False, gen throughput (token/s): 699.29, #queue-req: 0,
[2026-01-17 09:30:55] INFO:     127.0.0.1:55516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:55] INFO:     127.0.0.1:55594 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:31<00:19,  4.92it/s]
 52%|█████▏    | 103/200 [00:31<00:16,  5.79it/s][2026-01-17 09:30:55] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:32<00:15,  6.13it/s][2026-01-17 09:30:55] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:55] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:32<00:14,  6.34it/s]
 53%|█████▎    | 106/200 [00:32<00:11,  8.26it/s][2026-01-17 09:30:55] INFO:     127.0.0.1:59524 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:32<00:11,  7.77it/s][2026-01-17 09:30:55] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:32<00:11,  7.72it/s][2026-01-17 09:30:55] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:55] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:55] INFO:     127.0.0.1:43618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:55] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:32<00:11,  7.62it/s]
 56%|█████▌    | 112/200 [00:32<00:04, 17.89it/s]
 56%|█████▌    | 112/200 [00:32<00:04, 17.89it/s]
 56%|█████▌    | 112/200 [00:32<00:04, 17.89it/s][2026-01-17 09:30:56] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:56] INFO:     127.0.0.1:59256 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:32<00:05, 17.19it/s][2026-01-17 09:30:56] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:56] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:56] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:33<00:10,  8.25it/s]
 58%|█████▊    | 117/200 [00:33<00:12,  6.73it/s][2026-01-17 09:30:56] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:57] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:33<00:12,  6.28it/s][2026-01-17 09:30:57] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:34<00:15,  5.15it/s][2026-01-17 09:30:57] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:57] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:34<00:18,  4.30it/s]
 61%|██████    | 122/200 [00:34<00:17,  4.49it/s][2026-01-17 09:30:57] INFO:     127.0.0.1:55822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:57] INFO:     127.0.0.1:59368 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:34<00:15,  4.89it/s]
 62%|██████▏   | 124/200 [00:34<00:11,  6.48it/s][2026-01-17 09:30:58] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:34<00:11,  6.68it/s][2026-01-17 09:30:58] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:58] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:34<00:10,  6.87it/s]
 64%|██████▎   | 127/200 [00:34<00:08,  8.76it/s][2026-01-17 09:30:58] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:58] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:35<00:10,  6.79it/s][2026-01-17 09:30:59] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:35<00:15,  4.51it/s][2026-01-17 09:30:59] INFO:     127.0.0.1:43628 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [00:35<00:13,  4.97it/s][2026-01-17 09:30:59] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:59] INFO:     127.0.0.1:56310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:30:59] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:36<00:14,  4.61it/s]
 67%|██████▋   | 134/200 [00:36<00:09,  7.21it/s]
 67%|██████▋   | 134/200 [00:36<00:09,  7.21it/s][2026-01-17 09:30:59] INFO:     127.0.0.1:44402 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:36<00:09,  7.09it/s][2026-01-17 09:30:59] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [00:36<00:08,  7.28it/s][2026-01-17 09:30:59] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:36<00:08,  7.27it/s][2026-01-17 09:31:00] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:36<00:08,  7.28it/s][2026-01-17 09:31:00] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:36<00:08,  7.31it/s][2026-01-17 09:31:00] INFO:     127.0.0.1:44420 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:37<00:10,  5.79it/s][2026-01-17 09:31:00 TP0] Decode batch, #running-req: 61, #token: 17792, token usage: 0.03, npu graph: False, gen throughput (token/s): 576.28, #queue-req: 0,
[2026-01-17 09:31:00] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:37<00:09,  6.07it/s][2026-01-17 09:31:00] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:31:00] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:37<00:09,  6.41it/s]
 72%|███████▏  | 143/200 [00:37<00:06,  8.62it/s][2026-01-17 09:31:00] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:37<00:06,  8.27it/s][2026-01-17 09:31:01] INFO:     127.0.0.1:44304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:31:01] INFO:     127.0.0.1:44320 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:37<00:06,  8.08it/s]
 73%|███████▎  | 146/200 [00:37<00:05, 10.07it/s][2026-01-17 09:31:01] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:31:01] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:31:01] INFO:     127.0.0.1:59310 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:38<00:09,  5.31it/s]
 74%|███████▍  | 149/200 [00:38<00:10,  4.90it/s][2026-01-17 09:31:01] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:38<00:09,  5.23it/s][2026-01-17 09:31:02] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:38<00:09,  4.90it/s][2026-01-17 09:31:02] INFO:     127.0.0.1:43682 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:39<00:11,  4.11it/s][2026-01-17 09:31:02] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:39<00:10,  4.60it/s][2026-01-17 09:31:03] INFO:     127.0.0.1:44444 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:39<00:13,  3.35it/s][2026-01-17 09:31:03] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:39<00:11,  3.94it/s][2026-01-17 09:31:03] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:40<00:11,  3.90it/s][2026-01-17 09:31:03] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:40<00:12,  3.42it/s][2026-01-17 09:31:04] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:40<00:10,  4.06it/s][2026-01-17 09:31:04] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:31:04] INFO:     127.0.0.1:44454 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:40<00:10,  3.99it/s]
 80%|████████  | 160/200 [00:40<00:07,  5.11it/s][2026-01-17 09:31:04] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:41<00:07,  5.55it/s][2026-01-17 09:31:04] INFO:     127.0.0.1:59296 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:41<00:06,  5.95it/s][2026-01-17 09:31:04] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:41<00:05,  6.18it/s][2026-01-17 09:31:05] INFO:     127.0.0.1:44288 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:41<00:09,  3.74it/s][2026-01-17 09:31:05] INFO:     127.0.0.1:44374 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:42<00:09,  3.81it/s][2026-01-17 09:31:05] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:42<00:09,  3.75it/s][2026-01-17 09:31:05 TP0] Decode batch, #running-req: 35, #token: 11264, token usage: 0.02, npu graph: False, gen throughput (token/s): 347.03, #queue-req: 0,
[2026-01-17 09:31:05] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:42<00:07,  4.37it/s][2026-01-17 09:31:06] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:43<00:12,  2.51it/s][2026-01-17 09:31:08] INFO:     127.0.0.1:59356 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:45<00:25,  1.20it/s][2026-01-17 09:31:08] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:45<00:19,  1.53it/s][2026-01-17 09:31:09] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:31:09] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:31:09] INFO:     127.0.0.1:43602 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:46<00:18,  1.54it/s]
 86%|████████▋ | 173/200 [00:46<00:09,  2.79it/s]
 86%|████████▋ | 173/200 [00:46<00:09,  2.79it/s][2026-01-17 09:31:09] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:46<00:08,  2.93it/s][2026-01-17 09:31:09] INFO:     127.0.0.1:44344 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:46<00:07,  3.38it/s][2026-01-17 09:31:10] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:47<00:08,  2.88it/s][2026-01-17 09:31:10] INFO:     127.0.0.1:43594 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:47<00:06,  3.42it/s][2026-01-17 09:31:10] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:47<00:05,  3.98it/s][2026-01-17 09:31:10] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:47<00:05,  3.93it/s][2026-01-17 09:31:11] INFO:     127.0.0.1:44458 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:48<00:08,  2.32it/s][2026-01-17 09:31:11] INFO:     127.0.0.1:44398 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:48<00:06,  2.89it/s][2026-01-17 09:31:13 TP0] Decode batch, #running-req: 19, #token: 7552, token usage: 0.01, npu graph: False, gen throughput (token/s): 143.64, #queue-req: 0,
[2026-01-17 09:31:13] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:50<00:14,  1.22it/s][2026-01-17 09:31:14] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:51<00:13,  1.30it/s][2026-01-17 09:31:15] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:51<00:11,  1.44it/s][2026-01-17 09:31:15] INFO:     127.0.0.1:44460 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:52<00:08,  1.68it/s][2026-01-17 09:31:16] INFO:     127.0.0.1:59592 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:52<00:08,  1.65it/s][2026-01-17 09:31:17] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:31:17] INFO:     127.0.0.1:44334 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:54<00:10,  1.19it/s]
 94%|█████████▍| 188/200 [00:54<00:09,  1.30it/s][2026-01-17 09:31:17] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:31:17] INFO:     127.0.0.1:44322 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:54<00:07,  1.55it/s]
 95%|█████████▌| 190/200 [00:54<00:04,  2.32it/s][2026-01-17 09:31:18 TP0] Decode batch, #running-req: 10, #token: 4864, token usage: 0.01, npu graph: False, gen throughput (token/s): 124.44, #queue-req: 0,
[2026-01-17 09:31:18] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:54<00:03,  2.25it/s][2026-01-17 09:31:18] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:55<00:03,  2.52it/s][2026-01-17 09:31:18] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:55<00:02,  3.08it/s][2026-01-17 09:31:19] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:56<00:03,  1.94it/s][2026-01-17 09:31:21] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:57<00:04,  1.21it/s][2026-01-17 09:31:22] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:59<00:04,  1.05s/it][2026-01-17 09:31:23 TP0] Decode batch, #running-req: 4, #token: 2560, token usage: 0.00, npu graph: False, gen throughput (token/s): 44.57, #queue-req: 0,
[2026-01-17 09:31:23] INFO:     127.0.0.1:43670 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [01:00<00:03,  1.06s/it][2026-01-17 09:31:25] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [01:02<00:02,  1.28s/it][2026-01-17 09:31:29 TP0] Decode batch, #running-req: 2, #token: 1792, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.18, #queue-req: 0,
[2026-01-17 09:31:30] INFO:     127.0.0.1:44414 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [01:07<00:02,  2.34s/it][2026-01-17 09:31:35 TP0] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, npu graph: False, gen throughput (token/s): 8.81, #queue-req: 0,
[2026-01-17 09:31:35] INFO:     127.0.0.1:44274 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [01:12<00:00,  3.13s/it]
100%|██████████| 200/200 [01:12<00:00,  2.77it/s]
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/llm_models/gsm8k_ascend_mixin.py", line 66, in test_gsm8k
    self.assertGreaterEqual(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1277, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: np.float64(0.94) not greater than or equal to 0.96 : Accuracy of /root/.cache/modelscope/hub/models/Qwen3-Coder-480B-A35B-Instruct-w8a8-QuaRot is 0.94, is lower than 0.96
E
======================================================================
ERROR: test_gsm8k (__main__.TestMistral7B.test_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: np.float64(0.94) not greater than or equal to 0.96 : Accuracy of /root/.cache/modelscope/hub/models/Qwen3-Coder-480B-A35B-Instruct-w8a8-QuaRot is 0.94, is lower than 0.96

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 1015.469s

FAILED (errors=1)
Accuracy: 0.940
Invalid: 0.000
Latency: 72.591 s
Output throughput: 356.130 token/s
.
.
End (23/62):
filename='ascend/llm_models/test_ascend_qwen3_coder_480b_a35b_instruct_w8a8_quaRot.py', elapsed=1027, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_ascend_qwen3_coder_480b_a35b_instruct_w8a8_quaRot.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/llm_models/test_ascend_qwen3_coder_480b_a35b_instruct_w8a8_quaRot.py

.
.
Begin (23/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_qwen3_coder_480b_a35b_instruct_w8a8_quaRot.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:32:57] WARNING model_config.py:820: modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:32:58] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen3-Coder-480B-A35B-Instruct-w8a8-QuaRot', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen3-Coder-480B-A35B-Instruct-w8a8-QuaRot', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='modelslim', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=353135970, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Qwen3-Coder-480B-A35B-Instruct-w8a8-QuaRot', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='modelslim', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 09:32:58] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:32:58] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:33:09 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:33:09 TP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:09 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:33:10 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:10 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:33:10 TP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:10 TP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:10 TP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:10 TP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:33:10 TP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:33:10 TP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:33:10 TP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:10 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:10 TP3] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:10 TP0] Init torch distributed begin.
[2026-01-17 09:33:10 TP3] Init torch distributed begin.
[2026-01-17 09:33:10 TP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:10 TP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:10 TP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:10 TP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:10 TP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:33:10 TP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:33:11 TP5] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP5] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:33:11 TP2] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP14] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP2] Init torch distributed begin.
[2026-01-17 09:33:11 TP14] Init torch distributed begin.
[2026-01-17 09:33:11 TP4] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP4] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:33:11 TP7] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP15] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP15] Init torch distributed begin.
[2026-01-17 09:33:11 TP7] Init torch distributed begin.
[2026-01-17 09:33:11 TP8] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP8] Init torch distributed begin.
[2026-01-17 09:33:11 TP10] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP10] Init torch distributed begin.
[2026-01-17 09:33:11 TP9] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP9] Init torch distributed begin.
[2026-01-17 09:33:11 TP6] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP6] Init torch distributed begin.
[2026-01-17 09:33:11 TP12] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP12] Init torch distributed begin.
[2026-01-17 09:33:11 TP13] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP13] Init torch distributed begin.
[2026-01-17 09:33:11 TP11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:33:11 TP11] Init torch distributed begin.
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[2026-01-17 09:33:13 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP15] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP14] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP13] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP10] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP12] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP8] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP11] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP9] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:13 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:33:14 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP15] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP9] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP11] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP13] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP14] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP8] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP12] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP10] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:33:14 TP1] Load weight begin. avail mem=61.11 GB
[2026-01-17 09:33:14 TP2] Load weight begin. avail mem=60.84 GB
[2026-01-17 09:33:14 TP13] Load weight begin. avail mem=61.11 GB
[2026-01-17 09:33:14 TP7] Load weight begin. avail mem=61.10 GB
[2026-01-17 09:33:14 TP6] Load weight begin. avail mem=60.85 GB
[2026-01-17 09:33:14 TP8] Load weight begin. avail mem=60.84 GB
[2026-01-17 09:33:14 TP9] Load weight begin. avail mem=61.12 GB
[2026-01-17 09:33:14 TP4] Load weight begin. avail mem=60.84 GB
[2026-01-17 09:33:14 TP0] Load weight begin. avail mem=60.78 GB
[2026-01-17 09:33:14 TP14] Load weight begin. avail mem=60.85 GB
[2026-01-17 09:33:14 TP15] Load weight begin. avail mem=61.10 GB
[2026-01-17 09:33:14 TP12] Load weight begin. avail mem=60.84 GB
[2026-01-17 09:33:14 TP3] Load weight begin. avail mem=61.11 GB
[2026-01-17 09:33:14 TP10] Load weight begin. avail mem=60.85 GB
[2026-01-17 09:33:14 TP11] Load weight begin. avail mem=61.10 GB
[2026-01-17 09:33:14 TP5] Load weight begin. avail mem=61.11 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)

Loading safetensors checkpoint shards:   0% Completed | 0/113 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   1% Completed | 1/113 [00:00<00:44,  2.54it/s]

Loading safetensors checkpoint shards:   2% Completed | 2/113 [00:00<00:48,  2.29it/s]

Loading safetensors checkpoint shards:   3% Completed | 3/113 [00:01<00:48,  2.27it/s]

Loading safetensors checkpoint shards:   4% Completed | 4/113 [00:01<00:48,  2.23it/s]

Loading safetensors checkpoint shards:   4% Completed | 5/113 [00:02<00:48,  2.24it/s]

Loading safetensors checkpoint shards:   5% Completed | 6/113 [00:02<00:46,  2.29it/s]

Loading safetensors checkpoint shards:   6% Completed | 7/113 [00:03<00:45,  2.34it/s]

Loading safetensors checkpoint shards:   7% Completed | 8/113 [00:03<00:45,  2.28it/s]

Loading safetensors checkpoint shards:   8% Completed | 9/113 [00:03<00:46,  2.25it/s]

Loading safetensors checkpoint shards:   9% Completed | 10/113 [00:04<00:46,  2.20it/s]

Loading safetensors checkpoint shards:  10% Completed | 11/113 [00:04<00:46,  2.19it/s]

Loading safetensors checkpoint shards:  11% Completed | 12/113 [00:05<00:46,  2.15it/s]

Loading safetensors checkpoint shards:  12% Completed | 13/113 [00:05<00:46,  2.13it/s]

Loading safetensors checkpoint shards:  12% Completed | 14/113 [00:06<00:46,  2.11it/s]

Loading safetensors checkpoint shards:  13% Completed | 15/113 [00:07<00:52,  1.86it/s]

Loading safetensors checkpoint shards:  14% Completed | 16/113 [00:07<01:02,  1.55it/s]

Loading safetensors checkpoint shards:  15% Completed | 17/113 [00:08<01:09,  1.39it/s]

Loading safetensors checkpoint shards:  16% Completed | 18/113 [00:09<01:05,  1.45it/s]

Loading safetensors checkpoint shards:  17% Completed | 19/113 [00:09<00:51,  1.83it/s]

Loading safetensors checkpoint shards:  18% Completed | 20/113 [00:10<00:46,  1.99it/s]

Loading safetensors checkpoint shards:  19% Completed | 21/113 [00:10<00:54,  1.70it/s]

Loading safetensors checkpoint shards:  19% Completed | 22/113 [00:12<01:15,  1.20it/s]

Loading safetensors checkpoint shards:  20% Completed | 23/113 [00:13<01:17,  1.17it/s]

Loading safetensors checkpoint shards:  21% Completed | 24/113 [00:14<01:17,  1.14it/s]

Loading safetensors checkpoint shards:  22% Completed | 25/113 [00:14<01:07,  1.31it/s]

Loading safetensors checkpoint shards:  23% Completed | 26/113 [00:14<00:55,  1.57it/s]

Loading safetensors checkpoint shards:  24% Completed | 27/113 [00:15<00:59,  1.45it/s]

Loading safetensors checkpoint shards:  25% Completed | 28/113 [00:16<01:00,  1.40it/s]

Loading safetensors checkpoint shards:  26% Completed | 29/113 [00:16<00:53,  1.58it/s]

Loading safetensors checkpoint shards:  27% Completed | 30/113 [00:17<00:47,  1.75it/s]

Loading safetensors checkpoint shards:  27% Completed | 31/113 [00:17<00:44,  1.86it/s]

Loading safetensors checkpoint shards:  28% Completed | 32/113 [00:18<00:42,  1.92it/s]

Loading safetensors checkpoint shards:  29% Completed | 33/113 [00:18<00:40,  2.00it/s]

Loading safetensors checkpoint shards:  30% Completed | 34/113 [00:19<00:38,  2.06it/s]

Loading safetensors checkpoint shards:  31% Completed | 35/113 [00:19<00:36,  2.11it/s]

Loading safetensors checkpoint shards:  32% Completed | 36/113 [00:20<00:36,  2.12it/s]

Loading safetensors checkpoint shards:  33% Completed | 37/113 [00:20<00:35,  2.13it/s]

Loading safetensors checkpoint shards:  34% Completed | 38/113 [00:21<00:34,  2.17it/s]

Loading safetensors checkpoint shards:  35% Completed | 39/113 [00:21<00:33,  2.18it/s]

Loading safetensors checkpoint shards:  35% Completed | 40/113 [00:21<00:33,  2.18it/s]

Loading safetensors checkpoint shards:  36% Completed | 41/113 [00:22<00:32,  2.19it/s]

Loading safetensors checkpoint shards:  37% Completed | 42/113 [00:22<00:32,  2.16it/s]

Loading safetensors checkpoint shards:  38% Completed | 43/113 [00:23<00:32,  2.16it/s]

Loading safetensors checkpoint shards:  39% Completed | 44/113 [00:23<00:32,  2.15it/s]

Loading safetensors checkpoint shards:  40% Completed | 45/113 [00:24<00:31,  2.16it/s]

Loading safetensors checkpoint shards:  41% Completed | 46/113 [00:24<00:31,  2.15it/s]

Loading safetensors checkpoint shards:  42% Completed | 47/113 [00:25<00:30,  2.16it/s]

Loading safetensors checkpoint shards:  42% Completed | 48/113 [00:25<00:30,  2.15it/s]

Loading safetensors checkpoint shards:  43% Completed | 49/113 [00:26<00:29,  2.16it/s]

Loading safetensors checkpoint shards:  44% Completed | 50/113 [00:26<00:29,  2.17it/s]

Loading safetensors checkpoint shards:  45% Completed | 51/113 [00:27<00:28,  2.16it/s]

Loading safetensors checkpoint shards:  46% Completed | 52/113 [00:27<00:28,  2.16it/s]

Loading safetensors checkpoint shards:  47% Completed | 53/113 [00:27<00:27,  2.16it/s]

Loading safetensors checkpoint shards:  48% Completed | 54/113 [00:28<00:26,  2.20it/s]

Loading safetensors checkpoint shards:  49% Completed | 55/113 [00:28<00:26,  2.20it/s]

Loading safetensors checkpoint shards:  50% Completed | 56/113 [00:29<00:26,  2.18it/s]

Loading safetensors checkpoint shards:  50% Completed | 57/113 [00:29<00:25,  2.17it/s]

Loading safetensors checkpoint shards:  51% Completed | 58/113 [00:30<00:26,  2.10it/s]

Loading safetensors checkpoint shards:  52% Completed | 59/113 [00:30<00:25,  2.08it/s]

Loading safetensors checkpoint shards:  53% Completed | 60/113 [00:31<00:25,  2.07it/s]

Loading safetensors checkpoint shards:  54% Completed | 61/113 [00:31<00:25,  2.04it/s]

Loading safetensors checkpoint shards:  55% Completed | 62/113 [00:32<00:24,  2.08it/s]

Loading safetensors checkpoint shards:  56% Completed | 63/113 [00:32<00:23,  2.12it/s]

Loading safetensors checkpoint shards:  57% Completed | 64/113 [00:33<00:22,  2.20it/s]

Loading safetensors checkpoint shards:  58% Completed | 65/113 [00:33<00:21,  2.28it/s]

Loading safetensors checkpoint shards:  58% Completed | 66/113 [00:33<00:20,  2.30it/s]

Loading safetensors checkpoint shards:  59% Completed | 67/113 [00:34<00:20,  2.26it/s]

Loading safetensors checkpoint shards:  60% Completed | 68/113 [00:34<00:19,  2.28it/s]

Loading safetensors checkpoint shards:  61% Completed | 69/113 [00:35<00:19,  2.27it/s]

Loading safetensors checkpoint shards:  62% Completed | 70/113 [00:35<00:18,  2.28it/s]

Loading safetensors checkpoint shards:  63% Completed | 71/113 [00:36<00:17,  2.33it/s]

Loading safetensors checkpoint shards:  64% Completed | 72/113 [00:36<00:17,  2.37it/s]

Loading safetensors checkpoint shards:  65% Completed | 73/113 [00:36<00:16,  2.37it/s]

Loading safetensors checkpoint shards:  65% Completed | 74/113 [00:37<00:16,  2.35it/s]

Loading safetensors checkpoint shards:  66% Completed | 75/113 [00:37<00:15,  2.38it/s]

Loading safetensors checkpoint shards:  67% Completed | 76/113 [00:38<00:15,  2.39it/s]

Loading safetensors checkpoint shards:  68% Completed | 77/113 [00:38<00:15,  2.40it/s]

Loading safetensors checkpoint shards:  69% Completed | 78/113 [00:39<00:14,  2.38it/s]

Loading safetensors checkpoint shards:  70% Completed | 79/113 [00:39<00:14,  2.33it/s]

Loading safetensors checkpoint shards:  71% Completed | 80/113 [00:39<00:14,  2.33it/s]

Loading safetensors checkpoint shards:  72% Completed | 81/113 [00:40<00:13,  2.35it/s]

Loading safetensors checkpoint shards:  73% Completed | 82/113 [00:40<00:13,  2.35it/s]

Loading safetensors checkpoint shards:  73% Completed | 83/113 [00:41<00:12,  2.37it/s]

Loading safetensors checkpoint shards:  74% Completed | 84/113 [00:41<00:12,  2.39it/s]

Loading safetensors checkpoint shards:  75% Completed | 85/113 [00:42<00:11,  2.38it/s]

Loading safetensors checkpoint shards:  76% Completed | 86/113 [00:42<00:11,  2.37it/s]

Loading safetensors checkpoint shards:  77% Completed | 87/113 [00:42<00:11,  2.36it/s]

Loading safetensors checkpoint shards:  78% Completed | 88/113 [00:43<00:10,  2.36it/s]

Loading safetensors checkpoint shards:  79% Completed | 89/113 [00:43<00:10,  2.36it/s]

Loading safetensors checkpoint shards:  80% Completed | 90/113 [00:44<00:09,  2.36it/s]

Loading safetensors checkpoint shards:  81% Completed | 91/113 [00:44<00:09,  2.34it/s]

Loading safetensors checkpoint shards:  81% Completed | 92/113 [00:44<00:08,  2.34it/s]

Loading safetensors checkpoint shards:  82% Completed | 93/113 [00:45<00:08,  2.30it/s]

Loading safetensors checkpoint shards:  83% Completed | 94/113 [00:45<00:08,  2.28it/s]

Loading safetensors checkpoint shards:  84% Completed | 95/113 [00:46<00:07,  2.28it/s]

Loading safetensors checkpoint shards:  85% Completed | 96/113 [00:46<00:07,  2.28it/s]

Loading safetensors checkpoint shards:  86% Completed | 97/113 [00:47<00:06,  2.29it/s]

Loading safetensors checkpoint shards:  87% Completed | 98/113 [00:47<00:06,  2.29it/s]

Loading safetensors checkpoint shards:  88% Completed | 99/113 [00:48<00:06,  2.28it/s]

Loading safetensors checkpoint shards:  88% Completed | 100/113 [00:48<00:05,  2.31it/s]

Loading safetensors checkpoint shards:  89% Completed | 101/113 [00:48<00:05,  2.29it/s]

Loading safetensors checkpoint shards:  90% Completed | 102/113 [00:49<00:04,  2.32it/s]

Loading safetensors checkpoint shards:  91% Completed | 103/113 [00:49<00:04,  2.30it/s]

Loading safetensors checkpoint shards:  92% Completed | 104/113 [00:50<00:03,  2.32it/s]

Loading safetensors checkpoint shards:  93% Completed | 105/113 [00:50<00:03,  2.33it/s]

Loading safetensors checkpoint shards:  94% Completed | 106/113 [00:51<00:03,  2.31it/s]

Loading safetensors checkpoint shards:  95% Completed | 107/113 [00:51<00:02,  2.34it/s]

Loading safetensors checkpoint shards:  96% Completed | 108/113 [00:51<00:02,  2.30it/s]

Loading safetensors checkpoint shards:  96% Completed | 109/113 [00:52<00:01,  2.27it/s]

Loading safetensors checkpoint shards:  97% Completed | 110/113 [00:52<00:01,  2.25it/s]

Loading safetensors checkpoint shards:  98% Completed | 111/113 [00:53<00:00,  2.27it/s]

Loading safetensors checkpoint shards:  99% Completed | 112/113 [00:53<00:00,  2.27it/s]

Loading safetensors checkpoint shards: 100% Completed | 113/113 [00:54<00:00,  2.31it/s]

Loading safetensors checkpoint shards: 100% Completed | 113/113 [00:54<00:00,  2.09it/s]

[2026-01-17 09:34:33 TP8] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.86 GB, mem usage=29.98 GB.
[2026-01-17 09:34:33 TP11] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.13 GB, mem usage=29.98 GB.
[2026-01-17 09:34:34 TP13] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.14 GB, mem usage=29.98 GB.
[2026-01-17 09:34:35 TP10] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.88 GB, mem usage=29.98 GB.
[2026-01-17 09:34:36 TP6] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.88 GB, mem usage=29.98 GB.
[2026-01-17 09:34:36 TP4] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.86 GB, mem usage=29.98 GB.
[2026-01-17 09:34:36 TP0] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.81 GB, mem usage=29.98 GB.
[2026-01-17 09:34:36 TP9] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.14 GB, mem usage=29.98 GB.
[2026-01-17 09:34:37 TP2] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.86 GB, mem usage=29.98 GB.
[2026-01-17 09:34:37 TP3] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.14 GB, mem usage=29.98 GB.
[2026-01-17 09:34:37 TP1] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.14 GB, mem usage=29.98 GB.
[2026-01-17 09:34:38 TP14] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.88 GB, mem usage=29.98 GB.
[2026-01-17 09:34:38 TP5] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.14 GB, mem usage=29.98 GB.
[2026-01-17 09:34:39 TP15] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.13 GB, mem usage=29.98 GB.
[2026-01-17 09:34:39 TP7] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=31.13 GB, mem usage=29.98 GB.
[2026-01-17 09:34:39 TP12] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=30.86 GB, mem usage=29.98 GB.
[2026-01-17 09:34:39 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 09:34:39 TP0] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP15] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP14] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP12] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP13] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP11] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP9] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP10] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP7] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP8] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP6] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP5] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP4] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP3] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP1] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:39 TP2] The available memory for KV cache is 18.61 GB.
[2026-01-17 09:34:40 TP4] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP12] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP2] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP4] Memory pool end. avail mem=10.22 GB
[2026-01-17 09:34:40 TP12] Memory pool end. avail mem=10.22 GB
[2026-01-17 09:34:40 TP2] Memory pool end. avail mem=10.22 GB
[2026-01-17 09:34:40 TP0] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP10] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP8] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP0] Memory pool end. avail mem=10.16 GB
[2026-01-17 09:34:40 TP10] Memory pool end. avail mem=10.23 GB
[2026-01-17 09:34:40 TP8] Memory pool end. avail mem=10.22 GB
[2026-01-17 09:34:40 TP14] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP5] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP1] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP13] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP14] Memory pool end. avail mem=10.23 GB
[2026-01-17 09:34:40 TP9] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP11] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP6] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP5] Memory pool end. avail mem=10.49 GB
[2026-01-17 09:34:40 TP3] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP1] Memory pool end. avail mem=10.49 GB
[2026-01-17 09:34:40 TP15] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP9] Memory pool end. avail mem=10.50 GB
[2026-01-17 09:34:40 TP13] Memory pool end. avail mem=10.49 GB
[2026-01-17 09:34:40 TP11] Memory pool end. avail mem=10.48 GB
[2026-01-17 09:34:40 TP6] Memory pool end. avail mem=10.23 GB
[2026-01-17 09:34:40 TP3] Memory pool end. avail mem=10.49 GB
[2026-01-17 09:34:40 TP15] Memory pool end. avail mem=10.48 GB
[2026-01-17 09:34:40 TP7] KV Cache is allocated. #tokens: 629248, K size: 9.30 GB, V size: 9.30 GB
[2026-01-17 09:34:40 TP7] Memory pool end. avail mem=10.48 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 09:34:41 TP0] max_total_num_tokens=629248, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=262144, available_gpu_mem=10.16 GB
[2026-01-17 09:34:41] INFO:     Started server process [177144]
[2026-01-17 09:34:41] INFO:     Waiting for application startup.
[2026-01-17 09:34:41] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 09:34:41] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 09:34:41] INFO:     Application startup complete.
[2026-01-17 09:34:41] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 09:34:42] INFO:     127.0.0.1:37144 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 09:34:43 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
................[rank2]:[W117 09:34:46.132545333 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank5]:[W117 09:34:46.327701910 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank12]:[W117 09:34:47.622004237 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank3]:[W117 09:34:47.694557961 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank0]:[W117 09:34:47.714567110 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank9]:[W117 09:34:47.732051190 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank6]:[W117 09:34:47.796465168 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank14]:[W117 09:34:47.808343890 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank15]:[W117 09:34:47.817774257 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank13]:[W117 09:34:47.851627255 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank1]:[W117 09:34:47.033747395 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank10]:[W117 09:34:47.056276792 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank8]:[W117 09:34:47.074805093 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank11]:[W117 09:34:47.108470764 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank7]:[W117 09:34:47.108471974 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank4]:[W117 09:34:47.118075038 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[2026-01-17 09:34:48] INFO:     127.0.0.1:46932 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 09:34:49] INFO:     127.0.0.1:37146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:34:49] The server is fired up and ready to roll!
[2026-01-17 09:34:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:34:59] INFO:     127.0.0.1:39940 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 09:34:59] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 09:34:59] INFO:     127.0.0.1:39944 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 09:34:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:34:59] INFO:     127.0.0.1:39954 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen3-Coder-480B-A35B-Instruct-w8a8-QuaRot --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 16 --quantization modelslim --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 09:34:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:34:59 TP0] Prefill batch, #new-seq: 32, #new-token: 4352, #cached-token: 24576, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 09:34:59 TP0] Prefill batch, #new-seq: 61, #new-token: 8192, #cached-token: 46848, token usage: 0.01, #running-req: 33, #queue-req: 22,
[2026-01-17 09:35:00 TP0] Prefill batch, #new-seq: 34, #new-token: 4480, #cached-token: 26112, token usage: 0.02, #running-req: 94, #queue-req: 0,
[2026-01-17 09:35:05 TP0] Decode batch, #running-req: 128, #token: 21376, token usage: 0.03, npu graph: False, gen throughput (token/s): 34.39, #queue-req: 0,
[2026-01-17 09:35:07] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:08<27:32,  8.30s/it][2026-01-17 09:35:08] INFO:     127.0.0.1:39970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:08] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:08<11:35,  3.51s/it]
  2%|▏         | 3/200 [00:08<04:27,  1.36s/it][2026-01-17 09:35:08 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:08 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:09] INFO:     127.0.0.1:40864 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:09<04:34,  1.40s/it][2026-01-17 09:35:09] INFO:     127.0.0.1:40568 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:10<03:16,  1.01s/it][2026-01-17 09:35:09 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:09] INFO:     127.0.0.1:40276 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:10<02:25,  1.34it/s][2026-01-17 09:35:09 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:09 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 09:35:10] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:10<02:11,  1.47it/s][2026-01-17 09:35:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:10] INFO:     127.0.0.1:39976 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:11<01:48,  1.77it/s][2026-01-17 09:35:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:10] INFO:     127.0.0.1:40676 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:11<01:30,  2.10it/s][2026-01-17 09:35:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:11] INFO:     127.0.0.1:40270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:11] INFO:     127.0.0.1:41074 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:11<01:18,  2.42it/s]
  6%|▌         | 11/200 [00:11<00:53,  3.51it/s][2026-01-17 09:35:11 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:11] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:11] INFO:     127.0.0.1:40876 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:12<01:07,  2.80it/s]
  6%|▋         | 13/200 [00:12<01:01,  3.05it/s][2026-01-17 09:35:11] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:11] INFO:     127.0.0.1:40644 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:12<00:53,  3.50it/s]
  8%|▊         | 15/200 [00:12<00:37,  4.98it/s][2026-01-17 09:35:11 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:12 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:12] INFO:     127.0.0.1:40998 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:12<00:52,  3.49it/s][2026-01-17 09:35:12] INFO:     127.0.0.1:40854 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:13<00:45,  4.00it/s][2026-01-17 09:35:12 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:12 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:13] INFO:     127.0.0.1:40562 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:13<01:00,  3.00it/s][2026-01-17 09:35:13 TP0] Decode batch, #running-req: 128, #token: 30464, token usage: 0.05, npu graph: False, gen throughput (token/s): 658.54, #queue-req: 0,
[2026-01-17 09:35:13] INFO:     127.0.0.1:40512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:13] INFO:     127.0.0.1:40648 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:13<00:51,  3.54it/s]
 10%|█         | 20/200 [00:13<00:34,  5.23it/s][2026-01-17 09:35:13 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:13 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:14] INFO:     127.0.0.1:40286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:14] INFO:     127.0.0.1:40642 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:14<00:57,  3.11it/s]
 11%|█         | 22/200 [00:14<01:00,  2.93it/s][2026-01-17 09:35:14 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:14] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:14] INFO:     127.0.0.1:40532 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:14<00:57,  3.06it/s]
 12%|█▏        | 24/200 [00:14<00:44,  3.96it/s][2026-01-17 09:35:14 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:15] INFO:     127.0.0.1:40358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:15] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:15<01:00,  2.87it/s]
 13%|█▎        | 26/200 [00:15<01:00,  2.88it/s][2026-01-17 09:35:15] INFO:     127.0.0.1:40428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:15] INFO:     127.0.0.1:40882 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:15<00:52,  3.32it/s]
 14%|█▍        | 28/200 [00:15<00:36,  4.70it/s][2026-01-17 09:35:15 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:15 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:15] INFO:     127.0.0.1:40476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:15] INFO:     127.0.0.1:40708 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:16<00:49,  3.46it/s]
 15%|█▌        | 30/200 [00:16<00:48,  3.48it/s][2026-01-17 09:35:15] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:16<00:43,  3.93it/s][2026-01-17 09:35:15 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:16 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:16] INFO:     127.0.0.1:40374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:16] INFO:     127.0.0.1:40672 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:17<01:05,  2.57it/s]
 16%|█▋        | 33/200 [00:17<01:06,  2.50it/s][2026-01-17 09:35:16] INFO:     127.0.0.1:40726 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:17<00:56,  2.92it/s][2026-01-17 09:35:16 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:17 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:17] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:17<01:10,  2.33it/s][2026-01-17 09:35:17] INFO:     127.0.0.1:40170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:17] INFO:     127.0.0.1:41004 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:18<00:58,  2.81it/s]
 18%|█▊        | 37/200 [00:18<00:38,  4.26it/s][2026-01-17 09:35:17 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:17] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:18<00:35,  4.53it/s][2026-01-17 09:35:17 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:18 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 130, #queue-req: 0,
[2026-01-17 09:35:18] INFO:     127.0.0.1:41036 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:18<00:51,  3.10it/s][2026-01-17 09:35:18] INFO:     127.0.0.1:40606 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:19<00:43,  3.65it/s][2026-01-17 09:35:18 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:18 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:19] INFO:     127.0.0.1:40160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:19] INFO:     127.0.0.1:40448 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:19<00:56,  2.82it/s]
 21%|██        | 42/200 [00:19<00:51,  3.09it/s][2026-01-17 09:35:19] INFO:     127.0.0.1:40204 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:19<00:43,  3.59it/s][2026-01-17 09:35:19 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:19 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:20] INFO:     127.0.0.1:40426 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:20<01:00,  2.58it/s][2026-01-17 09:35:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:20] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:20<00:55,  2.78it/s][2026-01-17 09:35:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:20] INFO:     127.0.0.1:41132 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:21<00:52,  2.94it/s][2026-01-17 09:35:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:20] INFO:     127.0.0.1:40142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:20] INFO:     127.0.0.1:41020 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:21<00:49,  3.12it/s]
 24%|██▍       | 48/200 [00:21<00:36,  4.20it/s][2026-01-17 09:35:21 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:21] INFO:     127.0.0.1:41052 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:21<00:37,  4.00it/s][2026-01-17 09:35:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:21] INFO:     127.0.0.1:40618 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:21<00:39,  3.80it/s][2026-01-17 09:35:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:22] INFO:     127.0.0.1:40734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:22] INFO:     127.0.0.1:41170 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:22<00:50,  2.97it/s]
 26%|██▌       | 52/200 [00:22<00:45,  3.25it/s][2026-01-17 09:35:22] INFO:     127.0.0.1:40502 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:22<00:39,  3.74it/s][2026-01-17 09:35:22 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:22 TP0] Decode batch, #running-req: 126, #token: 28672, token usage: 0.05, npu graph: False, gen throughput (token/s): 568.42, #queue-req: 0,
[2026-01-17 09:35:22] INFO:     127.0.0.1:41124 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:22<00:34,  4.26it/s][2026-01-17 09:35:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 09:35:23] INFO:     127.0.0.1:40446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:23] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:23<00:53,  2.69it/s]
 28%|██▊       | 56/200 [00:23<00:53,  2.68it/s][2026-01-17 09:35:23] INFO:     127.0.0.1:40028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:23] INFO:     127.0.0.1:40768 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:23<00:44,  3.20it/s]
 29%|██▉       | 58/200 [00:23<00:29,  4.74it/s][2026-01-17 09:35:23 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:23] INFO:     127.0.0.1:40552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:23] INFO:     127.0.0.1:41116 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:23<00:27,  5.18it/s]
 30%|███       | 60/200 [00:23<00:20,  6.98it/s][2026-01-17 09:35:23 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:23 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.05, #running-req: 130, #queue-req: 0,
[2026-01-17 09:35:23] INFO:     127.0.0.1:40294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:23] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:24<00:26,  5.30it/s][2026-01-17 09:35:23] INFO:     127.0.0.1:40060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:23] INFO:     127.0.0.1:40412 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:24<00:24,  5.63it/s]
 32%|███▏      | 64/200 [00:24<00:18,  7.30it/s][2026-01-17 09:35:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 09:35:24] INFO:     127.0.0.1:40318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:24] INFO:     127.0.0.1:41154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:24] INFO:     127.0.0.1:41160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:24] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:24<00:15,  8.56it/s]
 34%|███▍      | 68/200 [00:24<00:08, 14.93it/s]
 34%|███▍      | 68/200 [00:24<00:08, 14.93it/s][2026-01-17 09:35:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 09:35:24 TP0] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 3072, token usage: 0.04, #running-req: 130, #queue-req: 0,
[2026-01-17 09:35:24] INFO:     127.0.0.1:40786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:25] INFO:     127.0.0.1:40022 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:25<00:20,  6.25it/s][2026-01-17 09:35:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:35:25] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:25] INFO:     127.0.0.1:40912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:25] INFO:     127.0.0.1:40928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 128, #queue-req: 0,

 36%|███▌      | 72/200 [00:25<00:17,  7.31it/s]
 36%|███▋      | 73/200 [00:25<00:13,  9.53it/s][2026-01-17 09:35:25 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 09:35:25] INFO:     127.0.0.1:40986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:25] INFO:     127.0.0.1:40810 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:26<00:19,  6.31it/s][2026-01-17 09:35:25] INFO:     127.0.0.1:40076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:25] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:26<00:16,  7.44it/s][2026-01-17 09:35:26] INFO:     127.0.0.1:40392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:26] INFO:     127.0.0.1:40664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:26] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:26<00:16,  7.52it/s]
 40%|████      | 80/200 [00:26<00:13,  8.64it/s][2026-01-17 09:35:26] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:26] INFO:     127.0.0.1:40254 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:26<00:13,  8.46it/s][2026-01-17 09:35:26] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:26] INFO:     127.0.0.1:40956 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:26<00:11,  9.67it/s][2026-01-17 09:35:26] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:26] INFO:     127.0.0.1:41084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:26] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:27<00:12,  9.14it/s]
 44%|████▎     | 87/200 [00:27<00:11, 10.03it/s][2026-01-17 09:35:27] INFO:     127.0.0.1:40246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:27] INFO:     127.0.0.1:40020 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:27<00:13,  8.11it/s][2026-01-17 09:35:27] INFO:     127.0.0.1:40096 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:27<00:13,  8.09it/s][2026-01-17 09:35:27] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:27<00:16,  6.78it/s][2026-01-17 09:35:27] INFO:     127.0.0.1:40844 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:28<00:18,  5.96it/s][2026-01-17 09:35:27] INFO:     127.0.0.1:40174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:27] INFO:     127.0.0.1:40486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:27] INFO:     127.0.0.1:41100 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:28<00:16,  6.35it/s]
 48%|████▊     | 95/200 [00:28<00:08, 11.99it/s]
 48%|████▊     | 95/200 [00:28<00:08, 11.99it/s][2026-01-17 09:35:28] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:28] INFO:     127.0.0.1:41146 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:28<00:11,  8.65it/s][2026-01-17 09:35:28] INFO:     127.0.0.1:40590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:28] INFO:     127.0.0.1:40658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:28] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:28<00:12,  8.41it/s]
 50%|█████     | 100/200 [00:28<00:10,  9.50it/s][2026-01-17 09:35:28] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:28] INFO:     127.0.0.1:41066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:28] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:29<00:10,  8.97it/s]
 52%|█████▏    | 103/200 [00:29<00:09,  9.82it/s][2026-01-17 09:35:28] INFO:     127.0.0.1:40148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:29 TP0] Decode batch, #running-req: 97, #token: 23424, token usage: 0.04, npu graph: False, gen throughput (token/s): 689.59, #queue-req: 0,
[2026-01-17 09:35:29] INFO:     127.0.0.1:40050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:29] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:29<00:10,  9.31it/s]
 53%|█████▎    | 106/200 [00:29<00:09, 10.16it/s][2026-01-17 09:35:29] INFO:     127.0.0.1:40376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:29] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:29<00:09,  9.52it/s][2026-01-17 09:35:29] INFO:     127.0.0.1:40526 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:29<00:11,  7.88it/s][2026-01-17 09:35:29] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:29] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:30<00:11,  7.88it/s]
 56%|█████▌    | 111/200 [00:30<00:09,  9.48it/s][2026-01-17 09:35:29] INFO:     127.0.0.1:56934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:29] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:30<00:07, 10.90it/s][2026-01-17 09:35:29] INFO:     127.0.0.1:40896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:30] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:30<00:08,  9.75it/s][2026-01-17 09:35:30] INFO:     127.0.0.1:40464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:30] INFO:     127.0.0.1:40720 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:30] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:30<00:09,  9.01it/s]
 59%|█████▉    | 118/200 [00:30<00:08,  9.89it/s][2026-01-17 09:35:30] INFO:     127.0.0.1:40242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:30] INFO:     127.0.0.1:36248 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:31<00:08,  9.07it/s][2026-01-17 09:35:31] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:31] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:31<00:15,  4.99it/s]
 61%|██████    | 122/200 [00:31<00:18,  4.15it/s][2026-01-17 09:35:31] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:32<00:22,  3.44it/s][2026-01-17 09:35:31] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [00:32<00:19,  3.89it/s][2026-01-17 09:35:32] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:32] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:32<00:17,  4.33it/s]
 63%|██████▎   | 126/200 [00:32<00:12,  5.96it/s][2026-01-17 09:35:32] INFO:     127.0.0.1:40418 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:32<00:13,  5.28it/s][2026-01-17 09:35:32] INFO:     127.0.0.1:36396 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:32<00:12,  5.71it/s][2026-01-17 09:35:33] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:33<00:21,  3.33it/s][2026-01-17 09:35:33] INFO:     127.0.0.1:36230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:33] INFO:     127.0.0.1:36466 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:33<00:22,  3.06it/s]
 66%|██████▌   | 131/200 [00:33<00:18,  3.69it/s][2026-01-17 09:35:33] INFO:     127.0.0.1:40638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:33] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:34<00:16,  4.14it/s]
 66%|██████▋   | 133/200 [00:34<00:11,  5.77it/s][2026-01-17 09:35:33] INFO:     127.0.0.1:40192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:33] INFO:     127.0.0.1:40826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:33] INFO:     127.0.0.1:36314 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:34<00:12,  5.19it/s]
 68%|██████▊   | 136/200 [00:34<00:08,  7.63it/s]
 68%|██████▊   | 136/200 [00:34<00:08,  7.63it/s][2026-01-17 09:35:34] INFO:     127.0.0.1:36362 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:34<00:08,  7.66it/s][2026-01-17 09:35:34] INFO:     127.0.0.1:36492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:34] INFO:     127.0.0.1:36508 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:34<00:09,  6.41it/s]
 70%|██████▉   | 139/200 [00:34<00:08,  6.86it/s][2026-01-17 09:35:34 TP0] Decode batch, #running-req: 63, #token: 18176, token usage: 0.03, npu graph: False, gen throughput (token/s): 584.40, #queue-req: 0,
[2026-01-17 09:35:34] INFO:     127.0.0.1:40032 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:34<00:08,  7.01it/s][2026-01-17 09:35:34] INFO:     127.0.0.1:40878 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:34<00:08,  7.04it/s][2026-01-17 09:35:34] INFO:     127.0.0.1:40104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:34] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:35<00:08,  7.06it/s]
 72%|███████▏  | 143/200 [00:35<00:06,  8.92it/s][2026-01-17 09:35:35] INFO:     127.0.0.1:40214 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:35<00:08,  6.68it/s][2026-01-17 09:35:35] INFO:     127.0.0.1:36450 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:35<00:10,  5.46it/s][2026-01-17 09:35:35] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:35] INFO:     127.0.0.1:36338 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:35<00:11,  4.81it/s]
 74%|███████▎  | 147/200 [00:35<00:09,  5.60it/s][2026-01-17 09:35:36] INFO:     127.0.0.1:40692 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:36<00:13,  3.81it/s][2026-01-17 09:35:36] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:36<00:13,  3.82it/s][2026-01-17 09:35:36] INFO:     127.0.0.1:36276 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:37<00:15,  3.13it/s][2026-01-17 09:35:37] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:37<00:18,  2.69it/s][2026-01-17 09:35:37] INFO:     127.0.0.1:40700 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:37<00:15,  3.14it/s][2026-01-17 09:35:37] INFO:     127.0.0.1:36256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:37] INFO:     127.0.0.1:36524 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:38<00:16,  2.93it/s]
 77%|███████▋  | 154/200 [00:38<00:12,  3.61it/s][2026-01-17 09:35:38] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:38<00:12,  3.66it/s][2026-01-17 09:35:38] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:38<00:13,  3.30it/s][2026-01-17 09:35:38] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:39<00:11,  3.85it/s][2026-01-17 09:35:39] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:39<00:12,  3.36it/s][2026-01-17 09:35:39] INFO:     127.0.0.1:41090 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:39<00:10,  3.96it/s][2026-01-17 09:35:39] INFO:     127.0.0.1:36278 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:39<00:10,  3.93it/s][2026-01-17 09:35:39] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:40<00:08,  4.53it/s][2026-01-17 09:35:39] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:40<00:08,  4.31it/s][2026-01-17 09:35:40 TP0] Decode batch, #running-req: 39, #token: 12672, token usage: 0.02, npu graph: False, gen throughput (token/s): 351.30, #queue-req: 0,
[2026-01-17 09:35:40] INFO:     127.0.0.1:36340 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:40<00:07,  4.94it/s][2026-01-17 09:35:40] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:40<00:09,  3.94it/s][2026-01-17 09:35:40] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:40] INFO:     127.0.0.1:36440 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:40<00:07,  4.58it/s]
 83%|████████▎ | 166/200 [00:40<00:05,  6.70it/s][2026-01-17 09:35:40] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:41<00:04,  6.82it/s][2026-01-17 09:35:40] INFO:     127.0.0.1:40164 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:41<00:05,  5.61it/s][2026-01-17 09:35:41] INFO:     127.0.0.1:40326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:41] INFO:     127.0.0.1:36218 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:41<00:07,  4.18it/s]
 85%|████████▌ | 170/200 [00:41<00:06,  4.49it/s][2026-01-17 09:35:41] INFO:     127.0.0.1:36284 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:41<00:05,  4.97it/s][2026-01-17 09:35:41] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:42<00:06,  4.63it/s][2026-01-17 09:35:41] INFO:     127.0.0.1:40048 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:42<00:05,  5.17it/s][2026-01-17 09:35:42] INFO:     127.0.0.1:40112 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:42<00:08,  3.14it/s][2026-01-17 09:35:42] INFO:     127.0.0.1:40310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:42] INFO:     127.0.0.1:36286 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:43<00:06,  3.73it/s]
 88%|████████▊ | 176/200 [00:43<00:04,  5.58it/s][2026-01-17 09:35:42] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:43<00:03,  6.04it/s][2026-01-17 09:35:43] INFO:     127.0.0.1:36436 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:43<00:07,  3.09it/s][2026-01-17 09:35:44] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:44<00:07,  2.65it/s][2026-01-17 09:35:44] INFO:     127.0.0.1:36476 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:44<00:06,  3.24it/s][2026-01-17 09:35:44] INFO:     127.0.0.1:36252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:44] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:44<00:04,  3.86it/s]
 91%|█████████ | 182/200 [00:44<00:03,  5.77it/s][2026-01-17 09:35:44] INFO:     127.0.0.1:36368 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:45<00:03,  5.06it/s][2026-01-17 09:35:44] INFO:     127.0.0.1:40728 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:45<00:03,  4.67it/s][2026-01-17 09:35:45] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:45<00:03,  4.42it/s][2026-01-17 09:35:45 TP0] Decode batch, #running-req: 16, #token: 6144, token usage: 0.01, npu graph: False, gen throughput (token/s): 200.33, #queue-req: 0,
[2026-01-17 09:35:46] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:46<00:06,  2.05it/s][2026-01-17 09:35:47] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:48<00:09,  1.36it/s][2026-01-17 09:35:47] INFO:     127.0.0.1:36288 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:48<00:06,  1.79it/s][2026-01-17 09:35:49] INFO:     127.0.0.1:36546 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:50<00:10,  1.07it/s][2026-01-17 09:35:49] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:50<00:07,  1.36it/s][2026-01-17 09:35:50 TP0] Decode batch, #running-req: 10, #token: 4864, token usage: 0.01, npu graph: False, gen throughput (token/s): 97.90, #queue-req: 0,
[2026-01-17 09:35:50] INFO:     127.0.0.1:36384 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:51<00:06,  1.33it/s][2026-01-17 09:35:50] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:35:50] INFO:     127.0.0.1:36398 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:51<00:04,  1.65it/s]
 96%|█████████▋| 193/200 [00:51<00:02,  2.57it/s][2026-01-17 09:35:51] INFO:     127.0.0.1:36432 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:51<00:01,  3.06it/s][2026-01-17 09:35:52] INFO:     127.0.0.1:36302 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:52<00:02,  1.84it/s][2026-01-17 09:35:56 TP0] Decode batch, #running-req: 5, #token: 3072, token usage: 0.00, npu graph: False, gen throughput (token/s): 41.72, #queue-req: 0,
[2026-01-17 09:35:56] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:56<00:05,  1.46s/it][2026-01-17 09:36:01] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [01:01<00:07,  2.41s/it][2026-01-17 09:36:01 TP0] Decode batch, #running-req: 3, #token: 1792, token usage: 0.00, npu graph: False, gen throughput (token/s): 29.61, #queue-req: 0,
[2026-01-17 09:36:01] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [01:01<00:03,  1.84s/it][2026-01-17 09:36:07 TP0] Decode batch, #running-req: 2, #token: 2048, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.32, #queue-req: 0,
[2026-01-17 09:36:08] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [01:08<00:03,  3.26s/it][2026-01-17 09:36:12 TP0] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 9.13, #queue-req: 0,
[2026-01-17 09:36:16] INFO:     127.0.0.1:40938 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [01:16<00:00,  4.76s/it]
100%|██████████| 200/200 [01:17<00:00,  2.60it/s]
.
----------------------------------------------------------------------
Ran 1 test in 209.376s

OK
Accuracy: 0.960
Invalid: 0.000
Latency: 77.234 s
Output throughput: 337.855 token/s
.
.
End (23/62):
filename='ascend/llm_models/test_ascend_qwen3_coder_480b_a35b_instruct_w8a8_quaRot.py', elapsed=220, estimated_time=400
.
.


✓ PASSED on retry (attempt 2): ascend/llm_models/test_ascend_qwen3_coder_480b_a35b_instruct_w8a8_quaRot.py

.
.
Begin (24/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_qwq_32b_w8a8.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:36:37] WARNING model_config.py:820: modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:36:37] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/vllm-ascend/QWQ-32B-W8A8', tokenizer_path='/root/.cache/modelscope/hub/models/vllm-ascend/QWQ-32B-W8A8', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='modelslim', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=492087281, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/vllm-ascend/QWQ-32B-W8A8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='modelslim', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 09:36:37] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:36:38] Using default HuggingFace chat template with detected content format: string
[2026-01-17 09:36:38] Detected the force reasoning pattern in chat template.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:36:46 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:36:46 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:36:47 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:36:47 TP0] Init torch distributed begin.
[2026-01-17 09:36:47 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:36:47 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2026-01-17 09:36:48 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:36:48 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:36:48 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 09:36:49 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:36:49 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:36:49 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:36:49 TP0] Load weight begin. avail mem=60.81 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [01:04<00:00, 64.89s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [01:04<00:00, 64.89s/it]

[2026-01-17 09:38:12 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=40.56 GB, mem usage=20.25 GB.
[2026-01-17 09:38:13 TP1] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=40.89 GB, mem usage=20.25 GB.
[2026-01-17 09:38:13 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 09:38:13 TP0] The available memory for KV cache is 28.38 GB.
[2026-01-17 09:38:13 TP1] The available memory for KV cache is 28.38 GB.
[2026-01-17 09:38:14 TP0] KV Cache is allocated. #tokens: 232448, K size: 14.20 GB, V size: 14.20 GB
[2026-01-17 09:38:14 TP0] Memory pool end. avail mem=11.71 GB
[2026-01-17 09:38:14 TP1] KV Cache is allocated. #tokens: 232448, K size: 14.20 GB, V size: 14.20 GB
[2026-01-17 09:38:14 TP1] Memory pool end. avail mem=12.04 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 09:38:15 TP0] max_total_num_tokens=232448, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2905, context_len=40960, available_gpu_mem=11.71 GB
[2026-01-17 09:38:16] INFO:     Started server process [199216]
[2026-01-17 09:38:16] INFO:     Waiting for application startup.
[2026-01-17 09:38:16] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 40, 'top_p': 0.95}
[2026-01-17 09:38:16] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 40, 'top_p': 0.95}
[2026-01-17 09:38:16] INFO:     Application startup complete.
[2026-01-17 09:38:16] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 09:38:17] INFO:     127.0.0.1:52658 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 09:38:17 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:38:18] INFO:     127.0.0.1:41126 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
..[2026-01-17 09:38:23] INFO:     127.0.0.1:52664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:38:23] The server is fired up and ready to roll!
[2026-01-17 09:38:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:38:29] INFO:     127.0.0.1:33232 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 09:38:29] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 09:38:29] INFO:     127.0.0.1:33248 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 09:38:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:38:29] INFO:     127.0.0.1:33258 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/vllm-ascend/QWQ-32B-W8A8 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 2 --quantization modelslim --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 09:38:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:38:29 TP0] Prefill batch, #new-seq: 34, #new-token: 4608, #cached-token: 26112, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 09:38:29 TP0] Prefill batch, #new-seq: 23, #new-token: 3072, #cached-token: 17664, token usage: 0.02, #running-req: 35, #queue-req: 0,
[2026-01-17 09:38:30 TP0] Prefill batch, #new-seq: 61, #new-token: 8192, #cached-token: 46848, token usage: 0.04, #running-req: 58, #queue-req: 9,
[2026-01-17 09:38:30 TP0] Prefill batch, #new-seq: 9, #new-token: 1152, #cached-token: 6912, token usage: 0.07, #running-req: 119, #queue-req: 0,
[2026-01-17 09:38:33 TP0] Decode batch, #running-req: 128, #token: 21376, token usage: 0.09, npu graph: False, gen throughput (token/s): 37.17, #queue-req: 0,
[2026-01-17 09:38:36] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:06<22:55,  6.91s/it][2026-01-17 09:38:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:37 TP0] Decode batch, #running-req: 128, #token: 32384, token usage: 0.14, npu graph: False, gen throughput (token/s): 1482.56, #queue-req: 0,
[2026-01-17 09:38:37] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:08<11:44,  3.56s/it][2026-01-17 09:38:37 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:38] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:08<07:06,  2.16s/it][2026-01-17 09:38:38 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:39] INFO:     127.0.0.1:34230 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:09<05:29,  1.68s/it][2026-01-17 09:38:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:39] INFO:     127.0.0.1:33960 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:09<03:42,  1.14s/it][2026-01-17 09:38:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:41 TP0] Decode batch, #running-req: 128, #token: 33280, token usage: 0.14, npu graph: False, gen throughput (token/s): 1358.09, #queue-req: 0,
[2026-01-17 09:38:41] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:38:41] INFO:     127.0.0.1:33504 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:11<04:12,  1.30s/it]
  4%|▎         | 7/200 [00:11<03:24,  1.06s/it][2026-01-17 09:38:41 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.14, #running-req: 126, #queue-req: 0,
[2026-01-17 09:38:41] INFO:     127.0.0.1:33526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:38:41] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:11<03:01,  1.06it/s]
  4%|▍         | 9/200 [00:11<02:08,  1.49it/s][2026-01-17 09:38:41 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.14, #running-req: 126, #queue-req: 0,
[2026-01-17 09:38:42] INFO:     127.0.0.1:33706 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:12<02:19,  1.36it/s][2026-01-17 09:38:42 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:43] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:13<02:27,  1.28it/s][2026-01-17 09:38:43 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.15, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:43] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:14<02:04,  1.51it/s][2026-01-17 09:38:43 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.15, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:44] INFO:     127.0.0.1:34008 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:14<01:38,  1.89it/s][2026-01-17 09:38:44 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.15, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:44] INFO:     127.0.0.1:33642 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:15<01:46,  1.75it/s][2026-01-17 09:38:44 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.15, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:44 TP0] Decode batch, #running-req: 127, #token: 35328, token usage: 0.15, npu graph: False, gen throughput (token/s): 1308.97, #queue-req: 0,
[2026-01-17 09:38:45] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:15<01:45,  1.76it/s][2026-01-17 09:38:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.15, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:46] INFO:     127.0.0.1:33352 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:17<02:40,  1.15it/s][2026-01-17 09:38:47 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:47] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:17<02:10,  1.40it/s][2026-01-17 09:38:47 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:48] INFO:     127.0.0.1:34174 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:18<02:27,  1.24it/s][2026-01-17 09:38:48 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:48] INFO:     127.0.0.1:33404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:38:48] INFO:     127.0.0.1:34170 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:18<02:01,  1.48it/s]
 10%|█         | 20/200 [00:18<01:20,  2.25it/s][2026-01-17 09:38:48 TP0] Decode batch, #running-req: 128, #token: 43520, token usage: 0.19, npu graph: False, gen throughput (token/s): 1338.14, #queue-req: 0,
[2026-01-17 09:38:48 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.19, #running-req: 126, #queue-req: 0,
[2026-01-17 09:38:49] INFO:     127.0.0.1:33666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:38:49] INFO:     127.0.0.1:33816 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:20<01:53,  1.58it/s]
 11%|█         | 22/200 [00:20<01:50,  1.61it/s][2026-01-17 09:38:49 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.20, #running-req: 126, #queue-req: 0,
[2026-01-17 09:38:50] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:38:50] INFO:     127.0.0.1:34082 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:20<01:42,  1.73it/s]
 12%|█▏        | 24/200 [00:20<01:16,  2.30it/s][2026-01-17 09:38:50 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.20, #running-req: 126, #queue-req: 0,
[2026-01-17 09:38:52] INFO:     127.0.0.1:34250 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:22<02:07,  1.37it/s][2026-01-17 09:38:52 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:52] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:38:52] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:22<01:50,  1.58it/s]
 14%|█▎        | 27/200 [00:22<01:16,  2.26it/s][2026-01-17 09:38:52 TP0] Decode batch, #running-req: 128, #token: 45824, token usage: 0.20, npu graph: False, gen throughput (token/s): 1368.21, #queue-req: 0,
[2026-01-17 09:38:52 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.20, #running-req: 126, #queue-req: 0,
[2026-01-17 09:38:52] INFO:     127.0.0.1:34314 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:23<01:15,  2.29it/s][2026-01-17 09:38:52 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:52] INFO:     127.0.0.1:34042 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:23<01:03,  2.70it/s][2026-01-17 09:38:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:53] INFO:     127.0.0.1:33792 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:23<01:07,  2.52it/s][2026-01-17 09:38:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:54] INFO:     127.0.0.1:33870 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:24<01:22,  2.05it/s][2026-01-17 09:38:54 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:54] INFO:     127.0.0.1:33474 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:25<01:26,  1.95it/s][2026-01-17 09:38:54 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:55] INFO:     127.0.0.1:34028 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:25<01:16,  2.18it/s][2026-01-17 09:38:55 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:56 TP0] Decode batch, #running-req: 128, #token: 47488, token usage: 0.20, npu graph: False, gen throughput (token/s): 1337.17, #queue-req: 0,
[2026-01-17 09:38:56] INFO:     127.0.0.1:34352 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:27<02:16,  1.21it/s][2026-01-17 09:38:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.21, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:57] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:27<01:52,  1.47it/s][2026-01-17 09:38:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.21, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:57] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:38:57] INFO:     127.0.0.1:34364 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:27<01:26,  1.89it/s]
 18%|█▊        | 37/200 [00:27<00:53,  3.07it/s][2026-01-17 09:38:57 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.21, #running-req: 126, #queue-req: 0,
[2026-01-17 09:38:57] INFO:     127.0.0.1:34096 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:27<00:46,  3.49it/s][2026-01-17 09:38:57 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.21, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:57] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:38:57] INFO:     127.0.0.1:33890 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:28<00:51,  3.10it/s]
 20%|██        | 40/200 [00:28<00:44,  3.63it/s][2026-01-17 09:38:57 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.20, #running-req: 126, #queue-req: 0,
[2026-01-17 09:38:58] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:28<00:39,  3.99it/s][2026-01-17 09:38:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:58] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:28<00:42,  3.70it/s][2026-01-17 09:38:58] INFO:     127.0.0.1:33308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:38:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.21, #running-req: 128, #queue-req: 0,
[2026-01-17 09:38:59] INFO:     127.0.0.1:33578 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:29<00:46,  3.36it/s][2026-01-17 09:38:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.21, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:59] INFO:     127.0.0.1:33966 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:29<00:58,  2.64it/s][2026-01-17 09:38:59] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:38:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.21, #running-req: 127, #queue-req: 0,
[2026-01-17 09:38:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.21, #running-req: 128, #queue-req: 0,
[2026-01-17 09:39:00] INFO:     127.0.0.1:34020 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:30<00:47,  3.24it/s][2026-01-17 09:39:00 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:00 TP0] Decode batch, #running-req: 128, #token: 51584, token usage: 0.22, npu graph: False, gen throughput (token/s): 1188.88, #queue-req: 0,
[2026-01-17 09:39:00] INFO:     127.0.0.1:33784 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:30<00:52,  2.90it/s][2026-01-17 09:39:00 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:01] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:31<01:11,  2.12it/s][2026-01-17 09:39:01 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:01] INFO:     127.0.0.1:34006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:01] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:32<01:05,  2.29it/s]
 26%|██▌       | 51/200 [00:32<00:48,  3.10it/s][2026-01-17 09:39:01] INFO:     127.0.0.1:39310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:01 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.23, #running-req: 126, #queue-req: 0,
[2026-01-17 09:39:01] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:32<00:34,  4.32it/s][2026-01-17 09:39:01 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 128, #queue-req: 0,
[2026-01-17 09:39:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 129, #queue-req: 0,
[2026-01-17 09:39:02] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:32<00:38,  3.77it/s][2026-01-17 09:39:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:02] INFO:     127.0.0.1:34030 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:32<00:34,  4.14it/s][2026-01-17 09:39:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:02] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:33<00:37,  3.80it/s][2026-01-17 09:39:02 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:03] INFO:     127.0.0.1:34034 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:33<00:33,  4.23it/s][2026-01-17 09:39:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:03] INFO:     127.0.0.1:33680 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:33<00:36,  3.88it/s][2026-01-17 09:39:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:03] INFO:     127.0.0.1:34182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:03 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.22, #running-req: 128, #queue-req: 0,
[2026-01-17 09:39:03] INFO:     127.0.0.1:34018 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:34<00:35,  3.99it/s][2026-01-17 09:39:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:04] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:34<00:36,  3.76it/s][2026-01-17 09:39:04 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:04] INFO:     127.0.0.1:33264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:04] INFO:     127.0.0.1:34086 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:34<00:43,  3.14it/s]
 32%|███▏      | 63/200 [00:34<00:38,  3.52it/s][2026-01-17 09:39:04 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.22, #running-req: 126, #queue-req: 0,
[2026-01-17 09:39:04] INFO:     127.0.0.1:34236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:04 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 128, #queue-req: 0,
[2026-01-17 09:39:05 TP0] Decode batch, #running-req: 128, #token: 51712, token usage: 0.22, npu graph: False, gen throughput (token/s): 1158.54, #queue-req: 0,
[2026-01-17 09:39:05] INFO:     127.0.0.1:39340 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:35<00:34,  3.92it/s][2026-01-17 09:39:05] INFO:     127.0.0.1:33844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:05 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:05 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.22, #running-req: 128, #queue-req: 0,
[2026-01-17 09:39:05] INFO:     127.0.0.1:33744 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:36<00:44,  3.01it/s][2026-01-17 09:39:06 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:06] INFO:     127.0.0.1:34194 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:37<01:01,  2.13it/s][2026-01-17 09:39:07 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:07] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:37<01:08,  1.93it/s][2026-01-17 09:39:07 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:08 TP0] Decode batch, #running-req: 128, #token: 52992, token usage: 0.23, npu graph: False, gen throughput (token/s): 1451.27, #queue-req: 0,
[2026-01-17 09:39:08] INFO:     127.0.0.1:33532 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:38<01:20,  1.61it/s][2026-01-17 09:39:08 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:09] INFO:     127.0.0.1:33656 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:39<01:12,  1.77it/s][2026-01-17 09:39:09] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:09 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:39:09 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 128, #queue-req: 0,
[2026-01-17 09:39:10] INFO:     127.0.0.1:34304 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:40<01:26,  1.46it/s][2026-01-17 09:39:11 TP0] Decode batch, #running-req: 127, #token: 58368, token usage: 0.25, npu graph: False, gen throughput (token/s): 1521.03, #queue-req: 0,
[2026-01-17 09:39:12] INFO:     127.0.0.1:33754 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:42<01:45,  1.20it/s][2026-01-17 09:39:12] INFO:     127.0.0.1:34488 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:43<01:47,  1.16it/s][2026-01-17 09:39:13] INFO:     127.0.0.1:39356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:13] INFO:     127.0.0.1:34476 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [00:43<01:26,  1.43it/s]
 38%|███▊      | 77/200 [00:43<00:55,  2.21it/s][2026-01-17 09:39:13] INFO:     127.0.0.1:33950 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:44<01:03,  1.93it/s][2026-01-17 09:39:14] INFO:     127.0.0.1:34520 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:44<00:52,  2.32it/s][2026-01-17 09:39:14] INFO:     127.0.0.1:34420 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:44<00:49,  2.44it/s][2026-01-17 09:39:14] INFO:     127.0.0.1:33992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:14] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:14] INFO:     127.0.0.1:34200 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:44<00:32,  3.68it/s]
 42%|████▏     | 83/200 [00:44<00:19,  5.88it/s][2026-01-17 09:39:15 TP0] Decode batch, #running-req: 117, #token: 60160, token usage: 0.26, npu graph: False, gen throughput (token/s): 1493.71, #queue-req: 0,
[2026-01-17 09:39:16] INFO:     127.0.0.1:39314 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:46<00:59,  1.96it/s][2026-01-17 09:39:17] INFO:     127.0.0.1:33616 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:48<01:13,  1.56it/s][2026-01-17 09:39:17] INFO:     127.0.0.1:34110 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:48<01:01,  1.86it/s][2026-01-17 09:39:18] INFO:     127.0.0.1:33596 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:48<00:57,  1.96it/s][2026-01-17 09:39:18] INFO:     127.0.0.1:33328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18 TP0] Decode batch, #running-req: 113, #token: 22400, token usage: 0.10, npu graph: False, gen throughput (token/s): 1323.06, #queue-req: 0,
[2026-01-17 09:39:18] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:18] INFO:     127.0.0.1:34338 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:49<00:51,  2.17it/s]
 64%|██████▎   | 127/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s]
 72%|███████▎  | 145/200 [00:49<00:01, 51.27it/s][2026-01-17 09:39:20] INFO:     127.0.0.1:34046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:22 TP0] Decode batch, #running-req: 54, #token: 23808, token usage: 0.10, npu graph: False, gen throughput (token/s): 653.00, #queue-req: 0,
[2026-01-17 09:39:23] INFO:     127.0.0.1:34098 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:24] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:24] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:25 TP0] Decode batch, #running-req: 51, #token: 24576, token usage: 0.11, npu graph: False, gen throughput (token/s): 616.31, #queue-req: 0,
[2026-01-17 09:39:25] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:26] INFO:     127.0.0.1:34010 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:57<00:09,  5.12it/s][2026-01-17 09:39:27] INFO:     127.0.0.1:44646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:28] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:28] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:28] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:58<00:10,  4.42it/s][2026-01-17 09:39:28 TP0] Decode batch, #running-req: 47, #token: 23040, token usage: 0.10, npu graph: False, gen throughput (token/s): 623.26, #queue-req: 0,
[2026-01-17 09:39:28] INFO:     127.0.0.1:34032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:29] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:30] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [01:00<00:11,  3.75it/s][2026-01-17 09:39:30] INFO:     127.0.0.1:34058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:31] INFO:     127.0.0.1:34080 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [01:01<00:11,  3.63it/s][2026-01-17 09:39:31 TP0] Decode batch, #running-req: 40, #token: 21760, token usage: 0.09, npu graph: False, gen throughput (token/s): 537.38, #queue-req: 0,
[2026-01-17 09:39:32] INFO:     127.0.0.1:39304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:33] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [01:03<00:13,  2.83it/s][2026-01-17 09:39:34] INFO:     127.0.0.1:39266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:34] INFO:     127.0.0.1:39270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:34] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [01:04<00:14,  2.51it/s]
 82%|████████▎ | 165/200 [01:04<00:14,  2.44it/s][2026-01-17 09:39:34] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [01:05<00:14,  2.36it/s][2026-01-17 09:39:34 TP0] Decode batch, #running-req: 35, #token: 19584, token usage: 0.08, npu graph: False, gen throughput (token/s): 485.46, #queue-req: 0,
[2026-01-17 09:39:35] INFO:     127.0.0.1:39298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:35] INFO:     127.0.0.1:39282 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [01:05<00:14,  2.33it/s]
 84%|████████▍ | 168/200 [01:05<00:12,  2.63it/s][2026-01-17 09:39:37] INFO:     127.0.0.1:39300 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [01:07<00:18,  1.63it/s][2026-01-17 09:39:37] INFO:     127.0.0.1:39344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:37] INFO:     127.0.0.1:34548 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [01:07<00:13,  2.19it/s][2026-01-17 09:39:38] INFO:     127.0.0.1:39316 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [01:09<00:18,  1.54it/s][2026-01-17 09:39:39 TP0] Decode batch, #running-req: 28, #token: 17152, token usage: 0.07, npu graph: False, gen throughput (token/s): 260.54, #queue-req: 0,
[2026-01-17 09:39:39] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [01:10<00:18,  1.45it/s][2026-01-17 09:39:40] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [01:10<00:17,  1.52it/s][2026-01-17 09:39:41] INFO:     127.0.0.1:39368 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [01:11<00:16,  1.51it/s][2026-01-17 09:39:41] INFO:     127.0.0.1:34444 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [01:11<00:13,  1.81it/s][2026-01-17 09:39:41] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [01:12<00:13,  1.77it/s][2026-01-17 09:39:42] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [01:12<00:13,  1.67it/s][2026-01-17 09:39:42 TP0] Decode batch, #running-req: 23, #token: 14208, token usage: 0.06, npu graph: False, gen throughput (token/s): 329.42, #queue-req: 0,
[2026-01-17 09:39:42] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [01:13<00:10,  2.03it/s][2026-01-17 09:39:42] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [01:13<00:07,  2.54it/s][2026-01-17 09:39:43] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [01:13<00:06,  2.74it/s][2026-01-17 09:39:43] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:43] INFO:     127.0.0.1:34396 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [01:13<00:04,  3.69it/s][2026-01-17 09:39:45] INFO:     127.0.0.1:34450 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [01:15<00:09,  1.72it/s][2026-01-17 09:39:45 TP0] Decode batch, #running-req: 16, #token: 11136, token usage: 0.05, npu graph: False, gen throughput (token/s): 239.87, #queue-req: 0,
[2026-01-17 09:39:46] INFO:     127.0.0.1:34460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:46] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [01:16<00:10,  1.47it/s]
 93%|█████████▎| 186/200 [01:16<00:08,  1.68it/s][2026-01-17 09:39:46] INFO:     127.0.0.1:34494 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [01:16<00:06,  1.88it/s][2026-01-17 09:39:46] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:46] INFO:     127.0.0.1:34510 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [01:16<00:04,  2.64it/s][2026-01-17 09:39:46] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:46] INFO:     127.0.0.1:34528 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [01:17<00:02,  3.39it/s][2026-01-17 09:39:47] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [01:17<00:02,  3.40it/s][2026-01-17 09:39:47] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [01:17<00:01,  3.56it/s][2026-01-17 09:39:47] INFO:     127.0.0.1:34556 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [01:18<00:01,  3.31it/s][2026-01-17 09:39:48] INFO:     127.0.0.1:34586 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [01:18<00:01,  3.56it/s][2026-01-17 09:39:48 TP0] Decode batch, #running-req: 5, #token: 3968, token usage: 0.02, npu graph: False, gen throughput (token/s): 133.42, #queue-req: 0,
[2026-01-17 09:39:48] INFO:     127.0.0.1:34600 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [01:19<00:01,  2.44it/s][2026-01-17 09:39:49] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [01:20<00:01,  1.84it/s][2026-01-17 09:39:50] INFO:     127.0.0.1:44644 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [01:20<00:01,  1.80it/s][2026-01-17 09:39:51] INFO:     127.0.0.1:44650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:39:51] INFO:     127.0.0.1:44652 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [01:21<00:00,  1.38it/s]
100%|██████████| 200/200 [01:21<00:00,  1.52it/s]
100%|██████████| 200/200 [01:21<00:00,  2.45it/s]
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/llm_models/gsm8k_ascend_mixin.py", line 66, in test_gsm8k
    self.assertGreaterEqual(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1277, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: np.float64(0.585) not greater than or equal to 0.59 : Accuracy of /root/.cache/modelscope/hub/models/vllm-ascend/QWQ-32B-W8A8 is 0.585, is lower than 0.59
E
======================================================================
ERROR: test_gsm8k (__main__.TestMistral7B.test_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: np.float64(0.585) not greater than or equal to 0.59 : Accuracy of /root/.cache/modelscope/hub/models/vllm-ascend/QWQ-32B-W8A8 is 0.585, is lower than 0.59

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 203.233s

FAILED (errors=1)
Accuracy: 0.585
Invalid: 0.010
Latency: 81.883 s
Output throughput: 948.215 token/s
.
.
End (24/62):
filename='ascend/llm_models/test_ascend_qwq_32b_w8a8.py', elapsed=213, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_ascend_qwq_32b_w8a8.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/llm_models/test_ascend_qwq_32b_w8a8.py

.
.
Begin (24/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_qwq_32b_w8a8.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:41:10] WARNING model_config.py:820: modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:41:11] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/vllm-ascend/QWQ-32B-W8A8', tokenizer_path='/root/.cache/modelscope/hub/models/vllm-ascend/QWQ-32B-W8A8', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='modelslim', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=951653021, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/vllm-ascend/QWQ-32B-W8A8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='modelslim', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 09:41:11] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:41:11] Using default HuggingFace chat template with detected content format: string
[2026-01-17 09:41:11] Detected the force reasoning pattern in chat template.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:41:20 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:41:20 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:41:21 TP1] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:41:21 TP1] Init torch distributed begin.
[2026-01-17 09:41:21 TP0] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2026-01-17 09:41:21 TP0] Init torch distributed begin.
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2026-01-17 09:41:22 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:41:22 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:41:22 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 09:41:23 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:41:23 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:41:23 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:41:23 TP0] Load weight begin. avail mem=60.81 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:3219: UserWarning: Torchaudio's I/O functions now support per-call backend dispatch. Importing backend implementation directly is no longer guaranteed to work. Please use `backend` keyword with load/save/info function, instead of calling the underlying implementation directly.
  and hasattr(value, target_function)

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.17s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.17s/it]

[2026-01-17 09:41:44 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=40.56 GB, mem usage=20.25 GB.
[2026-01-17 09:41:45 TP1] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=40.89 GB, mem usage=20.25 GB.
[2026-01-17 09:41:45 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 09:41:45 TP0] The available memory for KV cache is 28.37 GB.
[2026-01-17 09:41:45 TP1] The available memory for KV cache is 28.37 GB.
[2026-01-17 09:41:46 TP0] KV Cache is allocated. #tokens: 232320, K size: 14.19 GB, V size: 14.19 GB
[2026-01-17 09:41:46 TP0] Memory pool end. avail mem=11.73 GB
[2026-01-17 09:41:46 TP1] KV Cache is allocated. #tokens: 232320, K size: 14.19 GB, V size: 14.19 GB
[2026-01-17 09:41:46 TP1] Memory pool end. avail mem=12.06 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 09:41:47 TP0] max_total_num_tokens=232320, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2905, context_len=40960, available_gpu_mem=11.73 GB
[2026-01-17 09:41:47] INFO:     Started server process [203421]
[2026-01-17 09:41:47] INFO:     Waiting for application startup.
[2026-01-17 09:41:47] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 40, 'top_p': 0.95}
[2026-01-17 09:41:47] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 40, 'top_p': 0.95}
[2026-01-17 09:41:47] INFO:     Application startup complete.
[2026-01-17 09:41:47] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 09:41:48] INFO:     127.0.0.1:49922 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 09:41:48 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:41:51] INFO:     127.0.0.1:49946 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 09:41:51] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:41:51] The server is fired up and ready to roll!
[2026-01-17 09:42:01 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:42:02] INFO:     127.0.0.1:56966 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 09:42:02] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 09:42:02] INFO:     127.0.0.1:56970 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 09:42:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:42:02] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/vllm-ascend/QWQ-32B-W8A8 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 2 --quantization modelslim --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 09:42:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:42:03 TP0] Prefill batch, #new-seq: 32, #new-token: 4352, #cached-token: 24576, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 09:42:03 TP0] Prefill batch, #new-seq: 25, #new-token: 3328, #cached-token: 19200, token usage: 0.02, #running-req: 33, #queue-req: 0,
[2026-01-17 09:42:03 TP0] Prefill batch, #new-seq: 61, #new-token: 8192, #cached-token: 46848, token usage: 0.04, #running-req: 58, #queue-req: 9,
[2026-01-17 09:42:03 TP0] Prefill batch, #new-seq: 9, #new-token: 1152, #cached-token: 6912, token usage: 0.07, #running-req: 119, #queue-req: 0,
[2026-01-17 09:42:06 TP0] Decode batch, #running-req: 128, #token: 21376, token usage: 0.09, npu graph: False, gen throughput (token/s): 85.59, #queue-req: 0,
[2026-01-17 09:42:09] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:07<23:17,  7.02s/it][2026-01-17 09:42:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:10 TP0] Decode batch, #running-req: 128, #token: 32384, token usage: 0.14, npu graph: False, gen throughput (token/s): 1419.11, #queue-req: 0,
[2026-01-17 09:42:11] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:08<11:53,  3.61s/it][2026-01-17 09:42:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:11] INFO:     127.0.0.1:57152 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:08<07:12,  2.20s/it][2026-01-17 09:42:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:12] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:09<05:33,  1.70s/it][2026-01-17 09:42:12 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:12] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:09<03:44,  1.15s/it][2026-01-17 09:42:12 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:14 TP0] Decode batch, #running-req: 128, #token: 33280, token usage: 0.14, npu graph: False, gen throughput (token/s): 1371.91, #queue-req: 0,
[2026-01-17 09:42:14] INFO:     127.0.0.1:57086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:14] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:11<04:11,  1.30s/it]
  4%|▎         | 7/200 [00:11<03:22,  1.05s/it][2026-01-17 09:42:14 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.14, #running-req: 126, #queue-req: 0,
[2026-01-17 09:42:14] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:14] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:12<02:59,  1.07it/s]
  4%|▍         | 9/200 [00:12<02:06,  1.51it/s][2026-01-17 09:42:15 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.14, #running-req: 126, #queue-req: 0,
[2026-01-17 09:42:15] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:12<02:17,  1.38it/s][2026-01-17 09:42:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:16] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:13<02:26,  1.29it/s][2026-01-17 09:42:16 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.15, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:17] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:14<02:03,  1.52it/s][2026-01-17 09:42:17 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.15, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:17] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:14<01:37,  1.91it/s][2026-01-17 09:42:17 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.15, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:18] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:15<01:45,  1.76it/s][2026-01-17 09:42:18 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.15, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:18 TP0] Decode batch, #running-req: 127, #token: 35328, token usage: 0.15, npu graph: False, gen throughput (token/s): 1324.32, #queue-req: 0,
[2026-01-17 09:42:18] INFO:     127.0.0.1:57602 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:15<01:44,  1.78it/s][2026-01-17 09:42:18 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.15, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:18] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:15<01:30,  2.04it/s][2026-01-17 09:42:18 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.16, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:19] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:16<01:32,  1.97it/s][2026-01-17 09:42:19 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:20] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:18<02:28,  1.22it/s][2026-01-17 09:42:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.18, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:21] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:18<02:16,  1.33it/s][2026-01-17 09:42:21] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:21] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:21 TP0] Decode batch, #running-req: 127, #token: 43392, token usage: 0.19, npu graph: False, gen throughput (token/s): 1411.16, #queue-req: 0,
[2026-01-17 09:42:21 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-17 09:42:22] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:19<01:30,  1.96it/s][2026-01-17 09:42:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:23] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:20<01:34,  1.88it/s][2026-01-17 09:42:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:24] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:21<01:47,  1.64it/s][2026-01-17 09:42:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:25] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:22<02:26,  1.19it/s][2026-01-17 09:42:25] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:25 TP0] Decode batch, #running-req: 127, #token: 46208, token usage: 0.20, npu graph: False, gen throughput (token/s): 1296.90, #queue-req: 0,
[2026-01-17 09:42:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 128, #queue-req: 0,
[2026-01-17 09:42:26] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:23<01:52,  1.53it/s][2026-01-17 09:42:26 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:26] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:23<01:43,  1.65it/s][2026-01-17 09:42:26 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:28] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:25<02:12,  1.29it/s][2026-01-17 09:42:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:28] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:25<01:56,  1.46it/s][2026-01-17 09:42:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:28] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:25<01:32,  1.82it/s][2026-01-17 09:42:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:29] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:26<01:34,  1.78it/s][2026-01-17 09:42:29] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:29] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 128, #queue-req: 0,

 17%|█▋        | 34/200 [00:26<00:58,  2.85it/s][2026-01-17 09:42:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 129, #queue-req: 0,
[2026-01-17 09:42:30 TP0] Decode batch, #running-req: 128, #token: 47360, token usage: 0.20, npu graph: False, gen throughput (token/s): 1182.55, #queue-req: 0,
[2026-01-17 09:42:30] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:27<01:14,  2.21it/s][2026-01-17 09:42:30] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:30 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:30 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.20, #running-req: 128, #queue-req: 0,
[2026-01-17 09:42:31] INFO:     127.0.0.1:57278 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:28<01:10,  2.32it/s][2026-01-17 09:42:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.21, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:31] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:28<01:06,  2.42it/s][2026-01-17 09:42:31 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.21, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:31] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:28<00:57,  2.82it/s][2026-01-17 09:42:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.21, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:31] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:29<01:00,  2.65it/s][2026-01-17 09:42:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.21, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:32] INFO:     127.0.0.1:57022 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:29<00:51,  3.10it/s][2026-01-17 09:42:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.21, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:32] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:29<00:51,  3.06it/s][2026-01-17 09:42:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.21, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:33] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:30<01:15,  2.08it/s][2026-01-17 09:42:33 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:33] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:30<01:01,  2.55it/s][2026-01-17 09:42:33 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:34 TP0] Decode batch, #running-req: 128, #token: 52864, token usage: 0.23, npu graph: False, gen throughput (token/s): 1174.93, #queue-req: 0,
[2026-01-17 09:42:34] INFO:     127.0.0.1:57506 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:31<01:25,  1.81it/s][2026-01-17 09:42:34] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:34 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:34] INFO:     127.0.0.1:60146 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:31<00:52,  2.91it/s][2026-01-17 09:42:34 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 128, #queue-req: 0,
[2026-01-17 09:42:34 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.22, #running-req: 129, #queue-req: 0,
[2026-01-17 09:42:35] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:32<01:04,  2.34it/s][2026-01-17 09:42:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:35] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:32<01:01,  2.45it/s][2026-01-17 09:42:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:36] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:33<01:05,  2.28it/s][2026-01-17 09:42:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:36] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:33<01:05,  2.28it/s][2026-01-17 09:42:36] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:36] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 128, #queue-req: 0,

 26%|██▋       | 53/200 [00:33<00:41,  3.53it/s][2026-01-17 09:42:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 129, #queue-req: 0,
[2026-01-17 09:42:37] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:34<01:07,  2.16it/s][2026-01-17 09:42:37 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:38] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:35<01:02,  2.31it/s][2026-01-17 09:42:38 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:38] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:35<00:52,  2.73it/s][2026-01-17 09:42:38 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:38 TP0] Decode batch, #running-req: 128, #token: 53888, token usage: 0.23, npu graph: False, gen throughput (token/s): 1129.46, #queue-req: 0,
[2026-01-17 09:42:39] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:36<01:08,  2.09it/s][2026-01-17 09:42:39] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 128, #queue-req: 0,
[2026-01-17 09:42:39] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:37<01:01,  2.29it/s][2026-01-17 09:42:40 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:40] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:37<01:09,  2.02it/s][2026-01-17 09:42:40 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:41] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:38<01:06,  2.09it/s][2026-01-17 09:42:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:41] INFO:     127.0.0.1:57460 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:38<00:54,  2.51it/s][2026-01-17 09:42:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:41] INFO:     127.0.0.1:57966 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:38<00:46,  2.98it/s][2026-01-17 09:42:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:42] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:39<00:55,  2.44it/s][2026-01-17 09:42:42 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:43 TP0] Decode batch, #running-req: 128, #token: 53760, token usage: 0.23, npu graph: False, gen throughput (token/s): 1246.89, #queue-req: 0,
[2026-01-17 09:42:43] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:40<01:29,  1.52it/s][2026-01-17 09:42:43 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:44] INFO:     127.0.0.1:57336 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:41<01:36,  1.39it/s][2026-01-17 09:42:44 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.24, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:44] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:41<01:14,  1.79it/s][2026-01-17 09:42:44 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.24, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:44] INFO:     127.0.0.1:47348 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:41<01:04,  2.03it/s][2026-01-17 09:42:44 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.24, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:45] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:42<01:21,  1.60it/s][2026-01-17 09:42:45] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.24, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:45] INFO:     127.0.0.1:47250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.24, #running-req: 128, #queue-req: 0,

 36%|███▌      | 71/200 [00:42<00:49,  2.62it/s][2026-01-17 09:42:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.24, #running-req: 129, #queue-req: 0,
[2026-01-17 09:42:46] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:44<01:15,  1.71it/s][2026-01-17 09:42:47 TP0] Decode batch, #running-req: 128, #token: 57856, token usage: 0.25, npu graph: False, gen throughput (token/s): 1263.03, #queue-req: 0,
[2026-01-17 09:42:47 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.25, #running-req: 127, #queue-req: 0,
[2026-01-17 09:42:47] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:44<01:00,  2.09it/s][2026-01-17 09:42:48] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:45<01:22,  1.54it/s][2026-01-17 09:42:48] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:45<01:13,  1.70it/s][2026-01-17 09:42:49] INFO:     127.0.0.1:60180 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [00:46<01:07,  1.83it/s][2026-01-17 09:42:49] INFO:     127.0.0.1:57764 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:46<01:06,  1.86it/s][2026-01-17 09:42:50] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:50] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:47<01:08,  1.79it/s]
 40%|███▉      | 79/200 [00:47<00:53,  2.25it/s][2026-01-17 09:42:50 TP0] Decode batch, #running-req: 121, #token: 60928, token usage: 0.26, npu graph: False, gen throughput (token/s): 1414.07, #queue-req: 0,
[2026-01-17 09:42:52] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:49<01:34,  1.27it/s][2026-01-17 09:42:53] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:50<01:45,  1.13it/s][2026-01-17 09:42:53] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53 TP0] Decode batch, #running-req: 118, #token: 23936, token usage: 0.10, npu graph: False, gen throughput (token/s): 1431.48, #queue-req: 0,
[2026-01-17 09:42:53] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:53] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:57988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:42:54] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:51<01:17,  1.51it/s]
 57%|█████▊    | 115/200 [00:51<00:10,  7.77it/s]
 70%|███████   | 140/200 [00:51<00:07,  7.77it/s]
 70%|███████   | 140/200 [00:51<00:07,  7.77it/s]
 70%|███████   | 140/200 [00:51<00:07,  7.77it/s]
 70%|███████   | 140/200 [00:51<00:07,  7.77it/s]
 70%|███████   | 140/200 [00:51<00:07,  7.77it/s][2026-01-17 09:42:55] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:52<00:11,  5.31it/s][2026-01-17 09:42:56] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:53<00:16,  3.44it/s][2026-01-17 09:42:56] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:53<00:17,  3.33it/s][2026-01-17 09:42:57 TP0] Decode batch, #running-req: 57, #token: 24832, token usage: 0.11, npu graph: False, gen throughput (token/s): 765.05, #queue-req: 0,
[2026-01-17 09:42:57] INFO:     127.0.0.1:36490 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:54<00:22,  2.54it/s][2026-01-17 09:42:58] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:55<00:24,  2.20it/s][2026-01-17 09:42:58] INFO:     127.0.0.1:47238 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:55<00:21,  2.52it/s][2026-01-17 09:42:59] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:56<00:22,  2.34it/s][2026-01-17 09:43:00] INFO:     127.0.0.1:47450 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:57<00:27,  1.92it/s][2026-01-17 09:43:00] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:00 TP0] Decode batch, #running-req: 52, #token: 23552, token usage: 0.10, npu graph: False, gen throughput (token/s): 725.13, #queue-req: 0,
[2026-01-17 09:43:00] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:57<00:17,  2.91it/s][2026-01-17 09:43:00] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:57<00:17,  2.85it/s][2026-01-17 09:43:01] INFO:     127.0.0.1:60098 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:58<00:22,  2.14it/s][2026-01-17 09:43:02] INFO:     127.0.0.1:36588 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:59<00:28,  1.63it/s][2026-01-17 09:43:02] INFO:     127.0.0.1:59510 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:59<00:24,  1.89it/s][2026-01-17 09:43:03] INFO:     127.0.0.1:36582 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [01:00<00:21,  2.05it/s][2026-01-17 09:43:03 TP0] Decode batch, #running-req: 46, #token: 22784, token usage: 0.10, npu graph: False, gen throughput (token/s): 642.78, #queue-req: 0,
[2026-01-17 09:43:03] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:03] INFO:     127.0.0.1:60220 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [01:00<00:18,  2.38it/s][2026-01-17 09:43:03] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [01:00<00:15,  2.80it/s][2026-01-17 09:43:04] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [01:02<00:21,  1.88it/s][2026-01-17 09:43:05] INFO:     127.0.0.1:36542 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [01:02<00:17,  2.22it/s][2026-01-17 09:43:05] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:05] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [01:03<00:20,  1.88it/s]
 81%|████████  | 162/200 [01:03<00:17,  2.16it/s][2026-01-17 09:43:06 TP0] Decode batch, #running-req: 38, #token: 20096, token usage: 0.09, npu graph: False, gen throughput (token/s): 558.49, #queue-req: 0,
[2026-01-17 09:43:06] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [01:03<00:15,  2.46it/s][2026-01-17 09:43:06] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [01:03<00:14,  2.42it/s][2026-01-17 09:43:07] INFO:     127.0.0.1:47262 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [01:04<00:21,  1.61it/s][2026-01-17 09:43:08] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:08] INFO:     127.0.0.1:60164 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [01:05<00:22,  1.49it/s]
 84%|████████▎ | 167/200 [01:05<00:18,  1.80it/s][2026-01-17 09:43:09 TP0] Decode batch, #running-req: 33, #token: 18688, token usage: 0.08, npu graph: False, gen throughput (token/s): 448.00, #queue-req: 0,
[2026-01-17 09:43:09] INFO:     127.0.0.1:60182 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [01:06<00:22,  1.45it/s][2026-01-17 09:43:10] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [01:07<00:21,  1.43it/s][2026-01-17 09:43:11] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [01:08<00:26,  1.15it/s][2026-01-17 09:43:12] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [01:09<00:22,  1.28it/s][2026-01-17 09:43:12 TP0] Decode batch, #running-req: 29, #token: 17664, token usage: 0.08, npu graph: False, gen throughput (token/s): 394.50, #queue-req: 0,
[2026-01-17 09:43:12] INFO:     127.0.0.1:60238 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [01:09<00:18,  1.54it/s][2026-01-17 09:43:13] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [01:10<00:21,  1.28it/s][2026-01-17 09:43:14] INFO:     127.0.0.1:47222 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [01:11<00:16,  1.55it/s][2026-01-17 09:43:14] INFO:     127.0.0.1:47246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:14] INFO:     127.0.0.1:47260 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [01:11<00:15,  1.63it/s]
 88%|████████▊ | 176/200 [01:11<00:10,  2.18it/s][2026-01-17 09:43:14] INFO:     127.0.0.1:36504 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [01:11<00:09,  2.49it/s][2026-01-17 09:43:15 TP0] Decode batch, #running-req: 23, #token: 14336, token usage: 0.06, npu graph: False, gen throughput (token/s): 335.84, #queue-req: 0,
[2026-01-17 09:43:15] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [01:12<00:12,  1.79it/s][2026-01-17 09:43:15] INFO:     127.0.0.1:47286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:15] INFO:     127.0.0.1:36584 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [01:13<00:09,  2.21it/s]
 90%|█████████ | 180/200 [01:13<00:05,  3.43it/s][2026-01-17 09:43:16] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:16] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [01:13<00:04,  3.91it/s][2026-01-17 09:43:17] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [01:14<00:07,  2.31it/s][2026-01-17 09:43:18] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:18] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [01:15<00:08,  1.95it/s]
 92%|█████████▎| 185/200 [01:15<00:06,  2.16it/s][2026-01-17 09:43:18 TP0] Decode batch, #running-req: 15, #token: 10112, token usage: 0.04, npu graph: False, gen throughput (token/s): 235.03, #queue-req: 0,
[2026-01-17 09:43:19] INFO:     127.0.0.1:47428 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [01:16<00:08,  1.62it/s][2026-01-17 09:43:19] INFO:     127.0.0.1:47436 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [01:16<00:07,  1.85it/s][2026-01-17 09:43:20] INFO:     127.0.0.1:36432 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [01:17<00:07,  1.61it/s][2026-01-17 09:43:20] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [01:17<00:05,  1.93it/s][2026-01-17 09:43:20] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:21] INFO:     127.0.0.1:36462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:21] INFO:     127.0.0.1:36476 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [01:18<00:04,  2.23it/s]
 96%|█████████▌| 192/200 [01:18<00:02,  2.87it/s][2026-01-17 09:43:21 TP0] Decode batch, #running-req: 8, #token: 6016, token usage: 0.03, npu graph: False, gen throughput (token/s): 160.78, #queue-req: 0,
[2026-01-17 09:43:22] INFO:     127.0.0.1:36480 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [01:19<00:02,  2.60it/s][2026-01-17 09:43:22] INFO:     127.0.0.1:36560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:22] INFO:     127.0.0.1:36516 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [01:20<00:02,  2.44it/s][2026-01-17 09:43:23] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:43:24] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [01:21<00:01,  1.85it/s][2026-01-17 09:43:24 TP0] Decode batch, #running-req: 3, #token: 2688, token usage: 0.01, npu graph: False, gen throughput (token/s): 64.78, #queue-req: 0,
[2026-01-17 09:43:25] INFO:     127.0.0.1:36574 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [01:22<00:01,  1.71it/s][2026-01-17 09:43:26] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [01:23<00:00,  1.49it/s][2026-01-17 09:43:27] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [01:24<00:00,  1.35it/s]
100%|██████████| 200/200 [01:24<00:00,  2.37it/s]
.
----------------------------------------------------------------------
Ran 1 test in 145.861s

OK
Accuracy: 0.605
Invalid: 0.000
Latency: 84.519 s
Output throughput: 926.999 token/s
.
.
End (24/62):
filename='ascend/llm_models/test_ascend_qwq_32b_w8a8.py', elapsed=157, estimated_time=400
.
.


✓ PASSED on retry (attempt 2): ascend/llm_models/test_ascend_qwq_32b_w8a8.py

.
.
Begin (25/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_qwen3_32b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:43:52] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-32B', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-32B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=711774764, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-32B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 09:43:53] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:44:02 TP2] Init torch distributed begin.
[2026-01-17 09:44:02 TP1] Init torch distributed begin.
[2026-01-17 09:44:02 TP0] Init torch distributed begin.
[2026-01-17 09:44:03 TP3] Init torch distributed begin.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 09:44:04 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:44:04 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:44:04 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:44:04 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:44:04 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 09:44:05 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:44:05 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:44:05 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:44:05 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:44:05 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 09:44:05 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:44:05 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 09:44:05 TP3] Load weight begin. avail mem=61.14 GB

Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:04<01:08,  4.31s/it]

Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:08<01:06,  4.40s/it]

Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:12<00:56,  4.02s/it]

Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:16<00:51,  3.94s/it]

Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:18<00:41,  3.43s/it]

Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:22<00:40,  3.69s/it]

Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:27<00:38,  3.86s/it]

Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:31<00:36,  4.06s/it]

Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:36<00:33,  4.19s/it]

Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:40<00:29,  4.17s/it]

Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:43<00:24,  4.03s/it]

Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:47<00:19,  3.93s/it]

Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:50<00:15,  3.77s/it]

Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:54<00:11,  3.70s/it]

Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:57<00:07,  3.52s/it]

Loading safetensors checkpoint shards:  94% Completed | 16/17 [01:01<00:03,  3.49s/it]
[2026-01-17 09:45:10 TP3] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=45.86 GB, mem usage=15.28 GB.

Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:04<00:00,  3.36s/it]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:04<00:00,  3.77s/it]

[2026-01-17 09:45:10 TP0] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=45.53 GB, mem usage=15.28 GB.
[2026-01-17 09:45:10 TP2] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=45.58 GB, mem usage=15.28 GB.
[2026-01-17 09:45:10 TP1] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=45.86 GB, mem usage=15.28 GB.
[2026-01-17 09:45:10 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 09:45:10 TP0] The available memory for KV cache is 33.35 GB.
[2026-01-17 09:45:10 TP2] The available memory for KV cache is 33.35 GB.
[2026-01-17 09:45:10 TP3] The available memory for KV cache is 33.35 GB.
[2026-01-17 09:45:10 TP1] The available memory for KV cache is 33.35 GB.
[2026-01-17 09:45:11 TP0] KV Cache is allocated. #tokens: 546304, K size: 16.68 GB, V size: 16.68 GB
[2026-01-17 09:45:11 TP0] Memory pool end. avail mem=11.55 GB
[2026-01-17 09:45:11 TP2] KV Cache is allocated. #tokens: 546304, K size: 16.68 GB, V size: 16.68 GB
[2026-01-17 09:45:11 TP2] Memory pool end. avail mem=11.60 GB
[2026-01-17 09:45:11 TP1] KV Cache is allocated. #tokens: 546304, K size: 16.68 GB, V size: 16.68 GB
[2026-01-17 09:45:11 TP1] Memory pool end. avail mem=11.88 GB
[2026-01-17 09:45:11 TP3] KV Cache is allocated. #tokens: 546304, K size: 16.68 GB, V size: 16.68 GB
[2026-01-17 09:45:11 TP3] Memory pool end. avail mem=11.88 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 09:45:12 TP0] max_total_num_tokens=546304, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=40960, available_gpu_mem=11.55 GB
[2026-01-17 09:45:13] INFO:     Started server process [207195]
[2026-01-17 09:45:13] INFO:     Waiting for application startup.
[2026-01-17 09:45:13] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-17 09:45:13] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-17 09:45:13] INFO:     Application startup complete.
[2026-01-17 09:45:13] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 09:45:14] INFO:     127.0.0.1:36852 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 09:45:14 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:45:18] INFO:     127.0.0.1:42312 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 09:45:28] INFO:     127.0.0.1:54150 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 09:45:29] INFO:     127.0.0.1:36866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:29] The server is fired up and ready to roll!
[2026-01-17 09:45:38 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:45:39] INFO:     127.0.0.1:45760 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 09:45:39] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 09:45:39] INFO:     127.0.0.1:45774 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 09:45:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:45:39] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-32B --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 09:45:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:45:39 TP0] Prefill batch, #new-seq: 33, #new-token: 4480, #cached-token: 25344, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 09:45:39 TP0] Prefill batch, #new-seq: 20, #new-token: 2688, #cached-token: 15360, token usage: 0.01, #running-req: 34, #queue-req: 0,
[2026-01-17 09:45:39 TP0] Prefill batch, #new-seq: 61, #new-token: 8192, #cached-token: 46848, token usage: 0.01, #running-req: 54, #queue-req: 13,
[2026-01-17 09:45:39 TP0] Prefill batch, #new-seq: 13, #new-token: 1664, #cached-token: 9984, token usage: 0.03, #running-req: 115, #queue-req: 0,
[2026-01-17 09:45:43 TP0] Decode batch, #running-req: 128, #token: 21376, token usage: 0.04, npu graph: False, gen throughput (token/s): 39.35, #queue-req: 0,
[2026-01-17 09:45:44] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:44] INFO:     127.0.0.1:45832 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:05<17:36,  5.31s/it]
  1%|          | 2/200 [00:05<11:01,  3.34s/it][2026-01-17 09:45:44 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:45:45] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:05<07:39,  2.33s/it][2026-01-17 09:45:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:45] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:06<05:39,  1.73s/it][2026-01-17 09:45:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:45] INFO:     127.0.0.1:45800 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:06<04:11,  1.29s/it][2026-01-17 09:45:45] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:46] INFO:     127.0.0.1:46460 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:06<02:17,  1.40it/s][2026-01-17 09:45:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:45:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 09:45:46] INFO:     127.0.0.1:46248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:46] INFO:     127.0.0.1:46840 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:07<02:07,  1.50it/s]
  4%|▍         | 9/200 [00:07<01:34,  2.02it/s][2026-01-17 09:45:46] INFO:     127.0.0.1:45914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:46 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 09:45:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 09:45:47 TP0] Decode batch, #running-req: 128, #token: 31616, token usage: 0.06, npu graph: False, gen throughput (token/s): 1240.66, #queue-req: 0,
[2026-01-17 09:45:47] INFO:     127.0.0.1:46648 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:08<01:25,  2.20it/s][2026-01-17 09:45:47 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:48] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:08<01:41,  1.85it/s][2026-01-17 09:45:48 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:48] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:09<01:25,  2.19it/s][2026-01-17 09:45:48 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:48] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:09<01:27,  2.11it/s][2026-01-17 09:45:49 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:49] INFO:     127.0.0.1:46034 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:09<01:21,  2.28it/s][2026-01-17 09:45:49] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:49] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:49 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:49 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 09:45:49] INFO:     127.0.0.1:46866 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:10<00:51,  3.51it/s][2026-01-17 09:45:49 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:50] INFO:     127.0.0.1:46210 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:10<00:53,  3.38it/s][2026-01-17 09:45:50 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:50] INFO:     127.0.0.1:46006 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:10<00:48,  3.72it/s][2026-01-17 09:45:50 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:50] INFO:     127.0.0.1:46312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:50] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:11<00:55,  3.22it/s]
 11%|█         | 22/200 [00:11<00:48,  3.66it/s][2026-01-17 09:45:50] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:50 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 09:45:50 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 09:45:51] INFO:     127.0.0.1:45838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:51] INFO:     127.0.0.1:46422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:51] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:12<00:58,  3.00it/s]
 13%|█▎        | 26/200 [00:12<00:47,  3.69it/s]
 13%|█▎        | 26/200 [00:12<00:47,  3.69it/s][2026-01-17 09:45:51 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.06, #running-req: 125, #queue-req: 0,
[2026-01-17 09:45:51 TP0] Decode batch, #running-req: 125, #token: 31488, token usage: 0.06, npu graph: False, gen throughput (token/s): 1149.38, #queue-req: 0,
[2026-01-17 09:45:52] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:12<00:54,  3.20it/s][2026-01-17 09:45:52 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:52] INFO:     127.0.0.1:46510 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:13<00:59,  2.91it/s][2026-01-17 09:45:52] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:52 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:52] INFO:     127.0.0.1:46508 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:13<00:44,  3.86it/s][2026-01-17 09:45:52 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 09:45:52 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 09:45:53] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:13<00:48,  3.50it/s][2026-01-17 09:45:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:53] INFO:     127.0.0.1:46198 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:14<00:43,  3.85it/s][2026-01-17 09:45:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:53] INFO:     127.0.0.1:46744 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:14<00:49,  3.35it/s][2026-01-17 09:45:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:54] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:14<00:56,  2.91it/s][2026-01-17 09:45:54 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:54] INFO:     127.0.0.1:46046 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:15<01:10,  2.34it/s][2026-01-17 09:45:54 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:55] INFO:     127.0.0.1:46782 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:16<01:20,  2.04it/s][2026-01-17 09:45:55 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:55 TP0] Decode batch, #running-req: 128, #token: 35072, token usage: 0.06, npu graph: False, gen throughput (token/s): 1226.85, #queue-req: 0,
[2026-01-17 09:45:56] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:16<01:22,  1.96it/s][2026-01-17 09:45:56] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:56 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 09:45:56] INFO:     127.0.0.1:46032 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:17<01:00,  2.65it/s][2026-01-17 09:45:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:56] INFO:     127.0.0.1:46176 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:17<00:52,  3.04it/s][2026-01-17 09:45:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:57] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:57] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:18<01:08,  2.32it/s]
 21%|██        | 42/200 [00:18<01:03,  2.48it/s][2026-01-17 09:45:57 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:45:57] INFO:     127.0.0.1:46320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:57] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:18<00:55,  2.84it/s]
 22%|██▏       | 44/200 [00:18<00:38,  4.02it/s][2026-01-17 09:45:57 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:45:58] INFO:     127.0.0.1:41244 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:19<00:55,  2.77it/s][2026-01-17 09:45:58] INFO:     127.0.0.1:45998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:45:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:58] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:19<00:39,  3.91it/s][2026-01-17 09:45:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:45:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 129, #queue-req: 0,
[2026-01-17 09:45:59] INFO:     127.0.0.1:45910 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:19<00:47,  3.22it/s][2026-01-17 09:45:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:59] INFO:     127.0.0.1:46232 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:19<00:41,  3.61it/s][2026-01-17 09:45:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:45:59] INFO:     127.0.0.1:46336 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:20<00:57,  2.62it/s][2026-01-17 09:46:00] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:00 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:00 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:46:00 TP0] Decode batch, #running-req: 128, #token: 40704, token usage: 0.07, npu graph: False, gen throughput (token/s): 1144.31, #queue-req: 0,
[2026-01-17 09:46:00] INFO:     127.0.0.1:45934 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:21<01:01,  2.39it/s][2026-01-17 09:46:00 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:01] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:21<00:53,  2.76it/s][2026-01-17 09:46:01 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:01] INFO:     127.0.0.1:46454 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:22<00:54,  2.66it/s][2026-01-17 09:46:01] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:01 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:01 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-17 09:46:01] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:22<00:46,  3.07it/s][2026-01-17 09:46:02 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:02] INFO:     127.0.0.1:41384 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:22<00:41,  3.44it/s][2026-01-17 09:46:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:02] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:23<00:46,  3.08it/s][2026-01-17 09:46:02] INFO:     127.0.0.1:45896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:02] INFO:     127.0.0.1:46140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:02 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:46:03] INFO:     127.0.0.1:46764 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:23<00:32,  4.32it/s][2026-01-17 09:46:03] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:46:03] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:03] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:24<00:34,  3.96it/s]
 32%|███▏      | 64/200 [00:24<00:31,  4.35it/s][2026-01-17 09:46:03 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:46:04] INFO:     127.0.0.1:41304 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:24<00:35,  3.82it/s][2026-01-17 09:46:04] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:04 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:04] INFO:     127.0.0.1:46574 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:24<00:27,  4.88it/s][2026-01-17 09:46:04 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:46:04 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 129, #queue-req: 0,
[2026-01-17 09:46:04 TP0] Decode batch, #running-req: 128, #token: 40704, token usage: 0.07, npu graph: False, gen throughput (token/s): 1109.92, #queue-req: 0,
[2026-01-17 09:46:05] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:25<00:42,  3.09it/s][2026-01-17 09:46:05 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:05] INFO:     127.0.0.1:46534 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:26<00:42,  3.08it/s][2026-01-17 09:46:05] INFO:     127.0.0.1:41286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:05 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:05] INFO:     127.0.0.1:46394 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:26<00:30,  4.22it/s][2026-01-17 09:46:05 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:46:05 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 129, #queue-req: 0,
[2026-01-17 09:46:05] INFO:     127.0.0.1:45946 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:26<00:32,  3.88it/s][2026-01-17 09:46:05 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:46:06] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:27<00:37,  3.40it/s][2026-01-17 09:46:06] INFO:     127.0.0.1:46260 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:27<00:39,  3.17it/s][2026-01-17 09:46:06] INFO:     127.0.0.1:45968 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:27<00:32,  3.83it/s][2026-01-17 09:46:06] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:07] INFO:     127.0.0.1:45884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:07] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:27<00:26,  4.70it/s]
 39%|███▉      | 78/200 [00:27<00:19,  6.31it/s][2026-01-17 09:46:07] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:07] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:27<00:16,  7.14it/s][2026-01-17 09:46:07] INFO:     127.0.0.1:46718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:07] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:28<00:17,  6.63it/s]
 41%|████      | 82/200 [00:28<00:15,  7.51it/s][2026-01-17 09:46:08] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:28<00:30,  3.85it/s][2026-01-17 09:46:08] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:29<00:31,  3.72it/s][2026-01-17 09:46:08] INFO:     127.0.0.1:46102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:08] INFO:     127.0.0.1:47506 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:29<00:31,  3.64it/s]
 43%|████▎     | 86/200 [00:29<00:25,  4.48it/s][2026-01-17 09:46:08] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:09 TP0] Decode batch, #running-req: 113, #token: 38144, token usage: 0.07, npu graph: False, gen throughput (token/s): 1167.82, #queue-req: 0,
[2026-01-17 09:46:09] INFO:     127.0.0.1:47588 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:29<00:23,  4.71it/s][2026-01-17 09:46:09] INFO:     127.0.0.1:46886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:10] INFO:     127.0.0.1:46978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:10] INFO:     127.0.0.1:47546 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:30<00:30,  3.65it/s]
 46%|████▌     | 91/200 [00:30<00:29,  3.73it/s][2026-01-17 09:46:10] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:31<00:29,  3.67it/s][2026-01-17 09:46:10] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:11] INFO:     127.0.0.1:46322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:11] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:31<00:32,  3.23it/s]
 48%|████▊     | 95/200 [00:31<00:30,  3.46it/s][2026-01-17 09:46:11] INFO:     127.0.0.1:46114 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:32<00:33,  3.11it/s][2026-01-17 09:46:11] INFO:     127.0.0.1:45958 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:32<00:30,  3.39it/s][2026-01-17 09:46:12] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:12] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:32<00:30,  3.39it/s]
 50%|████▉     | 99/200 [00:32<00:24,  4.17it/s][2026-01-17 09:46:12] INFO:     127.0.0.1:45904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:12] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:12] INFO:     127.0.0.1:45880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:12] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:33<00:17,  5.71it/s]
 52%|█████▏    | 103/200 [00:33<00:12,  7.83it/s][2026-01-17 09:46:12] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:13 TP0] Decode batch, #running-req: 96, #token: 37888, token usage: 0.07, npu graph: False, gen throughput (token/s): 1089.08, #queue-req: 0,
[2026-01-17 09:46:13] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:33<00:18,  5.23it/s][2026-01-17 09:46:13] INFO:     127.0.0.1:46438 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:34<00:24,  3.82it/s][2026-01-17 09:46:14] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:35<00:34,  2.71it/s][2026-01-17 09:46:14] INFO:     127.0.0.1:41320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:15] INFO:     127.0.0.1:46368 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:35<00:28,  3.23it/s][2026-01-17 09:46:15] INFO:     127.0.0.1:45972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:15] INFO:     127.0.0.1:47560 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:36<00:27,  3.27it/s]
 56%|█████▌    | 111/200 [00:36<00:22,  4.03it/s][2026-01-17 09:46:15] INFO:     127.0.0.1:45886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:15] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:36<00:22,  3.81it/s][2026-01-17 09:46:16] INFO:     127.0.0.1:41310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:16] INFO:     127.0.0.1:41352 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:37<00:31,  2.77it/s]
 57%|█████▊    | 115/200 [00:37<00:31,  2.70it/s][2026-01-17 09:46:16 TP0] Decode batch, #running-req: 85, #token: 36224, token usage: 0.07, npu graph: False, gen throughput (token/s): 952.81, #queue-req: 0,
[2026-01-17 09:46:16] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:37<00:29,  2.84it/s][2026-01-17 09:46:17] INFO:     127.0.0.1:46990 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:37<00:27,  2.97it/s][2026-01-17 09:46:17] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:38<00:24,  3.32it/s][2026-01-17 09:46:17] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:17] INFO:     127.0.0.1:47712 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:38<00:17,  4.57it/s][2026-01-17 09:46:17] INFO:     127.0.0.1:41348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:17] INFO:     127.0.0.1:47620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:18] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:38<00:16,  4.55it/s][2026-01-17 09:46:18] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [00:39<00:19,  4.00it/s][2026-01-17 09:46:18] INFO:     127.0.0.1:47436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:18] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:18] INFO:     127.0.0.1:41260 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:39<00:15,  4.66it/s]
 64%|██████▎   | 127/200 [00:39<00:12,  6.06it/s][2026-01-17 09:46:19] INFO:     127.0.0.1:46908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:19] INFO:     127.0.0.1:46966 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:39<00:10,  6.93it/s][2026-01-17 09:46:19] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:19] INFO:     127.0.0.1:47692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:19] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:40<00:11,  5.97it/s][2026-01-17 09:46:20] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:40<00:13,  4.81it/s][2026-01-17 09:46:20] INFO:     127.0.0.1:46024 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:41<00:14,  4.62it/s][2026-01-17 09:46:20 TP0] Decode batch, #running-req: 67, #token: 29824, token usage: 0.05, npu graph: False, gen throughput (token/s): 815.90, #queue-req: 0,
[2026-01-17 09:46:20] INFO:     127.0.0.1:46182 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:41<00:16,  3.83it/s][2026-01-17 09:46:21] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [00:41<00:15,  4.17it/s][2026-01-17 09:46:21] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:21] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:42<00:15,  4.07it/s][2026-01-17 09:46:21] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:42<00:15,  4.04it/s][2026-01-17 09:46:22] INFO:     127.0.0.1:46366 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:42<00:15,  3.99it/s][2026-01-17 09:46:22] INFO:     127.0.0.1:46790 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:43<00:22,  2.67it/s][2026-01-17 09:46:23] INFO:     127.0.0.1:47554 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:43<00:19,  2.93it/s][2026-01-17 09:46:23] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:44<00:19,  2.95it/s][2026-01-17 09:46:23 TP0] Decode batch, #running-req: 57, #token: 29184, token usage: 0.05, npu graph: False, gen throughput (token/s): 727.61, #queue-req: 0,
[2026-01-17 09:46:24] INFO:     127.0.0.1:45926 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:45<00:30,  1.84it/s][2026-01-17 09:46:24] INFO:     127.0.0.1:46752 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:45<00:25,  2.17it/s][2026-01-17 09:46:25] INFO:     127.0.0.1:41470 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:46<00:31,  1.71it/s][2026-01-17 09:46:26] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:46<00:28,  1.85it/s][2026-01-17 09:46:26] INFO:     127.0.0.1:46088 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:47<00:25,  2.01it/s][2026-01-17 09:46:26] INFO:     127.0.0.1:47538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:27] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:47<00:21,  2.38it/s][2026-01-17 09:46:27 TP0] Decode batch, #running-req: 51, #token: 27776, token usage: 0.05, npu graph: False, gen throughput (token/s): 661.51, #queue-req: 0,
[2026-01-17 09:46:27] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:27] INFO:     127.0.0.1:47702 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:48<00:21,  2.30it/s]
 76%|███████▌  | 152/200 [00:48<00:16,  2.83it/s][2026-01-17 09:46:27] INFO:     127.0.0.1:45976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:27] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:48<00:13,  3.53it/s][2026-01-17 09:46:28] INFO:     127.0.0.1:41296 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:49<00:17,  2.56it/s][2026-01-17 09:46:29] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:50<00:19,  2.22it/s][2026-01-17 09:46:29] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:50<00:17,  2.51it/s][2026-01-17 09:46:29] INFO:     127.0.0.1:41372 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:50<00:16,  2.50it/s][2026-01-17 09:46:30] INFO:     127.0.0.1:45848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30 TP0] Decode batch, #running-req: 42, #token: 11904, token usage: 0.02, npu graph: False, gen throughput (token/s): 564.58, #queue-req: 0,
[2026-01-17 09:46:30] INFO:     127.0.0.1:46156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:41362 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:30] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:51<00:16,  2.44it/s]
 89%|████████▉ | 178/200 [00:51<00:00, 22.31it/s]
 89%|████████▉ | 178/200 [00:51<00:00, 22.31it/s]
 89%|████████▉ | 178/200 [00:51<00:00, 22.31it/s]
 89%|████████▉ | 178/200 [00:51<00:00, 22.31it/s]
 89%|████████▉ | 178/200 [00:51<00:00, 22.31it/s]
 89%|████████▉ | 178/200 [00:51<00:00, 22.31it/s]
 89%|████████▉ | 178/200 [00:51<00:00, 22.31it/s]
 89%|████████▉ | 178/200 [00:51<00:00, 22.31it/s]
 89%|████████▉ | 178/200 [00:51<00:00, 22.31it/s]
 89%|████████▉ | 178/200 [00:51<00:00, 22.31it/s]
 89%|████████▉ | 178/200 [00:51<00:00, 22.31it/s][2026-01-17 09:46:31] INFO:     127.0.0.1:47730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:31] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:32] INFO:     127.0.0.1:41416 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:52<00:02,  9.50it/s][2026-01-17 09:46:32] INFO:     127.0.0.1:46960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:33 TP0] Decode batch, #running-req: 18, #token: 10880, token usage: 0.02, npu graph: False, gen throughput (token/s): 256.89, #queue-req: 0,
[2026-01-17 09:46:34] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:54<00:03,  5.14it/s][2026-01-17 09:46:34] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:35] INFO:     127.0.0.1:46930 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:56<00:03,  4.01it/s][2026-01-17 09:46:35] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:35] INFO:     127.0.0.1:41228 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:56<00:03,  3.94it/s][2026-01-17 09:46:36] INFO:     127.0.0.1:47650 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:56<00:02,  4.06it/s][2026-01-17 09:46:36 TP0] Decode batch, #running-req: 12, #token: 7808, token usage: 0.01, npu graph: False, gen throughput (token/s): 189.82, #queue-req: 0,
[2026-01-17 09:46:37] INFO:     127.0.0.1:41254 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:57<00:03,  3.01it/s][2026-01-17 09:46:37] INFO:     127.0.0.1:41266 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:58<00:03,  2.56it/s][2026-01-17 09:46:38] INFO:     127.0.0.1:41274 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:58<00:03,  2.73it/s][2026-01-17 09:46:38] INFO:     127.0.0.1:47636 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:59<00:03,  2.56it/s][2026-01-17 09:46:39] INFO:     127.0.0.1:41332 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [01:00<00:03,  2.07it/s][2026-01-17 09:46:40 TP0] Decode batch, #running-req: 7, #token: 4864, token usage: 0.01, npu graph: False, gen throughput (token/s): 113.98, #queue-req: 0,
[2026-01-17 09:46:40] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [01:01<00:04,  1.46it/s][2026-01-17 09:46:41] INFO:     127.0.0.1:41422 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [01:02<00:04,  1.23it/s][2026-01-17 09:46:42] INFO:     127.0.0.1:41436 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [01:03<00:03,  1.15it/s][2026-01-17 09:46:43 TP0] Decode batch, #running-req: 4, #token: 3328, token usage: 0.01, npu graph: False, gen throughput (token/s): 73.26, #queue-req: 0,
[2026-01-17 09:46:45] INFO:     127.0.0.1:47528 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [01:06<00:04,  1.37s/it][2026-01-17 09:46:46 TP0] Decode batch, #running-req: 3, #token: 2688, token usage: 0.00, npu graph: False, gen throughput (token/s): 48.25, #queue-req: 0,
[2026-01-17 09:46:46] INFO:     127.0.0.1:47572 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [01:07<00:02,  1.40s/it][2026-01-17 09:46:48] INFO:     127.0.0.1:47666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:46:48] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [01:08<00:01,  1.35s/it]
100%|██████████| 200/200 [01:08<00:00,  1.02s/it]
100%|██████████| 200/200 [01:08<00:00,  2.90it/s]
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/llm_models/gsm8k_ascend_mixin.py", line 66, in test_gsm8k
    self.assertGreaterEqual(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1277, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: np.float64(0.875) not greater than or equal to 0.89 : Accuracy of /root/.cache/modelscope/hub/models/Qwen/Qwen3-32B is 0.875, is lower than 0.89
E
======================================================================
ERROR: test_gsm8k (__main__.TestMistral7B.test_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: np.float64(0.875) not greater than or equal to 0.89 : Accuracy of /root/.cache/modelscope/hub/models/Qwen/Qwen3-32B is 0.875, is lower than 0.89

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 190.567s

FAILED (errors=1)
Accuracy: 0.875
Invalid: 0.000
Latency: 69.069 s
Output throughput: 781.521 token/s
.
.
End (25/62):
filename='ascend/llm_models/test_ascend_qwen3_32b.py', elapsed=200, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_ascend_qwen3_32b.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/llm_models/test_ascend_qwen3_32b.py

.
.
Begin (25/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_qwen3_32b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:48:08] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-32B', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-32B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=69395673, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-32B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 09:48:08] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:48:18 TP0] Init torch distributed begin.
[2026-01-17 09:48:18 TP1] Init torch distributed begin.
[2026-01-17 09:48:18 TP2] Init torch distributed begin.
[2026-01-17 09:48:18 TP3] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 09:48:19 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:48:19 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:48:19 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:48:19 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:48:20 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 09:48:20 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:48:20 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:48:20 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:48:20 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:48:20 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:48:20 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 09:48:20 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:48:20 TP2] Load weight begin. avail mem=60.86 GB

Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:00<00:03,  4.48it/s]

Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:00<00:04,  3.60it/s]

Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:00<00:03,  3.52it/s]

Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:01<00:03,  3.65it/s]

Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:01<00:02,  4.18it/s]

Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:01<00:02,  4.27it/s]

Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:01<00:02,  4.19it/s]

Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:01<00:02,  4.18it/s]

Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:02<00:01,  4.15it/s]

Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:02<00:01,  3.90it/s]

Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:02<00:01,  3.85it/s]

Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:03<00:01,  3.56it/s]

Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:03<00:01,  3.57it/s]

Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:03<00:00,  3.68it/s]

Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:03<00:00,  3.78it/s]

Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:04<00:00,  3.84it/s]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:04<00:00,  3.84it/s]

Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:04<00:00,  3.86it/s]

[2026-01-17 09:48:26 TP0] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=45.53 GB, mem usage=15.28 GB.
[2026-01-17 09:48:28 TP1] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=45.86 GB, mem usage=15.28 GB.
[2026-01-17 09:48:28 TP2] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=45.59 GB, mem usage=15.28 GB.
[2026-01-17 09:48:29 TP3] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=45.86 GB, mem usage=15.28 GB.
[2026-01-17 09:48:29 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 09:48:29 TP0] The available memory for KV cache is 33.35 GB.
[2026-01-17 09:48:29 TP3] The available memory for KV cache is 33.35 GB.
[2026-01-17 09:48:29 TP2] The available memory for KV cache is 33.35 GB.
[2026-01-17 09:48:29 TP1] The available memory for KV cache is 33.35 GB.
[2026-01-17 09:48:30 TP0] KV Cache is allocated. #tokens: 546304, K size: 16.68 GB, V size: 16.68 GB
[2026-01-17 09:48:30 TP2] KV Cache is allocated. #tokens: 546304, K size: 16.68 GB, V size: 16.68 GB
[2026-01-17 09:48:30 TP0] Memory pool end. avail mem=11.55 GB
[2026-01-17 09:48:30 TP2] Memory pool end. avail mem=11.60 GB
[2026-01-17 09:48:30 TP3] KV Cache is allocated. #tokens: 546304, K size: 16.68 GB, V size: 16.68 GB
[2026-01-17 09:48:30 TP1] KV Cache is allocated. #tokens: 546304, K size: 16.68 GB, V size: 16.68 GB
[2026-01-17 09:48:30 TP3] Memory pool end. avail mem=11.88 GB
[2026-01-17 09:48:30 TP1] Memory pool end. avail mem=11.88 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 09:48:30 TP0] max_total_num_tokens=546304, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=40960, available_gpu_mem=11.55 GB
[2026-01-17 09:48:31] INFO:     Started server process [213075]
[2026-01-17 09:48:31] INFO:     Waiting for application startup.
[2026-01-17 09:48:31] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-17 09:48:31] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-17 09:48:31] INFO:     Application startup complete.
[2026-01-17 09:48:31] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 09:48:32] INFO:     127.0.0.1:53688 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 09:48:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:48:38] INFO:     127.0.0.1:56236 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 09:48:47] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:48:47] The server is fired up and ready to roll!
[2026-01-17 09:48:48 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:48:49] INFO:     127.0.0.1:50418 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 09:48:49] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 09:48:49] INFO:     127.0.0.1:50424 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 09:48:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:48:49] INFO:     127.0.0.1:50432 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-32B --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 09:48:49 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:48:49 TP0] Prefill batch, #new-seq: 33, #new-token: 4480, #cached-token: 25344, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 09:48:49 TP0] Prefill batch, #new-seq: 20, #new-token: 2688, #cached-token: 15360, token usage: 0.01, #running-req: 34, #queue-req: 0,
[2026-01-17 09:48:50 TP0] Prefill batch, #new-seq: 62, #new-token: 8192, #cached-token: 47616, token usage: 0.01, #running-req: 54, #queue-req: 12,
[2026-01-17 09:48:50 TP0] Prefill batch, #new-seq: 12, #new-token: 1664, #cached-token: 9216, token usage: 0.03, #running-req: 116, #queue-req: 0,
[2026-01-17 09:48:53 TP0] Decode batch, #running-req: 128, #token: 21376, token usage: 0.04, npu graph: False, gen throughput (token/s): 111.02, #queue-req: 0,
[2026-01-17 09:48:55] INFO:     127.0.0.1:50444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:48:55] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:05<17:55,  5.41s/it]
  1%|          | 2/200 [00:05<11:14,  3.40s/it][2026-01-17 09:48:55 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-17 09:48:55] INFO:     127.0.0.1:51498 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:05<07:48,  2.38s/it][2026-01-17 09:48:55 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:48:56] INFO:     127.0.0.1:51250 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:06<05:47,  1.77s/it][2026-01-17 09:48:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:48:56] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:06<04:16,  1.32s/it][2026-01-17 09:48:56] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:48:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 09:48:56] INFO:     127.0.0.1:51474 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:06<02:20,  1.37it/s][2026-01-17 09:48:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 09:48:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 09:48:57] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:48:57] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:07<02:10,  1.47it/s]
  4%|▍         | 9/200 [00:07<01:36,  1.98it/s][2026-01-17 09:48:57] INFO:     127.0.0.1:50608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:48:57 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 09:48:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 09:48:57] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:07<01:18,  2.42it/s][2026-01-17 09:48:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:48:57 TP0] Decode batch, #running-req: 127, #token: 31360, token usage: 0.06, npu graph: False, gen throughput (token/s): 1204.52, #queue-req: 0,
[2026-01-17 09:48:58] INFO:     127.0.0.1:51436 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:08<01:25,  2.19it/s][2026-01-17 09:48:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:48:58] INFO:     127.0.0.1:50758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:48:58] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:09<01:32,  2.02it/s]
  7%|▋         | 14/200 [00:09<01:18,  2.38it/s][2026-01-17 09:48:59 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 09:48:59] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:09<01:24,  2.19it/s][2026-01-17 09:48:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:48:59] INFO:     127.0.0.1:50750 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:10<01:18,  2.35it/s][2026-01-17 09:49:00] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:00 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:00 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 09:49:00] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:10<01:01,  2.94it/s][2026-01-17 09:49:00 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:00] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:10<01:01,  2.96it/s][2026-01-17 09:49:00] INFO:     127.0.0.1:50726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:00 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:00] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:11<00:42,  4.19it/s][2026-01-17 09:49:00 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 09:49:00 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 09:49:01] INFO:     127.0.0.1:51222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:01] INFO:     127.0.0.1:51290 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:11<00:53,  3.35it/s]
 12%|█▏        | 23/200 [00:11<00:49,  3.54it/s][2026-01-17 09:49:01 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 09:49:01] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:11<00:54,  3.21it/s][2026-01-17 09:49:01 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:01] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:12<00:48,  3.60it/s][2026-01-17 09:49:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:02] INFO:     127.0.0.1:50484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:02] INFO:     127.0.0.1:51536 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:12<00:51,  3.39it/s]
 14%|█▎        | 27/200 [00:12<00:42,  4.10it/s][2026-01-17 09:49:02 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 09:49:02 TP0] Decode batch, #running-req: 126, #token: 31360, token usage: 0.06, npu graph: False, gen throughput (token/s): 1130.64, #queue-req: 0,
[2026-01-17 09:49:03] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:13<01:05,  2.61it/s][2026-01-17 09:49:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:03] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:13<00:56,  3.04it/s][2026-01-17 09:49:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:03] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:13<00:48,  3.48it/s][2026-01-17 09:49:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:03] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:14<00:54,  3.11it/s][2026-01-17 09:49:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:04] INFO:     127.0.0.1:50992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:04] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:14<00:47,  3.52it/s]
 16%|█▋        | 33/200 [00:14<00:33,  5.02it/s][2026-01-17 09:49:04 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 09:49:04] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:15<00:58,  2.86it/s][2026-01-17 09:49:04 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:05] INFO:     127.0.0.1:50790 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:15<01:11,  2.29it/s][2026-01-17 09:49:05 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:05] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:16<01:06,  2.45it/s][2026-01-17 09:49:06 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:06] INFO:     127.0.0.1:51008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:06] INFO:     127.0.0.1:36686 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:16<01:07,  2.40it/s]
 19%|█▉        | 38/200 [00:16<00:53,  3.03it/s][2026-01-17 09:49:06 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 09:49:06] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:16<00:47,  3.40it/s][2026-01-17 09:49:06 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:06 TP0] Decode batch, #running-req: 127, #token: 34944, token usage: 0.06, npu graph: False, gen throughput (token/s): 1199.73, #queue-req: 0,
[2026-01-17 09:49:06] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:06] INFO:     127.0.0.1:51266 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:17<00:49,  3.22it/s]
 20%|██        | 41/200 [00:17<00:40,  3.92it/s][2026-01-17 09:49:06 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 09:49:07] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:17<00:48,  3.28it/s][2026-01-17 09:49:07 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:08] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:18<01:14,  2.11it/s][2026-01-17 09:49:08 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:08] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:08] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:18<01:02,  2.51it/s]
 22%|██▎       | 45/200 [00:18<00:41,  3.73it/s][2026-01-17 09:49:08 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:49:09] INFO:     127.0.0.1:50682 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:19<01:04,  2.40it/s][2026-01-17 09:49:09 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:09] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:19<00:54,  2.80it/s][2026-01-17 09:49:09 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:09] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:20<00:53,  2.82it/s][2026-01-17 09:49:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:10] INFO:     127.0.0.1:36700 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:20<00:47,  3.20it/s][2026-01-17 09:49:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:10] INFO:     127.0.0.1:50912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:10] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:21<01:03,  2.37it/s]
 26%|██▌       | 51/200 [00:21<00:58,  2.56it/s][2026-01-17 09:49:10] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:10 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:49:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:49:11 TP0] Decode batch, #running-req: 128, #token: 40320, token usage: 0.07, npu graph: False, gen throughput (token/s): 1104.10, #queue-req: 0,
[2026-01-17 09:49:11] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:21<00:48,  3.02it/s][2026-01-17 09:49:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:11] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:22<00:57,  2.54it/s][2026-01-17 09:49:12 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:12] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:12] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:12] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:22<00:50,  2.86it/s]
 28%|██▊       | 57/200 [00:22<00:23,  5.99it/s]
 28%|██▊       | 57/200 [00:22<00:23,  5.99it/s][2026-01-17 09:49:12 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-17 09:49:12] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:23<00:38,  3.64it/s][2026-01-17 09:49:13 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:13] INFO:     127.0.0.1:36844 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:23<00:36,  3.88it/s][2026-01-17 09:49:13 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:13] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:23<00:38,  3.60it/s][2026-01-17 09:49:13] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:13] INFO:     127.0.0.1:51610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:13 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:13 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:49:13] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:24<00:30,  4.45it/s][2026-01-17 09:49:14 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:14] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:24<00:30,  4.53it/s][2026-01-17 09:49:14 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:14] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:14] INFO:     127.0.0.1:51562 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:24<00:35,  3.81it/s]
 33%|███▎      | 66/200 [00:24<00:32,  4.14it/s][2026-01-17 09:49:14 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:49:15] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:15] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:25<00:42,  3.10it/s]
 34%|███▍      | 68/200 [00:25<00:41,  3.15it/s][2026-01-17 09:49:15] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:15 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:49:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 128, #queue-req: 0,
[2026-01-17 09:49:15] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:25<00:38,  3.40it/s][2026-01-17 09:49:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:15 TP0] Decode batch, #running-req: 127, #token: 40448, token usage: 0.07, npu graph: False, gen throughput (token/s): 1120.38, #queue-req: 0,
[2026-01-17 09:49:16] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:26<00:39,  3.29it/s][2026-01-17 09:49:16 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-17 09:49:16] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:16] INFO:     127.0.0.1:36764 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:26<00:35,  3.56it/s]
 36%|███▋      | 73/200 [00:26<00:26,  4.74it/s][2026-01-17 09:49:16 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.07, #running-req: 126, #queue-req: 0,
[2026-01-17 09:49:16] INFO:     127.0.0.1:36906 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:26<00:25,  4.98it/s][2026-01-17 09:49:16] INFO:     127.0.0.1:50620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:16] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:26<00:29,  4.30it/s]
 38%|███▊      | 76/200 [00:26<00:25,  4.85it/s][2026-01-17 09:49:17] INFO:     127.0.0.1:51522 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:27<00:28,  4.25it/s][2026-01-17 09:49:17] INFO:     127.0.0.1:50976 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:27<00:29,  4.09it/s][2026-01-17 09:49:17] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:17] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:17] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:27<00:21,  5.57it/s]
 40%|████      | 81/200 [00:27<00:14,  8.17it/s][2026-01-17 09:49:17] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:27<00:17,  6.92it/s][2026-01-17 09:49:17] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:18] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:28<00:17,  6.59it/s][2026-01-17 09:49:18] INFO:     127.0.0.1:36776 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:29<00:29,  3.90it/s][2026-01-17 09:49:19] INFO:     127.0.0.1:51292 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:29<00:29,  3.93it/s][2026-01-17 09:49:19] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:19] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:29<00:21,  5.15it/s][2026-01-17 09:49:19] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:19] INFO:     127.0.0.1:50488 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:29<00:17,  6.42it/s][2026-01-17 09:49:19 TP0] Decode batch, #running-req: 110, #token: 37632, token usage: 0.07, npu graph: False, gen throughput (token/s): 1295.95, #queue-req: 0,
[2026-01-17 09:49:19] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:19] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:29<00:20,  5.24it/s]
 46%|████▌     | 92/200 [00:29<00:19,  5.51it/s][2026-01-17 09:49:19] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:20] INFO:     127.0.0.1:50866 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:30<00:26,  4.01it/s][2026-01-17 09:49:20] INFO:     127.0.0.1:48068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:21] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:31<00:36,  2.86it/s][2026-01-17 09:49:21] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:21] INFO:     127.0.0.1:51276 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:31<00:27,  3.77it/s][2026-01-17 09:49:21] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:22] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:32<00:22,  4.53it/s][2026-01-17 09:49:22] INFO:     127.0.0.1:36808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:22] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:32<00:17,  5.62it/s][2026-01-17 09:49:22] INFO:     127.0.0.1:48084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:22] INFO:     127.0.0.1:48224 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:32<00:15,  6.07it/s]
 52%|█████▏    | 104/200 [00:32<00:12,  7.90it/s][2026-01-17 09:49:22] INFO:     127.0.0.1:48096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:22] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:33<00:17,  5.37it/s][2026-01-17 09:49:23 TP0] Decode batch, #running-req: 95, #token: 37248, token usage: 0.07, npu graph: False, gen throughput (token/s): 1208.61, #queue-req: 0,
[2026-01-17 09:49:23] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:33<00:17,  5.42it/s][2026-01-17 09:49:23] INFO:     127.0.0.1:51496 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:33<00:20,  4.47it/s][2026-01-17 09:49:23] INFO:     127.0.0.1:36918 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:33<00:19,  4.70it/s][2026-01-17 09:49:24] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:24] INFO:     127.0.0.1:36778 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:34<00:36,  2.47it/s]
 56%|█████▌    | 111/200 [00:34<00:39,  2.28it/s][2026-01-17 09:49:24] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:35<00:33,  2.64it/s][2026-01-17 09:49:24] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:35<00:28,  3.04it/s][2026-01-17 09:49:25] INFO:     127.0.0.1:51042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:25] INFO:     127.0.0.1:50658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:25] INFO:     127.0.0.1:36904 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:35<00:19,  4.36it/s]
 58%|█████▊    | 116/200 [00:35<00:12,  6.70it/s][2026-01-17 09:49:25] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:35<00:18,  4.56it/s][2026-01-17 09:49:26] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:36<00:21,  3.79it/s][2026-01-17 09:49:26] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:26] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:36<00:15,  5.04it/s][2026-01-17 09:49:26] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:36<00:14,  5.60it/s][2026-01-17 09:49:26 TP0] Decode batch, #running-req: 79, #token: 33664, token usage: 0.06, npu graph: False, gen throughput (token/s): 988.47, #queue-req: 0,
[2026-01-17 09:49:26] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:36<00:15,  5.10it/s][2026-01-17 09:49:26] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:37<00:14,  5.24it/s][2026-01-17 09:49:26] INFO:     127.0.0.1:36810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:26] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [00:37<00:14,  5.37it/s]
 62%|██████▎   | 125/200 [00:37<00:10,  6.96it/s][2026-01-17 09:49:27] INFO:     127.0.0.1:51394 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:37<00:12,  5.88it/s][2026-01-17 09:49:27] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:37<00:12,  5.72it/s][2026-01-17 09:49:27] INFO:     127.0.0.1:48070 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:27] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:38<00:17,  4.14it/s]
 64%|██████▍   | 129/200 [00:38<00:16,  4.35it/s][2026-01-17 09:49:28] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:28] INFO:     127.0.0.1:36942 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:38<00:15,  4.62it/s]
 66%|██████▌   | 131/200 [00:38<00:11,  6.09it/s][2026-01-17 09:49:28] INFO:     127.0.0.1:36730 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:38<00:15,  4.42it/s][2026-01-17 09:49:28] INFO:     127.0.0.1:51612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:28] INFO:     127.0.0.1:51456 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:39<00:13,  4.81it/s][2026-01-17 09:49:28] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:28] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:28] INFO:     127.0.0.1:48296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:29] INFO:     127.0.0.1:36750 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:39<00:08,  7.53it/s][2026-01-17 09:49:29] INFO:     127.0.0.1:51544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:30 TP0] Decode batch, #running-req: 61, #token: 28032, token usage: 0.05, npu graph: False, gen throughput (token/s): 791.97, #queue-req: 0,
[2026-01-17 09:49:30] INFO:     127.0.0.1:36890 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:40<00:14,  4.02it/s][2026-01-17 09:49:30] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:41<00:19,  3.00it/s][2026-01-17 09:49:31] INFO:     127.0.0.1:36828 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:41<00:18,  3.15it/s][2026-01-17 09:49:31] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:31] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:41<00:19,  2.91it/s]
 72%|███████▏  | 144/200 [00:41<00:16,  3.37it/s][2026-01-17 09:49:32] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:42<00:16,  3.30it/s][2026-01-17 09:49:32] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:42<00:14,  3.69it/s][2026-01-17 09:49:32] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:42<00:18,  2.85it/s][2026-01-17 09:49:32] INFO:     127.0.0.1:48098 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:43<00:15,  3.31it/s][2026-01-17 09:49:33 TP0] Decode batch, #running-req: 52, #token: 26624, token usage: 0.05, npu graph: False, gen throughput (token/s): 671.55, #queue-req: 0,
[2026-01-17 09:49:33] INFO:     127.0.0.1:36758 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:43<00:21,  2.35it/s][2026-01-17 09:49:33] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:33] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:44<00:18,  2.67it/s]
 76%|███████▌  | 151/200 [00:44<00:12,  3.81it/s][2026-01-17 09:49:34] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:34] INFO:     127.0.0.1:48144 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:45<00:17,  2.71it/s][2026-01-17 09:49:35] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:45<00:17,  2.65it/s][2026-01-17 09:49:35] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:45<00:16,  2.71it/s][2026-01-17 09:49:35] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:46<00:13,  3.15it/s][2026-01-17 09:49:36] INFO:     127.0.0.1:36800 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:46<00:15,  2.76it/s][2026-01-17 09:49:36] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:46<00:13,  3.21it/s][2026-01-17 09:49:36 TP0] Decode batch, #running-req: 42, #token: 23040, token usage: 0.04, npu graph: False, gen throughput (token/s): 571.42, #queue-req: 0,
[2026-01-17 09:49:36] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:47<00:12,  3.18it/s][2026-01-17 09:49:38] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:48<00:22,  1.81it/s][2026-01-17 09:49:38] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:48<00:17,  2.27it/s][2026-01-17 09:49:38] INFO:     127.0.0.1:48188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:39] INFO:     127.0.0.1:36932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:39] INFO:     127.0.0.1:48324 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:49<00:17,  2.15it/s]
 82%|████████▏ | 164/200 [00:49<00:14,  2.50it/s][2026-01-17 09:49:39] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:49<00:13,  2.52it/s][2026-01-17 09:49:39] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:49<00:11,  2.85it/s][2026-01-17 09:49:40] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40 TP0] Decode batch, #running-req: 34, #token: 11520, token usage: 0.02, npu graph: False, gen throughput (token/s): 443.31, #queue-req: 0,
[2026-01-17 09:49:40] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:50846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:51516 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:50<00:12,  2.63it/s]
 90%|████████▉ | 179/200 [00:50<00:01, 14.27it/s]
 90%|████████▉ | 179/200 [00:50<00:01, 14.27it/s]
 90%|████████▉ | 179/200 [00:50<00:01, 14.27it/s]
 90%|████████▉ | 179/200 [00:50<00:01, 14.27it/s]
 90%|████████▉ | 179/200 [00:50<00:01, 14.27it/s]
 90%|████████▉ | 179/200 [00:50<00:01, 14.27it/s]
 90%|████████▉ | 179/200 [00:50<00:01, 14.27it/s]
 90%|████████▉ | 179/200 [00:50<00:01, 14.27it/s][2026-01-17 09:49:40] INFO:     127.0.0.1:36842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:40] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:51<00:01,  9.58it/s][2026-01-17 09:49:41] INFO:     127.0.0.1:48336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:41] INFO:     127.0.0.1:36718 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:51<00:02,  7.08it/s][2026-01-17 09:49:42] INFO:     127.0.0.1:36814 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:52<00:02,  5.42it/s][2026-01-17 09:49:43] INFO:     127.0.0.1:36728 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:53<00:03,  3.92it/s][2026-01-17 09:49:43] INFO:     127.0.0.1:48262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:43 TP0] Decode batch, #running-req: 14, #token: 8448, token usage: 0.02, npu graph: False, gen throughput (token/s): 192.69, #queue-req: 0,
[2026-01-17 09:49:44] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:44] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:54<00:04,  3.08it/s]
 94%|█████████▍| 188/200 [00:54<00:04,  2.97it/s][2026-01-17 09:49:44] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:54<00:03,  3.19it/s][2026-01-17 09:49:45] INFO:     127.0.0.1:48072 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:55<00:03,  2.53it/s][2026-01-17 09:49:45] INFO:     127.0.0.1:36678 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:55<00:03,  2.35it/s][2026-01-17 09:49:45] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:49:47 TP0] Decode batch, #running-req: 8, #token: 5376, token usage: 0.01, npu graph: False, gen throughput (token/s): 114.24, #queue-req: 0,
[2026-01-17 09:49:48] INFO:     127.0.0.1:48240 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:58<00:05,  1.24it/s][2026-01-17 09:49:49] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:59<00:04,  1.38it/s][2026-01-17 09:49:49] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:59<00:03,  1.52it/s][2026-01-17 09:49:50 TP0] Decode batch, #running-req: 5, #token: 3584, token usage: 0.01, npu graph: False, gen throughput (token/s): 81.54, #queue-req: 0,
[2026-01-17 09:49:51] INFO:     127.0.0.1:36846 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [01:01<00:03,  1.12it/s][2026-01-17 09:49:52] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [01:02<00:03,  1.03s/it][2026-01-17 09:49:54 TP0] Decode batch, #running-req: 3, #token: 2688, token usage: 0.00, npu graph: False, gen throughput (token/s): 44.19, #queue-req: 0,
[2026-01-17 09:49:56] INFO:     127.0.0.1:48132 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [01:06<00:03,  1.83s/it][2026-01-17 09:49:57 TP0] Decode batch, #running-req: 2, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 32.86, #queue-req: 0,
[2026-01-17 09:49:57] INFO:     127.0.0.1:48190 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [01:07<00:01,  1.55s/it][2026-01-17 09:49:59] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [01:09<00:00,  1.61s/it]
100%|██████████| 200/200 [01:09<00:00,  2.88it/s]
.
----------------------------------------------------------------------
Ran 1 test in 120.985s

OK
Accuracy: 0.895
Invalid: 0.000
Latency: 69.605 s
Output throughput: 748.324 token/s
.
.
End (25/62):
filename='ascend/llm_models/test_ascend_qwen3_32b.py', elapsed=132, estimated_time=400
.
.


✓ PASSED on retry (attempt 2): ascend/llm_models/test_ascend_qwen3_32b.py

.
.
Begin (26/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_llama4_models.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:50:20] INFO model_config.py:150: Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2026-01-17 09:50:20] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/meta-llama/Llama-4-Scout-17B-16E-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/meta-llama/Llama-4-Scout-17B-16E-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=8192, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.9, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=831191087, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/meta-llama/Llama-4-Scout-17B-16E-Instruct', weight_version='default', chat_template='llama-4', completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 09:50:20] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2026-01-17 09:50:21] Loading chat template from argument: llama-4
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:50:29 TP1] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:50:30 TP3] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2026-01-17 09:50:30 TP2] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2026-01-17 09:50:30 TP0] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:50:31 TP1] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2026-01-17 09:50:31 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:50:31 TP3] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2026-01-17 09:50:31 TP3] Init torch distributed begin.
[2026-01-17 09:50:31 TP0] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2026-01-17 09:50:31 TP2] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2026-01-17 09:50:31 TP0] Init torch distributed begin.
[2026-01-17 09:50:31 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 09:50:33 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:50:33 TP0] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 09:50:33 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:50:33 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:50:34 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:50:34 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:50:34 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:50:34 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:50:34 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:50:34 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:50:34 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 09:50:34 TP0] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/50 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   2% Completed | 1/50 [00:09<07:22,  9.03s/it]

Loading safetensors checkpoint shards:   4% Completed | 2/50 [00:16<06:37,  8.29s/it]

Loading safetensors checkpoint shards:   6% Completed | 3/50 [00:24<06:22,  8.14s/it]

Loading safetensors checkpoint shards:   8% Completed | 4/50 [00:33<06:17,  8.21s/it]

Loading safetensors checkpoint shards:  10% Completed | 5/50 [00:41<06:13,  8.30s/it]

Loading safetensors checkpoint shards:  12% Completed | 6/50 [00:49<06:03,  8.27s/it]

Loading safetensors checkpoint shards:  14% Completed | 7/50 [00:58<05:55,  8.26s/it]

Loading safetensors checkpoint shards:  16% Completed | 8/50 [01:06<05:46,  8.25s/it]

Loading safetensors checkpoint shards:  18% Completed | 9/50 [01:14<05:36,  8.22s/it]

Loading safetensors checkpoint shards:  20% Completed | 10/50 [01:22<05:30,  8.25s/it]

Loading safetensors checkpoint shards:  22% Completed | 11/50 [01:30<05:21,  8.24s/it]

Loading safetensors checkpoint shards:  24% Completed | 12/50 [01:39<05:16,  8.32s/it]

Loading safetensors checkpoint shards:  26% Completed | 13/50 [01:48<05:11,  8.42s/it]

Loading safetensors checkpoint shards:  28% Completed | 14/50 [01:57<05:10,  8.64s/it]

Loading safetensors checkpoint shards:  30% Completed | 15/50 [02:07<05:17,  9.06s/it]

Loading safetensors checkpoint shards:  32% Completed | 16/50 [02:15<05:01,  8.85s/it]

Loading safetensors checkpoint shards:  34% Completed | 17/50 [02:24<04:51,  8.84s/it]

Loading safetensors checkpoint shards:  36% Completed | 18/50 [02:33<04:47,  8.97s/it]

Loading safetensors checkpoint shards:  38% Completed | 19/50 [02:42<04:32,  8.79s/it]

Loading safetensors checkpoint shards:  40% Completed | 20/50 [02:51<04:26,  8.88s/it]

Loading safetensors checkpoint shards:  42% Completed | 21/50 [03:00<04:18,  8.93s/it]

Loading safetensors checkpoint shards:  44% Completed | 22/50 [03:10<04:21,  9.34s/it]

Loading safetensors checkpoint shards:  46% Completed | 23/50 [03:18<04:03,  9.02s/it]

Loading safetensors checkpoint shards:  48% Completed | 24/50 [03:27<03:55,  9.04s/it]

Loading safetensors checkpoint shards:  50% Completed | 25/50 [03:37<03:47,  9.10s/it]

Loading safetensors checkpoint shards:  52% Completed | 26/50 [03:45<03:35,  8.98s/it]

Loading safetensors checkpoint shards:  54% Completed | 27/50 [03:54<03:27,  9.03s/it]

Loading safetensors checkpoint shards:  56% Completed | 28/50 [04:04<03:20,  9.12s/it]

Loading safetensors checkpoint shards:  58% Completed | 29/50 [04:14<03:15,  9.31s/it]

Loading safetensors checkpoint shards:  60% Completed | 30/50 [04:23<03:08,  9.45s/it]

Loading safetensors checkpoint shards:  62% Completed | 31/50 [04:25<02:15,  7.12s/it]

Loading safetensors checkpoint shards:  64% Completed | 32/50 [04:34<02:18,  7.70s/it]

Loading safetensors checkpoint shards:  66% Completed | 33/50 [04:43<02:17,  8.12s/it]

Loading safetensors checkpoint shards:  68% Completed | 34/50 [04:53<02:19,  8.73s/it]

Loading safetensors checkpoint shards:  70% Completed | 35/50 [05:02<02:12,  8.86s/it]

Loading safetensors checkpoint shards:  72% Completed | 36/50 [05:11<02:04,  8.89s/it]

Loading safetensors checkpoint shards:  74% Completed | 37/50 [05:21<01:56,  8.95s/it]

Loading safetensors checkpoint shards:  76% Completed | 38/50 [05:30<01:48,  9.06s/it]

Loading safetensors checkpoint shards:  78% Completed | 39/50 [05:39<01:39,  9.07s/it]

Loading safetensors checkpoint shards:  80% Completed | 40/50 [05:48<01:30,  9.10s/it]

Loading safetensors checkpoint shards:  82% Completed | 41/50 [05:58<01:22,  9.22s/it]

Loading safetensors checkpoint shards:  84% Completed | 42/50 [06:00<00:58,  7.26s/it]

Loading safetensors checkpoint shards:  86% Completed | 43/50 [06:09<00:53,  7.63s/it]

Loading safetensors checkpoint shards:  88% Completed | 44/50 [06:18<00:49,  8.19s/it]

Loading safetensors checkpoint shards:  90% Completed | 45/50 [06:29<00:44,  8.92s/it]

Loading safetensors checkpoint shards:  92% Completed | 46/50 [06:38<00:35,  8.87s/it]

Loading safetensors checkpoint shards:  94% Completed | 47/50 [06:47<00:26,  8.92s/it]

Loading safetensors checkpoint shards:  96% Completed | 48/50 [06:56<00:18,  9.08s/it]

Loading safetensors checkpoint shards:  98% Completed | 49/50 [07:06<00:09,  9.22s/it]

Loading safetensors checkpoint shards: 100% Completed | 50/50 [07:15<00:00,  9.17s/it]

Loading safetensors checkpoint shards: 100% Completed | 50/50 [07:15<00:00,  8.70s/it]

[2026-01-17 09:57:51 TP0] Some weights are not initialized from checkpoints {'language_model.model.layers.25.self_attn.qk_norm.weight', 'language_model.model.layers.37.self_attn.qk_norm.weight', 'language_model.model.layers.46.self_attn.qk_norm.weight', 'language_model.model.layers.14.self_attn.qk_norm.weight', 'language_model.model.layers.16.self_attn.qk_norm.weight', 'language_model.model.layers.44.self_attn.qk_norm.weight', 'language_model.model.layers.29.self_attn.qk_norm.weight', 'language_model.model.layers.24.self_attn.qk_norm.weight', 'language_model.model.layers.30.self_attn.qk_norm.weight', 'language_model.model.layers.10.self_attn.qk_norm.weight', 'language_model.model.layers.26.self_attn.qk_norm.weight', 'language_model.model.layers.22.self_attn.qk_norm.weight', 'language_model.model.layers.40.self_attn.qk_norm.weight', 'language_model.model.layers.13.self_attn.qk_norm.weight', 'language_model.model.layers.38.self_attn.qk_norm.weight', 'language_model.model.layers.34.self_attn.qk_norm.weight', 'language_model.model.layers.33.self_attn.qk_norm.weight', 'language_model.model.layers.8.self_attn.qk_norm.weight', 'language_model.model.layers.9.self_attn.qk_norm.weight', 'language_model.model.layers.21.self_attn.qk_norm.weight', 'language_model.model.layers.45.self_attn.qk_norm.weight', 'language_model.model.layers.6.self_attn.qk_norm.weight', 'language_model.model.layers.0.self_attn.qk_norm.weight', 'language_model.model.layers.4.self_attn.qk_norm.weight', 'language_model.model.layers.17.self_attn.qk_norm.weight', 'language_model.model.layers.36.self_attn.qk_norm.weight', 'language_model.model.layers.2.self_attn.qk_norm.weight', 'language_model.model.layers.18.self_attn.qk_norm.weight', 'language_model.model.layers.32.self_attn.qk_norm.weight', 'language_model.model.layers.41.self_attn.qk_norm.weight', 'language_model.model.layers.28.self_attn.qk_norm.weight', 'language_model.model.layers.42.self_attn.qk_norm.weight', 'language_model.model.layers.20.self_attn.qk_norm.weight', 'language_model.model.layers.5.self_attn.qk_norm.weight', 'language_model.model.layers.12.self_attn.qk_norm.weight', 'language_model.model.layers.1.self_attn.qk_norm.weight'}
[2026-01-17 09:57:51 TP1] Some weights are not initialized from checkpoints {'language_model.model.layers.9.self_attn.qk_norm.weight', 'language_model.model.layers.2.self_attn.qk_norm.weight', 'language_model.model.layers.45.self_attn.qk_norm.weight', 'language_model.model.layers.36.self_attn.qk_norm.weight', 'language_model.model.layers.30.self_attn.qk_norm.weight', 'language_model.model.layers.37.self_attn.qk_norm.weight', 'language_model.model.layers.33.self_attn.qk_norm.weight', 'language_model.model.layers.16.self_attn.qk_norm.weight', 'language_model.model.layers.1.self_attn.qk_norm.weight', 'language_model.model.layers.42.self_attn.qk_norm.weight', 'language_model.model.layers.28.self_attn.qk_norm.weight', 'language_model.model.layers.0.self_attn.qk_norm.weight', 'language_model.model.layers.14.self_attn.qk_norm.weight', 'language_model.model.layers.25.self_attn.qk_norm.weight', 'language_model.model.layers.46.self_attn.qk_norm.weight', 'language_model.model.layers.41.self_attn.qk_norm.weight', 'language_model.model.layers.29.self_attn.qk_norm.weight', 'language_model.model.layers.44.self_attn.qk_norm.weight', 'language_model.model.layers.18.self_attn.qk_norm.weight', 'language_model.model.layers.5.self_attn.qk_norm.weight', 'language_model.model.layers.26.self_attn.qk_norm.weight', 'language_model.model.layers.38.self_attn.qk_norm.weight', 'language_model.model.layers.13.self_attn.qk_norm.weight', 'language_model.model.layers.20.self_attn.qk_norm.weight', 'language_model.model.layers.34.self_attn.qk_norm.weight', 'language_model.model.layers.10.self_attn.qk_norm.weight', 'language_model.model.layers.40.self_attn.qk_norm.weight', 'language_model.model.layers.17.self_attn.qk_norm.weight', 'language_model.model.layers.21.self_attn.qk_norm.weight', 'language_model.model.layers.32.self_attn.qk_norm.weight', 'language_model.model.layers.22.self_attn.qk_norm.weight', 'language_model.model.layers.4.self_attn.qk_norm.weight', 'language_model.model.layers.6.self_attn.qk_norm.weight', 'language_model.model.layers.24.self_attn.qk_norm.weight', 'language_model.model.layers.12.self_attn.qk_norm.weight', 'language_model.model.layers.8.self_attn.qk_norm.weight'}
[2026-01-17 09:57:51 TP2] Some weights are not initialized from checkpoints {'language_model.model.layers.42.self_attn.qk_norm.weight', 'language_model.model.layers.10.self_attn.qk_norm.weight', 'language_model.model.layers.45.self_attn.qk_norm.weight', 'language_model.model.layers.34.self_attn.qk_norm.weight', 'language_model.model.layers.5.self_attn.qk_norm.weight', 'language_model.model.layers.26.self_attn.qk_norm.weight', 'language_model.model.layers.24.self_attn.qk_norm.weight', 'language_model.model.layers.28.self_attn.qk_norm.weight', 'language_model.model.layers.12.self_attn.qk_norm.weight', 'language_model.model.layers.38.self_attn.qk_norm.weight', 'language_model.model.layers.6.self_attn.qk_norm.weight', 'language_model.model.layers.18.self_attn.qk_norm.weight', 'language_model.model.layers.14.self_attn.qk_norm.weight', 'language_model.model.layers.25.self_attn.qk_norm.weight', 'language_model.model.layers.37.self_attn.qk_norm.weight', 'language_model.model.layers.16.self_attn.qk_norm.weight', 'language_model.model.layers.44.self_attn.qk_norm.weight', 'language_model.model.layers.0.self_attn.qk_norm.weight', 'language_model.model.layers.9.self_attn.qk_norm.weight', 'language_model.model.layers.32.self_attn.qk_norm.weight', 'language_model.model.layers.17.self_attn.qk_norm.weight', 'language_model.model.layers.30.self_attn.qk_norm.weight', 'language_model.model.layers.4.self_attn.qk_norm.weight', 'language_model.model.layers.36.self_attn.qk_norm.weight', 'language_model.model.layers.8.self_attn.qk_norm.weight', 'language_model.model.layers.41.self_attn.qk_norm.weight', 'language_model.model.layers.20.self_attn.qk_norm.weight', 'language_model.model.layers.22.self_attn.qk_norm.weight', 'language_model.model.layers.46.self_attn.qk_norm.weight', 'language_model.model.layers.1.self_attn.qk_norm.weight', 'language_model.model.layers.2.self_attn.qk_norm.weight', 'language_model.model.layers.13.self_attn.qk_norm.weight', 'language_model.model.layers.29.self_attn.qk_norm.weight', 'language_model.model.layers.21.self_attn.qk_norm.weight', 'language_model.model.layers.40.self_attn.qk_norm.weight', 'language_model.model.layers.33.self_attn.qk_norm.weight'}
[2026-01-17 09:57:51 TP3] Some weights are not initialized from checkpoints {'language_model.model.layers.32.self_attn.qk_norm.weight', 'language_model.model.layers.1.self_attn.qk_norm.weight', 'language_model.model.layers.25.self_attn.qk_norm.weight', 'language_model.model.layers.28.self_attn.qk_norm.weight', 'language_model.model.layers.45.self_attn.qk_norm.weight', 'language_model.model.layers.46.self_attn.qk_norm.weight', 'language_model.model.layers.22.self_attn.qk_norm.weight', 'language_model.model.layers.36.self_attn.qk_norm.weight', 'language_model.model.layers.29.self_attn.qk_norm.weight', 'language_model.model.layers.0.self_attn.qk_norm.weight', 'language_model.model.layers.14.self_attn.qk_norm.weight', 'language_model.model.layers.30.self_attn.qk_norm.weight', 'language_model.model.layers.12.self_attn.qk_norm.weight', 'language_model.model.layers.16.self_attn.qk_norm.weight', 'language_model.model.layers.21.self_attn.qk_norm.weight', 'language_model.model.layers.17.self_attn.qk_norm.weight', 'language_model.model.layers.13.self_attn.qk_norm.weight', 'language_model.model.layers.9.self_attn.qk_norm.weight', 'language_model.model.layers.42.self_attn.qk_norm.weight', 'language_model.model.layers.34.self_attn.qk_norm.weight', 'language_model.model.layers.5.self_attn.qk_norm.weight', 'language_model.model.layers.24.self_attn.qk_norm.weight', 'language_model.model.layers.38.self_attn.qk_norm.weight', 'language_model.model.layers.44.self_attn.qk_norm.weight', 'language_model.model.layers.6.self_attn.qk_norm.weight', 'language_model.model.layers.41.self_attn.qk_norm.weight', 'language_model.model.layers.37.self_attn.qk_norm.weight', 'language_model.model.layers.20.self_attn.qk_norm.weight', 'language_model.model.layers.8.self_attn.qk_norm.weight', 'language_model.model.layers.10.self_attn.qk_norm.weight', 'language_model.model.layers.18.self_attn.qk_norm.weight', 'language_model.model.layers.2.self_attn.qk_norm.weight', 'language_model.model.layers.40.self_attn.qk_norm.weight', 'language_model.model.layers.33.self_attn.qk_norm.weight', 'language_model.model.layers.26.self_attn.qk_norm.weight', 'language_model.model.layers.4.self_attn.qk_norm.weight'}
[2026-01-17 09:58:05 TP2] Setting sliding_window_size to be attention_chunk_size: 8192
[2026-01-17 09:58:05 TP2] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=7.50 GB, mem usage=53.36 GB.
[2026-01-17 09:58:05 TP0] Setting sliding_window_size to be attention_chunk_size: 8192
[2026-01-17 09:58:05 TP0] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=7.45 GB, mem usage=53.36 GB.
[2026-01-17 09:58:05 TP1] Setting sliding_window_size to be attention_chunk_size: 8192
[2026-01-17 09:58:05 TP1] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=7.78 GB, mem usage=53.36 GB.
[2026-01-17 09:58:05 TP3] Setting sliding_window_size to be attention_chunk_size: 8192
[2026-01-17 09:58:05 TP3] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=7.78 GB, mem usage=53.36 GB.
[2026-01-17 09:58:05 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 09:58:05 TP3] The available memory for KV cache is 1.37 GB.
[2026-01-17 09:58:05 TP0] The available memory for KV cache is 1.37 GB.
[2026-01-17 09:58:05 TP2] The available memory for KV cache is 1.37 GB.
[2026-01-17 09:58:05 TP1] The available memory for KV cache is 1.37 GB.
[2026-01-17 09:58:05 TP3] KV Cache is allocated. #tokens: 29824, K size: 0.69 GB, V size: 0.69 GB
[2026-01-17 09:58:05 TP2] KV Cache is allocated. #tokens: 29824, K size: 0.69 GB, V size: 0.69 GB
[2026-01-17 09:58:05 TP1] KV Cache is allocated. #tokens: 29824, K size: 0.69 GB, V size: 0.69 GB
[2026-01-17 09:58:05 TP0] KV Cache is allocated. #tokens: 29824, K size: 0.69 GB, V size: 0.69 GB
[2026-01-17 09:58:05 TP3] Memory pool end. avail mem=6.40 GB
[2026-01-17 09:58:05 TP2] Memory pool end. avail mem=6.13 GB
[2026-01-17 09:58:05 TP1] Memory pool end. avail mem=6.41 GB
[2026-01-17 09:58:05 TP0] Memory pool end. avail mem=6.07 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 09:58:07 TP0] max_total_num_tokens=29824, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=8192, available_gpu_mem=6.08 GB
[2026-01-17 09:58:09] INFO:     Started server process [218955]
[2026-01-17 09:58:09] INFO:     Waiting for application startup.
[2026-01-17 09:58:09] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-17 09:58:09] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-17 09:58:09] INFO:     Application startup complete.
[2026-01-17 09:58:09] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 09:58:10] INFO:     127.0.0.1:47154 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 09:58:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:58:10] INFO:     127.0.0.1:47182 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
....[2026-01-17 09:58:17] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:17] The server is fired up and ready to roll!
[2026-01-17 09:58:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:58:21] INFO:     127.0.0.1:54684 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 09:58:21] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 09:58:21] INFO:     127.0.0.1:54694 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 09:58:21 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 09:58:21] INFO:     127.0.0.1:54698 - "POST /generate HTTP/1.1" 200 OK
[CI Test Method] TestLlama4.test_gsm8k
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/meta-llama/Llama-4-Scout-17B-16E-Instruct --chat-template llama-4 --tp-size 4 --mem-fraction-static 0.9 --context-length 8192 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 09:58:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 0, #queue-req: 0,
[2026-01-17 09:58:21 TP0] Prefill batch, #new-seq: 39, #new-token: 5248, #cached-token: 24960, token usage: 0.03, #running-req: 1, #queue-req: 0,
[2026-01-17 09:58:21 TP0] Prefill batch, #new-seq: 14, #new-token: 1920, #cached-token: 8960, token usage: 0.20, #running-req: 40, #queue-req: 0,
[2026-01-17 09:58:21 TP0] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.27, #running-req: 54, #queue-req: 70,
[2026-01-17 09:58:24 TP0] Decode batch, #running-req: 58, #token: 9600, token usage: 0.32, npu graph: False, gen throughput (token/s): 3.81, #queue-req: 70,
[2026-01-17 09:58:25] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:04<13:33,  4.09s/it][2026-01-17 09:58:25] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:25] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.37, #running-req: 57, #queue-req: 70,
[2026-01-17 09:58:25 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.38, #running-req: 56, #queue-req: 70,
[2026-01-17 09:58:25] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:04<02:50,  1.15it/s][2026-01-17 09:58:26 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.39, #running-req: 57, #queue-req: 70,
[2026-01-17 09:58:26] INFO:     127.0.0.1:54726 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:04<02:26,  1.33it/s][2026-01-17 09:58:26] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:26 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.40, #running-req: 57, #queue-req: 70,
[2026-01-17 09:58:26] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:05<01:35,  2.02it/s][2026-01-17 09:58:26] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:26] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:05<01:23,  2.29it/s]
  4%|▍         | 9/200 [00:05<00:59,  3.22it/s][2026-01-17 09:58:27] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:27 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.41, #running-req: 54, #queue-req: 72,
[2026-01-17 09:58:27 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.42, #running-req: 55, #queue-req: 72,
[2026-01-17 09:58:27] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:06<00:55,  3.39it/s][2026-01-17 09:58:27 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.43, #running-req: 55, #queue-req: 72,
[2026-01-17 09:58:27] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:06<00:57,  3.28it/s][2026-01-17 09:58:27] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:27] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:27 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.42, #running-req: 55, #queue-req: 72,
[2026-01-17 09:58:28 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.42, #running-req: 54, #queue-req: 72,
[2026-01-17 09:58:28] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:06<00:45,  4.11it/s][2026-01-17 09:58:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.44, #running-req: 55, #queue-req: 72,
[2026-01-17 09:58:28] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:07<00:47,  3.84it/s][2026-01-17 09:58:28] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.43, #running-req: 55, #queue-req: 72,
[2026-01-17 09:58:28] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:07<00:36,  4.99it/s][2026-01-17 09:58:28 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.43, #running-req: 54, #queue-req: 72,
[2026-01-17 09:58:29 TP0] Decode batch, #running-req: 56, #token: 13312, token usage: 0.45, npu graph: False, gen throughput (token/s): 502.82, #queue-req: 72,
[2026-01-17 09:58:29] INFO:     127.0.0.1:55028 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:07<00:47,  3.82it/s][2026-01-17 09:58:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.44, #running-req: 55, #queue-req: 72,
[2026-01-17 09:58:29] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:08<00:43,  4.11it/s][2026-01-17 09:58:29 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.43, #running-req: 55, #queue-req: 71,
[2026-01-17 09:58:29] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:08<00:40,  4.42it/s][2026-01-17 09:58:29 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.43, #running-req: 56, #queue-req: 71,
[2026-01-17 09:58:30] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:30] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:08<00:45,  3.89it/s]
 12%|█▏        | 23/200 [00:08<00:39,  4.51it/s][2026-01-17 09:58:30] INFO:     127.0.0.1:55016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:30 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.42, #running-req: 55, #queue-req: 70,
[2026-01-17 09:58:30 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.43, #running-req: 57, #queue-req: 70,
[2026-01-17 09:58:30] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:08<00:36,  4.83it/s][2026-01-17 09:58:30 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.42, #running-req: 57, #queue-req: 70,
[2026-01-17 09:58:30] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:30] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:30] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:09<00:34,  4.99it/s]
 14%|█▍        | 28/200 [00:09<00:19,  8.81it/s]
 14%|█▍        | 28/200 [00:09<00:19,  8.81it/s][2026-01-17 09:58:30 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.41, #running-req: 55, #queue-req: 70,
[2026-01-17 09:58:31] INFO:     127.0.0.1:55120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:31 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.42, #running-req: 57, #queue-req: 69,
[2026-01-17 09:58:31] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:09<00:28,  5.92it/s][2026-01-17 09:58:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.42, #running-req: 58, #queue-req: 69,
[2026-01-17 09:58:31] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:10<00:34,  4.95it/s][2026-01-17 09:58:31] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:31 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.40, #running-req: 58, #queue-req: 68,
[2026-01-17 09:58:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.41, #running-req: 59, #queue-req: 68,
[2026-01-17 09:58:31] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:10<00:32,  5.19it/s][2026-01-17 09:58:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.41, #running-req: 59, #queue-req: 68,
[2026-01-17 09:58:32] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:32] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:32] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:10<00:42,  3.91it/s]
 18%|█▊        | 36/200 [00:10<00:36,  4.51it/s]
 18%|█▊        | 36/200 [00:10<00:36,  4.51it/s][2026-01-17 09:58:32] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:32 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.39, #running-req: 57, #queue-req: 68,
[2026-01-17 09:58:32] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:11<00:29,  5.48it/s][2026-01-17 09:58:32 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.39, #running-req: 58, #queue-req: 67,
[2026-01-17 09:58:33] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:11<00:36,  4.44it/s][2026-01-17 09:58:33 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.40, #running-req: 60, #queue-req: 66,
[2026-01-17 09:58:33] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:33] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:12<00:47,  3.34it/s]
 20%|██        | 41/200 [00:12<00:47,  3.33it/s][2026-01-17 09:58:33 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.41, #running-req: 60, #queue-req: 66,
[2026-01-17 09:58:33] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:33] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:12<00:43,  3.63it/s]
 22%|██▏       | 43/200 [00:12<00:32,  4.85it/s][2026-01-17 09:58:33 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.40, #running-req: 60, #queue-req: 66,
[2026-01-17 09:58:34] INFO:     127.0.0.1:54868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:34] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:12<00:31,  5.01it/s]
 22%|██▎       | 45/200 [00:12<00:24,  6.35it/s][2026-01-17 09:58:34 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.40, #running-req: 60, #queue-req: 66,
[2026-01-17 09:58:34 TP0] Decode batch, #running-req: 60, #token: 12288, token usage: 0.41, npu graph: False, gen throughput (token/s): 471.52, #queue-req: 66,
[2026-01-17 09:58:34] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:12<00:33,  4.64it/s][2026-01-17 09:58:34] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:34 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.42, #running-req: 61, #queue-req: 66,
[2026-01-17 09:58:34 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.42, #running-req: 61, #queue-req: 66,
[2026-01-17 09:58:34] INFO:     127.0.0.1:54982 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:13<00:30,  5.01it/s][2026-01-17 09:58:34 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.42, #running-req: 61, #queue-req: 66,
[2026-01-17 09:58:35] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:13<00:40,  3.76it/s][2026-01-17 09:58:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.43, #running-req: 61, #queue-req: 66,
[2026-01-17 09:58:35] INFO:     127.0.0.1:55416 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:14<00:42,  3.51it/s][2026-01-17 09:58:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.44, #running-req: 61, #queue-req: 66,
[2026-01-17 09:58:35] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:35] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:14<00:38,  3.86it/s]
 26%|██▌       | 52/200 [00:14<00:27,  5.30it/s][2026-01-17 09:58:35 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.44, #running-req: 60, #queue-req: 67,
[2026-01-17 09:58:36] INFO:     127.0.0.1:54778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:36] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:14<00:27,  5.40it/s]
 27%|██▋       | 54/200 [00:14<00:21,  6.84it/s][2026-01-17 09:58:36 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.43, #running-req: 59, #queue-req: 67,
[2026-01-17 09:58:36] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:14<00:30,  4.72it/s][2026-01-17 09:58:36] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.45, #running-req: 60, #queue-req: 67,
[2026-01-17 09:58:36] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:15<00:23,  6.03it/s][2026-01-17 09:58:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.45, #running-req: 59, #queue-req: 68,
[2026-01-17 09:58:36] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:15<00:23,  5.94it/s][2026-01-17 09:58:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.45, #running-req: 59, #queue-req: 68,
[2026-01-17 09:58:37] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:15<00:24,  5.86it/s][2026-01-17 09:58:37 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.44, #running-req: 59, #queue-req: 67,
[2026-01-17 09:58:37] INFO:     127.0.0.1:54748 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:15<00:24,  5.80it/s][2026-01-17 09:58:37 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.45, #running-req: 60, #queue-req: 67,
[2026-01-17 09:58:37] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:15<00:24,  5.71it/s][2026-01-17 09:58:37 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.45, #running-req: 60, #queue-req: 67,
[2026-01-17 09:58:37] INFO:     127.0.0.1:55280 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:16<00:37,  3.69it/s][2026-01-17 09:58:37] INFO:     127.0.0.1:55588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:37 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.45, #running-req: 60, #queue-req: 67,
[2026-01-17 09:58:38 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.46, #running-req: 60, #queue-req: 67,
[2026-01-17 09:58:38] INFO:     127.0.0.1:55338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:38] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:16<00:31,  4.33it/s]
 32%|███▎      | 65/200 [00:16<00:23,  5.65it/s][2026-01-17 09:58:38 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.45, #running-req: 59, #queue-req: 67,
[2026-01-17 09:58:38] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:38] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:38] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:16<00:24,  5.58it/s]
 34%|███▍      | 68/200 [00:16<00:14,  8.89it/s]
 34%|███▍      | 68/200 [00:16<00:14,  8.89it/s][2026-01-17 09:58:38 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.43, #running-req: 58, #queue-req: 67,
[2026-01-17 09:58:38] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:17<00:24,  5.42it/s][2026-01-17 09:58:39 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.45, #running-req: 60, #queue-req: 66,
[2026-01-17 09:58:39 TP0] Decode batch, #running-req: 62, #token: 13440, token usage: 0.45, npu graph: False, gen throughput (token/s): 465.43, #queue-req: 66,
[2026-01-17 09:58:39] INFO:     127.0.0.1:55412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:39] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:17<00:30,  4.25it/s]
 36%|███▌      | 71/200 [00:17<00:29,  4.33it/s][2026-01-17 09:58:39 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.45, #running-req: 60, #queue-req: 66,
[2026-01-17 09:58:39] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:18<00:28,  4.50it/s][2026-01-17 09:58:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.46, #running-req: 61, #queue-req: 66,
[2026-01-17 09:58:39] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:18<00:26,  4.71it/s][2026-01-17 09:58:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.46, #running-req: 61, #queue-req: 65,
[2026-01-17 09:58:40] INFO:     127.0.0.1:55368 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:18<00:36,  3.44it/s][2026-01-17 09:58:40] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:40] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:40 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.45, #running-req: 61, #queue-req: 63,
[2026-01-17 09:58:40 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.47, #running-req: 61, #queue-req: 62,
[2026-01-17 09:58:40] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:19<00:30,  4.02it/s][2026-01-17 09:58:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.47, #running-req: 61, #queue-req: 61,
[2026-01-17 09:58:41] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:19<00:28,  4.26it/s][2026-01-17 09:58:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.46, #running-req: 61, #queue-req: 60,
[2026-01-17 09:58:41] INFO:     127.0.0.1:55540 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:19<00:31,  3.82it/s][2026-01-17 09:58:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.47, #running-req: 61, #queue-req: 59,
[2026-01-17 09:58:41] INFO:     127.0.0.1:55558 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:20<00:34,  3.53it/s][2026-01-17 09:58:41] INFO:     127.0.0.1:55562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:41 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.46, #running-req: 61, #queue-req: 57,
[2026-01-17 09:58:42 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.47, #running-req: 62, #queue-req: 56,
[2026-01-17 09:58:42] INFO:     127.0.0.1:55420 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:20<00:28,  4.14it/s][2026-01-17 09:58:42 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.46, #running-req: 62, #queue-req: 55,
[2026-01-17 09:58:42] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:42] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:42] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:21<00:36,  3.23it/s]
 42%|████▎     | 85/200 [00:21<00:28,  4.10it/s]
 42%|████▎     | 85/200 [00:21<00:28,  4.10it/s][2026-01-17 09:58:42 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.45, #running-req: 60, #queue-req: 52,
[2026-01-17 09:58:42] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:42] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:21<00:26,  4.29it/s]
 44%|████▎     | 87/200 [00:21<00:20,  5.41it/s][2026-01-17 09:58:43 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.45, #running-req: 61, #queue-req: 50,
[2026-01-17 09:58:43] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:21<00:20,  5.44it/s][2026-01-17 09:58:43 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.46, #running-req: 62, #queue-req: 48,
[2026-01-17 09:58:43] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:43] INFO:     127.0.0.1:55694 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:21<00:20,  5.45it/s]
 45%|████▌     | 90/200 [00:21<00:16,  6.75it/s][2026-01-17 09:58:43 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.45, #running-req: 62, #queue-req: 46,
[2026-01-17 09:58:43] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:22<00:21,  5.08it/s][2026-01-17 09:58:43 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.46, #running-req: 63, #queue-req: 44,
[2026-01-17 09:58:44] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:22<00:25,  4.26it/s][2026-01-17 09:58:44] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:44 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.45, #running-req: 64, #queue-req: 42,
[2026-01-17 09:58:44 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.46, #running-req: 65, #queue-req: 41,
[2026-01-17 09:58:44 TP0] Decode batch, #running-req: 66, #token: 14720, token usage: 0.49, npu graph: False, gen throughput (token/s): 476.77, #queue-req: 41,
[2026-01-17 09:58:44] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:23<00:28,  3.73it/s][2026-01-17 09:58:44] INFO:     127.0.0.1:55530 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [00:23<00:26,  4.04it/s][2026-01-17 09:58:45] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:45] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:23<00:23,  4.34it/s]
 48%|████▊     | 97/200 [00:23<00:17,  5.79it/s][2026-01-17 09:58:45] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:45 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.48, #running-req: 62, #queue-req: 39,
[2026-01-17 09:58:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.48, #running-req: 63, #queue-req: 38,
[2026-01-17 09:58:45] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:23<00:17,  5.65it/s][2026-01-17 09:58:45 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.48, #running-req: 63, #queue-req: 36,
[2026-01-17 09:58:45] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:24<00:21,  4.64it/s][2026-01-17 09:58:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.48, #running-req: 64, #queue-req: 35,
[2026-01-17 09:58:46] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:46] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:46] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [00:24<00:24,  4.06it/s]
 52%|█████▏    | 103/200 [00:24<00:16,  5.82it/s]
 52%|█████▏    | 103/200 [00:24<00:16,  5.82it/s][2026-01-17 09:58:46] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:46] INFO:     127.0.0.1:55630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:46 TP0] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 2560, token usage: 0.44, #running-req: 62, #queue-req: 31,
[2026-01-17 09:58:46 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.46, #running-req: 64, #queue-req: 29,
[2026-01-17 09:58:46] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:25<00:16,  5.79it/s][2026-01-17 09:58:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.47, #running-req: 65, #queue-req: 28,
[2026-01-17 09:58:46] INFO:     127.0.0.1:55768 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:25<00:16,  5.78it/s][2026-01-17 09:58:46 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.47, #running-req: 65, #queue-req: 26,
[2026-01-17 09:58:46] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:25<00:15,  5.79it/s][2026-01-17 09:58:47 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.47, #running-req: 66, #queue-req: 25,
[2026-01-17 09:58:47] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:25<00:15,  5.79it/s][2026-01-17 09:58:47 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.47, #running-req: 66, #queue-req: 24,
[2026-01-17 09:58:47] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:25<00:15,  5.82it/s][2026-01-17 09:58:47 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.47, #running-req: 66, #queue-req: 23,
[2026-01-17 09:58:47] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:26<00:19,  4.57it/s][2026-01-17 09:58:47] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:47 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.48, #running-req: 66, #queue-req: 22,
[2026-01-17 09:58:47] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:26<00:14,  6.08it/s][2026-01-17 09:58:47 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.48, #running-req: 65, #queue-req: 20,
[2026-01-17 09:58:48] INFO:     127.0.0.1:55648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:48] INFO:     127.0.0.1:55794 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:26<00:14,  6.04it/s]
 57%|█████▊    | 115/200 [00:26<00:11,  7.48it/s][2026-01-17 09:58:48 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.47, #running-req: 65, #queue-req: 18,
[2026-01-17 09:58:48] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:48] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:26<00:11,  7.02it/s]
 58%|█████▊    | 117/200 [00:26<00:10,  8.25it/s][2026-01-17 09:58:48 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.47, #running-req: 65, #queue-req: 16,
[2026-01-17 09:58:48] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:26<00:10,  7.57it/s][2026-01-17 09:58:48 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.47, #running-req: 66, #queue-req: 15,
[2026-01-17 09:58:48] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:48] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:27<00:16,  4.92it/s]
 60%|██████    | 120/200 [00:27<00:16,  4.78it/s][2026-01-17 09:58:48 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.45, #running-req: 65, #queue-req: 12,
[2026-01-17 09:58:48] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:27<00:15,  4.98it/s][2026-01-17 09:58:49 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.47, #running-req: 67, #queue-req: 11,
[2026-01-17 09:58:49] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:27<00:20,  3.89it/s][2026-01-17 09:58:49 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.48, #running-req: 67, #queue-req: 10,
[2026-01-17 09:58:49] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:28<00:18,  4.21it/s][2026-01-17 09:58:49 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.48, #running-req: 67, #queue-req: 9,
[2026-01-17 09:58:49 TP0] Decode batch, #running-req: 68, #token: 14464, token usage: 0.48, npu graph: False, gen throughput (token/s): 499.22, #queue-req: 9,
[2026-01-17 09:58:49] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [00:28<00:20,  3.79it/s][2026-01-17 09:58:49 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.48, #running-req: 67, #queue-req: 8,
[2026-01-17 09:58:50] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:28<00:22,  3.26it/s][2026-01-17 09:58:50 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.49, #running-req: 67, #queue-req: 7,
[2026-01-17 09:58:50] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:29<00:25,  2.91it/s][2026-01-17 09:58:50 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.51, #running-req: 67, #queue-req: 6,
[2026-01-17 09:58:51] INFO:     127.0.0.1:55780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:51] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:29<00:28,  2.56it/s]
 64%|██████▍   | 128/200 [00:29<00:23,  3.04it/s][2026-01-17 09:58:51] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:51 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.52, #running-req: 66, #queue-req: 5,
[2026-01-17 09:58:51 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.52, #running-req: 66, #queue-req: 4,
[2026-01-17 09:58:51] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:30<00:19,  3.66it/s][2026-01-17 09:58:51 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.52, #running-req: 66, #queue-req: 3,
[2026-01-17 09:58:52] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:52] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:52] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:52] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:52] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [00:30<00:22,  3.03it/s]
 68%|██████▊   | 135/200 [00:30<00:10,  6.44it/s]
 68%|██████▊   | 135/200 [00:30<00:10,  6.44it/s]
 68%|██████▊   | 135/200 [00:30<00:10,  6.44it/s]
 68%|██████▊   | 135/200 [00:30<00:10,  6.44it/s][2026-01-17 09:58:52 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.49, #running-req: 62, #queue-req: 0,
[2026-01-17 09:58:52] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [00:31<00:11,  5.56it/s][2026-01-17 09:58:52] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:31<00:12,  4.85it/s][2026-01-17 09:58:53] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:31<00:12,  4.96it/s][2026-01-17 09:58:53] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:31<00:12,  5.07it/s][2026-01-17 09:58:53] INFO:     127.0.0.1:55684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:53] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:31<00:09,  6.22it/s][2026-01-17 09:58:53] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:32<00:09,  6.03it/s][2026-01-17 09:58:53] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:53] INFO:     127.0.0.1:44770 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:32<00:10,  5.30it/s]
 72%|███████▏  | 144/200 [00:32<00:09,  5.99it/s][2026-01-17 09:58:54 TP0] Decode batch, #running-req: 56, #token: 13824, token usage: 0.46, npu graph: False, gen throughput (token/s): 616.86, #queue-req: 0,
[2026-01-17 09:58:54] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:32<00:11,  4.81it/s][2026-01-17 09:58:54] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:54] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:54] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:33<00:10,  5.10it/s][2026-01-17 09:58:55] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:33<00:10,  4.76it/s][2026-01-17 09:58:55] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:33<00:11,  4.50it/s][2026-01-17 09:58:55] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:55] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:34<00:09,  5.24it/s][2026-01-17 09:58:55] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:55] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:34<00:07,  5.81it/s][2026-01-17 09:58:56] INFO:     127.0.0.1:55700 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:56] INFO:     127.0.0.1:44778 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:34<00:08,  5.26it/s][2026-01-17 09:58:56] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:35<00:11,  3.89it/s][2026-01-17 09:58:57] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:57] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:35<00:11,  3.60it/s]
 80%|███████▉  | 159/200 [00:35<00:09,  4.19it/s][2026-01-17 09:58:57] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:57] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:57] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:36<00:07,  4.94it/s]
 81%|████████  | 162/200 [00:36<00:05,  6.48it/s][2026-01-17 09:58:57 TP0] Decode batch, #running-req: 38, #token: 10624, token usage: 0.36, npu graph: False, gen throughput (token/s): 528.03, #queue-req: 0,
[2026-01-17 09:58:57] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:57] INFO:     127.0.0.1:55360 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:36<00:05,  6.29it/s]
 82%|████████▏ | 164/200 [00:36<00:04,  7.31it/s][2026-01-17 09:58:57] INFO:     127.0.0.1:55344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:57] INFO:     127.0.0.1:44806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:57] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:57] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:36<00:03,  9.58it/s]
 84%|████████▍ | 168/200 [00:36<00:02, 12.87it/s][2026-01-17 09:58:58] INFO:     127.0.0.1:55660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:58] INFO:     127.0.0.1:44812 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:36<00:02, 11.03it/s][2026-01-17 09:58:58] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:58] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:36<00:02,  9.90it/s][2026-01-17 09:58:58] INFO:     127.0.0.1:55696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:59] INFO:     127.0.0.1:55862 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:37<00:04,  5.97it/s][2026-01-17 09:58:59] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:59] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:37<00:04,  5.86it/s][2026-01-17 09:58:59] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:58:59] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:38<00:04,  5.12it/s][2026-01-17 09:59:00] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:59:00] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:59:00] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:38<00:03,  5.65it/s]
 90%|█████████ | 181/200 [00:38<00:02,  7.01it/s][2026-01-17 09:59:00] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:59:00] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:39<00:02,  6.20it/s]
 92%|█████████▏| 183/200 [00:39<00:02,  6.57it/s][2026-01-17 09:59:00] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:39<00:02,  6.41it/s][2026-01-17 09:59:00] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:59:00] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:39<00:02,  5.66it/s]
 93%|█████████▎| 186/200 [00:39<00:02,  6.29it/s][2026-01-17 09:59:01 TP0] Decode batch, #running-req: 14, #token: 4992, token usage: 0.17, npu graph: False, gen throughput (token/s): 281.41, #queue-req: 0,
[2026-01-17 09:59:01] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:39<00:02,  4.68it/s][2026-01-17 09:59:02] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:41<00:05,  2.19it/s][2026-01-17 09:59:02] INFO:     127.0.0.1:44792 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:41<00:04,  2.58it/s][2026-01-17 09:59:03] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:41<00:03,  2.83it/s][2026-01-17 09:59:03] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:59:03] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:42<00:04,  2.15it/s]
 96%|█████████▌| 192/200 [00:42<00:03,  2.33it/s][2026-01-17 09:59:04 TP0] Decode batch, #running-req: 8, #token: 3584, token usage: 0.12, npu graph: False, gen throughput (token/s): 131.65, #queue-req: 0,
[2026-01-17 09:59:04] INFO:     127.0.0.1:44802 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:43<00:03,  1.76it/s][2026-01-17 09:59:04] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:59:04] INFO:     127.0.0.1:44846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 09:59:05] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:43<00:01,  2.83it/s][2026-01-17 09:59:06] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:44<00:01,  2.08it/s][2026-01-17 09:59:06] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:45<00:00,  2.28it/s][2026-01-17 09:59:07] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:46<00:00,  1.66it/s][2026-01-17 09:59:08 TP0] Decode batch, #running-req: 1, #token: 1152, token usage: 0.04, npu graph: False, gen throughput (token/s): 41.97, #queue-req: 0,
[2026-01-17 09:59:09] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:47<00:00,  1.17it/s]
100%|██████████| 200/200 [00:47<00:00,  4.18it/s]
/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 218955 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 539.591s

OK
Accuracy: 0.940
Invalid: 0.000
Latency: 47.938 s
Output throughput: 418.959 token/s
metrics={'accuracy': np.float64(0.94), 'invalid': np.float64(0.0), 'latency': 47.937849698995706, 'output_throughput': 418.95913408941993}
Cleaning up process 218955
.
.
End (26/62):
filename='ascend/llm_models/test_ascend_llama4_models.py', elapsed=550, estimated_time=400
.
.

.
.
Begin (27/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_llm_models_grok.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:59:29] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/huihui-ai/grok-2', tokenizer_path='/root/.cache/modelscope/hub/models/huihui-ai/grok-2/tokenizer.tok.json', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=878986660, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/huihui-ai/grok-2', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 09:59:30] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 09:59:41 TP10] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:59:42 TP0] Init torch distributed begin.
[2026-01-17 09:59:42 TP8] Init torch distributed begin.
[2026-01-17 09:59:42 TP7] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:59:43 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:59:43 TP5] Init torch distributed begin.
[2026-01-17 09:59:43 TP11] Init torch distributed begin.
[2026-01-17 09:59:43 TP9] Init torch distributed begin.
[2026-01-17 09:59:43 TP12] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 09:59:43 TP15] Init torch distributed begin.
[2026-01-17 09:59:43 TP4] Init torch distributed begin.
[2026-01-17 09:59:43 TP13] Init torch distributed begin.
[2026-01-17 09:59:43 TP1] Init torch distributed begin.
[2026-01-17 09:59:43 TP2] Init torch distributed begin.
[2026-01-17 09:59:43 TP6] Init torch distributed begin.
[2026-01-17 09:59:43 TP14] Init torch distributed begin.
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[2026-01-17 09:59:44 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP15] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP13] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP14] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP11] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP12] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP10] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP9] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP8] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:44 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 09:59:45 TP10] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP9] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP11] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP13] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP8] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP14] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP12] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP15] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 09:59:45 TP5] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:59:45 TP9] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:59:45 TP10] Load weight begin. avail mem=60.88 GB
[2026-01-17 09:59:45 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:59:45 TP7] Load weight begin. avail mem=61.13 GB
[2026-01-17 09:59:45 TP13] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:59:45 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 09:59:45 TP11] Load weight begin. avail mem=61.13 GB
[2026-01-17 09:59:45 TP8] Load weight begin. avail mem=60.86 GB
[2026-01-17 09:59:45 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 09:59:45 TP4] Load weight begin. avail mem=60.87 GB
[2026-01-17 09:59:45 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 09:59:45 TP12] Load weight begin. avail mem=60.86 GB
[2026-01-17 09:59:45 TP14] Load weight begin. avail mem=60.88 GB
[2026-01-17 09:59:45 TP15] Load weight begin. avail mem=61.13 GB
[2026-01-17 09:59:45 TP6] Load weight begin. avail mem=60.88 GB

Loading safetensors checkpoint shards:   0% Completed | 0/64 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   2% Completed | 1/64 [00:11<12:27, 11.86s/it]

Loading safetensors checkpoint shards:   3% Completed | 2/64 [00:24<12:54, 12.49s/it]

Loading safetensors checkpoint shards:   5% Completed | 3/64 [00:37<12:51, 12.66s/it]

Loading safetensors checkpoint shards:   6% Completed | 4/64 [00:49<12:11, 12.19s/it]

Loading safetensors checkpoint shards:   8% Completed | 5/64 [01:00<11:43, 11.92s/it]

Loading safetensors checkpoint shards:   9% Completed | 6/64 [01:11<11:04, 11.46s/it]

Loading safetensors checkpoint shards:  11% Completed | 7/64 [01:22<10:44, 11.30s/it]

Loading safetensors checkpoint shards:  12% Completed | 8/64 [01:32<10:25, 11.17s/it]

Loading safetensors checkpoint shards:  14% Completed | 9/64 [01:43<10:11, 11.11s/it]

Loading safetensors checkpoint shards:  16% Completed | 10/64 [01:54<09:53, 11.00s/it]

Loading safetensors checkpoint shards:  17% Completed | 11/64 [02:05<09:31, 10.78s/it]

Loading safetensors checkpoint shards:  19% Completed | 12/64 [02:15<09:08, 10.55s/it]

Loading safetensors checkpoint shards:  20% Completed | 13/64 [02:26<09:07, 10.73s/it]

Loading safetensors checkpoint shards:  22% Completed | 14/64 [02:37<08:58, 10.77s/it]

Loading safetensors checkpoint shards:  23% Completed | 15/64 [02:48<09:03, 11.09s/it]

Loading safetensors checkpoint shards:  25% Completed | 16/64 [03:02<09:29, 11.87s/it]

Loading safetensors checkpoint shards:  27% Completed | 17/64 [03:16<09:53, 12.63s/it]

Loading safetensors checkpoint shards:  28% Completed | 18/64 [03:31<10:08, 13.22s/it]

Loading safetensors checkpoint shards:  30% Completed | 19/64 [03:45<10:03, 13.42s/it]

Loading safetensors checkpoint shards:  31% Completed | 20/64 [04:12<12:46, 17.41s/it]

Loading safetensors checkpoint shards:  33% Completed | 21/64 [04:25<11:40, 16.30s/it]

Loading safetensors checkpoint shards:  34% Completed | 22/64 [04:39<10:57, 15.65s/it]

Loading safetensors checkpoint shards:  36% Completed | 23/64 [04:54<10:23, 15.20s/it]

Loading safetensors checkpoint shards:  38% Completed | 24/64 [05:08<09:53, 14.84s/it]

Loading safetensors checkpoint shards:  39% Completed | 25/64 [05:22<09:27, 14.55s/it]

Loading safetensors checkpoint shards:  41% Completed | 26/64 [05:36<09:15, 14.63s/it]

Loading safetensors checkpoint shards:  42% Completed | 27/64 [05:50<08:51, 14.38s/it]

Loading safetensors checkpoint shards:  44% Completed | 28/64 [06:04<08:31, 14.22s/it]

Loading safetensors checkpoint shards:  45% Completed | 29/64 [06:19<08:24, 14.40s/it]

Loading safetensors checkpoint shards:  47% Completed | 30/64 [06:33<08:09, 14.40s/it]

Loading safetensors checkpoint shards:  48% Completed | 31/64 [06:48<07:55, 14.41s/it]

Loading safetensors checkpoint shards:  50% Completed | 32/64 [07:02<07:39, 14.37s/it]

Loading safetensors checkpoint shards:  52% Completed | 33/64 [07:15<07:13, 13.98s/it]

Loading safetensors checkpoint shards:  53% Completed | 34/64 [07:30<07:07, 14.26s/it]

Loading safetensors checkpoint shards:  55% Completed | 35/64 [07:43<06:45, 13.98s/it]

Loading safetensors checkpoint shards:  56% Completed | 36/64 [07:57<06:34, 14.07s/it]

Loading safetensors checkpoint shards:  58% Completed | 37/64 [08:11<06:16, 13.96s/it]

Loading safetensors checkpoint shards:  59% Completed | 38/64 [08:25<05:58, 13.80s/it]

Loading safetensors checkpoint shards:  61% Completed | 39/64 [08:39<05:47, 13.89s/it]

Loading safetensors checkpoint shards:  62% Completed | 40/64 [08:52<05:30, 13.77s/it]

Loading safetensors checkpoint shards:  64% Completed | 41/64 [09:06<05:16, 13.74s/it]

Loading safetensors checkpoint shards:  66% Completed | 42/64 [09:20<05:02, 13.77s/it]

Loading safetensors checkpoint shards:  67% Completed | 43/64 [09:33<04:44, 13.55s/it]

Loading safetensors checkpoint shards:  69% Completed | 44/64 [09:46<04:32, 13.61s/it]

Loading safetensors checkpoint shards:  70% Completed | 45/64 [10:01<04:22, 13.81s/it]

Loading safetensors checkpoint shards:  72% Completed | 46/64 [10:14<04:08, 13.78s/it]

Loading safetensors checkpoint shards:  73% Completed | 47/64 [10:29<03:55, 13.86s/it]

Loading safetensors checkpoint shards:  75% Completed | 48/64 [10:43<03:43, 13.98s/it]

Loading safetensors checkpoint shards:  77% Completed | 49/64 [10:56<03:27, 13.85s/it]

Loading safetensors checkpoint shards:  78% Completed | 50/64 [11:10<03:14, 13.87s/it]

Loading safetensors checkpoint shards:  80% Completed | 51/64 [11:24<03:01, 13.95s/it]

Loading safetensors checkpoint shards:  81% Completed | 52/64 [11:39<02:48, 14.03s/it]

Loading safetensors checkpoint shards:  83% Completed | 53/64 [11:52<02:33, 13.92s/it]

Loading safetensors checkpoint shards:  84% Completed | 54/64 [12:06<02:17, 13.79s/it]

Loading safetensors checkpoint shards:  86% Completed | 55/64 [12:20<02:04, 13.83s/it]

Loading safetensors checkpoint shards:  88% Completed | 56/64 [12:34<01:51, 13.93s/it]

Loading safetensors checkpoint shards:  89% Completed | 57/64 [12:48<01:37, 13.87s/it]

Loading safetensors checkpoint shards:  91% Completed | 58/64 [13:03<01:25, 14.30s/it]

Loading safetensors checkpoint shards:  92% Completed | 59/64 [13:17<01:10, 14.15s/it]

Loading safetensors checkpoint shards:  94% Completed | 60/64 [13:31<00:56, 14.08s/it]

Loading safetensors checkpoint shards:  95% Completed | 61/64 [13:45<00:42, 14.18s/it]

Loading safetensors checkpoint shards:  97% Completed | 62/64 [13:59<00:28, 14.16s/it]

Loading safetensors checkpoint shards:  98% Completed | 63/64 [14:14<00:14, 14.36s/it]

Loading safetensors checkpoint shards: 100% Completed | 64/64 [14:28<00:00, 14.14s/it]

Loading safetensors checkpoint shards: 100% Completed | 64/64 [14:28<00:00, 13.56s/it]

[2026-01-17 10:14:15 TP1] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP0] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP7] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP6] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP9] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP8] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP13] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP12] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP5] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP4] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP10] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP11] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP3] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP2] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP15] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:15 TP14] #all_names: 707, #hit_names: 707, #missing_exclude_scales: 0
[2026-01-17 10:14:36 TP12] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.25 GB, mem usage=33.61 GB.
[2026-01-17 10:14:36 TP0] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.20 GB, mem usage=33.61 GB.
[2026-01-17 10:14:36 TP11] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.52 GB, mem usage=33.61 GB.
[2026-01-17 10:14:36 TP2] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.25 GB, mem usage=33.61 GB.
[2026-01-17 10:14:36 TP14] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.27 GB, mem usage=33.61 GB.
[2026-01-17 10:14:36 TP4] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.26 GB, mem usage=33.61 GB.
[2026-01-17 10:14:36 TP3] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.53 GB, mem usage=33.61 GB.
[2026-01-17 10:14:36 TP6] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.27 GB, mem usage=33.61 GB.
[2026-01-17 10:14:36 TP1] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.53 GB, mem usage=33.61 GB.
[2026-01-17 10:14:36 TP10] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.27 GB, mem usage=33.61 GB.
[2026-01-17 10:14:37 TP8] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.25 GB, mem usage=33.61 GB.
[2026-01-17 10:14:37 TP15] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.52 GB, mem usage=33.61 GB.
[2026-01-17 10:14:37 TP13] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.53 GB, mem usage=33.61 GB.
[2026-01-17 10:14:37 TP7] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.52 GB, mem usage=33.61 GB.
[2026-01-17 10:14:37 TP5] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.53 GB, mem usage=33.61 GB.
[2026-01-17 10:14:39 TP9] Load weight end. type=Grok1ForCausalLM, dtype=torch.bfloat16, avail mem=27.53 GB, mem usage=33.61 GB.
[2026-01-17 10:14:39 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 10:14:39 TP0] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP15] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP14] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP13] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP12] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP10] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP11] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP9] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP8] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP5] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP6] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP7] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP4] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP3] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP1] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP2] The available memory for KV cache is 13.77 GB.
[2026-01-17 10:14:39 TP0] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:39 TP4] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:39 TP10] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:39 TP12] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:39 TP6] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:39 TP0] Memory pool end. avail mem=11.16 GB
[2026-01-17 10:14:39 TP4] Memory pool end. avail mem=11.22 GB
[2026-01-17 10:14:39 TP9] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:39 TP15] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:39 TP10] Memory pool end. avail mem=11.23 GB
[2026-01-17 10:14:39 TP12] Memory pool end. avail mem=11.22 GB
[2026-01-17 10:14:39 TP2] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:39 TP6] Memory pool end. avail mem=11.23 GB
[2026-01-17 10:14:39 TP15] Memory pool end. avail mem=11.48 GB
[2026-01-17 10:14:39 TP9] Memory pool end. avail mem=11.50 GB
[2026-01-17 10:14:40 TP2] Memory pool end. avail mem=11.22 GB
[2026-01-17 10:14:40 TP14] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:40 TP1] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:40 TP5] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:40 TP14] Memory pool end. avail mem=11.23 GB
[2026-01-17 10:14:40 TP3] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:40 TP11] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:40 TP13] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:40 TP1] Memory pool end. avail mem=11.50 GB
[2026-01-17 10:14:40 TP5] Memory pool end. avail mem=11.49 GB
[2026-01-17 10:14:40 TP8] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:40 TP7] KV Cache is allocated. #tokens: 451072, K size: 6.88 GB, V size: 6.88 GB
[2026-01-17 10:14:40 TP3] Memory pool end. avail mem=11.50 GB
[2026-01-17 10:14:40 TP11] Memory pool end. avail mem=11.48 GB
[2026-01-17 10:14:40 TP13] Memory pool end. avail mem=11.50 GB
[2026-01-17 10:14:40 TP8] Memory pool end. avail mem=11.22 GB
[2026-01-17 10:14:40 TP7] Memory pool end. avail mem=11.48 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 10:14:41 TP0] max_total_num_tokens=451072, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=11.16 GB
[2026-01-17 10:14:41] INFO:     Started server process [229404]
[2026-01-17 10:14:41] INFO:     Waiting for application startup.
[2026-01-17 10:14:41] INFO:     Application startup complete.
[2026-01-17 10:14:41] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 10:14:42] INFO:     127.0.0.1:38890 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 10:14:42 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
................[2026-01-17 10:14:50] INFO:     127.0.0.1:46714 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
................[2026-01-17 10:14:58] INFO:     127.0.0.1:38902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 10:14:58] The server is fired up and ready to roll!
[2026-01-17 10:15:00 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:15:01] INFO:     127.0.0.1:52176 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 10:15:01] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 10:15:01] INFO:     127.0.0.1:52188 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 10:15:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:15:01] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/huihui-ai/grok-2 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-radix-cache --disable-cuda-graph --tokenizer-path /root/.cache/modelscope/hub/models/huihui-ai/grok-2/tokenizer.tok.json --tp-size 16 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestGrok.test_gsm8k

  0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 10:15:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:15:01 TP0] Prefill batch, #new-seq: 9, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 40,
[2026-01-17 10:15:03 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, token usage: 0.02, #running-req: 10, #queue-req: 30,
[2026-01-17 10:15:05 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, token usage: 0.04, #running-req: 19, #queue-req: 21,
[2026-01-17 10:15:06 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, token usage: 0.06, #running-req: 28, #queue-req: 12,
[2026-01-17 10:15:08 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, token usage: 0.07, #running-req: 37, #queue-req: 3,
[2026-01-17 10:15:09 TP0] Prefill batch, #new-seq: 4, #new-token: 3200, #cached-token: 0, token usage: 0.09, #running-req: 46, #queue-req: 0,
[2026-01-17 10:16:56 TP0] Decode batch, #running-req: 50, #token: 45824, token usage: 0.10, npu graph: False, gen throughput (token/s): 1.51, #queue-req: 0,
[2026-01-17 10:17:36] INFO:     127.0.0.1:52422 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 1/50 [02:35<2:06:43, 155.18s/it][2026-01-17 10:17:54] INFO:     127.0.0.1:52206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:17:54] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 2/50 [02:52<59:11, 74.00s/it]
  6%|▌         | 3/50 [02:52<25:50, 32.99s/it][2026-01-17 10:18:11] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 4/50 [03:09<21:35, 28.17s/it][2026-01-17 10:18:14] INFO:     127.0.0.1:52550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:18:14] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 5/50 [03:12<15:23, 20.52s/it]
 12%|█▏        | 6/50 [03:12<08:34, 11.68s/it][2026-01-17 10:18:24] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 7/50 [03:22<08:00, 11.18s/it][2026-01-17 10:18:33] INFO:     127.0.0.1:52216 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 8/50 [03:31<07:24, 10.58s/it][2026-01-17 10:18:39] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 9/50 [03:37<06:21,  9.29s/it][2026-01-17 10:18:42] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 10/50 [03:40<04:59,  7.50s/it][2026-01-17 10:18:44] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 11/50 [03:43<03:59,  6.13s/it][2026-01-17 10:18:52] INFO:     127.0.0.1:52444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:18:52] INFO:     127.0.0.1:52652 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 12/50 [03:51<04:15,  6.71s/it]
 26%|██▌       | 13/50 [03:51<03:24,  5.52s/it][2026-01-17 10:19:03 TP0] Decode batch, #running-req: 37, #token: 37632, token usage: 0.08, npu graph: False, gen throughput (token/s): 14.42, #queue-req: 0,
[2026-01-17 10:19:16] INFO:     127.0.0.1:52378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:19:16] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 14/50 [04:14<05:58,  9.97s/it]
 30%|███       | 15/50 [04:14<06:16, 10.75s/it][2026-01-17 10:19:34] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 16/50 [04:32<06:57, 12.29s/it][2026-01-17 10:19:51] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 17/50 [04:49<07:26, 13.53s/it][2026-01-17 10:19:56] INFO:     127.0.0.1:52362 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 18/50 [04:54<06:00, 11.26s/it][2026-01-17 10:19:58] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 19/50 [04:56<04:33,  8.83s/it][2026-01-17 10:20:02] INFO:     127.0.0.1:52262 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 20/50 [05:01<03:47,  7.60s/it][2026-01-17 10:20:05] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 21/50 [05:03<02:56,  6.08s/it][2026-01-17 10:20:11] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 22/50 [05:09<02:52,  6.16s/it][2026-01-17 10:20:15] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 23/50 [05:13<02:29,  5.55s/it][2026-01-17 10:20:25] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 24/50 [05:23<02:58,  6.87s/it][2026-01-17 10:20:31] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 25/50 [05:29<02:44,  6.59s/it][2026-01-17 10:20:33] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 26/50 [05:31<02:03,  5.15s/it][2026-01-17 10:20:35 TP0] Decode batch, #running-req: 25, #token: 24576, token usage: 0.05, npu graph: False, gen throughput (token/s): 13.92, #queue-req: 0,
[2026-01-17 10:20:36] INFO:     127.0.0.1:52314 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 27/50 [05:34<01:45,  4.61s/it][2026-01-17 10:20:41] INFO:     127.0.0.1:52660 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 28/50 [05:40<01:44,  4.75s/it][2026-01-17 10:21:08] INFO:     127.0.0.1:52306 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 29/50 [06:06<03:55, 11.21s/it][2026-01-17 10:21:11] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 30/50 [06:09<02:56,  8.84s/it][2026-01-17 10:21:16] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:21:16] INFO:     127.0.0.1:52624 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 31/50 [06:14<02:24,  7.58s/it]
 64%|██████▍   | 32/50 [06:14<01:32,  5.16s/it][2026-01-17 10:21:17] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 33/50 [06:15<01:11,  4.23s/it][2026-01-17 10:21:18] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:21:18] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 34/50 [06:17<00:55,  3.47s/it]
 70%|███████   | 35/50 [06:17<00:34,  2.27s/it][2026-01-17 10:21:19] INFO:     127.0.0.1:52642 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 36/50 [06:18<00:28,  2.02s/it][2026-01-17 10:21:27] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 37/50 [06:25<00:44,  3.44s/it][2026-01-17 10:21:29] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 38/50 [06:28<00:36,  3.07s/it][2026-01-17 10:21:30] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 39/50 [06:28<00:27,  2.50s/it][2026-01-17 10:21:32 TP0] Decode batch, #running-req: 11, #token: 11520, token usage: 0.03, npu graph: False, gen throughput (token/s): 13.35, #queue-req: 0,
[2026-01-17 10:21:35] INFO:     127.0.0.1:52368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:21:35] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 40/50 [06:33<00:30,  3.09s/it]
 82%|████████▏ | 41/50 [06:33<00:24,  2.74s/it][2026-01-17 10:21:39] INFO:     127.0.0.1:52326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:21:39] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 42/50 [06:37<00:24,  3.11s/it]
 86%|████████▌ | 43/50 [06:37<00:19,  2.71s/it][2026-01-17 10:21:43] INFO:     127.0.0.1:52632 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 44/50 [06:41<00:17,  2.89s/it][2026-01-17 10:21:43] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 45/50 [06:41<00:11,  2.34s/it][2026-01-17 10:21:58 TP0] Decode batch, #running-req: 5, #token: 5760, token usage: 0.01, npu graph: False, gen throughput (token/s): 9.87, #queue-req: 0,
[2026-01-17 10:21:58] INFO:     127.0.0.1:52280 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 46/50 [06:57<00:22,  5.67s/it][2026-01-17 10:22:03] INFO:     127.0.0.1:52562 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 47/50 [07:01<00:15,  5.32s/it][2026-01-17 10:22:04] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 48/50 [07:02<00:08,  4.20s/it][2026-01-17 10:22:07] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 49/50 [07:05<00:03,  3.77s/it][2026-01-17 10:22:11 TP0] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, npu graph: False, gen throughput (token/s): 6.82, #queue-req: 0,
[2026-01-17 10:22:12] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 50/50 [07:11<00:00,  4.26s/it]
100%|██████████| 50/50 [07:11<00:00,  8.62s/it]
.
----------------------------------------------------------------------
Ran 1 test in 1373.664s

OK
Accuracy: 0.920
Invalid: 0.000
Latency: 431.332 s
Output throughput: 13.391 token/s
.
.
End (27/62):
filename='ascend/llm_models/test_ascend_llm_models_grok.py', elapsed=1385, estimated_time=400
.
.

.
.
Begin (28/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_llm_models_XVERSE.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 10:22:34] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/xverse/XVERSE-MoE-A36B', tokenizer_path='/root/.cache/modelscope/hub/models/xverse/XVERSE-MoE-A36B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=2048, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=782792031, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/xverse/XVERSE-MoE-A36B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 10:22:35] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 10:22:46 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 10:22:46 TP9] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 10:22:47 TP7] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 10:22:47 TP15] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 10:22:47 TP14] Init torch distributed begin.
[2026-01-17 10:22:47 TP13] Init torch distributed begin.
[2026-01-17 10:22:47 TP8] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 10:22:47 TP3] Init torch distributed begin.
[2026-01-17 10:22:47 TP5] Init torch distributed begin.
[2026-01-17 10:22:47 TP4] Init torch distributed begin.
[2026-01-17 10:22:47 TP0] Init torch distributed begin.
[2026-01-17 10:22:47 TP1] Init torch distributed begin.
[2026-01-17 10:22:47 TP11] Init torch distributed begin.
[2026-01-17 10:22:47 TP6] Init torch distributed begin.
[2026-01-17 10:22:48 TP10] Init torch distributed begin.
[2026-01-17 10:22:48 TP12] Init torch distributed begin.
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[2026-01-17 10:22:50 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP15] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP13] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP14] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP12] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP11] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP9] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP10] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP8] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:22:50 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 10:22:50 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP14] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP8] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP13] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP9] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP10] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP15] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP12] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP11] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:50 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:22:51 TP9] Load weight begin. avail mem=61.14 GB
[2026-01-17 10:22:51 TP13] Load weight begin. avail mem=61.14 GB
[2026-01-17 10:22:51 TP5] Load weight begin. avail mem=61.14 GB
[2026-01-17 10:22:51 TP7] Load weight begin. avail mem=61.13 GB
[2026-01-17 10:22:51 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 10:22:51 TP8] Load weight begin. avail mem=60.86 GB
[2026-01-17 10:22:51 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 10:22:51 TP10] Load weight begin. avail mem=60.88 GB
[2026-01-17 10:22:51 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 10:22:51 TP6] Load weight begin. avail mem=60.88 GB
[2026-01-17 10:22:51 TP14] Load weight begin. avail mem=60.88 GB
[2026-01-17 10:22:51 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 10:22:51 TP4] Load weight begin. avail mem=60.86 GB
[2026-01-17 10:22:51 TP11] Load weight begin. avail mem=61.13 GB
[2026-01-17 10:22:51 TP12] Load weight begin. avail mem=60.86 GB
[2026-01-17 10:22:51 TP15] Load weight begin. avail mem=61.13 GB

Loading safetensors checkpoint shards:   0% Completed | 0/51 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   2% Completed | 1/51 [00:11<09:51, 11.84s/it]

Loading safetensors checkpoint shards:   4% Completed | 2/51 [00:22<09:14, 11.31s/it]

Loading safetensors checkpoint shards:   6% Completed | 3/51 [00:34<09:09, 11.45s/it]

Loading safetensors checkpoint shards:   8% Completed | 4/51 [00:46<09:05, 11.60s/it]

Loading safetensors checkpoint shards:  10% Completed | 5/51 [00:57<08:51, 11.55s/it]

Loading safetensors checkpoint shards:  12% Completed | 6/51 [01:09<08:46, 11.70s/it]

Loading safetensors checkpoint shards:  14% Completed | 7/51 [01:21<08:38, 11.79s/it]

Loading safetensors checkpoint shards:  16% Completed | 8/51 [01:33<08:27, 11.80s/it]

Loading safetensors checkpoint shards:  18% Completed | 9/51 [01:44<08:09, 11.65s/it]

Loading safetensors checkpoint shards:  20% Completed | 10/51 [01:57<08:06, 11.87s/it]

Loading safetensors checkpoint shards:  22% Completed | 11/51 [02:10<08:07, 12.18s/it]

Loading safetensors checkpoint shards:  24% Completed | 12/51 [02:22<08:01, 12.34s/it]

Loading safetensors checkpoint shards:  25% Completed | 13/51 [02:34<07:45, 12.25s/it]

Loading safetensors checkpoint shards:  27% Completed | 14/51 [02:46<07:29, 12.14s/it]

Loading safetensors checkpoint shards:  29% Completed | 15/51 [02:58<07:16, 12.12s/it]

Loading safetensors checkpoint shards:  31% Completed | 16/51 [03:11<07:05, 12.17s/it]

Loading safetensors checkpoint shards:  33% Completed | 17/51 [03:23<06:54, 12.20s/it]

Loading safetensors checkpoint shards:  35% Completed | 18/51 [03:35<06:40, 12.13s/it]

Loading safetensors checkpoint shards:  37% Completed | 19/51 [03:47<06:24, 12.03s/it]

Loading safetensors checkpoint shards:  39% Completed | 20/51 [03:58<06:08, 11.87s/it]

Loading safetensors checkpoint shards:  41% Completed | 21/51 [04:10<05:53, 11.78s/it]

Loading safetensors checkpoint shards:  43% Completed | 22/51 [04:21<05:41, 11.79s/it]

Loading safetensors checkpoint shards:  45% Completed | 23/51 [04:31<05:08, 11.00s/it]

Loading safetensors checkpoint shards:  47% Completed | 24/51 [04:43<05:05, 11.30s/it]

Loading safetensors checkpoint shards:  49% Completed | 25/51 [04:54<04:57, 11.45s/it]

Loading safetensors checkpoint shards:  51% Completed | 26/51 [05:06<04:48, 11.53s/it]

Loading safetensors checkpoint shards:  53% Completed | 27/51 [05:18<04:39, 11.65s/it]

Loading safetensors checkpoint shards:  55% Completed | 28/51 [05:30<04:29, 11.74s/it]

Loading safetensors checkpoint shards:  57% Completed | 29/51 [05:43<04:26, 12.11s/it]

Loading safetensors checkpoint shards:  59% Completed | 30/51 [06:03<05:01, 14.36s/it]

Loading safetensors checkpoint shards:  61% Completed | 31/51 [06:22<05:15, 15.76s/it]

Loading safetensors checkpoint shards:  63% Completed | 32/51 [06:42<05:28, 17.27s/it]

Loading safetensors checkpoint shards:  65% Completed | 33/51 [07:04<05:34, 18.61s/it]

Loading safetensors checkpoint shards:  67% Completed | 34/51 [07:29<05:50, 20.61s/it]

Loading safetensors checkpoint shards:  69% Completed | 35/51 [07:55<05:51, 21.97s/it]

Loading safetensors checkpoint shards:  71% Completed | 36/51 [08:18<05:38, 22.54s/it]

Loading safetensors checkpoint shards:  73% Completed | 37/51 [08:41<05:16, 22.60s/it]

Loading safetensors checkpoint shards:  75% Completed | 38/51 [09:26<06:18, 29.12s/it]

Loading safetensors checkpoint shards:  76% Completed | 39/51 [10:11<06:49, 34.10s/it]

Loading safetensors checkpoint shards:  78% Completed | 40/51 [10:35<05:41, 31.02s/it]

Loading safetensors checkpoint shards:  80% Completed | 41/51 [10:51<04:23, 26.39s/it]

Loading safetensors checkpoint shards:  82% Completed | 42/51 [11:06<03:28, 23.17s/it]

Loading safetensors checkpoint shards:  84% Completed | 43/51 [11:22<02:48, 21.01s/it]

Loading safetensors checkpoint shards:  86% Completed | 44/51 [11:38<02:15, 19.29s/it]

Loading safetensors checkpoint shards:  88% Completed | 45/51 [11:53<01:48, 18.02s/it]

Loading safetensors checkpoint shards:  90% Completed | 46/51 [12:08<01:25, 17.18s/it]

Loading safetensors checkpoint shards:  92% Completed | 47/51 [12:23<01:06, 16.53s/it]

Loading safetensors checkpoint shards:  94% Completed | 48/51 [12:38<00:47, 15.98s/it]

Loading safetensors checkpoint shards:  96% Completed | 49/51 [12:52<00:31, 15.62s/it]

Loading safetensors checkpoint shards:  98% Completed | 50/51 [13:07<00:15, 15.48s/it]

Loading safetensors checkpoint shards: 100% Completed | 51/51 [13:24<00:00, 15.93s/it]

Loading safetensors checkpoint shards: 100% Completed | 51/51 [13:24<00:00, 15.78s/it]

[rank0]:[W117 10:36:19.465942855 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank2]:[W117 10:36:20.073621465 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank7]:[W117 10:36:20.074185796 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank14]:[W117 10:36:20.074570791 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank10]:[W117 10:36:20.074936165 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank8]:[W117 10:36:20.075826089 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank6]:[W117 10:36:20.076304397 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank13]:[W117 10:36:20.076993184 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank4]:[W117 10:36:20.078805703 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank11]:[W117 10:36:20.079734318 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank1]:[W117 10:36:20.080663464 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank5]:[W117 10:36:20.080916133 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank3]:[W117 10:36:20.080916743 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank9]:[W117 10:36:20.081001967 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank15]:[W117 10:36:20.081449714 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[rank12]:[W117 10:36:20.081507096 compiler_depend.ts:62] Warning: Cannot create tensor with NZ format while dim < 2, tensor will be created with ND format. (function operator())
[2026-01-17 10:36:35 TP0] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.00 GB, mem usage=29.81 GB.
[2026-01-17 10:36:36 TP11] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.32 GB, mem usage=29.81 GB.
[2026-01-17 10:36:37 TP2] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.05 GB, mem usage=29.81 GB.
[2026-01-17 10:36:38 TP7] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.32 GB, mem usage=29.81 GB.
[2026-01-17 10:36:38 TP15] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.32 GB, mem usage=29.81 GB.
[2026-01-17 10:36:38 TP1] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.33 GB, mem usage=29.81 GB.
[2026-01-17 10:36:38 TP4] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.06 GB, mem usage=29.81 GB.
[2026-01-17 10:36:39 TP6] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.07 GB, mem usage=29.81 GB.
[2026-01-17 10:36:39 TP14] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.07 GB, mem usage=29.81 GB.
[2026-01-17 10:36:39 TP10] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.07 GB, mem usage=29.81 GB.
[2026-01-17 10:36:40 TP5] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.33 GB, mem usage=29.81 GB.
[2026-01-17 10:36:41 TP3] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.33 GB, mem usage=29.81 GB.
[2026-01-17 10:36:41 TP9] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.34 GB, mem usage=29.81 GB.
[2026-01-17 10:36:41 TP13] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.33 GB, mem usage=29.81 GB.
[2026-01-17 10:36:41 TP12] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.06 GB, mem usage=29.81 GB.
[2026-01-17 10:36:41 TP8] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.06 GB, mem usage=29.81 GB.
[2026-01-17 10:36:41 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 10:36:41 TP0] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP15] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP13] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP12] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP14] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP11] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP9] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP8] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP10] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP7] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP6] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP5] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP4] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP3] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP2] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:41 TP1] The available memory for KV cache is 18.84 GB.
[2026-01-17 10:36:42 TP4] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP2] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP8] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP10] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP2] Memory pool end. avail mem=12.17 GB
[2026-01-17 10:36:42 TP4] Memory pool end. avail mem=12.17 GB
[2026-01-17 10:36:42 TP8] Memory pool end. avail mem=12.17 GB
[2026-01-17 10:36:42 TP10] Memory pool end. avail mem=12.18 GB
[2026-01-17 10:36:42 TP14] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP0] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP12] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP0] Memory pool end. avail mem=12.11 GB
[2026-01-17 10:36:42 TP14] Memory pool end. avail mem=12.18 GB
[2026-01-17 10:36:42 TP6] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP12] Memory pool end. avail mem=12.17 GB
[2026-01-17 10:36:42 TP5] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP15] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP3] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP13] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP11] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP1] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP6] Memory pool end. avail mem=12.19 GB
[2026-01-17 10:36:42 TP5] Memory pool end. avail mem=12.45 GB
[2026-01-17 10:36:42 TP15] Memory pool end. avail mem=12.44 GB
[2026-01-17 10:36:42 TP3] Memory pool end. avail mem=12.45 GB
[2026-01-17 10:36:42 TP7] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP13] Memory pool end. avail mem=12.45 GB
[2026-01-17 10:36:42 TP11] Memory pool end. avail mem=12.43 GB
[2026-01-17 10:36:42 TP1] Memory pool end. avail mem=12.45 GB
[2026-01-17 10:36:42 TP9] KV Cache is allocated. #tokens: 790144, K size: 9.42 GB, V size: 9.42 GB
[2026-01-17 10:36:42 TP7] Memory pool end. avail mem=12.44 GB
[2026-01-17 10:36:42 TP9] Memory pool end. avail mem=12.45 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 10:36:43 TP0] max_total_num_tokens=790144, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=2048, available_gpu_mem=12.11 GB
[2026-01-17 10:36:43] INFO:     Started server process [253478]
[2026-01-17 10:36:43] INFO:     Waiting for application startup.
[2026-01-17 10:36:43] INFO:     Application startup complete.
[2026-01-17 10:36:43] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 10:36:44] INFO:     127.0.0.1:56288 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 10:36:44 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:36:45] INFO:     127.0.0.1:56312 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 10:36:51] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:36:51] The server is fired up and ready to roll!
[2026-01-17 10:36:55 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:36:56] INFO:     127.0.0.1:49008 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 10:36:56] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 10:36:56] INFO:     127.0.0.1:49022 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 10:36:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:36:59] INFO:     127.0.0.1:49032 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/xverse/XVERSE-MoE-A36B --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 16 --context-length 2048 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestXVERSE.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 10:36:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:36:59 TP0] Prefill batch, #new-seq: 29, #new-token: 4736, #cached-token: 29696, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 10:36:59 TP0] Prefill batch, #new-seq: 49, #new-token: 8192, #cached-token: 50176, token usage: 0.01, #running-req: 30, #queue-req: 0,
[2026-01-17 10:36:59 TP0] Prefill batch, #new-seq: 46, #new-token: 8192, #cached-token: 46080, token usage: 0.02, #running-req: 78, #queue-req: 4,
[2026-01-17 10:36:59 TP0] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 4096, token usage: 0.03, #running-req: 124, #queue-req: 0,
[2026-01-17 10:37:02 TP0] Decode batch, #running-req: 128, #token: 29568, token usage: 0.04, npu graph: False, gen throughput (token/s): 4.65, #queue-req: 0,
[2026-01-17 10:37:03] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:04<13:51,  4.18s/it][2026-01-17 10:37:03 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:03] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:04<06:28,  1.96s/it][2026-01-17 10:37:03] INFO:     127.0.0.1:56134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:03] INFO:     127.0.0.1:56358 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:04<02:35,  1.26it/s][2026-01-17 10:37:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 10:37:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 10:37:04] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:05<02:17,  1.42it/s][2026-01-17 10:37:04 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:04] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:05<02:08,  1.51it/s][2026-01-17 10:37:05 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:05] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:05] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:06<02:31,  1.27it/s]
  4%|▍         | 8/200 [00:06<02:08,  1.50it/s][2026-01-17 10:37:06] INFO:     127.0.0.1:55892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:06] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:06 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 10:37:06] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:07<01:08,  2.75it/s][2026-01-17 10:37:06 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 2048, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 10:37:06 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 130, #queue-req: 0,
[2026-01-17 10:37:06] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:07<01:07,  2.78it/s][2026-01-17 10:37:06] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:06] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:06] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:06] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:06 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:06] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:07<00:31,  5.82it/s][2026-01-17 10:37:06 TP0] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 4096, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 10:37:06 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 132, #queue-req: 0,
[2026-01-17 10:37:07] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:07] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:07<00:31,  5.80it/s][2026-01-17 10:37:07] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:07 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 10:37:07] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:07] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:07] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:08<00:26,  6.70it/s]
 12%|█▏        | 23/200 [00:08<00:15, 11.65it/s]
 12%|█▏        | 23/200 [00:08<00:15, 11.65it/s][2026-01-17 10:37:07 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 10:37:07 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 3072, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 10:37:07 TP0] Decode batch, #running-req: 128, #token: 32896, token usage: 0.04, npu graph: False, gen throughput (token/s): 1101.12, #queue-req: 0,
[2026-01-17 10:37:07] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:07 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:07] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:08<00:21,  8.06it/s][2026-01-17 10:37:07 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:07] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:07 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:08] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:08<00:23,  7.36it/s][2026-01-17 10:37:08 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:08] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:08] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:09<00:21,  8.05it/s][2026-01-17 10:37:08 TP0] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 10:37:08] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:08] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:08] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:09<00:19,  8.85it/s]
 16%|█▌        | 32/200 [00:09<00:15, 10.90it/s][2026-01-17 10:37:08 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 3072, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-17 10:37:08] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:08] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:08] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:08] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:09<00:15, 10.99it/s]
 18%|█▊        | 36/200 [00:09<00:10, 15.29it/s]
 18%|█▊        | 36/200 [00:09<00:10, 15.29it/s][2026-01-17 10:37:08 TP0] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 4096, token usage: 0.04, #running-req: 124, #queue-req: 0,
[2026-01-17 10:37:08] INFO:     127.0.0.1:56626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:09] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:10<00:17,  9.47it/s][2026-01-17 10:37:09 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:09] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:09] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:09 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 10:37:09 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 2048, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 10:37:09] INFO:     127.0.0.1:56516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:09] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:10<00:18,  8.56it/s]
 21%|██        | 42/200 [00:10<00:17,  8.90it/s][2026-01-17 10:37:09] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:09 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 10:37:09 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 10:37:09] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:10<00:20,  7.58it/s][2026-01-17 10:37:09] INFO:     127.0.0.1:55874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:09 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:10] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:11<00:18,  8.22it/s][2026-01-17 10:37:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 10:37:10 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 10:37:10] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:10] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:11<00:28,  5.41it/s]
 24%|██▍       | 48/200 [00:11<00:32,  4.70it/s][2026-01-17 10:37:10] INFO:     127.0.0.1:56524 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:11<00:29,  5.10it/s][2026-01-17 10:37:10 TP0] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 10:37:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 10:37:11] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:12<00:36,  4.16it/s][2026-01-17 10:37:11] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:11 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:11] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:12<00:27,  5.38it/s][2026-01-17 10:37:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 10:37:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-17 10:37:11] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:12<00:32,  4.58it/s][2026-01-17 10:37:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:11] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:11] INFO:     127.0.0.1:56146 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:12<00:35,  4.15it/s]
 28%|██▊       | 55/200 [00:12<00:30,  4.79it/s][2026-01-17 10:37:12] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:12] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:12] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:12] INFO:     127.0.0.1:56768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:12 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 10:37:12] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:12] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:12] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:13<00:15,  9.29it/s]
 31%|███       | 62/200 [00:13<00:06, 20.39it/s]
 31%|███       | 62/200 [00:13<00:06, 20.39it/s][2026-01-17 10:37:12 TP0] Prefill batch, #new-seq: 4, #new-token: 768, #cached-token: 4096, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-17 10:37:12 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 3072, token usage: 0.04, #running-req: 132, #queue-req: 0,
[2026-01-17 10:37:12] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:12 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:12] INFO:     127.0.0.1:56934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:13 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:13] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:14<00:14,  9.26it/s][2026-01-17 10:37:13 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:13] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:13 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:13 TP0] Decode batch, #running-req: 127, #token: 33920, token usage: 0.04, npu graph: False, gen throughput (token/s): 842.51, #queue-req: 0,
[2026-01-17 10:37:13] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:14<00:16,  7.83it/s][2026-01-17 10:37:13 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:13] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:13] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:14<00:16,  8.07it/s][2026-01-17 10:37:13 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 10:37:14] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:14] INFO:     127.0.0.1:57114 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:15<00:17,  7.35it/s][2026-01-17 10:37:14 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 10:37:14] INFO:     127.0.0.1:55900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:14 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 10:37:14] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:15<00:20,  6.14it/s][2026-01-17 10:37:14] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:14] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:14] INFO:     127.0.0.1:57136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:14] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:14] INFO:     127.0.0.1:56250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:14] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:14] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:15<00:13,  8.88it/s]
 40%|████      | 80/200 [00:15<00:05, 23.56it/s]
 40%|████      | 80/200 [00:15<00:05, 23.56it/s]
 40%|████      | 80/200 [00:15<00:05, 23.56it/s][2026-01-17 10:37:14] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:14] INFO:     127.0.0.1:57150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:14] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:15] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:16<00:05, 20.86it/s][2026-01-17 10:37:15] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:15] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:15] INFO:     127.0.0.1:52878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:15] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:16<00:05, 21.46it/s][2026-01-17 10:37:15] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:15] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:15] INFO:     127.0.0.1:56228 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:16<00:08, 12.76it/s][2026-01-17 10:37:15] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:15] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:15] INFO:     127.0.0.1:56296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:15] INFO:     127.0.0.1:56758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:15] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [00:16<00:07, 14.79it/s]
 48%|████▊     | 96/200 [00:16<00:05, 17.89it/s][2026-01-17 10:37:16] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:16] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:16] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:16] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:17<00:06, 14.44it/s]
 50%|█████     | 100/200 [00:17<00:07, 13.47it/s][2026-01-17 10:37:16] INFO:     127.0.0.1:56656 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:16] INFO:     127.0.0.1:56308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:16] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:16] INFO:     127.0.0.1:53154 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:17<00:08, 12.11it/s][2026-01-17 10:37:16] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:16] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:16] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:16] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:17<00:05, 15.81it/s]
 54%|█████▍    | 108/200 [00:17<00:04, 20.44it/s][2026-01-17 10:37:16] INFO:     127.0.0.1:56060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:16] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:17] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:18<00:06, 13.42it/s][2026-01-17 10:37:17 TP0] Decode batch, #running-req: 89, #token: 28288, token usage: 0.04, npu graph: False, gen throughput (token/s): 1145.38, #queue-req: 0,
[2026-01-17 10:37:17] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:17] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:18<00:07, 11.47it/s][2026-01-17 10:37:17] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:17] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:18<00:09,  8.92it/s][2026-01-17 10:37:18] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:18] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:18] INFO:     127.0.0.1:56204 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:19<00:12,  6.81it/s]
 59%|█████▉    | 118/200 [00:19<00:12,  6.36it/s][2026-01-17 10:37:18] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:19<00:12,  6.63it/s][2026-01-17 10:37:18] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:19<00:13,  6.15it/s][2026-01-17 10:37:18] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:19<00:12,  6.13it/s][2026-01-17 10:37:19] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:20<00:12,  6.07it/s][2026-01-17 10:37:19] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:20<00:12,  6.10it/s][2026-01-17 10:37:19] INFO:     127.0.0.1:52958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:19] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:19] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:19] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:19] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:20<00:10,  7.43it/s]
 64%|██████▍   | 128/200 [00:20<00:04, 17.51it/s]
 64%|██████▍   | 128/200 [00:20<00:04, 17.51it/s]
 64%|██████▍   | 128/200 [00:20<00:04, 17.51it/s][2026-01-17 10:37:19] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:19] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:20<00:06, 10.52it/s][2026-01-17 10:37:20] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:20] INFO:     127.0.0.1:52982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:20] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:21<00:08,  8.21it/s]
 66%|██████▋   | 133/200 [00:21<00:08,  7.80it/s][2026-01-17 10:37:20] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:21<00:10,  6.48it/s][2026-01-17 10:37:20] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:21<00:10,  5.93it/s][2026-01-17 10:37:21 TP0] Decode batch, #running-req: 66, #token: 22528, token usage: 0.03, npu graph: False, gen throughput (token/s): 843.02, #queue-req: 0,
[2026-01-17 10:37:21] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:21] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:21] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:22<00:09,  6.96it/s]
 69%|██████▉   | 138/200 [00:22<00:06,  9.11it/s][2026-01-17 10:37:21] INFO:     127.0.0.1:52864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:21] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:21] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:22<00:06,  8.65it/s]
 70%|███████   | 141/200 [00:22<00:06,  9.52it/s][2026-01-17 10:37:21] INFO:     127.0.0.1:56200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:21] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:21] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:22<00:06,  8.79it/s]
 72%|███████▏  | 144/200 [00:22<00:05,  9.43it/s][2026-01-17 10:37:21] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:22<00:06,  7.94it/s][2026-01-17 10:37:22] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:23<00:07,  6.86it/s][2026-01-17 10:37:22] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:23<00:07,  6.70it/s][2026-01-17 10:37:22] INFO:     127.0.0.1:57030 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:23<00:07,  6.51it/s][2026-01-17 10:37:23] INFO:     127.0.0.1:53064 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:24<00:12,  3.99it/s][2026-01-17 10:37:23] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:23] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:24<00:13,  3.67it/s]
 76%|███████▌  | 151/200 [00:24<00:11,  4.37it/s][2026-01-17 10:37:23] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:23] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:23] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:23] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:24<00:10,  4.70it/s]
 78%|███████▊  | 155/200 [00:24<00:03, 12.84it/s]
 78%|███████▊  | 155/200 [00:24<00:03, 12.84it/s]
 78%|███████▊  | 155/200 [00:24<00:03, 12.84it/s][2026-01-17 10:37:23] INFO:     127.0.0.1:57016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:23] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:24<00:03, 12.68it/s][2026-01-17 10:37:24 TP0] Decode batch, #running-req: 43, #token: 17024, token usage: 0.02, npu graph: False, gen throughput (token/s): 631.35, #queue-req: 0,
[2026-01-17 10:37:24] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:24] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:25<00:06,  6.46it/s][2026-01-17 10:37:24] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:25] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:26<00:08,  4.55it/s][2026-01-17 10:37:25] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:25] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:26<00:07,  5.15it/s][2026-01-17 10:37:25] INFO:     127.0.0.1:52800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:25] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:26<00:05,  6.18it/s][2026-01-17 10:37:26] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:26<00:06,  4.99it/s][2026-01-17 10:37:26] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:27<00:07,  4.46it/s][2026-01-17 10:37:26] INFO:     127.0.0.1:57152 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:27<00:09,  3.41it/s][2026-01-17 10:37:26] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:26] INFO:     127.0.0.1:53178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:27] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:28<00:05,  5.65it/s][2026-01-17 10:37:27] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:28<00:05,  4.89it/s][2026-01-17 10:37:27 TP0] Decode batch, #running-req: 28, #token: 12928, token usage: 0.02, npu graph: False, gen throughput (token/s): 446.73, #queue-req: 0,
[2026-01-17 10:37:28] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:29<00:10,  2.65it/s][2026-01-17 10:37:28] INFO:     127.0.0.1:57076 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:29<00:08,  3.06it/s][2026-01-17 10:37:30 TP0] Decode batch, #running-req: 26, #token: 13056, token usage: 0.02, npu graph: False, gen throughput (token/s): 341.87, #queue-req: 0,
[2026-01-17 10:37:31] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:31] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:32<00:24,  1.02it/s]
 88%|████████▊ | 176/200 [00:32<00:28,  1.19s/it][2026-01-17 10:37:33] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:34<00:32,  1.41s/it][2026-01-17 10:37:33 TP0] Decode batch, #running-req: 23, #token: 12032, token usage: 0.02, npu graph: False, gen throughput (token/s): 311.20, #queue-req: 0,
[2026-01-17 10:37:36] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:37<00:36,  1.67s/it][2026-01-17 10:37:36 TP0] Decode batch, #running-req: 22, #token: 12544, token usage: 0.02, npu graph: False, gen throughput (token/s): 290.70, #queue-req: 0,
[2026-01-17 10:37:39] INFO:     127.0.0.1:53136 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:40<00:41,  2.00s/it][2026-01-17 10:37:40 TP0] Decode batch, #running-req: 21, #token: 13568, token usage: 0.02, npu graph: False, gen throughput (token/s): 244.22, #queue-req: 0,
[2026-01-17 10:37:43 TP0] Decode batch, #running-req: 21, #token: 13824, token usage: 0.02, npu graph: False, gen throughput (token/s): 269.71, #queue-req: 0,
[2026-01-17 10:37:46 TP0] Decode batch, #running-req: 21, #token: 7040, token usage: 0.01, npu graph: False, gen throughput (token/s): 271.97, #queue-req: 0,
[2026-01-17 10:37:46] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:46] INFO:     127.0.0.1:55954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:46] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:46] INFO:     127.0.0.1:56028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:46] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:46] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:46] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:46] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:46] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:46] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:46] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:47<01:10,  3.53s/it]
 95%|█████████▌| 190/200 [00:47<00:09,  1.04it/s]
 95%|█████████▌| 190/200 [00:47<00:09,  1.04it/s]
 95%|█████████▌| 190/200 [00:47<00:09,  1.04it/s]
 95%|█████████▌| 190/200 [00:47<00:09,  1.04it/s]
 95%|█████████▌| 190/200 [00:47<00:09,  1.04it/s]
 95%|█████████▌| 190/200 [00:47<00:09,  1.04it/s]
 95%|█████████▌| 190/200 [00:47<00:09,  1.04it/s]
 95%|█████████▌| 190/200 [00:47<00:09,  1.04it/s]
 95%|█████████▌| 190/200 [00:47<00:09,  1.04it/s][2026-01-17 10:37:49 TP0] Decode batch, #running-req: 10, #token: 7680, token usage: 0.01, npu graph: False, gen throughput (token/s): 135.01, #queue-req: 0,
[2026-01-17 10:37:51] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:52<00:10,  1.17s/it][2026-01-17 10:37:51] INFO:     127.0.0.1:57128 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:52<00:08,  1.11s/it][2026-01-17 10:37:52] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:53<00:07,  1.05s/it][2026-01-17 10:37:52] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:53<00:05,  1.02it/s][2026-01-17 10:37:52 TP0] Decode batch, #running-req: 7, #token: 5248, token usage: 0.01, npu graph: False, gen throughput (token/s): 117.47, #queue-req: 0,
[2026-01-17 10:37:54] INFO:     127.0.0.1:53080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:54] INFO:     127.0.0.1:53088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:54] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:54] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:54] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:37:54] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:55<00:05,  1.12s/it]
100%|██████████| 200/200 [00:55<00:00,  2.07it/s]
100%|██████████| 200/200 [00:55<00:00,  2.07it/s]
100%|██████████| 200/200 [00:55<00:00,  2.07it/s]
100%|██████████| 200/200 [00:55<00:00,  2.07it/s]
100%|██████████| 200/200 [00:55<00:00,  2.07it/s]
100%|██████████| 200/200 [00:55<00:00,  3.59it/s]
.
----------------------------------------------------------------------
Ran 1 test in 930.788s

OK
Accuracy: 0.265
Invalid: 0.005
Latency: 58.112 s
Output throughput: 543.758 token/s
.
.
End (28/62):
filename='ascend/llm_models/test_ascend_llm_models_XVERSE.py', elapsed=941, estimated_time=400
.
.

.
.
Begin (29/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_dbrx_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 10:38:18] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/AI-ModelScope/dbrx-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/AI-ModelScope/dbrx-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=992719691, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/AI-ModelScope/dbrx-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 10:38:19] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 10:38:29 TP5] Init torch distributed begin.
[2026-01-17 10:38:29 TP4] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 10:38:29 TP6] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 10:38:29 TP2] Init torch distributed begin.
[2026-01-17 10:38:29 TP0] Init torch distributed begin.
[2026-01-17 10:38:29 TP1] Init torch distributed begin.
[2026-01-17 10:38:29 TP3] Init torch distributed begin.
[2026-01-17 10:38:29 TP7] Init torch distributed begin.
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2026-01-17 10:38:31 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:38:31 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:38:31 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:38:31 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:38:31 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:38:31 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:38:31 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:38:31 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:38:31 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 10:38:32 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:38:32 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:38:32 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:38:32 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:38:32 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:38:32 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:38:32 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:38:32 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:38:32 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 10:38:32 TP5] Load weight begin. avail mem=61.14 GB
[2026-01-17 10:38:32 TP7] Load weight begin. avail mem=61.13 GB
[2026-01-17 10:38:32 TP4] Load weight begin. avail mem=60.86 GB
[2026-01-17 10:38:32 TP6] Load weight begin. avail mem=60.88 GB
[2026-01-17 10:38:32 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 10:38:32 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 10:38:32 TP0] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/61 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   2% Completed | 1/61 [00:03<03:46,  3.78s/it]

Loading safetensors checkpoint shards:   3% Completed | 2/61 [00:07<03:40,  3.73s/it]

Loading safetensors checkpoint shards:   5% Completed | 3/61 [00:11<03:30,  3.64s/it]

Loading safetensors checkpoint shards:   7% Completed | 4/61 [00:14<03:26,  3.62s/it]

Loading safetensors checkpoint shards:   8% Completed | 5/61 [00:18<03:18,  3.55s/it]

Loading safetensors checkpoint shards:  10% Completed | 6/61 [00:21<03:17,  3.58s/it]

Loading safetensors checkpoint shards:  11% Completed | 7/61 [00:24<03:08,  3.49s/it]

Loading safetensors checkpoint shards:  13% Completed | 8/61 [00:28<03:09,  3.58s/it]

Loading safetensors checkpoint shards:  15% Completed | 9/61 [00:32<03:08,  3.62s/it]

Loading safetensors checkpoint shards:  16% Completed | 10/61 [00:36<03:04,  3.62s/it]

Loading safetensors checkpoint shards:  18% Completed | 11/61 [00:39<02:54,  3.50s/it]

Loading safetensors checkpoint shards:  20% Completed | 12/61 [00:42<02:52,  3.53s/it]

Loading safetensors checkpoint shards:  21% Completed | 13/61 [00:46<02:52,  3.60s/it]

Loading safetensors checkpoint shards:  23% Completed | 14/61 [00:49<02:38,  3.37s/it]

Loading safetensors checkpoint shards:  25% Completed | 15/61 [00:52<02:31,  3.29s/it]

Loading safetensors checkpoint shards:  26% Completed | 16/61 [00:55<02:28,  3.30s/it]

Loading safetensors checkpoint shards:  28% Completed | 17/61 [00:58<02:18,  3.16s/it]

Loading safetensors checkpoint shards:  30% Completed | 18/61 [01:02<02:19,  3.24s/it]

Loading safetensors checkpoint shards:  31% Completed | 19/61 [01:05<02:17,  3.26s/it]

Loading safetensors checkpoint shards:  33% Completed | 20/61 [01:09<02:19,  3.41s/it]

Loading safetensors checkpoint shards:  34% Completed | 21/61 [01:13<02:21,  3.53s/it]

Loading safetensors checkpoint shards:  36% Completed | 22/61 [01:16<02:21,  3.63s/it]

Loading safetensors checkpoint shards:  38% Completed | 23/61 [01:20<02:18,  3.66s/it]

Loading safetensors checkpoint shards:  39% Completed | 24/61 [01:24<02:15,  3.67s/it]

Loading safetensors checkpoint shards:  41% Completed | 25/61 [01:28<02:13,  3.70s/it]

Loading safetensors checkpoint shards:  43% Completed | 26/61 [01:31<02:09,  3.71s/it]

Loading safetensors checkpoint shards:  44% Completed | 27/61 [01:35<02:02,  3.60s/it]

Loading safetensors checkpoint shards:  46% Completed | 28/61 [01:38<01:59,  3.63s/it]

Loading safetensors checkpoint shards:  48% Completed | 29/61 [01:42<01:57,  3.68s/it]

Loading safetensors checkpoint shards:  49% Completed | 30/61 [01:46<01:55,  3.73s/it]

Loading safetensors checkpoint shards:  51% Completed | 31/61 [01:49<01:48,  3.61s/it]

Loading safetensors checkpoint shards:  52% Completed | 32/61 [01:53<01:47,  3.71s/it]

Loading safetensors checkpoint shards:  54% Completed | 33/61 [01:57<01:40,  3.59s/it]

Loading safetensors checkpoint shards:  56% Completed | 34/61 [02:00<01:36,  3.56s/it]

Loading safetensors checkpoint shards:  57% Completed | 35/61 [02:04<01:35,  3.67s/it]

Loading safetensors checkpoint shards:  59% Completed | 36/61 [02:08<01:32,  3.72s/it]

Loading safetensors checkpoint shards:  61% Completed | 37/61 [02:11<01:27,  3.63s/it]

Loading safetensors checkpoint shards:  62% Completed | 38/61 [02:15<01:22,  3.58s/it]

Loading safetensors checkpoint shards:  64% Completed | 39/61 [02:19<01:21,  3.68s/it]

Loading safetensors checkpoint shards:  66% Completed | 40/61 [02:22<01:16,  3.63s/it]

Loading safetensors checkpoint shards:  67% Completed | 41/61 [02:26<01:11,  3.56s/it]

Loading safetensors checkpoint shards:  69% Completed | 42/61 [02:29<01:09,  3.66s/it]

Loading safetensors checkpoint shards:  70% Completed | 43/61 [02:33<01:03,  3.55s/it]

Loading safetensors checkpoint shards:  72% Completed | 44/61 [02:37<01:02,  3.68s/it]

Loading safetensors checkpoint shards:  74% Completed | 45/61 [02:41<01:00,  3.80s/it]

Loading safetensors checkpoint shards:  75% Completed | 46/61 [02:44<00:55,  3.68s/it]

Loading safetensors checkpoint shards:  77% Completed | 47/61 [02:48<00:50,  3.62s/it]

Loading safetensors checkpoint shards:  79% Completed | 48/61 [02:52<00:48,  3.71s/it]

Loading safetensors checkpoint shards:  80% Completed | 49/61 [02:56<00:45,  3.79s/it]

Loading safetensors checkpoint shards:  82% Completed | 50/61 [03:00<00:42,  3.85s/it]

Loading safetensors checkpoint shards:  84% Completed | 51/61 [03:03<00:38,  3.84s/it]

Loading safetensors checkpoint shards:  85% Completed | 52/61 [03:07<00:34,  3.84s/it]

Loading safetensors checkpoint shards:  87% Completed | 53/61 [03:11<00:31,  3.88s/it]

Loading safetensors checkpoint shards:  89% Completed | 54/61 [03:14<00:24,  3.56s/it]

Loading safetensors checkpoint shards:  90% Completed | 55/61 [03:17<00:21,  3.50s/it]

Loading safetensors checkpoint shards:  92% Completed | 56/61 [03:21<00:17,  3.45s/it]

Loading safetensors checkpoint shards:  93% Completed | 57/61 [03:24<00:13,  3.41s/it]

Loading safetensors checkpoint shards:  95% Completed | 58/61 [03:27<00:10,  3.37s/it]

Loading safetensors checkpoint shards:  97% Completed | 59/61 [03:31<00:06,  3.50s/it]

Loading safetensors checkpoint shards:  98% Completed | 60/61 [03:35<00:03,  3.59s/it]

Loading safetensors checkpoint shards: 100% Completed | 61/61 [03:39<00:00,  3.66s/it]

Loading safetensors checkpoint shards: 100% Completed | 61/61 [03:39<00:00,  3.59s/it]

[2026-01-17 10:42:31 TP7] Load weight end. type=DbrxForCausalLM, dtype=torch.bfloat16, avail mem=30.43 GB, mem usage=30.69 GB.
[2026-01-17 10:42:31 TP5] Load weight end. type=DbrxForCausalLM, dtype=torch.bfloat16, avail mem=30.44 GB, mem usage=30.69 GB.
[2026-01-17 10:42:31 TP4] Load weight end. type=DbrxForCausalLM, dtype=torch.bfloat16, avail mem=30.17 GB, mem usage=30.70 GB.
[2026-01-17 10:42:31 TP2] Load weight end. type=DbrxForCausalLM, dtype=torch.bfloat16, avail mem=30.16 GB, mem usage=30.69 GB.
[2026-01-17 10:42:32 TP0] Load weight end. type=DbrxForCausalLM, dtype=torch.bfloat16, avail mem=30.11 GB, mem usage=30.69 GB.
[2026-01-17 10:42:32 TP1] Load weight end. type=DbrxForCausalLM, dtype=torch.bfloat16, avail mem=30.44 GB, mem usage=30.69 GB.
[2026-01-17 10:42:32 TP3] Load weight end. type=DbrxForCausalLM, dtype=torch.bfloat16, avail mem=30.44 GB, mem usage=30.69 GB.
[2026-01-17 10:42:32 TP6] Load weight end. type=DbrxForCausalLM, dtype=torch.bfloat16, avail mem=30.18 GB, mem usage=30.70 GB.
[2026-01-17 10:42:32 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 10:42:32 TP0] The available memory for KV cache is 17.95 GB.
[2026-01-17 10:42:32 TP7] The available memory for KV cache is 17.95 GB.
[2026-01-17 10:42:32 TP5] The available memory for KV cache is 17.95 GB.
[2026-01-17 10:42:32 TP4] The available memory for KV cache is 17.95 GB.
[2026-01-17 10:42:32 TP6] The available memory for KV cache is 17.95 GB.
[2026-01-17 10:42:32 TP2] The available memory for KV cache is 17.95 GB.
[2026-01-17 10:42:32 TP3] The available memory for KV cache is 17.95 GB.
[2026-01-17 10:42:32 TP1] The available memory for KV cache is 17.95 GB.
[2026-01-17 10:42:33 TP0] KV Cache is allocated. #tokens: 941056, K size: 8.98 GB, V size: 8.98 GB
[2026-01-17 10:42:33 TP4] KV Cache is allocated. #tokens: 941056, K size: 8.98 GB, V size: 8.98 GB
[2026-01-17 10:42:33 TP0] Memory pool end. avail mem=11.66 GB
[2026-01-17 10:42:33 TP4] Memory pool end. avail mem=11.71 GB
[2026-01-17 10:42:33 TP2] KV Cache is allocated. #tokens: 941056, K size: 8.98 GB, V size: 8.98 GB
[2026-01-17 10:42:33 TP2] Memory pool end. avail mem=11.71 GB
[2026-01-17 10:42:33 TP1] KV Cache is allocated. #tokens: 941056, K size: 8.98 GB, V size: 8.98 GB
[2026-01-17 10:42:33 TP5] KV Cache is allocated. #tokens: 941056, K size: 8.98 GB, V size: 8.98 GB
[2026-01-17 10:42:33 TP6] KV Cache is allocated. #tokens: 941056, K size: 8.98 GB, V size: 8.98 GB
[2026-01-17 10:42:33 TP3] KV Cache is allocated. #tokens: 941056, K size: 8.98 GB, V size: 8.98 GB
[2026-01-17 10:42:33 TP1] Memory pool end. avail mem=11.99 GB
[2026-01-17 10:42:33 TP5] Memory pool end. avail mem=11.99 GB
[2026-01-17 10:42:33 TP6] Memory pool end. avail mem=11.72 GB
[2026-01-17 10:42:33 TP3] Memory pool end. avail mem=11.99 GB
[2026-01-17 10:42:33 TP7] KV Cache is allocated. #tokens: 941056, K size: 8.98 GB, V size: 8.98 GB
[2026-01-17 10:42:33 TP7] Memory pool end. avail mem=11.98 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 10:42:33 TP0] max_total_num_tokens=941056, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.66 GB
[2026-01-17 10:42:34] INFO:     Started server process [275539]
[2026-01-17 10:42:34] INFO:     Waiting for application startup.
[2026-01-17 10:42:34] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 10:42:34] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 10:42:34] INFO:     Application startup complete.
[2026-01-17 10:42:34] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 10:42:35] INFO:     127.0.0.1:44538 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 10:42:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:42:36] INFO:     127.0.0.1:44558 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
........[2026-01-17 10:42:42] INFO:     127.0.0.1:44554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:42] The server is fired up and ready to roll!
[2026-01-17 10:42:46 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:42:47] INFO:     127.0.0.1:53108 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 10:42:47] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 10:42:47] INFO:     127.0.0.1:47378 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 10:42:47 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:42:48] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/AI-ModelScope/dbrx-instruct --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 8 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 10:42:48 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:42:48 TP0] Prefill batch, #new-seq: 29, #new-token: 3968, #cached-token: 18560, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 10:42:48 TP0] Prefill batch, #new-seq: 16, #new-token: 2176, #cached-token: 10240, token usage: 0.01, #running-req: 30, #queue-req: 0,
[2026-01-17 10:42:48 TP0] Prefill batch, #new-seq: 62, #new-token: 8192, #cached-token: 39680, token usage: 0.01, #running-req: 46, #queue-req: 7,
[2026-01-17 10:42:48 TP0] Prefill batch, #new-seq: 20, #new-token: 2688, #cached-token: 12800, token usage: 0.02, #running-req: 108, #queue-req: 0,
[2026-01-17 10:42:52 TP0] Decode batch, #running-req: 128, #token: 21504, token usage: 0.02, npu graph: False, gen throughput (token/s): 15.09, #queue-req: 0,
[2026-01-17 10:42:52] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:52] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:04<15:48,  4.77s/it]
  1%|          | 2/200 [00:04<09:54,  3.00s/it][2026-01-17 10:42:52 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 10:42:52] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:52] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:04<06:36,  2.01s/it]
  2%|▏         | 4/200 [00:04<03:24,  1.05s/it][2026-01-17 10:42:53 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 10:42:53] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:05<02:37,  1.24it/s][2026-01-17 10:42:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 10:42:53] INFO:     127.0.0.1:48368 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:05<02:00,  1.61it/s][2026-01-17 10:42:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 10:42:53] INFO:     127.0.0.1:48376 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:05<01:33,  2.06it/s][2026-01-17 10:42:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 10:42:53] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:53] INFO:     127.0.0.1:47894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:53] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:05<01:20,  2.39it/s]
  5%|▌         | 10/200 [00:05<00:35,  5.34it/s]
  5%|▌         | 10/200 [00:05<00:35,  5.34it/s][2026-01-17 10:42:53 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.03, #running-req: 125, #queue-req: 0,
[2026-01-17 10:42:53] INFO:     127.0.0.1:47986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:53] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:53] INFO:     127.0.0.1:48132 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:05<00:33,  5.58it/s]
  6%|▋         | 13/200 [00:05<00:18,  9.87it/s]
  6%|▋         | 13/200 [00:05<00:18,  9.87it/s][2026-01-17 10:42:53 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.03, #running-req: 125, #queue-req: 0,
[2026-01-17 10:42:53] INFO:     127.0.0.1:48156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 10:42:54] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:06<00:25,  7.34it/s][2026-01-17 10:42:54] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:54] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:54 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 10:42:54] INFO:     127.0.0.1:47386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:54] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:06<00:18,  9.94it/s]
 10%|▉         | 19/200 [00:06<00:12, 13.93it/s][2026-01-17 10:42:54 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:54 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 130, #queue-req: 0,
[2026-01-17 10:42:54] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:54] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:06<00:16, 10.80it/s][2026-01-17 10:42:54 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 10:42:54] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:54] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:54] INFO:     127.0.0.1:48190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:54 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:54 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.03, #running-req: 129, #queue-req: 0,
[2026-01-17 10:42:54] INFO:     127.0.0.1:47786 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:06<00:15, 11.37it/s][2026-01-17 10:42:55] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:55 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 10:42:55 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:55] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:55] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:07<00:17,  9.64it/s]
 14%|█▍        | 28/200 [00:07<00:17,  9.56it/s][2026-01-17 10:42:55] INFO:     127.0.0.1:48050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:55] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:55] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:55] INFO:     127.0.0.1:48118 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:55] INFO:     127.0.0.1:48486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:55 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-17 10:42:55 TP0] Prefill batch, #new-seq: 5, #new-token: 640, #cached-token: 3200, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:55] INFO:     127.0.0.1:48226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:55] INFO:     127.0.0.1:48514 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:07<00:13, 12.29it/s]
 18%|█▊        | 35/200 [00:07<00:10, 15.00it/s][2026-01-17 10:42:55 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-17 10:42:55] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:55] INFO:     127.0.0.1:48434 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:07<00:10, 14.92it/s][2026-01-17 10:42:55 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-17 10:42:56] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:56] INFO:     127.0.0.1:48364 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:07<00:12, 12.94it/s][2026-01-17 10:42:56] INFO:     127.0.0.1:48428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:56 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-17 10:42:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:56] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:08<00:14, 10.61it/s][2026-01-17 10:42:56] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:56] INFO:     127.0.0.1:48340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 10:42:56] INFO:     127.0.0.1:47482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:56] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:08<00:12, 12.74it/s]
 22%|██▎       | 45/200 [00:08<00:09, 16.29it/s][2026-01-17 10:42:56 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:56 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 130, #queue-req: 0,
[2026-01-17 10:42:56] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:56] INFO:     127.0.0.1:48560 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:08<00:12, 12.24it/s][2026-01-17 10:42:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 10:42:56] INFO:     127.0.0.1:47502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:56] INFO:     127.0.0.1:48412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:56 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 129, #queue-req: 0,
[2026-01-17 10:42:57 TP0] Decode batch, #running-req: 128, #token: 28288, token usage: 0.03, npu graph: False, gen throughput (token/s): 1156.64, #queue-req: 0,
[2026-01-17 10:42:57] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:09<00:13, 11.24it/s][2026-01-17 10:42:57] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 10:42:57] INFO:     127.0.0.1:47586 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:09<00:12, 11.99it/s][2026-01-17 10:42:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 129, #queue-req: 0,
[2026-01-17 10:42:57] INFO:     127.0.0.1:47780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:57] INFO:     127.0.0.1:48580 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:09<00:14,  9.90it/s][2026-01-17 10:42:57] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:57] INFO:     127.0.0.1:48296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:57] INFO:     127.0.0.1:48590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:57 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-17 10:42:57] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:57] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:57] INFO:     127.0.0.1:48112 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:09<00:10, 13.56it/s]
 30%|███       | 60/200 [00:09<00:05, 24.26it/s]
 30%|███       | 60/200 [00:09<00:05, 24.26it/s][2026-01-17 10:42:57 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:57 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.03, #running-req: 131, #queue-req: 0,
[2026-01-17 10:42:57] INFO:     127.0.0.1:48212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:58] INFO:     127.0.0.1:47696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-17 10:42:58] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:10<00:08, 16.23it/s][2026-01-17 10:42:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 129, #queue-req: 0,
[2026-01-17 10:42:58] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:58] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:58] INFO:     127.0.0.1:47684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:58] INFO:     127.0.0.1:48622 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:10<00:09, 13.85it/s][2026-01-17 10:42:58 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-17 10:42:58 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:58] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:58] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:10<00:10, 12.02it/s]
 34%|███▍      | 69/200 [00:10<00:11, 11.89it/s][2026-01-17 10:42:58] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:58] INFO:     127.0.0.1:47758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:58] INFO:     127.0.0.1:47906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:58] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:58 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-17 10:42:58 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-17 10:42:58] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:58] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:10<00:09, 13.15it/s]
 38%|███▊      | 75/200 [00:10<00:08, 14.97it/s][2026-01-17 10:42:59] INFO:     127.0.0.1:47854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:59] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:11<00:08, 14.02it/s][2026-01-17 10:42:59] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:59] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:59] INFO:     127.0.0.1:48684 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:11<00:08, 14.24it/s]
 40%|████      | 80/200 [00:11<00:07, 15.98it/s][2026-01-17 10:42:59] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:59] INFO:     127.0.0.1:47822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:59] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:59] INFO:     127.0.0.1:48224 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:11<00:07, 15.80it/s]
 42%|████▏     | 84/200 [00:11<00:05, 20.82it/s]
 42%|████▏     | 84/200 [00:11<00:05, 20.82it/s][2026-01-17 10:42:59] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:59] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:59] INFO:     127.0.0.1:47464 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:11<00:07, 15.05it/s][2026-01-17 10:42:59] INFO:     127.0.0.1:47564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:59] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:11<00:07, 15.20it/s][2026-01-17 10:42:59] INFO:     127.0.0.1:48526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:42:59] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:47778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48326 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:11<00:06, 16.97it/s]
 47%|████▋     | 94/200 [00:11<00:04, 24.94it/s]
 47%|████▋     | 94/200 [00:11<00:04, 24.94it/s][2026-01-17 10:43:00] INFO:     127.0.0.1:47656 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:12<00:03, 28.81it/s][2026-01-17 10:43:00] INFO:     127.0.0.1:48660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:12<00:03, 29.28it/s][2026-01-17 10:43:00] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:47742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48730 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:12<00:03, 29.93it/s][2026-01-17 10:43:00] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00 TP0] Decode batch, #running-req: 93, #token: 20864, token usage: 0.02, npu graph: False, gen throughput (token/s): 1385.95, #queue-req: 0,
[2026-01-17 10:43:00] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48880 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:12<00:03, 23.51it/s][2026-01-17 10:43:00] INFO:     127.0.0.1:48742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:47522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48928 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:12<00:03, 22.87it/s]
 57%|█████▊    | 115/200 [00:12<00:03, 24.29it/s][2026-01-17 10:43:00] INFO:     127.0.0.1:47534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:00] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:12<00:03, 24.06it/s][2026-01-17 10:43:01] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:39476 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:12<00:03, 23.95it/s][2026-01-17 10:43:01] INFO:     127.0.0.1:47636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:48104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:48634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:48940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:39568 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [00:13<00:03, 23.67it/s]
 63%|██████▎   | 126/200 [00:13<00:02, 29.34it/s]
 63%|██████▎   | 126/200 [00:13<00:02, 29.34it/s][2026-01-17 10:43:01] INFO:     127.0.0.1:48308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:47602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:48914 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:13<00:02, 27.43it/s]
 65%|██████▌   | 130/200 [00:13<00:02, 28.32it/s][2026-01-17 10:43:01] INFO:     127.0.0.1:48392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:39572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:47490 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:13<00:02, 23.46it/s][2026-01-17 10:43:01] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:01] INFO:     127.0.0.1:49062 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [00:13<00:03, 18.24it/s]
 70%|██████▉   | 139/200 [00:13<00:03, 19.33it/s]
 70%|██████▉   | 139/200 [00:13<00:03, 19.33it/s]
 70%|██████▉   | 139/200 [00:13<00:03, 19.33it/s][2026-01-17 10:43:01] INFO:     127.0.0.1:39588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:39424 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:14<00:03, 15.82it/s]
 72%|███████▏  | 143/200 [00:14<00:03, 14.79it/s][2026-01-17 10:43:02] INFO:     127.0.0.1:48608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:47736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:48862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:39600 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:14<00:03, 14.94it/s]
 74%|███████▍  | 148/200 [00:14<00:02, 24.33it/s]
 74%|███████▍  | 148/200 [00:14<00:02, 24.33it/s]
 74%|███████▍  | 148/200 [00:14<00:02, 24.33it/s][2026-01-17 10:43:02] INFO:     127.0.0.1:47936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:47990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:14<00:02, 17.76it/s]
 76%|███████▋  | 153/200 [00:14<00:02, 15.97it/s]
 76%|███████▋  | 153/200 [00:14<00:02, 15.97it/s][2026-01-17 10:43:02] INFO:     127.0.0.1:48974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:48708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:02] INFO:     127.0.0.1:39506 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:14<00:02, 15.81it/s]
 78%|███████▊  | 156/200 [00:14<00:02, 17.21it/s][2026-01-17 10:43:02] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:03] INFO:     127.0.0.1:39450 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:14<00:03, 11.82it/s][2026-01-17 10:43:03 TP0] Decode batch, #running-req: 43, #token: 11904, token usage: 0.01, npu graph: False, gen throughput (token/s): 1006.95, #queue-req: 0,
[2026-01-17 10:43:03] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:03] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:15<00:04,  9.33it/s][2026-01-17 10:43:03] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:03] INFO:     127.0.0.1:48954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:03] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:15<00:03,  9.65it/s]
 82%|████████▏ | 163/200 [00:15<00:03, 11.19it/s][2026-01-17 10:43:03] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:03] INFO:     127.0.0.1:48596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:03] INFO:     127.0.0.1:48892 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:15<00:02, 11.92it/s]
 83%|████████▎ | 166/200 [00:15<00:02, 14.20it/s][2026-01-17 10:43:03] INFO:     127.0.0.1:49016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:04] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:15<00:02, 10.92it/s][2026-01-17 10:43:04] INFO:     127.0.0.1:39534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:04] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:16<00:02, 10.03it/s][2026-01-17 10:43:04] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:04] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:04] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:16<00:02, 12.66it/s][2026-01-17 10:43:04] INFO:     127.0.0.1:39436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:04] INFO:     127.0.0.1:39488 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:16<00:02, 11.01it/s][2026-01-17 10:43:04] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:05] INFO:     127.0.0.1:49032 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:16<00:02,  8.60it/s][2026-01-17 10:43:05] INFO:     127.0.0.1:49002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:05] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:17<00:02,  8.48it/s][2026-01-17 10:43:05] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:05] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:17<00:02,  8.41it/s][2026-01-17 10:43:05 TP0] Decode batch, #running-req: 20, #token: 6272, token usage: 0.01, npu graph: False, gen throughput (token/s): 490.25, #queue-req: 0,
[2026-01-17 10:43:05] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:05] INFO:     127.0.0.1:39520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:05] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:17<00:01,  8.23it/s][2026-01-17 10:43:05] INFO:     127.0.0.1:49026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:06] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:18<00:01,  8.17it/s][2026-01-17 10:43:06] INFO:     127.0.0.1:47968 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:18<00:03,  4.19it/s][2026-01-17 10:43:07] INFO:     127.0.0.1:48830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:07] INFO:     127.0.0.1:39614 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:19<00:02,  5.44it/s][2026-01-17 10:43:07] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:07] INFO:     127.0.0.1:47772 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:19<00:01,  6.41it/s][2026-01-17 10:43:07] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:43:08 TP0] Decode batch, #running-req: 8, #token: 3456, token usage: 0.00, npu graph: False, gen throughput (token/s): 204.06, #queue-req: 0,
[2026-01-17 10:43:08] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:20<00:01,  3.60it/s][2026-01-17 10:43:09] INFO:     127.0.0.1:39454 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:21<00:02,  2.54it/s][2026-01-17 10:43:09] INFO:     127.0.0.1:49050 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:21<00:01,  2.95it/s][2026-01-17 10:43:09] INFO:     127.0.0.1:47506 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:21<00:01,  2.76it/s][2026-01-17 10:43:10 TP0] Decode batch, #running-req: 4, #token: 2304, token usage: 0.00, npu graph: False, gen throughput (token/s): 99.21, #queue-req: 0,
[2026-01-17 10:43:11] INFO:     127.0.0.1:39560 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:23<00:01,  1.58it/s][2026-01-17 10:43:11] INFO:     127.0.0.1:48814 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:23<00:01,  1.99it/s][2026-01-17 10:43:12 TP0] Decode batch, #running-req: 2, #token: 1536, token usage: 0.00, npu graph: False, gen throughput (token/s): 45.47, #queue-req: 0,
[2026-01-17 10:43:13] INFO:     127.0.0.1:47474 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:25<00:01,  1.02s/it][2026-01-17 10:43:14] INFO:     127.0.0.1:39468 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:26<00:00,  1.02s/it]
100%|██████████| 200/200 [00:26<00:00,  7.43it/s]
.
----------------------------------------------------------------------
Ran 1 test in 308.763s

OK
Accuracy: 0.760
Invalid: 0.000
Latency: 27.029 s
Output throughput: 685.977 token/s
.
.
End (29/62):
filename='ascend/llm_models/test_ascend_dbrx_instruct.py', elapsed=319, estimated_time=400
.
.

.
.
Begin (30/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_baichuan2_13b_chat.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 10:43:35] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat', tokenizer_path='/root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=128, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=-1, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=992462196, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
[2026-01-17 10:43:35] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 10:43:45] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 10:43:46] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 10:43:46] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 10:43:47] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 10:43:47] Load weight begin. avail mem=60.81 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return func(*args, **kwargs)

Loading pt checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading pt checkpoint shards:  33% Completed | 1/3 [00:26<00:53, 26.63s/it]

Loading pt checkpoint shards:  67% Completed | 2/3 [00:53<00:26, 26.84s/it]

Loading pt checkpoint shards: 100% Completed | 3/3 [01:20<00:00, 26.71s/it]

Loading pt checkpoint shards: 100% Completed | 3/3 [01:20<00:00, 26.73s/it]

[2026-01-17 10:45:08] Load weight end. type=BaichuanForCausalLM, dtype=torch.bfloat16, avail mem=34.91 GB, mem usage=25.90 GB.
[2026-01-17 10:45:08] Using KV cache dtype: torch.bfloat16
[2026-01-17 10:45:08] The available memory for KV cache is 22.75 GB.
[2026-01-17 10:45:08] KV Cache is allocated. #tokens: 29696, K size: 11.38 GB, V size: 11.38 GB
[2026-01-17 10:45:08] Memory pool end. avail mem=12.16 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 10:45:09] max_total_num_tokens=29696, chunked_prefill_size=-1, max_prefill_tokens=16384, max_running_requests=128, context_len=4096, available_gpu_mem=12.16 GB
[2026-01-17 10:45:09] INFO:     Started server process [288214]
[2026-01-17 10:45:09] INFO:     Waiting for application startup.
[2026-01-17 10:45:09] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.3, 'top_k': 5, 'top_p': 0.85}
[2026-01-17 10:45:09] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.3, 'top_k': 5, 'top_p': 0.85}
[2026-01-17 10:45:09] INFO:     Application startup complete.
[2026-01-17 10:45:09] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 10:45:10] INFO:     127.0.0.1:54690 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 10:45:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:45:15] INFO:     127.0.0.1:54704 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 10:45:23] INFO:     127.0.0.1:54692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:45:23] The server is fired up and ready to roll!
[2026-01-17 10:45:25] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:45:26] INFO:     127.0.0.1:45358 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 10:45:26] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 10:45:26] INFO:     127.0.0.1:45362 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 10:45:26] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:45:26] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --max-running-requests 128 --disable-radix-cache --chunked-prefill-size -1 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestBaichuan.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 10:45:26] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 10:45:26] Prefill batch, #new-seq: 34, #new-token: 8704, #cached-token: 0, token usage: 0.01, #running-req: 1, #queue-req: 0,
[2026-01-17 10:45:28] Prefill batch, #new-seq: 10, #new-token: 2688, #cached-token: 0, token usage: 0.30, #running-req: 35, #queue-req: 83,
[2026-01-17 10:45:59] Decode batch, #running-req: 45, #token: 12160, token usage: 0.41, npu graph: False, gen throughput (token/s): 10.42, #queue-req: 83,
[2026-01-17 10:46:08] INFO:     127.0.0.1:45502 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:41<2:16:51, 41.26s/it][2026-01-17 10:46:09] Prefill batch, #new-seq: 5, #new-token: 1280, #cached-token: 0, token usage: 0.41, #running-req: 44, #queue-req: 79,
[2026-01-17 10:46:16] INFO:     127.0.0.1:45382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:46:16] INFO:     127.0.0.1:45582 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:49<1:12:54, 22.09s/it]
  2%|▏         | 3/200 [00:49<35:59, 10.96s/it]  [2026-01-17 10:46:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.47, #running-req: 47, #queue-req: 80,
[2026-01-17 10:46:20] INFO:     127.0.0.1:45632 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:53<28:28,  8.72s/it][2026-01-17 10:46:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.49, #running-req: 47, #queue-req: 80,
[2026-01-17 10:46:24] INFO:     127.0.0.1:45756 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:57<23:22,  7.19s/it][2026-01-17 10:46:26] INFO:     127.0.0.1:45684 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:59<18:34,  5.75s/it][2026-01-17 10:46:27] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.50, #running-req: 46, #queue-req: 81,
[2026-01-17 10:46:27] INFO:     127.0.0.1:45592 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [01:01<14:10,  4.41s/it][2026-01-17 10:46:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.51, #running-req: 46, #queue-req: 81,
[2026-01-17 10:46:42] INFO:     127.0.0.1:45378 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [01:16<24:12,  7.57s/it][2026-01-17 10:46:44] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.51, #running-req: 46, #queue-req: 81,
[2026-01-17 10:46:44] INFO:     127.0.0.1:45740 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [01:17<18:05,  5.68s/it][2026-01-17 10:46:45] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.53, #running-req: 46, #queue-req: 81,
[2026-01-17 10:46:49] Decode batch, #running-req: 47, #token: 16512, token usage: 0.56, npu graph: False, gen throughput (token/s): 37.35, #queue-req: 81,
[2026-01-17 10:47:00] INFO:     127.0.0.1:45566 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [01:33<28:03,  8.86s/it][2026-01-17 10:47:05] INFO:     127.0.0.1:45542 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [01:39<24:42,  7.84s/it][2026-01-17 10:47:07] Prefill batch, #new-seq: 2, #new-token: 640, #cached-token: 0, token usage: 0.53, #running-req: 45, #queue-req: 81,
[2026-01-17 10:47:07] INFO:     127.0.0.1:45508 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [01:40<18:28,  5.89s/it][2026-01-17 10:47:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.56, #running-req: 46, #queue-req: 81,
[2026-01-17 10:47:11] INFO:     127.0.0.1:45648 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [01:44<17:04,  5.48s/it][2026-01-17 10:47:13] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.55, #running-req: 46, #queue-req: 81,
[2026-01-17 10:47:17] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [01:50<17:26,  5.63s/it][2026-01-17 10:47:19] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.55, #running-req: 46, #queue-req: 81,
[2026-01-17 10:47:26] INFO:     127.0.0.1:45614 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [02:00<20:41,  6.71s/it][2026-01-17 10:47:28] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.55, #running-req: 46, #queue-req: 81,
[2026-01-17 10:47:28] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [02:01<16:00,  5.22s/it][2026-01-17 10:47:30] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.54, #running-req: 46, #queue-req: 80,
[2026-01-17 10:47:30] INFO:     127.0.0.1:45666 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [02:03<12:36,  4.13s/it][2026-01-17 10:47:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.55, #running-req: 47, #queue-req: 80,
[2026-01-17 10:47:36] INFO:     127.0.0.1:45490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:47:36] INFO:     127.0.0.1:45686 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [02:09<14:05,  4.65s/it]
 10%|▉         | 19/200 [02:09<11:37,  3.85s/it][2026-01-17 10:47:37] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.54, #running-req: 46, #queue-req: 80,
[2026-01-17 10:47:47] Decode batch, #running-req: 48, #token: 16768, token usage: 0.56, npu graph: False, gen throughput (token/s): 32.18, #queue-req: 80,
[2026-01-17 10:47:55] INFO:     127.0.0.1:45412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:47:55] INFO:     127.0.0.1:45770 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [02:28<23:02,  7.68s/it]
 10%|█         | 21/200 [02:28<25:20,  8.50s/it][2026-01-17 10:47:56] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.55, #running-req: 46, #queue-req: 80,
[2026-01-17 10:48:00] INFO:     127.0.0.1:45604 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [02:33<22:33,  7.60s/it][2026-01-17 10:48:01] Prefill batch, #new-seq: 2, #new-token: 640, #cached-token: 0, token usage: 0.55, #running-req: 47, #queue-req: 79,
[2026-01-17 10:48:01] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:48:01] INFO:     127.0.0.1:45524 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [02:34<18:04,  6.13s/it]
 12%|█▏        | 24/200 [02:34<11:31,  3.93s/it][2026-01-17 10:48:03] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.55, #running-req: 47, #queue-req: 79,
[2026-01-17 10:48:09] INFO:     127.0.0.1:45456 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [02:42<13:51,  4.75s/it][2026-01-17 10:48:10] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.56, #running-req: 48, #queue-req: 79,
[2026-01-17 10:48:10] INFO:     127.0.0.1:45692 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [02:43<11:27,  3.95s/it][2026-01-17 10:48:10] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [02:43<08:30,  2.95s/it][2026-01-17 10:48:12] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.55, #running-req: 47, #queue-req: 79,
[2026-01-17 10:48:16] INFO:     127.0.0.1:45420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:48:16] INFO:     127.0.0.1:45550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:48:16] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [02:49<10:47,  3.76s/it]
 15%|█▌        | 30/200 [02:49<07:43,  2.73s/it]
 15%|█▌        | 30/200 [02:49<07:43,  2.73s/it][2026-01-17 10:48:18] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 0, token usage: 0.52, #running-req: 46, #queue-req: 79,
[2026-01-17 10:48:18] INFO:     127.0.0.1:45664 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [02:51<06:57,  2.47s/it][2026-01-17 10:48:19] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.54, #running-req: 48, #queue-req: 78,
[2026-01-17 10:48:24] INFO:     127.0.0.1:45690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:48:24] INFO:     127.0.0.1:45890 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [02:57<09:00,  3.22s/it]
 16%|█▋        | 33/200 [02:57<08:44,  3.14s/it][2026-01-17 10:48:25] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.54, #running-req: 48, #queue-req: 79,
[2026-01-17 10:48:29] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [03:02<10:04,  3.64s/it][2026-01-17 10:48:31] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.53, #running-req: 48, #queue-req: 78,
[2026-01-17 10:48:31] INFO:     127.0.0.1:45912 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [03:04<08:44,  3.18s/it][2026-01-17 10:48:31] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:48:31] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [03:04<06:38,  2.43s/it]
 18%|█▊        | 37/200 [03:04<03:56,  1.45s/it][2026-01-17 10:48:32] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 0, token usage: 0.53, #running-req: 47, #queue-req: 78,
[2026-01-17 10:48:33] INFO:     127.0.0.1:45426 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [03:06<04:05,  1.52s/it][2026-01-17 10:48:34] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.54, #running-req: 49, #queue-req: 78,
[2026-01-17 10:48:34] INFO:     127.0.0.1:45838 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [03:08<04:11,  1.56s/it][2026-01-17 10:48:36] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.53, #running-req: 49, #queue-req: 77,
[2026-01-17 10:48:39] INFO:     127.0.0.1:45876 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [03:12<06:30,  2.44s/it][2026-01-17 10:48:41] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.55, #running-req: 50, #queue-req: 77,
[2026-01-17 10:48:45] INFO:     127.0.0.1:45798 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [03:18<08:53,  3.36s/it][2026-01-17 10:48:46] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.55, #running-req: 50, #queue-req: 77,
[2026-01-17 10:48:49] Decode batch, #running-req: 51, #token: 16640, token usage: 0.56, npu graph: False, gen throughput (token/s): 31.45, #queue-req: 77,
[2026-01-17 10:48:51] INFO:     127.0.0.1:45674 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [03:24<10:34,  4.02s/it][2026-01-17 10:48:52] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.55, #running-req: 50, #queue-req: 77,
[2026-01-17 10:48:55] INFO:     127.0.0.1:45440 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [03:28<10:47,  4.13s/it][2026-01-17 10:48:57] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.55, #running-req: 50, #queue-req: 77,
[2026-01-17 10:48:57] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [03:30<08:45,  3.37s/it][2026-01-17 10:48:58] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.54, #running-req: 50, #queue-req: 77,
[2026-01-17 10:49:05] INFO:     127.0.0.1:45474 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [03:39<12:45,  4.94s/it][2026-01-17 10:49:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.56, #running-req: 50, #queue-req: 77,
[2026-01-17 10:49:15] INFO:     127.0.0.1:45724 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [03:48<16:17,  6.34s/it][2026-01-17 10:49:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.56, #running-req: 50, #queue-req: 77,
[2026-01-17 10:49:22] INFO:     127.0.0.1:45622 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [03:55<16:25,  6.44s/it][2026-01-17 10:49:24] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.56, #running-req: 50, #queue-req: 76,
[2026-01-17 10:49:24] INFO:     127.0.0.1:45992 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [03:57<12:43,  5.03s/it][2026-01-17 10:49:25] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.57, #running-req: 51, #queue-req: 76,
[2026-01-17 10:49:29] INFO:     127.0.0.1:45980 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [04:02<12:53,  5.12s/it][2026-01-17 10:49:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.57, #running-req: 51, #queue-req: 76,
[2026-01-17 10:49:45] INFO:     127.0.0.1:46002 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [04:18<21:06,  8.44s/it][2026-01-17 10:49:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.58, #running-req: 51, #queue-req: 76,
[2026-01-17 10:49:47] INFO:     127.0.0.1:46122 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [04:20<15:48,  6.37s/it][2026-01-17 10:49:53] Decode batch, #running-req: 51, #token: 17920, token usage: 0.60, npu graph: False, gen throughput (token/s): 32.32, #queue-req: 77,
[2026-01-17 10:49:56] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [04:29<17:45,  7.20s/it][2026-01-17 10:49:57] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 50, #queue-req: 77,
[2026-01-17 10:50:01] INFO:     127.0.0.1:45826 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [04:34<15:50,  6.47s/it][2026-01-17 10:50:02] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 50, #queue-req: 77,
[2026-01-17 10:50:02] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [04:35<12:12,  5.02s/it][2026-01-17 10:50:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 50, #queue-req: 77,
[2026-01-17 10:50:04] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:50:04] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:50:04] INFO:     127.0.0.1:46160 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [04:37<09:39,  4.00s/it]
 28%|██▊       | 57/200 [04:37<03:39,  1.53s/it]
 28%|██▊       | 57/200 [04:37<03:39,  1.53s/it][2026-01-17 10:50:05] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.59, #running-req: 48, #queue-req: 78,
[2026-01-17 10:50:05] INFO:     127.0.0.1:46038 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [04:38<03:37,  1.53s/it][2026-01-17 10:50:07] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.59, #running-req: 49, #queue-req: 77,
[2026-01-17 10:50:17] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [04:50<08:39,  3.69s/it][2026-01-17 10:50:18] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 50, #queue-req: 77,
[2026-01-17 10:50:21] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [04:55<09:06,  3.90s/it][2026-01-17 10:50:23] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 50, #queue-req: 77,
[2026-01-17 10:50:26] INFO:     127.0.0.1:45704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:50:26] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [04:59<09:34,  4.13s/it]
 31%|███       | 62/200 [04:59<07:51,  3.41s/it][2026-01-17 10:50:28] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.58, #running-req: 49, #queue-req: 77,
[2026-01-17 10:50:28] INFO:     127.0.0.1:45820 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [05:01<06:47,  2.98s/it][2026-01-17 10:50:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.60, #running-req: 50, #queue-req: 77,
[2026-01-17 10:50:36] INFO:     127.0.0.1:46036 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [05:09<09:28,  4.18s/it][2026-01-17 10:50:37] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.60, #running-req: 50, #queue-req: 77,
[2026-01-17 10:50:37] INFO:     127.0.0.1:46244 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [05:11<08:02,  3.57s/it][2026-01-17 10:50:39] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.60, #running-req: 50, #queue-req: 76,
[2026-01-17 10:50:39] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [05:13<07:00,  3.14s/it][2026-01-17 10:50:41] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.60, #running-req: 51, #queue-req: 76,
[2026-01-17 10:50:45] INFO:     127.0.0.1:45796 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [05:18<08:20,  3.76s/it][2026-01-17 10:50:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.59, #running-req: 51, #queue-req: 76,
[2026-01-17 10:50:50] INFO:     127.0.0.1:45924 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [05:23<09:14,  4.20s/it][2026-01-17 10:50:52] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.59, #running-req: 51, #queue-req: 75,
[2026-01-17 10:50:57] INFO:     127.0.0.1:45908 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [05:30<10:57,  5.02s/it][2026-01-17 10:50:59] Decode batch, #running-req: 53, #token: 17920, token usage: 0.60, npu graph: False, gen throughput (token/s): 30.76, #queue-req: 75,
[2026-01-17 10:50:59] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.60, #running-req: 52, #queue-req: 75,
[2026-01-17 10:50:59] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [05:32<08:48,  4.07s/it][2026-01-17 10:51:00] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.60, #running-req: 52, #queue-req: 75,
[2026-01-17 10:51:07] INFO:     127.0.0.1:46044 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [05:40<11:06,  5.17s/it][2026-01-17 10:51:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.60, #running-req: 52, #queue-req: 75,
[2026-01-17 10:51:13] INFO:     127.0.0.1:46030 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [05:46<11:45,  5.51s/it][2026-01-17 10:51:15] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.59, #running-req: 52, #queue-req: 74,
[2026-01-17 10:51:15] INFO:     127.0.0.1:46098 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [05:48<09:08,  4.32s/it][2026-01-17 10:51:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 53, #queue-req: 73,
[2026-01-17 10:51:22] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [05:56<11:18,  5.39s/it][2026-01-17 10:51:24] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 53, #queue-req: 72,
[2026-01-17 10:51:24] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [05:58<09:06,  4.37s/it][2026-01-17 10:51:26] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 53, #queue-req: 71,
[2026-01-17 10:51:30] INFO:     127.0.0.1:45940 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [06:03<09:57,  4.82s/it][2026-01-17 10:51:32] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.60, #running-req: 53, #queue-req: 69,
[2026-01-17 10:51:32] INFO:     127.0.0.1:45898 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [06:05<08:04,  3.94s/it][2026-01-17 10:51:32] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:51:32] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [06:06<05:41,  2.80s/it]
 40%|███▉      | 79/200 [06:06<03:06,  1.54s/it][2026-01-17 10:51:34] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.59, #running-req: 52, #queue-req: 67,
[2026-01-17 10:51:38] INFO:     127.0.0.1:46376 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [06:11<05:03,  2.53s/it][2026-01-17 10:51:40] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.60, #running-req: 53, #queue-req: 66,
[2026-01-17 10:51:43] INFO:     127.0.0.1:46138 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [06:17<06:37,  3.34s/it][2026-01-17 10:51:45] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.60, #running-req: 53, #queue-req: 64,
[2026-01-17 10:51:45] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [06:19<05:48,  2.96s/it][2026-01-17 10:51:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 54, #queue-req: 63,
[2026-01-17 10:51:47] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [06:20<05:03,  2.60s/it][2026-01-17 10:51:49] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.60, #running-req: 54, #queue-req: 62,
[2026-01-17 10:51:53] INFO:     127.0.0.1:46216 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [06:27<07:04,  3.66s/it][2026-01-17 10:51:55] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.60, #running-req: 54, #queue-req: 61,
[2026-01-17 10:51:55] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [06:28<05:54,  3.08s/it][2026-01-17 10:51:57] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.59, #running-req: 54, #queue-req: 59,
[2026-01-17 10:52:00] INFO:     127.0.0.1:46306 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [06:33<06:51,  3.61s/it][2026-01-17 10:52:02] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.60, #running-req: 55, #queue-req: 58,
[2026-01-17 10:52:06] Decode batch, #running-req: 56, #token: 18304, token usage: 0.62, npu graph: False, gen throughput (token/s): 31.85, #queue-req: 58,
[2026-01-17 10:52:12] INFO:     127.0.0.1:46422 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [06:45<11:25,  6.07s/it][2026-01-17 10:52:14] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.60, #running-req: 55, #queue-req: 56,
[2026-01-17 10:52:14] INFO:     127.0.0.1:46128 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [06:47<09:05,  4.87s/it][2026-01-17 10:52:16] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.61, #running-req: 56, #queue-req: 55,
[2026-01-17 10:52:17] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [06:50<08:02,  4.34s/it][2026-01-17 10:52:19] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 56, #queue-req: 54,
[2026-01-17 10:52:26] INFO:     127.0.0.1:46112 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [06:59<10:15,  5.60s/it][2026-01-17 10:52:27] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 56, #queue-req: 53,
[2026-01-17 10:52:30] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [07:04<09:44,  5.36s/it][2026-01-17 10:52:32] Prefill batch, #new-seq: 2, #new-token: 640, #cached-token: 0, token usage: 0.60, #running-req: 56, #queue-req: 51,
[2026-01-17 10:52:32] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [07:05<07:37,  4.24s/it][2026-01-17 10:52:32] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [07:05<05:22,  3.01s/it][2026-01-17 10:52:34] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.61, #running-req: 56, #queue-req: 50,
[2026-01-17 10:52:37] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [07:10<06:19,  3.58s/it][2026-01-17 10:52:39] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.61, #running-req: 56, #queue-req: 48,
[2026-01-17 10:52:39] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [07:12<05:14,  2.99s/it][2026-01-17 10:52:40] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.63, #running-req: 57, #queue-req: 47,
[2026-01-17 10:52:47] INFO:     127.0.0.1:46454 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [07:20<08:01,  4.63s/it][2026-01-17 10:52:52] INFO:     127.0.0.1:46292 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [07:25<08:08,  4.74s/it][2026-01-17 10:52:54] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.61, #running-req: 56, #queue-req: 45,
[2026-01-17 10:52:54] INFO:     127.0.0.1:45950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:52:54] INFO:     127.0.0.1:46410 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [07:27<06:36,  3.89s/it]
 50%|████▉     | 99/200 [07:27<04:15,  2.53s/it][2026-01-17 10:52:54] INFO:     127.0.0.1:46478 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [07:27<03:14,  1.95s/it][2026-01-17 10:52:56] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.61, #running-req: 55, #queue-req: 43,
[2026-01-17 10:53:15] INFO:     127.0.0.1:46496 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [07:49<11:29,  6.96s/it][2026-01-17 10:53:19] Decode batch, #running-req: 56, #token: 19456, token usage: 0.66, npu graph: False, gen throughput (token/s): 31.01, #queue-req: 43,
[2026-01-17 10:53:19] INFO:     127.0.0.1:46516 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [07:53<10:06,  6.18s/it][2026-01-17 10:53:21] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.66, #running-req: 55, #queue-req: 42,
[2026-01-17 10:53:28] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [08:01<10:57,  6.78s/it][2026-01-17 10:53:30] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.65, #running-req: 55, #queue-req: 40,
[2026-01-17 10:53:30] INFO:     127.0.0.1:46384 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [08:03<08:40,  5.42s/it][2026-01-17 10:53:30] INFO:     127.0.0.1:46488 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [08:03<06:09,  3.89s/it][2026-01-17 10:53:32] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.66, #running-req: 55, #queue-req: 39,
[2026-01-17 10:53:36] INFO:     127.0.0.1:46192 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [08:09<07:09,  4.57s/it][2026-01-17 10:53:38] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.65, #running-req: 55, #queue-req: 38,
[2026-01-17 10:53:38] INFO:     127.0.0.1:46174 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [08:12<06:01,  3.89s/it][2026-01-17 10:53:40] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.65, #running-req: 55, #queue-req: 37,
[2026-01-17 10:53:45] INFO:     127.0.0.1:46288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:53:45] INFO:     127.0.0.1:46456 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [08:18<07:02,  4.59s/it]
 55%|█████▍    | 109/200 [08:18<05:56,  3.92s/it][2026-01-17 10:53:46] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.64, #running-req: 54, #queue-req: 35,
[2026-01-17 10:53:52] INFO:     127.0.0.1:46228 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [08:26<07:16,  4.86s/it][2026-01-17 10:53:54] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.65, #running-req: 55, #queue-req: 34,
[2026-01-17 10:54:01] INFO:     127.0.0.1:46460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:54:01] INFO:     127.0.0.1:56674 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [08:35<08:49,  5.95s/it]
 56%|█████▌    | 112/200 [08:35<07:49,  5.34s/it][2026-01-17 10:54:03] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 0, token usage: 0.62, #running-req: 54, #queue-req: 31,
[2026-01-17 10:54:03] INFO:     127.0.0.1:46100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:54:03] INFO:     127.0.0.1:37546 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [08:36<06:30,  4.49s/it]
 57%|█████▋    | 114/200 [08:36<04:20,  3.03s/it][2026-01-17 10:54:03] INFO:     127.0.0.1:46492 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [08:36<03:23,  2.39s/it][2026-01-17 10:54:05] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.64, #running-req: 54, #queue-req: 29,
[2026-01-17 10:54:05] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [08:38<03:08,  2.25s/it][2026-01-17 10:54:07] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.65, #running-req: 55, #queue-req: 27,
[2026-01-17 10:54:07] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [08:40<02:56,  2.13s/it][2026-01-17 10:54:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.66, #running-req: 56, #queue-req: 26,
[2026-01-17 10:54:12] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [08:45<04:01,  2.95s/it][2026-01-17 10:54:14] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.65, #running-req: 56, #queue-req: 24,
[2026-01-17 10:54:14] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [08:47<03:34,  2.65s/it][2026-01-17 10:54:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.67, #running-req: 57, #queue-req: 23,
[2026-01-17 10:54:22] INFO:     127.0.0.1:37556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:54:22] INFO:     127.0.0.1:34296 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [08:55<05:34,  4.18s/it]
 60%|██████    | 121/200 [08:55<05:25,  4.11s/it][2026-01-17 10:54:24] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.66, #running-req: 56, #queue-req: 21,
[2026-01-17 10:54:24] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [08:57<04:35,  3.53s/it][2026-01-17 10:54:24] INFO:     127.0.0.1:46400 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [08:57<03:24,  2.66s/it][2026-01-17 10:54:25] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.66, #running-req: 56, #queue-req: 20,
[2026-01-17 10:54:35] Decode batch, #running-req: 57, #token: 19712, token usage: 0.66, npu graph: False, gen throughput (token/s): 29.46, #queue-req: 20,
[2026-01-17 10:54:36] INFO:     127.0.0.1:39842 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [09:09<06:27,  5.10s/it][2026-01-17 10:54:37] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.67, #running-req: 56, #queue-req: 19,
[2026-01-17 10:54:37] INFO:     127.0.0.1:46432 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [09:11<05:17,  4.23s/it][2026-01-17 10:54:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.66, #running-req: 56, #queue-req: 18,
[2026-01-17 10:54:43] INFO:     127.0.0.1:46354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:54:43] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [09:17<05:47,  4.69s/it]
 64%|██████▎   | 127/200 [09:17<04:44,  3.90s/it][2026-01-17 10:54:45] Prefill batch, #new-seq: 3, #new-token: 896, #cached-token: 0, token usage: 0.63, #running-req: 55, #queue-req: 15,
[2026-01-17 10:54:45] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [09:18<04:04,  3.40s/it][2026-01-17 10:54:45] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:54:45] INFO:     127.0.0.1:60216 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [09:19<03:02,  2.57s/it]
 65%|██████▌   | 130/200 [09:19<01:46,  1.52s/it][2026-01-17 10:54:47] Prefill batch, #new-seq: 3, #new-token: 896, #cached-token: 0, token usage: 0.64, #running-req: 55, #queue-req: 12,
[2026-01-17 10:55:03] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:55:03] INFO:     127.0.0.1:39866 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [09:36<05:59,  5.21s/it]
 66%|██████▌   | 132/200 [09:36<07:29,  6.61s/it][2026-01-17 10:55:05] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.66, #running-req: 56, #queue-req: 10,
[2026-01-17 10:55:16] INFO:     127.0.0.1:46268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:55:16] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [09:50<09:06,  8.16s/it]
 67%|██████▋   | 134/200 [09:50<08:24,  7.64s/it][2026-01-17 10:55:18] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.66, #running-req: 56, #queue-req: 8,
[2026-01-17 10:55:18] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [09:52<06:58,  6.43s/it][2026-01-17 10:55:20] Prefill batch, #new-seq: 2, #new-token: 640, #cached-token: 0, token usage: 0.66, #running-req: 57, #queue-req: 6,
[2026-01-17 10:55:26] INFO:     127.0.0.1:34292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:55:26] INFO:     127.0.0.1:55090 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [09:59<07:05,  6.65s/it]
 68%|██████▊   | 137/200 [09:59<05:42,  5.44s/it][2026-01-17 10:55:27] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.66, #running-req: 57, #queue-req: 4,
[2026-01-17 10:55:44] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [10:17<08:34,  8.30s/it][2026-01-17 10:55:46] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.69, #running-req: 58, #queue-req: 2,
[2026-01-17 10:55:50] INFO:     127.0.0.1:34302 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:55:50] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [10:23<07:56,  7.81s/it]
 70%|███████   | 140/200 [10:23<05:53,  5.90s/it][2026-01-17 10:55:52] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 0, token usage: 0.69, #running-req: 58, #queue-req: 0,
[2026-01-17 10:55:52] Decode batch, #running-req: 58, #token: 20096, token usage: 0.68, npu graph: False, gen throughput (token/s): 30.09, #queue-req: 0,
[2026-01-17 10:55:52] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:55:52] INFO:     127.0.0.1:45216 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [10:26<04:58,  5.06s/it]
 71%|███████   | 142/200 [10:26<03:23,  3.50s/it][2026-01-17 10:56:08] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [10:41<05:51,  6.17s/it][2026-01-17 10:56:12] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:56:12] INFO:     127.0.0.1:45234 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [10:46<05:19,  5.70s/it]
 72%|███████▎  | 145/200 [10:46<03:53,  4.25s/it][2026-01-17 10:56:29] INFO:     127.0.0.1:34276 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [11:02<06:16,  6.97s/it][2026-01-17 10:56:35] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [11:08<05:55,  6.70s/it][2026-01-17 10:56:38] INFO:     127.0.0.1:41638 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [11:12<05:10,  5.97s/it][2026-01-17 10:56:44] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [11:17<05:01,  5.91s/it][2026-01-17 10:56:48] INFO:     127.0.0.1:37548 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [11:21<04:24,  5.30s/it][2026-01-17 10:56:52] INFO:     127.0.0.1:39800 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [11:25<03:58,  4.86s/it][2026-01-17 10:56:54] INFO:     127.0.0.1:39850 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [11:27<03:12,  4.02s/it][2026-01-17 10:57:07] INFO:     127.0.0.1:39854 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [11:40<05:15,  6.71s/it][2026-01-17 10:57:09] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:57:09] INFO:     127.0.0.1:46576 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [11:42<04:03,  5.29s/it]
 78%|███████▊  | 155/200 [11:42<02:28,  3.30s/it][2026-01-17 10:57:12] Decode batch, #running-req: 45, #token: 17792, token usage: 0.60, npu graph: False, gen throughput (token/s): 26.57, #queue-req: 0,
[2026-01-17 10:57:12] INFO:     127.0.0.1:58574 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [11:45<02:27,  3.36s/it][2026-01-17 10:57:23] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:57:23] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [11:56<03:43,  5.20s/it]
 79%|███████▉  | 158/200 [11:56<03:38,  5.21s/it][2026-01-17 10:57:24] INFO:     127.0.0.1:41644 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [11:57<02:57,  4.33s/it][2026-01-17 10:57:27] INFO:     127.0.0.1:46462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:57:27] INFO:     127.0.0.1:33012 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [12:00<02:37,  3.95s/it]
 80%|████████  | 161/200 [12:00<01:52,  2.89s/it][2026-01-17 10:57:28] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [12:01<01:36,  2.53s/it][2026-01-17 10:57:31] INFO:     127.0.0.1:45218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:57:31] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [12:04<01:34,  2.55s/it]
 82%|████████▏ | 164/200 [12:04<01:13,  2.03s/it][2026-01-17 10:57:32] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [12:05<01:04,  1.84s/it][2026-01-17 10:57:37] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [12:10<01:28,  2.59s/it][2026-01-17 10:57:38] INFO:     127.0.0.1:34010 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [12:12<01:15,  2.29s/it][2026-01-17 10:57:42] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [12:15<01:27,  2.72s/it][2026-01-17 10:57:46] INFO:     127.0.0.1:39116 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [12:19<01:32,  3.00s/it][2026-01-17 10:57:53] INFO:     127.0.0.1:57136 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [12:26<02:05,  4.18s/it][2026-01-17 10:57:54] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [12:27<01:34,  3.27s/it][2026-01-17 10:57:57] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [12:30<01:29,  3.19s/it][2026-01-17 10:58:03] Decode batch, #running-req: 28, #token: 12800, token usage: 0.43, npu graph: False, gen throughput (token/s): 27.49, #queue-req: 0,
[2026-01-17 10:58:10] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [12:43<02:42,  6.01s/it][2026-01-17 10:58:12] INFO:     127.0.0.1:46506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:58:12] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [12:45<02:04,  4.79s/it]
 88%|████████▊ | 175/200 [12:45<01:15,  3.02s/it][2026-01-17 10:58:13] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [12:46<00:59,  2.49s/it][2026-01-17 10:58:15] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [12:48<00:57,  2.50s/it][2026-01-17 10:58:16] INFO:     127.0.0.1:37286 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [12:49<00:44,  2.04s/it][2026-01-17 10:58:18] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [12:52<00:44,  2.12s/it][2026-01-17 10:58:22] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 10:58:22] INFO:     127.0.0.1:37262 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [12:55<00:51,  2.60s/it]
 90%|█████████ | 181/200 [12:55<00:43,  2.28s/it][2026-01-17 10:58:25] INFO:     127.0.0.1:39844 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [12:58<00:43,  2.40s/it][2026-01-17 10:58:26] INFO:     127.0.0.1:45652 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [12:59<00:36,  2.12s/it][2026-01-17 10:58:28] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [13:01<00:32,  2.06s/it][2026-01-17 10:58:33] INFO:     127.0.0.1:37272 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [13:06<00:41,  2.77s/it][2026-01-17 10:58:33] Decode batch, #running-req: 16, #token: 7552, token usage: 0.25, npu graph: False, gen throughput (token/s): 28.44, #queue-req: 0,
[2026-01-17 10:58:34] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [13:08<00:34,  2.44s/it][2026-01-17 10:58:44] INFO:     127.0.0.1:51954 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [13:17<00:58,  4.47s/it][2026-01-17 10:58:53] INFO:     127.0.0.1:39816 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [13:26<01:08,  5.72s/it][2026-01-17 10:58:54] INFO:     127.0.0.1:58584 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [13:27<00:48,  4.37s/it][2026-01-17 10:58:55] INFO:     127.0.0.1:46872 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [13:28<00:33,  3.39s/it][2026-01-17 10:58:56] Decode batch, #running-req: 10, #token: 5504, token usage: 0.19, npu graph: False, gen throughput (token/s): 23.80, #queue-req: 0,
[2026-01-17 10:58:59] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [13:32<00:31,  3.52s/it][2026-01-17 10:59:08] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [13:41<00:42,  5.34s/it][2026-01-17 10:59:11] INFO:     127.0.0.1:50460 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [13:44<00:31,  4.46s/it][2026-01-17 10:59:12] INFO:     127.0.0.1:39826 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [13:45<00:20,  3.40s/it][2026-01-17 10:59:12] Decode batch, #running-req: 7, #token: 3328, token usage: 0.11, npu graph: False, gen throughput (token/s): 22.05, #queue-req: 0,
[2026-01-17 10:59:15] INFO:     127.0.0.1:49872 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [13:48<00:17,  3.46s/it][2026-01-17 10:59:21] Decode batch, #running-req: 5, #token: 3072, token usage: 0.10, npu graph: False, gen throughput (token/s): 22.70, #queue-req: 0,
[2026-01-17 10:59:26] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [13:59<00:22,  5.65s/it][2026-01-17 10:59:28] INFO:     127.0.0.1:39664 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [14:01<00:13,  4.44s/it][2026-01-17 10:59:29] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [14:02<00:07,  3.60s/it][2026-01-17 10:59:29] Decode batch, #running-req: 2, #token: 1152, token usage: 0.04, npu graph: False, gen throughput (token/s): 20.70, #queue-req: 0,
[2026-01-17 10:59:35] Decode batch, #running-req: 2, #token: 1280, token usage: 0.04, npu graph: False, gen throughput (token/s): 15.49, #queue-req: 0,
[2026-01-17 10:59:40] Decode batch, #running-req: 2, #token: 1280, token usage: 0.04, npu graph: False, gen throughput (token/s): 14.13, #queue-req: 0,
[2026-01-17 10:59:43] INFO:     127.0.0.1:38222 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [14:16<00:06,  6.62s/it][2026-01-17 10:59:44] Decode batch, #running-req: 1, #token: 768, token usage: 0.03, npu graph: False, gen throughput (token/s): 14.34, #queue-req: 0,
[2026-01-17 10:59:47] Decode batch, #running-req: 1, #token: 768, token usage: 0.03, npu graph: False, gen throughput (token/s): 14.52, #queue-req: 0,
[2026-01-17 10:59:47] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [14:20<00:00,  5.96s/it]
100%|██████████| 200/200 [14:20<00:00,  4.30s/it]
.
----------------------------------------------------------------------
Ran 1 test in 982.320s

OK
Accuracy: 0.545
Invalid: 0.005
Latency: 861.043 s
Output throughput: 30.312 token/s
.
.
End (30/62):
filename='ascend/llm_models/test_ascend_baichuan2_13b_chat.py', elapsed=992, estimated_time=400
.
.

.
.
Begin (31/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_ernie_4_5_21b_a3b_pt.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 11:00:07] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/baidu/ERNIE-4.5-21B-A3B-PT', tokenizer_path='/root/.cache/modelscope/hub/models/baidu/ERNIE-4.5-21B-A3B-PT', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=488148945, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/baidu/ERNIE-4.5-21B-A3B-PT', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 11:00:08] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 11:00:17] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 11:00:18] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 11:00:19] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 11:00:19] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:07<00:58,  7.26s/it]

Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:14<00:52,  7.49s/it]

Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:22<00:45,  7.59s/it]

Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:30<00:38,  7.72s/it]

Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:37<00:29,  7.34s/it]

Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:45<00:22,  7.57s/it]

Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:52<00:15,  7.54s/it]

Loading safetensors checkpoint shards:  89% Completed | 8/9 [01:00<00:07,  7.59s/it]

Loading safetensors checkpoint shards: 100% Completed | 9/9 [01:08<00:00,  7.76s/it]

Loading safetensors checkpoint shards: 100% Completed | 9/9 [01:08<00:00,  7.61s/it]

[2026-01-17 11:01:44] Load weight end. type=Ernie4_5_MoeForCausalLM, dtype=torch.bfloat16, avail mem=20.10 GB, mem usage=40.71 GB.
[2026-01-17 11:01:44] Using KV cache dtype: torch.bfloat16
[2026-01-17 11:01:44] The available memory for KV cache is 7.92 GB.
[2026-01-17 11:01:45] KV Cache is allocated. #tokens: 148096, K size: 3.96 GB, V size: 3.96 GB
[2026-01-17 11:01:45] Memory pool end. avail mem=11.15 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 11:01:47] max_total_num_tokens=148096, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=11.15 GB
[2026-01-17 11:01:47] INFO:     Started server process [290566]
[2026-01-17 11:01:47] INFO:     Waiting for application startup.
[2026-01-17 11:01:47] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.8, 'top_k': 50, 'top_p': 0.8}
[2026-01-17 11:01:47] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.8, 'top_k': 50, 'top_p': 0.8}
[2026-01-17 11:01:47] INFO:     Application startup complete.
[2026-01-17 11:01:47] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 11:01:47] INFO:     127.0.0.1:58876 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 11:01:48] INFO:     127.0.0.1:58882 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 11:01:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
.[2026-01-17 11:01:50] INFO:     127.0.0.1:58894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:01:50] The server is fired up and ready to roll!
[2026-01-17 11:01:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:01:58] INFO:     127.0.0.1:42714 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 11:01:58] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 11:01:58] INFO:     127.0.0.1:42724 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 11:01:58] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:01:58] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/baidu/ERNIE-4.5-21B-A3B-PT --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestERNIE.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 11:01:58] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 11:01:58] Prefill batch, #new-seq: 22, #new-token: 3968, #cached-token: 16896, token usage: 0.01, #running-req: 1, #queue-req: 0,
[2026-01-17 11:01:59] Prefill batch, #new-seq: 32, #new-token: 5376, #cached-token: 24576, token usage: 0.03, #running-req: 23, #queue-req: 0,
[2026-01-17 11:01:59] Prefill batch, #new-seq: 27, #new-token: 4736, #cached-token: 20736, token usage: 0.07, #running-req: 55, #queue-req: 0,
[2026-01-17 11:01:59] Prefill batch, #new-seq: 45, #new-token: 8192, #cached-token: 34560, token usage: 0.10, #running-req: 82, #queue-req: 1,
[2026-01-17 11:01:59] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.16, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:01] Decode batch, #running-req: 128, #token: 32128, token usage: 0.22, npu graph: False, gen throughput (token/s): 37.80, #queue-req: 0,
[2026-01-17 11:02:01] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:03<10:03,  3.03s/it][2026-01-17 11:02:02] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.22, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:02] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:03<05:44,  1.74s/it][2026-01-17 11:02:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:03] Decode batch, #running-req: 128, #token: 33664, token usage: 0.23, npu graph: False, gen throughput (token/s): 2640.99, #queue-req: 0,
[2026-01-17 11:02:03] INFO:     127.0.0.1:43922 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:04<03:47,  1.15s/it][2026-01-17 11:02:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:03] INFO:     127.0.0.1:42966 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:04<02:34,  1.27it/s][2026-01-17 11:02:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:03] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:04<02:07,  1.53it/s][2026-01-17 11:02:03] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:03] INFO:     127.0.0.1:43612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:04] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:04] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.23, #running-req: 128, #queue-req: 0,
[2026-01-17 11:02:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 130, #queue-req: 0,
[2026-01-17 11:02:04] INFO:     127.0.0.1:43812 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:05<00:54,  3.52it/s][2026-01-17 11:02:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.23, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:04] INFO:     127.0.0.1:43578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:04] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:05<01:00,  3.14it/s]
  6%|▌         | 11/200 [00:05<00:54,  3.48it/s][2026-01-17 11:02:04] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.24, #running-req: 126, #queue-req: 0,
[2026-01-17 11:02:04] INFO:     127.0.0.1:43084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.24, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:05] INFO:     127.0.0.1:43360 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:06<00:48,  3.82it/s][2026-01-17 11:02:05] Decode batch, #running-req: 128, #token: 36224, token usage: 0.24, npu graph: False, gen throughput (token/s): 2299.70, #queue-req: 0,
[2026-01-17 11:02:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.24, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:05] INFO:     127.0.0.1:42776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:05] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:06<00:49,  3.77it/s]
  8%|▊         | 15/200 [00:06<00:40,  4.54it/s][2026-01-17 11:02:05] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.24, #running-req: 126, #queue-req: 0,
[2026-01-17 11:02:05] INFO:     127.0.0.1:43112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:05] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:05] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.24, #running-req: 126, #queue-req: 0,
[2026-01-17 11:02:05] INFO:     127.0.0.1:42926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:05] INFO:     127.0.0.1:43322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:05] INFO:     127.0.0.1:43436 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:06<00:27,  6.56it/s]
 10%|█         | 20/200 [00:06<00:13, 13.25it/s]
 10%|█         | 20/200 [00:06<00:13, 13.25it/s][2026-01-17 11:02:05] Prefill batch, #new-seq: 3, #new-token: 640, #cached-token: 2304, token usage: 0.24, #running-req: 125, #queue-req: 0,
[2026-01-17 11:02:05] INFO:     127.0.0.1:42772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:05] INFO:     127.0.0.1:43216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:05] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.24, #running-req: 126, #queue-req: 0,
[2026-01-17 11:02:05] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:05] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:06<00:12, 13.69it/s]
 12%|█▏        | 24/200 [00:06<00:11, 15.36it/s][2026-01-17 11:02:05] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.24, #running-req: 126, #queue-req: 0,
[2026-01-17 11:02:05] INFO:     127.0.0.1:43282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.24, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:06] INFO:     127.0.0.1:43678 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:07<00:12, 14.02it/s][2026-01-17 11:02:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.24, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:06] INFO:     127.0.0.1:43034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:06] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:06] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.24, #running-req: 126, #queue-req: 0,
[2026-01-17 11:02:06] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:07<00:11, 14.57it/s][2026-01-17 11:02:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.25, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:06] INFO:     127.0.0.1:43194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.25, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:06] INFO:     127.0.0.1:43012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:06] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:07<00:12, 13.39it/s]
 16%|█▌        | 32/200 [00:07<00:11, 14.03it/s][2026-01-17 11:02:06] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.25, #running-req: 126, #queue-req: 0,
[2026-01-17 11:02:06] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.25, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:06] INFO:     127.0.0.1:43138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:06] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:06] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:07<00:13, 12.18it/s]
 18%|█▊        | 36/200 [00:07<00:11, 13.83it/s]
 18%|█▊        | 36/200 [00:07<00:11, 13.83it/s][2026-01-17 11:02:06] INFO:     127.0.0.1:43832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:06] INFO:     127.0.0.1:43866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:06] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.25, #running-req: 128, #queue-req: 0,
[2026-01-17 11:02:06] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.25, #running-req: 131, #queue-req: 0,
[2026-01-17 11:02:06] INFO:     127.0.0.1:43046 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:07<00:12, 13.37it/s][2026-01-17 11:02:06] INFO:     127.0.0.1:43734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.25, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:06] INFO:     127.0.0.1:43778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:07] INFO:     127.0.0.1:42760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:07] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.25, #running-req: 128, #queue-req: 0,
[2026-01-17 11:02:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.25, #running-req: 130, #queue-req: 0,
[2026-01-17 11:02:07] INFO:     127.0.0.1:43576 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:07] INFO:     127.0.0.1:43702 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:08<00:11, 13.74it/s]
 22%|██▏       | 44/200 [00:08<00:10, 15.15it/s][2026-01-17 11:02:07] INFO:     127.0.0.1:43744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:07] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.25, #running-req: 126, #queue-req: 0,
[2026-01-17 11:02:07] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:07] INFO:     127.0.0.1:42956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:07] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.25, #running-req: 128, #queue-req: 0,
[2026-01-17 11:02:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.26, #running-req: 130, #queue-req: 0,
[2026-01-17 11:02:07] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:08<00:10, 14.79it/s][2026-01-17 11:02:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.26, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:07] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.26, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:07] INFO:     127.0.0.1:42744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:07] INFO:     127.0.0.1:43092 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:08<00:13, 11.27it/s]
 26%|██▌       | 51/200 [00:08<00:14, 10.17it/s][2026-01-17 11:02:07] INFO:     127.0.0.1:43206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:07] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.27, #running-req: 126, #queue-req: 0,
[2026-01-17 11:02:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.27, #running-req: 128, #queue-req: 0,
[2026-01-17 11:02:08] INFO:     127.0.0.1:43878 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:09<00:15,  9.78it/s][2026-01-17 11:02:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.27, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:08] INFO:     127.0.0.1:43448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.27, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:08] INFO:     127.0.0.1:43590 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:09<00:14,  9.86it/s][2026-01-17 11:02:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.27, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:08] Decode batch, #running-req: 128, #token: 40064, token usage: 0.27, npu graph: False, gen throughput (token/s): 1585.07, #queue-req: 0,
[2026-01-17 11:02:08] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:08] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:09<00:13, 10.22it/s][2026-01-17 11:02:08] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:08] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.27, #running-req: 126, #queue-req: 0,
[2026-01-17 11:02:08] INFO:     127.0.0.1:43628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:08] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.27, #running-req: 128, #queue-req: 0,
[2026-01-17 11:02:08] INFO:     127.0.0.1:43100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:08] INFO:     127.0.0.1:43376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:08] INFO:     127.0.0.1:43638 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:09<00:15,  9.25it/s]
 31%|███       | 62/200 [00:09<00:12, 10.86it/s]
 31%|███       | 62/200 [00:09<00:12, 10.86it/s][2026-01-17 11:02:08] Prefill batch, #new-seq: 3, #new-token: 640, #cached-token: 2304, token usage: 0.27, #running-req: 125, #queue-req: 0,
[2026-01-17 11:02:08] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:08] INFO:     127.0.0.1:43338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:08] INFO:     127.0.0.1:43594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:09] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.27, #running-req: 125, #queue-req: 0,
[2026-01-17 11:02:09] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:10<00:10, 12.80it/s][2026-01-17 11:02:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.27, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:09] INFO:     127.0.0.1:43390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.27, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:09] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:09] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:09] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:10<00:12, 10.55it/s]
 35%|███▌      | 70/200 [00:10<00:12, 10.64it/s]
 35%|███▌      | 70/200 [00:10<00:12, 10.64it/s][2026-01-17 11:02:09] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.26, #running-req: 125, #queue-req: 0,
[2026-01-17 11:02:09] INFO:     127.0.0.1:42850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.26, #running-req: 127, #queue-req: 0,
[2026-01-17 11:02:09] INFO:     127.0.0.1:43190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:09] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:10<00:13,  9.69it/s]
 36%|███▋      | 73/200 [00:10<00:12,  9.98it/s][2026-01-17 11:02:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.26, #running-req: 126, #queue-req: 0,
[2026-01-17 11:02:10] INFO:     127.0.0.1:43344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:11<00:14,  8.39it/s][2026-01-17 11:02:10] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:11<00:11, 10.44it/s][2026-01-17 11:02:10] INFO:     127.0.0.1:43762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43852 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:11<00:12,  9.96it/s]
 40%|████      | 81/200 [00:11<00:11, 10.81it/s][2026-01-17 11:02:10] INFO:     127.0.0.1:44022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:11<00:10, 10.80it/s]
 42%|████▏     | 84/200 [00:11<00:09, 12.17it/s][2026-01-17 11:02:10] INFO:     127.0.0.1:43760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43276 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:11<00:09, 12.63it/s][2026-01-17 11:02:10] Decode batch, #running-req: 115, #token: 35712, token usage: 0.24, npu graph: False, gen throughput (token/s): 2160.86, #queue-req: 0,
[2026-01-17 11:02:10] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43652 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:11<00:07, 14.73it/s]
 45%|████▌     | 90/200 [00:11<00:06, 18.24it/s][2026-01-17 11:02:10] INFO:     127.0.0.1:42826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:10] INFO:     127.0.0.1:43940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:43232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:44236 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:12<00:05, 19.57it/s]
 48%|████▊     | 97/200 [00:12<00:04, 21.59it/s][2026-01-17 11:02:11] INFO:     127.0.0.1:43260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:43556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:12<00:04, 20.34it/s]
 50%|█████     | 101/200 [00:12<00:04, 20.84it/s][2026-01-17 11:02:11] INFO:     127.0.0.1:43698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:42796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:42766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:42902 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:12<00:05, 16.26it/s]
 52%|█████▎    | 105/200 [00:12<00:06, 14.93it/s][2026-01-17 11:02:11] INFO:     127.0.0.1:42852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:43294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:43674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:43586 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:12<00:05, 16.55it/s][2026-01-17 11:02:11] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:11] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:12] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:13<00:04, 17.82it/s][2026-01-17 11:02:12] INFO:     127.0.0.1:43078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:12] INFO:     127.0.0.1:44064 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:13<00:06, 12.17it/s][2026-01-17 11:02:12] INFO:     127.0.0.1:44106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:12] INFO:     127.0.0.1:44266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:12] INFO:     127.0.0.1:59770 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:13<00:05, 13.88it/s][2026-01-17 11:02:12] Decode batch, #running-req: 83, #token: 27264, token usage: 0.18, npu graph: False, gen throughput (token/s): 2098.86, #queue-req: 0,
[2026-01-17 11:02:12] INFO:     127.0.0.1:43482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:12] INFO:     127.0.0.1:43130 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:13<00:05, 14.01it/s][2026-01-17 11:02:12] INFO:     127.0.0.1:44056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:13] INFO:     127.0.0.1:43300 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:14<00:07,  9.92it/s][2026-01-17 11:02:13] INFO:     127.0.0.1:43990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:13] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:13] INFO:     127.0.0.1:43910 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:14<00:06, 12.12it/s][2026-01-17 11:02:13] INFO:     127.0.0.1:44230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:13] INFO:     127.0.0.1:43496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:13] INFO:     127.0.0.1:43382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:13] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:14<00:05, 12.49it/s]
 64%|██████▍   | 129/200 [00:14<00:05, 14.09it/s][2026-01-17 11:02:13] INFO:     127.0.0.1:43600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:13] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [00:14<00:05, 13.40it/s][2026-01-17 11:02:13] INFO:     127.0.0.1:42838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:13] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:14<00:05, 11.41it/s][2026-01-17 11:02:13] INFO:     127.0.0.1:44274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:13] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:14] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [00:15<00:04, 13.69it/s][2026-01-17 11:02:14] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:14] INFO:     127.0.0.1:43636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:14] INFO:     127.0.0.1:44216 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:15<00:05, 11.54it/s]
 70%|██████▉   | 139/200 [00:15<00:05, 11.55it/s][2026-01-17 11:02:14] Decode batch, #running-req: 61, #token: 22784, token usage: 0.15, npu graph: False, gen throughput (token/s): 1642.71, #queue-req: 0,
[2026-01-17 11:02:14] INFO:     127.0.0.1:43094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:14] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:15<00:05, 11.58it/s][2026-01-17 11:02:14] INFO:     127.0.0.1:44316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:14] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:15<00:04, 12.36it/s][2026-01-17 11:02:14] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:14] INFO:     127.0.0.1:42964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:14] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:16<00:06,  9.01it/s]
 73%|███████▎  | 146/200 [00:16<00:06,  8.52it/s][2026-01-17 11:02:15] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:15] INFO:     127.0.0.1:44074 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:16<00:05,  9.66it/s][2026-01-17 11:02:15] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:15] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:16<00:04, 10.76it/s][2026-01-17 11:02:15] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:15] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:16<00:04, 10.42it/s][2026-01-17 11:02:15] INFO:     127.0.0.1:43960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:15] INFO:     127.0.0.1:43998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:15] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:16<00:03, 13.13it/s][2026-01-17 11:02:15] INFO:     127.0.0.1:44144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:15] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:15] INFO:     127.0.0.1:42882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:15] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:15] INFO:     127.0.0.1:44262 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:16<00:02, 17.81it/s][2026-01-17 11:02:15] INFO:     127.0.0.1:43984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:15] INFO:     127.0.0.1:44100 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:16<00:02, 15.22it/s][2026-01-17 11:02:16] Decode batch, #running-req: 38, #token: 16512, token usage: 0.11, npu graph: False, gen throughput (token/s): 1190.86, #queue-req: 0,
[2026-01-17 11:02:16] INFO:     127.0.0.1:44086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:16] INFO:     127.0.0.1:44198 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:17<00:03, 10.24it/s][2026-01-17 11:02:16] INFO:     127.0.0.1:44336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:16] INFO:     127.0.0.1:44026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:16] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:17<00:02, 12.61it/s][2026-01-17 11:02:16] INFO:     127.0.0.1:44186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:16] INFO:     127.0.0.1:44284 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:17<00:02, 13.34it/s][2026-01-17 11:02:16] INFO:     127.0.0.1:43526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:17] INFO:     127.0.0.1:43106 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:18<00:03,  8.34it/s][2026-01-17 11:02:17] INFO:     127.0.0.1:44040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:17] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:18<00:02,  9.62it/s][2026-01-17 11:02:17] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:17] INFO:     127.0.0.1:43958 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:18<00:03,  7.82it/s][2026-01-17 11:02:17] Decode batch, #running-req: 26, #token: 11904, token usage: 0.08, npu graph: False, gen throughput (token/s): 795.09, #queue-req: 0,
[2026-01-17 11:02:17] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:17] INFO:     127.0.0.1:44264 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:18<00:02,  9.20it/s][2026-01-17 11:02:17] INFO:     127.0.0.1:44212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:17] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:17] INFO:     127.0.0.1:44122 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:18<00:01, 10.85it/s][2026-01-17 11:02:18] INFO:     127.0.0.1:42940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:18] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:19<00:02,  8.49it/s][2026-01-17 11:02:18] INFO:     127.0.0.1:44320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:18] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:19<00:02,  7.02it/s][2026-01-17 11:02:19] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:20<00:02,  5.54it/s][2026-01-17 11:02:19] Decode batch, #running-req: 15, #token: 8064, token usage: 0.05, npu graph: False, gen throughput (token/s): 492.66, #queue-req: 0,
[2026-01-17 11:02:19] INFO:     127.0.0.1:44168 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:20<00:03,  4.06it/s][2026-01-17 11:02:20] INFO:     127.0.0.1:42810 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:21<00:03,  3.47it/s][2026-01-17 11:02:20] INFO:     127.0.0.1:44070 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:20] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:21<00:02,  4.79it/s][2026-01-17 11:02:20] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:20] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:21<00:01,  5.31it/s][2026-01-17 11:02:20] Decode batch, #running-req: 9, #token: 5504, token usage: 0.04, npu graph: False, gen throughput (token/s): 348.72, #queue-req: 0,
[2026-01-17 11:02:21] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:22<00:02,  3.55it/s][2026-01-17 11:02:21] INFO:     127.0.0.1:44256 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:22<00:02,  2.82it/s][2026-01-17 11:02:21] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:02:21] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:23<00:01,  3.97it/s][2026-01-17 11:02:22] Decode batch, #running-req: 5, #token: 3712, token usage: 0.03, npu graph: False, gen throughput (token/s): 220.92, #queue-req: 0,
[2026-01-17 11:02:22] INFO:     127.0.0.1:42868 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:23<00:01,  3.37it/s][2026-01-17 11:02:23] Decode batch, #running-req: 4, #token: 2432, token usage: 0.02, npu graph: False, gen throughput (token/s): 122.58, #queue-req: 0,
[2026-01-17 11:02:23] INFO:     127.0.0.1:43662 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:24<00:01,  2.08it/s][2026-01-17 11:02:23] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:24<00:00,  2.15it/s][2026-01-17 11:02:24] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:25<00:00,  1.81it/s][2026-01-17 11:02:24] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 66.82, #queue-req: 0,
[2026-01-17 11:02:24] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:25<00:00,  2.29it/s]
100%|██████████| 200/200 [00:25<00:00,  7.73it/s]
.
----------------------------------------------------------------------
Ran 1 test in 147.221s

OK
Accuracy: 0.900
Invalid: 0.000
Latency: 25.964 s
Output throughput: 1392.308 token/s
.
.
End (31/62):
filename='ascend/llm_models/test_ascend_ernie_4_5_21b_a3b_pt.py', elapsed=157, estimated_time=400
.
.

.
.
Begin (32/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_minicpm3_4b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 11:02:44] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/OpenBMB/MiniCPM3-4B', tokenizer_path='/root/.cache/modelscope/hub/models/OpenBMB/MiniCPM3-4B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=8, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=94931748, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/OpenBMB/MiniCPM3-4B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 11:02:44] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 11:02:53] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 11:02:54] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 11:02:54] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 11:02:55] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 11:02:55] Load weight begin. avail mem=60.81 GB

Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading pt checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.83s/it]

Loading pt checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.83s/it]

[2026-01-17 11:03:19] Load weight end. type=MiniCPM3ForCausalLM, dtype=torch.bfloat16, avail mem=53.05 GB, mem usage=7.76 GB.
[2026-01-17 11:03:19] Using KV cache dtype: torch.bfloat16
[2026-01-17 11:03:19] The available memory for KV cache is 40.89 GB.
[2026-01-17 11:03:20] KV Cache is allocated. #tokens: 1229312, KV size: 40.89 GB
[2026-01-17 11:03:20] Memory pool end. avail mem=12.15 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 11:03:21] max_total_num_tokens=1229312, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=8, context_len=32768, available_gpu_mem=12.15 GB
[2026-01-17 11:03:21] INFO:     Started server process [293169]
[2026-01-17 11:03:21] INFO:     Waiting for application startup.
[2026-01-17 11:03:21] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.8, 'top_k': 50, 'top_p': 0.8}
[2026-01-17 11:03:21] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.8, 'top_k': 50, 'top_p': 0.8}
[2026-01-17 11:03:21] INFO:     Application startup complete.
[2026-01-17 11:03:21] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 11:03:22] INFO:     127.0.0.1:55032 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 11:03:22] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:03:24] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:03:24] The server is fired up and ready to roll!
[2026-01-17 11:03:24] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:03:25] INFO:     127.0.0.1:55048 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 11:03:25] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 11:03:25] INFO:     127.0.0.1:55058 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 11:03:25] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:03:26] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/OpenBMB/MiniCPM3-4B --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --disable-radix-cache --disable-overlap-schedule --max-running-requests 8 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMiniCPM3.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 11:03:26] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:03:26] Prefill batch, #new-seq: 7, #new-token: 6528, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 52,
[2026-01-17 11:03:28] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<07:48,  2.35s/it][2026-01-17 11:03:28] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:03:29] INFO:     127.0.0.1:55108 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:03<04:47,  1.45s/it][2026-01-17 11:03:29] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:03:32] Decode batch, #running-req: 8, #token: 8064, token usage: 0.01, npu graph: False, gen throughput (token/s): 6.93, #queue-req: 120,
[2026-01-17 11:03:34] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:08<10:11,  3.10s/it][2026-01-17 11:03:34] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:03:36] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:10<08:24,  2.57s/it][2026-01-17 11:03:36] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:03:36] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 69.28, #queue-req: 120,
[2026-01-17 11:03:39] INFO:     127.0.0.1:55154 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:13<08:55,  2.75s/it][2026-01-17 11:03:39] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:03:41] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 74.92, #queue-req: 120,
[2026-01-17 11:03:44] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:18<11:58,  3.71s/it][2026-01-17 11:03:44] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:03:45] Decode batch, #running-req: 8, #token: 8448, token usage: 0.01, npu graph: False, gen throughput (token/s): 75.20, #queue-req: 120,
[2026-01-17 11:03:49] Decode batch, #running-req: 8, #token: 8832, token usage: 0.01, npu graph: False, gen throughput (token/s): 78.90, #queue-req: 120,
[2026-01-17 11:03:49] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:23<13:26,  4.18s/it][2026-01-17 11:03:49] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:03:51] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:25<10:42,  3.35s/it][2026-01-17 11:03:51] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:03:51] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:25<07:48,  2.45s/it][2026-01-17 11:03:51] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:03:54] Decode batch, #running-req: 8, #token: 8704, token usage: 0.01, npu graph: False, gen throughput (token/s): 67.13, #queue-req: 120,
[2026-01-17 11:03:55] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:29<08:34,  2.71s/it][2026-01-17 11:03:55] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:03:58] Decode batch, #running-req: 8, #token: 8576, token usage: 0.01, npu graph: False, gen throughput (token/s): 68.74, #queue-req: 120,
[2026-01-17 11:04:00] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:34<11:07,  3.53s/it][2026-01-17 11:04:00] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:01] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:35<08:30,  2.71s/it][2026-01-17 11:04:01] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:03] Decode batch, #running-req: 8, #token: 8832, token usage: 0.01, npu graph: False, gen throughput (token/s): 66.68, #queue-req: 120,
[2026-01-17 11:04:04] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:38<08:46,  2.82s/it][2026-01-17 11:04:04] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:05] INFO:     127.0.0.1:55212 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:39<06:53,  2.22s/it][2026-01-17 11:04:05] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:07] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:41<06:54,  2.24s/it][2026-01-17 11:04:07] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:08] Decode batch, #running-req: 8, #token: 8576, token usage: 0.01, npu graph: False, gen throughput (token/s): 63.52, #queue-req: 120,
[2026-01-17 11:04:11] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:45<08:40,  2.83s/it][2026-01-17 11:04:11] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:12] Decode batch, #running-req: 8, #token: 8704, token usage: 0.01, npu graph: False, gen throughput (token/s): 71.25, #queue-req: 120,
[2026-01-17 11:04:13] INFO:     127.0.0.1:55252 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:47<07:27,  2.45s/it][2026-01-17 11:04:13] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:14] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:48<06:29,  2.14s/it][2026-01-17 11:04:14] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:15] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:49<05:03,  1.68s/it][2026-01-17 11:04:15] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:17] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:51<05:21,  1.78s/it][2026-01-17 11:04:17] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:17] Decode batch, #running-req: 8, #token: 8704, token usage: 0.01, npu graph: False, gen throughput (token/s): 65.06, #queue-req: 120,
[2026-01-17 11:04:20] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:54<06:15,  2.10s/it][2026-01-17 11:04:20] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:22] Decode batch, #running-req: 8, #token: 9088, token usage: 0.01, npu graph: False, gen throughput (token/s): 73.54, #queue-req: 120,
[2026-01-17 11:04:26] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:04:26] INFO:     127.0.0.1:55142 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [01:00<09:58,  3.36s/it]
 12%|█▏        | 23/200 [01:00<09:38,  3.27s/it][2026-01-17 11:04:26] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.01, #running-req: 6, #queue-req: 120,
[2026-01-17 11:04:27] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 64.20, #queue-req: 120,
[2026-01-17 11:04:27] INFO:     127.0.0.1:55368 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [01:01<07:40,  2.61s/it][2026-01-17 11:04:27] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:30] INFO:     127.0.0.1:55362 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [01:04<07:51,  2.70s/it][2026-01-17 11:04:30] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:31] Decode batch, #running-req: 8, #token: 8448, token usage: 0.01, npu graph: False, gen throughput (token/s): 64.77, #queue-req: 120,
[2026-01-17 11:04:32] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [01:06<07:41,  2.65s/it][2026-01-17 11:04:32] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:33] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [01:07<06:06,  2.12s/it][2026-01-17 11:04:33] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:35] INFO:     127.0.0.1:55334 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [01:09<06:01,  2.10s/it][2026-01-17 11:04:35] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:36] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [01:10<05:25,  1.91s/it][2026-01-17 11:04:36] Decode batch, #running-req: 7, #token: 7296, token usage: 0.01, npu graph: False, gen throughput (token/s): 63.13, #queue-req: 120,
[2026-01-17 11:04:36] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:38] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [01:12<04:55,  1.74s/it][2026-01-17 11:04:38] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:39] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [01:13<04:20,  1.54s/it][2026-01-17 11:04:39] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:42] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [01:15<05:19,  1.90s/it][2026-01-17 11:04:42] Decode batch, #running-req: 7, #token: 7424, token usage: 0.01, npu graph: False, gen throughput (token/s): 61.39, #queue-req: 120,
[2026-01-17 11:04:42] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:43] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [01:17<04:59,  1.80s/it][2026-01-17 11:04:43] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:47] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 64.28, #queue-req: 120,
[2026-01-17 11:04:47] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [01:21<06:46,  2.45s/it][2026-01-17 11:04:47] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:49] INFO:     127.0.0.1:55426 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [01:23<06:27,  2.35s/it][2026-01-17 11:04:49] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:50] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [01:24<05:13,  1.91s/it][2026-01-17 11:04:50] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:51] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [01:25<04:45,  1.75s/it][2026-01-17 11:04:52] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:52] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 57.43, #queue-req: 120,
[2026-01-17 11:04:53] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [01:27<04:35,  1.70s/it][2026-01-17 11:04:53] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:54] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [01:28<04:04,  1.52s/it][2026-01-17 11:04:54] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:04:57] Decode batch, #running-req: 8, #token: 8576, token usage: 0.01, npu graph: False, gen throughput (token/s): 69.00, #queue-req: 120,
[2026-01-17 11:04:58] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [01:32<05:48,  2.18s/it][2026-01-17 11:04:58] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:01] Decode batch, #running-req: 8, #token: 8448, token usage: 0.01, npu graph: False, gen throughput (token/s): 73.63, #queue-req: 120,
[2026-01-17 11:05:01] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [01:35<06:50,  2.58s/it][2026-01-17 11:05:01] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:04] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [01:38<06:39,  2.53s/it][2026-01-17 11:05:04] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:05] Decode batch, #running-req: 8, #token: 8704, token usage: 0.01, npu graph: False, gen throughput (token/s): 72.47, #queue-req: 120,
[2026-01-17 11:05:06] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [01:39<06:01,  2.30s/it][2026-01-17 11:05:06] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:07] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [01:41<05:06,  1.97s/it][2026-01-17 11:05:07] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:07] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [01:41<03:38,  1.41s/it][2026-01-17 11:05:07] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:10] Decode batch, #running-req: 8, #token: 8576, token usage: 0.01, npu graph: False, gen throughput (token/s): 67.86, #queue-req: 120,
[2026-01-17 11:05:12] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [01:46<06:13,  2.43s/it][2026-01-17 11:05:12] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:12] INFO:     127.0.0.1:55516 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [01:46<04:46,  1.87s/it][2026-01-17 11:05:12] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:14] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [01:48<04:45,  1.88s/it][2026-01-17 11:05:14] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:15] Decode batch, #running-req: 8, #token: 8320, token usage: 0.01, npu graph: False, gen throughput (token/s): 67.96, #queue-req: 120,
[2026-01-17 11:05:15] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [01:49<04:21,  1.73s/it][2026-01-17 11:05:16] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:19] Decode batch, #running-req: 8, #token: 8576, token usage: 0.01, npu graph: False, gen throughput (token/s): 73.50, #queue-req: 120,
[2026-01-17 11:05:20] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [01:54<06:35,  2.64s/it][2026-01-17 11:05:20] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:23] INFO:     127.0.0.1:55660 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [01:57<06:48,  2.74s/it][2026-01-17 11:05:23] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:24] Decode batch, #running-req: 8, #token: 8576, token usage: 0.01, npu graph: False, gen throughput (token/s): 66.70, #queue-req: 120,
[2026-01-17 11:05:24] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [01:58<05:38,  2.29s/it][2026-01-17 11:05:25] INFO:     127.0.0.1:55540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:05:25] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,

 26%|██▋       | 53/200 [01:59<04:00,  1.63s/it][2026-01-17 11:05:25] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:29] Decode batch, #running-req: 8, #token: 8960, token usage: 0.01, npu graph: False, gen throughput (token/s): 62.17, #queue-req: 120,
[2026-01-17 11:05:31] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [02:05<07:38,  3.14s/it][2026-01-17 11:05:31] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:32] INFO:     127.0.0.1:55628 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [02:06<05:50,  2.42s/it][2026-01-17 11:05:32] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:33] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [02:07<05:01,  2.09s/it][2026-01-17 11:05:33] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:34] Decode batch, #running-req: 8, #token: 8704, token usage: 0.01, npu graph: False, gen throughput (token/s): 60.00, #queue-req: 120,
[2026-01-17 11:05:35] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [02:09<04:36,  1.93s/it][2026-01-17 11:05:35] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:38] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [02:12<05:18,  2.25s/it][2026-01-17 11:05:38] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:39] Decode batch, #running-req: 8, #token: 8704, token usage: 0.01, npu graph: False, gen throughput (token/s): 64.80, #queue-req: 120,
[2026-01-17 11:05:39] INFO:     127.0.0.1:55442 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [02:13<04:44,  2.02s/it][2026-01-17 11:05:39] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:40] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [02:14<03:58,  1.70s/it][2026-01-17 11:05:40] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:44] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [02:17<05:01,  2.17s/it][2026-01-17 11:05:44] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:44] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [02:18<04:04,  1.77s/it][2026-01-17 11:05:44] Decode batch, #running-req: 7, #token: 7040, token usage: 0.01, npu graph: False, gen throughput (token/s): 59.68, #queue-req: 120,
[2026-01-17 11:05:44] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:45] INFO:     127.0.0.1:55682 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [02:19<03:25,  1.50s/it][2026-01-17 11:05:45] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:46] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [02:20<03:05,  1.36s/it][2026-01-17 11:05:46] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:48] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [02:22<03:14,  1.44s/it][2026-01-17 11:05:48] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:49] INFO:     127.0.0.1:55706 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [02:23<02:49,  1.27s/it][2026-01-17 11:05:49] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:05:49] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 7, #queue-req: 120,

 34%|███▎      | 67/200 [02:23<02:02,  1.09it/s][2026-01-17 11:05:49] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:50] Decode batch, #running-req: 8, #token: 8064, token usage: 0.01, npu graph: False, gen throughput (token/s): 53.36, #queue-req: 120,
[2026-01-17 11:05:51] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [02:25<02:51,  1.30s/it][2026-01-17 11:05:51] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:55] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 65.50, #queue-req: 120,
[2026-01-17 11:05:57] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [02:31<06:03,  2.78s/it][2026-01-17 11:05:57] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:05:59] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [02:33<05:30,  2.54s/it][2026-01-17 11:05:59] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:06:00] Decode batch, #running-req: 8, #token: 8320, token usage: 0.01, npu graph: False, gen throughput (token/s): 70.81, #queue-req: 120,
[2026-01-17 11:06:01] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [02:35<05:02,  2.35s/it][2026-01-17 11:06:01] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,
[2026-01-17 11:06:03] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [02:37<04:35,  2.15s/it][2026-01-17 11:06:03] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:06:03] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 120,

 36%|███▋      | 73/200 [02:37<03:15,  1.54s/it][2026-01-17 11:06:03] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 119,
[2026-01-17 11:06:05] Decode batch, #running-req: 8, #token: 7296, token usage: 0.01, npu graph: False, gen throughput (token/s): 64.81, #queue-req: 119,
[2026-01-17 11:06:05] INFO:     127.0.0.1:55768 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [02:39<03:18,  1.58s/it][2026-01-17 11:06:05] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 118,
[2026-01-17 11:06:06] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [02:40<03:19,  1.60s/it][2026-01-17 11:06:06] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 117,
[2026-01-17 11:06:08] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [02:42<03:05,  1.50s/it][2026-01-17 11:06:08] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 116,
[2026-01-17 11:06:08] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [02:42<02:37,  1.28s/it][2026-01-17 11:06:08] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 115,
[2026-01-17 11:06:10] Decode batch, #running-req: 8, #token: 8064, token usage: 0.01, npu graph: False, gen throughput (token/s): 60.88, #queue-req: 115,
[2026-01-17 11:06:11] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [02:45<03:32,  1.74s/it][2026-01-17 11:06:11] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 114,
[2026-01-17 11:06:12] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [02:46<03:07,  1.55s/it][2026-01-17 11:06:12] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 113,
[2026-01-17 11:06:14] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [02:48<03:06,  1.56s/it][2026-01-17 11:06:14] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 112,
[2026-01-17 11:06:15] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 66.34, #queue-req: 112,
[2026-01-17 11:06:18] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [02:52<04:30,  2.27s/it][2026-01-17 11:06:18] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 111,
[2026-01-17 11:06:19] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 73.71, #queue-req: 111,
[2026-01-17 11:06:19] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [02:53<03:56,  2.01s/it][2026-01-17 11:06:19] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 110,
[2026-01-17 11:06:21] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [02:55<04:05,  2.10s/it][2026-01-17 11:06:22] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 109,
[2026-01-17 11:06:22] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [02:56<03:21,  1.74s/it][2026-01-17 11:06:22] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 108,
[2026-01-17 11:06:24] Decode batch, #running-req: 8, #token: 8448, token usage: 0.01, npu graph: False, gen throughput (token/s): 64.40, #queue-req: 108,
[2026-01-17 11:06:26] INFO:     127.0.0.1:55862 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [03:00<04:29,  2.34s/it][2026-01-17 11:06:26] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 107,
[2026-01-17 11:06:26] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [03:00<03:19,  1.75s/it][2026-01-17 11:06:27] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 7, #queue-req: 106,
[2026-01-17 11:06:27] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [03:01<02:21,  1.26s/it][2026-01-17 11:06:27] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 105,
[2026-01-17 11:06:28] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [03:02<02:17,  1.22s/it][2026-01-17 11:06:28] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 104,
[2026-01-17 11:06:29] Decode batch, #running-req: 8, #token: 7936, token usage: 0.01, npu graph: False, gen throughput (token/s): 63.80, #queue-req: 104,
[2026-01-17 11:06:29] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [03:03<02:14,  1.21s/it][2026-01-17 11:06:29] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 103,
[2026-01-17 11:06:31] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [03:05<02:39,  1.45s/it][2026-01-17 11:06:31] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 102,
[2026-01-17 11:06:32] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [03:06<02:39,  1.46s/it][2026-01-17 11:06:32] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 101,
[2026-01-17 11:06:34] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 66.22, #queue-req: 101,
[2026-01-17 11:06:34] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [03:08<02:55,  1.63s/it][2026-01-17 11:06:35] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 100,
[2026-01-17 11:06:35] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [03:09<02:26,  1.37s/it][2026-01-17 11:06:35] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 99,
[2026-01-17 11:06:36] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [03:10<02:13,  1.26s/it][2026-01-17 11:06:36] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 98,
[2026-01-17 11:06:38] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [03:12<02:32,  1.45s/it][2026-01-17 11:06:38] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 97,
[2026-01-17 11:06:39] Decode batch, #running-req: 8, #token: 7040, token usage: 0.01, npu graph: False, gen throughput (token/s): 62.48, #queue-req: 97,
[2026-01-17 11:06:39] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [03:13<02:03,  1.19s/it][2026-01-17 11:06:39] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 96,
[2026-01-17 11:06:39] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [03:13<01:46,  1.03s/it][2026-01-17 11:06:39] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 95,
[2026-01-17 11:06:43] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 72.58, #queue-req: 95,
[2026-01-17 11:06:43] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [03:17<03:10,  1.87s/it][2026-01-17 11:06:43] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 94,
[2026-01-17 11:06:44] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [03:18<02:29,  1.48s/it][2026-01-17 11:06:44] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 93,
[2026-01-17 11:06:45] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [03:19<02:10,  1.30s/it][2026-01-17 11:06:45] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 7, #queue-req: 92,
[2026-01-17 11:06:45] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [03:19<01:33,  1.06it/s][2026-01-17 11:06:45] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 91,
[2026-01-17 11:06:48] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 58.12, #queue-req: 91,
[2026-01-17 11:06:49] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [03:23<03:14,  1.98s/it][2026-01-17 11:06:49] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 7, #queue-req: 90,
[2026-01-17 11:06:49] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [03:23<02:17,  1.42s/it][2026-01-17 11:06:50] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 89,
[2026-01-17 11:06:51] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [03:25<02:31,  1.58s/it][2026-01-17 11:06:51] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 88,
[2026-01-17 11:06:53] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [03:27<02:37,  1.66s/it][2026-01-17 11:06:53] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 87,
[2026-01-17 11:06:54] Decode batch, #running-req: 8, #token: 8064, token usage: 0.01, npu graph: False, gen throughput (token/s): 56.80, #queue-req: 87,
[2026-01-17 11:06:58] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [03:32<04:06,  2.62s/it][2026-01-17 11:06:58] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 86,
[2026-01-17 11:06:59] Decode batch, #running-req: 8, #token: 7168, token usage: 0.01, npu graph: False, gen throughput (token/s): 67.74, #queue-req: 86,
[2026-01-17 11:06:59] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [03:33<03:14,  2.09s/it][2026-01-17 11:06:59] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 85,
[2026-01-17 11:06:59] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [03:33<02:33,  1.66s/it][2026-01-17 11:07:00] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 7, #queue-req: 84,
[2026-01-17 11:07:00] INFO:     127.0.0.1:56066 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [03:33<01:49,  1.20s/it][2026-01-17 11:07:00] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 83,
[2026-01-17 11:07:03] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [03:37<02:44,  1.83s/it][2026-01-17 11:07:03] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 82,
[2026-01-17 11:07:04] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [03:38<02:19,  1.57s/it][2026-01-17 11:07:04] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 81,
[2026-01-17 11:07:04] Decode batch, #running-req: 8, #token: 8064, token usage: 0.01, npu graph: False, gen throughput (token/s): 56.20, #queue-req: 81,
[2026-01-17 11:07:08] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [03:42<03:17,  2.24s/it][2026-01-17 11:07:08] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 80,
[2026-01-17 11:07:09] Decode batch, #running-req: 8, #token: 7296, token usage: 0.01, npu graph: False, gen throughput (token/s): 70.72, #queue-req: 80,
[2026-01-17 11:07:09] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [03:43<02:49,  1.95s/it][2026-01-17 11:07:09] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 7, #queue-req: 79,
[2026-01-17 11:07:09] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [03:43<02:00,  1.40s/it][2026-01-17 11:07:09] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 78,
[2026-01-17 11:07:10] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [03:44<01:43,  1.22s/it][2026-01-17 11:07:10] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 77,
[2026-01-17 11:07:11] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [03:45<01:49,  1.30s/it][2026-01-17 11:07:11] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 76,
[2026-01-17 11:07:14] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 61.63, #queue-req: 76,
[2026-01-17 11:07:14] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [03:48<02:33,  1.85s/it][2026-01-17 11:07:14] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 75,
[2026-01-17 11:07:15] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [03:49<01:55,  1.40s/it][2026-01-17 11:07:15] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 74,
[2026-01-17 11:07:18] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [03:51<02:26,  1.81s/it][2026-01-17 11:07:18] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 7, #queue-req: 73,
[2026-01-17 11:07:18] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [03:52<01:43,  1.30s/it][2026-01-17 11:07:18] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 72,
[2026-01-17 11:07:19] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 63.55, #queue-req: 72,
[2026-01-17 11:07:21] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [03:55<02:43,  2.07s/it][2026-01-17 11:07:22] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 71,
[2026-01-17 11:07:23] Decode batch, #running-req: 8, #token: 8448, token usage: 0.01, npu graph: False, gen throughput (token/s): 71.23, #queue-req: 71,
[2026-01-17 11:07:24] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [03:58<02:51,  2.20s/it][2026-01-17 11:07:24] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 70,
[2026-01-17 11:07:26] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [04:00<02:38,  2.06s/it][2026-01-17 11:07:26] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 69,
[2026-01-17 11:07:28] Decode batch, #running-req: 8, #token: 8448, token usage: 0.01, npu graph: False, gen throughput (token/s): 67.29, #queue-req: 69,
[2026-01-17 11:07:29] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [04:03<02:54,  2.30s/it][2026-01-17 11:07:29] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 68,
[2026-01-17 11:07:30] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [04:04<02:41,  2.16s/it][2026-01-17 11:07:31] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 67,
[2026-01-17 11:07:31] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [04:05<02:12,  1.79s/it][2026-01-17 11:07:31] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 66,
[2026-01-17 11:07:33] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 62.92, #queue-req: 66,
[2026-01-17 11:07:36] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [04:10<03:09,  2.59s/it][2026-01-17 11:07:36] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 65,
[2026-01-17 11:07:37] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [04:11<02:43,  2.27s/it][2026-01-17 11:07:37] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 64,
[2026-01-17 11:07:38] Decode batch, #running-req: 8, #token: 7168, token usage: 0.01, npu graph: False, gen throughput (token/s): 70.10, #queue-req: 64,
[2026-01-17 11:07:38] INFO:     127.0.0.1:35070 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [04:12<02:01,  1.71s/it][2026-01-17 11:07:38] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 63,
[2026-01-17 11:07:41] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [04:15<02:25,  2.08s/it][2026-01-17 11:07:41] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 62,
[2026-01-17 11:07:42] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [04:16<02:03,  1.79s/it][2026-01-17 11:07:42] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 61,
[2026-01-17 11:07:43] Decode batch, #running-req: 8, #token: 8448, token usage: 0.01, npu graph: False, gen throughput (token/s): 59.84, #queue-req: 61,
[2026-01-17 11:07:44] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [04:18<02:05,  1.85s/it][2026-01-17 11:07:44] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 60,
[2026-01-17 11:07:45] INFO:     127.0.0.1:43494 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [04:19<01:48,  1.61s/it][2026-01-17 11:07:45] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 59,
[2026-01-17 11:07:45] INFO:     127.0.0.1:43482 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [04:19<01:25,  1.29s/it][2026-01-17 11:07:45] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 58,
[2026-01-17 11:07:47] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [04:21<01:30,  1.40s/it][2026-01-17 11:07:47] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 7, #queue-req: 57,
[2026-01-17 11:07:47] INFO:     127.0.0.1:35048 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [04:21<01:04,  1.01s/it][2026-01-17 11:07:48] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 56,
[2026-01-17 11:07:49] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 56.83, #queue-req: 56,
[2026-01-17 11:07:50] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [04:24<01:35,  1.52s/it][2026-01-17 11:07:50] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 55,
[2026-01-17 11:07:52] INFO:     127.0.0.1:35036 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [04:26<01:40,  1.62s/it][2026-01-17 11:07:52] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 54,
[2026-01-17 11:07:53] Decode batch, #running-req: 8, #token: 8064, token usage: 0.01, npu graph: False, gen throughput (token/s): 65.01, #queue-req: 54,
[2026-01-17 11:07:57] INFO:     127.0.0.1:54008 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [04:31<02:46,  2.74s/it][2026-01-17 11:07:57] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 53,
[2026-01-17 11:07:58] INFO:     127.0.0.1:35052 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [04:32<02:04,  2.07s/it][2026-01-17 11:07:58] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 52,
[2026-01-17 11:07:59] Decode batch, #running-req: 8, #token: 8576, token usage: 0.01, npu graph: False, gen throughput (token/s): 62.77, #queue-req: 52,
[2026-01-17 11:08:03] Decode batch, #running-req: 8, #token: 8576, token usage: 0.01, npu graph: False, gen throughput (token/s): 74.87, #queue-req: 52,
[2026-01-17 11:08:05] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [04:39<03:42,  3.78s/it][2026-01-17 11:08:05] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 51,
[2026-01-17 11:08:06] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [04:40<02:39,  2.76s/it][2026-01-17 11:08:06] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 50,
[2026-01-17 11:08:08] Decode batch, #running-req: 8, #token: 8704, token usage: 0.01, npu graph: False, gen throughput (token/s): 67.27, #queue-req: 50,
[2026-01-17 11:08:08] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [04:42<02:26,  2.58s/it][2026-01-17 11:08:08] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 49,
[2026-01-17 11:08:10] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [04:44<02:24,  2.57s/it][2026-01-17 11:08:11] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 48,
[2026-01-17 11:08:11] INFO:     127.0.0.1:35034 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [04:45<01:56,  2.11s/it][2026-01-17 11:08:12] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 47,
[2026-01-17 11:08:13] Decode batch, #running-req: 8, #token: 7424, token usage: 0.01, npu graph: False, gen throughput (token/s): 62.69, #queue-req: 47,
[2026-01-17 11:08:13] INFO:     127.0.0.1:43508 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [04:47<01:38,  1.82s/it][2026-01-17 11:08:13] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 46,
[2026-01-17 11:08:15] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [04:49<01:44,  1.98s/it][2026-01-17 11:08:15] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 45,
[2026-01-17 11:08:16] INFO:     127.0.0.1:34900 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [04:50<01:31,  1.76s/it][2026-01-17 11:08:16] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 44,
[2026-01-17 11:08:18] Decode batch, #running-req: 8, #token: 8320, token usage: 0.01, npu graph: False, gen throughput (token/s): 61.23, #queue-req: 44,
[2026-01-17 11:08:18] INFO:     127.0.0.1:34884 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [04:52<01:35,  1.86s/it][2026-01-17 11:08:18] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 43,
[2026-01-17 11:08:19] INFO:     127.0.0.1:34876 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [04:53<01:22,  1.64s/it][2026-01-17 11:08:20] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 42,
[2026-01-17 11:08:23] Decode batch, #running-req: 8, #token: 8448, token usage: 0.01, npu graph: False, gen throughput (token/s): 65.36, #queue-req: 42,
[2026-01-17 11:08:24] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [04:58<01:59,  2.45s/it][2026-01-17 11:08:24] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 41,
[2026-01-17 11:08:27] Decode batch, #running-req: 8, #token: 8576, token usage: 0.01, npu graph: False, gen throughput (token/s): 69.57, #queue-req: 41,
[2026-01-17 11:08:29] INFO:     127.0.0.1:34910 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [05:03<02:37,  3.28s/it][2026-01-17 11:08:29] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 40,
[2026-01-17 11:08:30] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [05:04<02:03,  2.62s/it][2026-01-17 11:08:30] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 39,
[2026-01-17 11:08:32] Decode batch, #running-req: 8, #token: 8832, token usage: 0.01, npu graph: False, gen throughput (token/s): 62.95, #queue-req: 39,
[2026-01-17 11:08:33] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [05:07<02:02,  2.66s/it][2026-01-17 11:08:33] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 38,
[2026-01-17 11:08:34] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [05:08<01:34,  2.09s/it][2026-01-17 11:08:34] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 37,
[2026-01-17 11:08:36] INFO:     127.0.0.1:34898 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [05:10<01:31,  2.07s/it][2026-01-17 11:08:36] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 36,
[2026-01-17 11:08:37] Decode batch, #running-req: 8, #token: 8576, token usage: 0.01, npu graph: False, gen throughput (token/s): 60.94, #queue-req: 36,
[2026-01-17 11:08:38] INFO:     127.0.0.1:35582 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [05:12<01:29,  2.08s/it][2026-01-17 11:08:38] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 35,
[2026-01-17 11:08:39] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [05:13<01:17,  1.85s/it][2026-01-17 11:08:39] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 34,
[2026-01-17 11:08:40] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [05:14<01:10,  1.72s/it][2026-01-17 11:08:41] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 33,
[2026-01-17 11:08:43] Decode batch, #running-req: 8, #token: 8320, token usage: 0.01, npu graph: False, gen throughput (token/s): 60.82, #queue-req: 33,
[2026-01-17 11:08:43] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [05:17<01:19,  1.98s/it][2026-01-17 11:08:43] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 32,
[2026-01-17 11:08:44] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [05:18<01:02,  1.61s/it][2026-01-17 11:08:44] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 31,
[2026-01-17 11:08:48] Decode batch, #running-req: 8, #token: 8064, token usage: 0.01, npu graph: False, gen throughput (token/s): 63.33, #queue-req: 31,
[2026-01-17 11:08:49] INFO:     127.0.0.1:45402 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [05:23<01:39,  2.61s/it][2026-01-17 11:08:49] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 30,
[2026-01-17 11:08:49] INFO:     127.0.0.1:45406 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [05:23<01:08,  1.86s/it][2026-01-17 11:08:49] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 29,
[2026-01-17 11:08:50] INFO:     127.0.0.1:45428 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [05:24<00:59,  1.66s/it][2026-01-17 11:08:50] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 28,
[2026-01-17 11:08:51] INFO:     127.0.0.1:45396 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [05:24<00:46,  1.33s/it][2026-01-17 11:08:51] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 27,
[2026-01-17 11:08:51] INFO:     127.0.0.1:45420 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [05:25<00:37,  1.09s/it][2026-01-17 11:08:51] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 26,
[2026-01-17 11:08:52] INFO:     127.0.0.1:45422 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [05:26<00:35,  1.07s/it][2026-01-17 11:08:52] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 25,
[2026-01-17 11:08:54] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [05:28<00:40,  1.26s/it][2026-01-17 11:08:54] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 24,
[2026-01-17 11:08:54] Decode batch, #running-req: 8, #token: 8064, token usage: 0.01, npu graph: False, gen throughput (token/s): 46.38, #queue-req: 24,
[2026-01-17 11:08:59] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 71.28, #queue-req: 24,
[2026-01-17 11:09:02] INFO:     127.0.0.1:46910 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [05:36<01:43,  3.34s/it][2026-01-17 11:09:02] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 23,
[2026-01-17 11:09:03] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [05:37<01:17,  2.58s/it][2026-01-17 11:09:03] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 22,
[2026-01-17 11:09:04] Decode batch, #running-req: 8, #token: 8064, token usage: 0.01, npu graph: False, gen throughput (token/s): 68.41, #queue-req: 22,
[2026-01-17 11:09:04] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [05:38<01:00,  2.10s/it][2026-01-17 11:09:04] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 21,
[2026-01-17 11:09:07] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [05:41<01:07,  2.42s/it][2026-01-17 11:09:07] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 20,
[2026-01-17 11:09:08] INFO:     127.0.0.1:46886 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [05:42<00:54,  2.02s/it][2026-01-17 11:09:08] Decode batch, #running-req: 7, #token: 7296, token usage: 0.01, npu graph: False, gen throughput (token/s): 69.32, #queue-req: 20,
[2026-01-17 11:09:08] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 19,
[2026-01-17 11:09:10] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [05:44<00:53,  2.05s/it][2026-01-17 11:09:10] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 18,
[2026-01-17 11:09:11] INFO:     127.0.0.1:46918 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [05:45<00:40,  1.62s/it][2026-01-17 11:09:11] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 17,
[2026-01-17 11:09:12] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [05:46<00:32,  1.37s/it][2026-01-17 11:09:12] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 16,
[2026-01-17 11:09:13] Decode batch, #running-req: 8, #token: 8192, token usage: 0.01, npu graph: False, gen throughput (token/s): 61.88, #queue-req: 16,
[2026-01-17 11:09:15] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [05:49<00:46,  2.02s/it][2026-01-17 11:09:15] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 15,
[2026-01-17 11:09:17] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [05:51<00:46,  2.10s/it][2026-01-17 11:09:18] Decode batch, #running-req: 7, #token: 7296, token usage: 0.01, npu graph: False, gen throughput (token/s): 75.34, #queue-req: 15,
[2026-01-17 11:09:18] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 14,
[2026-01-17 11:09:18] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [05:52<00:36,  1.73s/it][2026-01-17 11:09:18] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 13,
[2026-01-17 11:09:19] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [05:53<00:31,  1.58s/it][2026-01-17 11:09:20] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 12,
[2026-01-17 11:09:22] Decode batch, #running-req: 8, #token: 8576, token usage: 0.01, npu graph: False, gen throughput (token/s): 66.19, #queue-req: 12,
[2026-01-17 11:09:24] INFO:     127.0.0.1:38298 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [05:58<00:46,  2.44s/it][2026-01-17 11:09:24] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 11,
[2026-01-17 11:09:24] INFO:     127.0.0.1:60944 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [05:58<00:32,  1.82s/it][2026-01-17 11:09:24] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 10,
[2026-01-17 11:09:26] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [06:00<00:30,  1.78s/it][2026-01-17 11:09:26] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 9,
[2026-01-17 11:09:27] Decode batch, #running-req: 8, #token: 8320, token usage: 0.01, npu graph: False, gen throughput (token/s): 67.28, #queue-req: 9,
[2026-01-17 11:09:31] Decode batch, #running-req: 8, #token: 8704, token usage: 0.01, npu graph: False, gen throughput (token/s): 78.40, #queue-req: 9,
[2026-01-17 11:09:33] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [06:06<00:51,  3.20s/it][2026-01-17 11:09:33] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 8,
[2026-01-17 11:09:35] Decode batch, #running-req: 8, #token: 8704, token usage: 0.01, npu graph: False, gen throughput (token/s): 74.89, #queue-req: 8,
[2026-01-17 11:09:35] INFO:     127.0.0.1:38314 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [06:09<00:46,  3.12s/it][2026-01-17 11:09:36] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 7,
[2026-01-17 11:09:36] INFO:     127.0.0.1:38334 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [06:10<00:33,  2.37s/it][2026-01-17 11:09:36] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 6,
[2026-01-17 11:09:37] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [06:11<00:23,  1.82s/it][2026-01-17 11:09:37] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 5,
[2026-01-17 11:09:38] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [06:12<00:18,  1.55s/it][2026-01-17 11:09:38] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 4,
[2026-01-17 11:09:40] INFO:     127.0.0.1:38310 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [06:13<00:18,  1.68s/it][2026-01-17 11:09:40] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 3,
[2026-01-17 11:09:41] Decode batch, #running-req: 8, #token: 8320, token usage: 0.01, npu graph: False, gen throughput (token/s): 56.67, #queue-req: 3,
[2026-01-17 11:09:42] INFO:     127.0.0.1:38346 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [06:16<00:18,  1.85s/it][2026-01-17 11:09:42] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 2,
[2026-01-17 11:09:44] INFO:     127.0.0.1:38330 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [06:18<00:17,  1.99s/it][2026-01-17 11:09:44] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 1,
[2026-01-17 11:09:45] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [06:19<00:12,  1.55s/it][2026-01-17 11:09:45] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 0,
[2026-01-17 11:09:46] Decode batch, #running-req: 8, #token: 8320, token usage: 0.01, npu graph: False, gen throughput (token/s): 64.51, #queue-req: 0,
[2026-01-17 11:09:46] INFO:     127.0.0.1:38172 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [06:20<00:11,  1.61s/it][2026-01-17 11:09:48] INFO:     127.0.0.1:38182 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [06:21<00:08,  1.47s/it][2026-01-17 11:09:50] Decode batch, #running-req: 6, #token: 6272, token usage: 0.01, npu graph: False, gen throughput (token/s): 62.84, #queue-req: 0,
[2026-01-17 11:09:51] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [06:24<00:09,  1.93s/it][2026-01-17 11:09:54] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [06:28<00:09,  2.31s/it][2026-01-17 11:09:58] INFO:     127.0.0.1:38162 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [06:32<00:08,  2.96s/it][2026-01-17 11:09:59] Decode batch, #running-req: 3, #token: 3200, token usage: 0.00, npu graph: False, gen throughput (token/s): 19.47, #queue-req: 0,
[2026-01-17 11:10:00] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [06:34<00:05,  2.74s/it][2026-01-17 11:10:09] Decode batch, #running-req: 2, #token: 2304, token usage: 0.00, npu graph: False, gen throughput (token/s): 8.68, #queue-req: 0,
[2026-01-17 11:10:12] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [06:46<00:05,  5.45s/it][2026-01-17 11:10:15] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, npu graph: False, gen throughput (token/s): 8.44, #queue-req: 0,
[2026-01-17 11:10:20] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [06:54<00:00,  6.03s/it]
100%|██████████| 200/200 [06:54<00:00,  2.07s/it]
.
----------------------------------------------------------------------
Ran 1 test in 465.405s

OK
Accuracy: 0.730
Invalid: 0.015
Latency: 414.214 s
Output throughput: 61.630 token/s
.
.
End (32/62):
filename='ascend/llm_models/test_ascend_minicpm3_4b.py', elapsed=475, estimated_time=400
.
.

.
.
Begin (33/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_pp_single_node.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 11:10:38] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.837, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=256, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=1037753302, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 11:10:39] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 11:10:48] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 11:10:48] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 11:10:48] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 11:10:49] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 11:10:49] Load weight begin. avail mem=60.81 GB

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.84s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.84s/it]

[2026-01-17 11:10:51] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=59.56 GB, mem usage=1.25 GB.
[2026-01-17 11:10:51] Using KV cache dtype: torch.bfloat16
[2026-01-17 11:10:51] The available memory for KV cache is 49.65 GB.
[2026-01-17 11:10:52] KV Cache is allocated. #tokens: 464768, K size: 24.83 GB, V size: 24.83 GB
[2026-01-17 11:10:52] Memory pool end. avail mem=9.27 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 11:10:53] max_total_num_tokens=464768, chunked_prefill_size=256, max_prefill_tokens=16384, max_running_requests=4096, context_len=40960, available_gpu_mem=9.25 GB
[2026-01-17 11:10:53] INFO:     Started server process [294998]
[2026-01-17 11:10:53] INFO:     Waiting for application startup.
[2026-01-17 11:10:53] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-17 11:10:53] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-17 11:10:53] INFO:     Application startup complete.
[2026-01-17 11:10:53] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 11:10:54] INFO:     127.0.0.1:33380 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-17 11:10:54] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:10:59] INFO:     127.0.0.1:37944 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 11:11:06] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:06] The server is fired up and ready to roll!
[2026-01-17 11:11:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:11:10] INFO:     127.0.0.1:34800 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 11:11:10] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 11:11:10] INFO:     127.0.0.1:34816 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 11:11:10] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:11:10] INFO:     127.0.0.1:34820 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B --chunked-prefill-size 256 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestQwenPPTieWeightsAccuracy.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 11:11:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 1, #queue-req: 9,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 3, #queue-req: 18,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 4, #queue-req: 28,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 6, #queue-req: 39,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 8, #queue-req: 48,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 9, #queue-req: 49,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 11, #queue-req: 47,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 13, #queue-req: 45,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 15, #queue-req: 49,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 17, #queue-req: 57,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 19, #queue-req: 65,
[2026-01-17 11:11:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 21, #queue-req: 74,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 23, #queue-req: 82,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 25, #queue-req: 90,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 27, #queue-req: 99,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 29, #queue-req: 97,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 31, #queue-req: 95,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 33, #queue-req: 93,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 35, #queue-req: 91,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 37, #queue-req: 89,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 39, #queue-req: 87,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.01, #running-req: 40, #queue-req: 86,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 42, #queue-req: 84,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 44, #queue-req: 82,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 46, #queue-req: 80,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 48, #queue-req: 78,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 50, #queue-req: 76,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 52, #queue-req: 74,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 54, #queue-req: 73,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 55, #queue-req: 71,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 57, #queue-req: 69,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 59, #queue-req: 67,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 61, #queue-req: 65,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 63, #queue-req: 63,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 65, #queue-req: 61,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 67, #queue-req: 59,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 69, #queue-req: 57,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 71, #queue-req: 55,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 72, #queue-req: 54,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 74, #queue-req: 52,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 76, #queue-req: 50,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 78, #queue-req: 48,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 80, #queue-req: 46,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 82, #queue-req: 44,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 84, #queue-req: 42,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 86, #queue-req: 40,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 88, #queue-req: 38,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 90, #queue-req: 36,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 92, #queue-req: 34,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 94, #queue-req: 32,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 96, #queue-req: 30,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 98, #queue-req: 28,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 100, #queue-req: 26,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 102, #queue-req: 24,
[2026-01-17 11:11:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 104, #queue-req: 22,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 106, #queue-req: 20,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 108, #queue-req: 18,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 110, #queue-req: 16,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 112, #queue-req: 14,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 114, #queue-req: 12,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 116, #queue-req: 10,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.04, #running-req: 118, #queue-req: 9,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 119, #queue-req: 7,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 121, #queue-req: 5,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 123, #queue-req: 3,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 125, #queue-req: 1,
[2026-01-17 11:11:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:12] INFO:     127.0.0.1:35156 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<07:01,  2.12s/it][2026-01-17 11:11:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:12] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:13] INFO:     127.0.0.1:35674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:13] INFO:     127.0.0.1:35688 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:02<02:05,  1.57it/s]
  2%|▏         | 4/200 [00:02<00:57,  3.39it/s][2026-01-17 11:11:13] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-17 11:11:13] INFO:     127.0.0.1:35950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:13] Decode batch, #running-req: 127, #token: 21120, token usage: 0.05, npu graph: False, gen throughput (token/s): 154.10, #queue-req: 0,
[2026-01-17 11:11:13] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:02<00:42,  4.52it/s][2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:13] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:13] INFO:     127.0.0.1:35350 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:02<00:34,  5.63it/s][2026-01-17 11:11:13] INFO:     127.0.0.1:35622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:13] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 11:11:13] INFO:     127.0.0.1:35876 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:02<00:23,  8.19it/s][2026-01-17 11:11:13] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:13] INFO:     127.0.0.1:35016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 11:11:13] INFO:     127.0.0.1:35010 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:03<00:17, 10.66it/s][2026-01-17 11:11:13] INFO:     127.0.0.1:35018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:13] INFO:     127.0.0.1:34822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:13] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 11:11:13] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-17 11:11:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.05, #running-req: 130, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:35712 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:04<00:36,  4.98it/s][2026-01-17 11:11:15] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:35668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 11:11:15] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35766 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:04<00:27,  6.58it/s]
 11%|█         | 22/200 [00:04<00:19,  9.30it/s][2026-01-17 11:11:15] INFO:     127.0.0.1:35032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:35168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-17 11:11:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 130, #queue-req: 4,
[2026-01-17 11:11:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 132, #queue-req: 2,
[2026-01-17 11:11:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 134, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:34976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:36008 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:04<00:10, 16.61it/s]
 16%|█▌        | 32/200 [00:04<00:06, 24.46it/s][2026-01-17 11:11:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:35604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:35052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:35996 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:04<00:07, 23.29it/s][2026-01-17 11:11:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:34990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 125, #queue-req: 1,

 20%|██        | 40/200 [00:05<00:06, 25.23it/s]
 20%|██        | 41/200 [00:05<00:05, 28.76it/s][2026-01-17 11:11:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 126, #queue-req: 2,
[2026-01-17 11:11:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:35514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:35436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] INFO:     127.0.0.1:35818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:35476 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:05<00:06, 25.52it/s][2026-01-17 11:11:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:15] Decode batch, #running-req: 127, #token: 28032, token usage: 0.06, npu graph: False, gen throughput (token/s): 1789.65, #queue-req: 0,
[2026-01-17 11:11:15] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:16] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35432 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:05<00:05, 26.43it/s][2026-01-17 11:11:16] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 125, #queue-req: 1,
[2026-01-17 11:11:16] INFO:     127.0.0.1:35888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 127, #queue-req: 1,
[2026-01-17 11:11:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 11:11:16] INFO:     127.0.0.1:34870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:36092 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:05<00:05, 26.01it/s]
 27%|██▋       | 54/200 [00:05<00:05, 27.50it/s][2026-01-17 11:11:16] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 11:11:16] INFO:     127.0.0.1:34968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 11:11:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 11:11:16] INFO:     127.0.0.1:35096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35322 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:05<00:05, 26.08it/s]
 29%|██▉       | 58/200 [00:05<00:05, 26.96it/s][2026-01-17 11:11:16] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-17 11:11:16] INFO:     127.0.0.1:35270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-17 11:11:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-17 11:11:16] INFO:     127.0.0.1:35660 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:05<00:05, 26.42it/s][2026-01-17 11:11:16] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:16] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:16] INFO:     127.0.0.1:35046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:36060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 125, #queue-req: 1,
[2026-01-17 11:11:16] INFO:     127.0.0.1:35080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:36080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 126, #queue-req: 0,

 34%|███▎      | 67/200 [00:05<00:04, 28.82it/s]
 36%|███▌      | 71/200 [00:05<00:02, 48.61it/s]
 36%|███▌      | 71/200 [00:05<00:02, 48.61it/s]
 36%|███▌      | 71/200 [00:05<00:02, 48.61it/s]
 36%|███▌      | 71/200 [00:05<00:02, 48.61it/s][2026-01-17 11:11:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 3,
[2026-01-17 11:11:16] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 130, #queue-req: 1,
[2026-01-17 11:11:16] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 132, #queue-req: 0,
[2026-01-17 11:11:16] INFO:     127.0.0.1:35830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-17 11:11:16] INFO:     127.0.0.1:34958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35880 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [00:06<00:03, 37.97it/s][2026-01-17 11:11:16] INFO:     127.0.0.1:35106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:35280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:16] INFO:     127.0.0.1:36038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36070 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:06<00:03, 38.88it/s]
 42%|████▏     | 84/200 [00:06<00:02, 51.78it/s]
 42%|████▏     | 84/200 [00:06<00:02, 51.78it/s]
 42%|████▏     | 84/200 [00:06<00:02, 51.78it/s][2026-01-17 11:11:17] INFO:     127.0.0.1:35008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:06<00:02, 38.96it/s]
 46%|████▌     | 91/200 [00:06<00:03, 34.42it/s][2026-01-17 11:11:17] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] Decode batch, #running-req: 108, #token: 23936, token usage: 0.05, npu graph: False, gen throughput (token/s): 3584.05, #queue-req: 0,
[2026-01-17 11:11:17] INFO:     127.0.0.1:35612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35864 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:06<00:02, 37.74it/s]
 49%|████▉     | 98/200 [00:06<00:02, 42.55it/s][2026-01-17 11:11:17] INFO:     127.0.0.1:35328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:34966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35466 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:06<00:01, 48.58it/s][2026-01-17 11:11:17] INFO:     127.0.0.1:36356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36372 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:06<00:01, 46.99it/s]
 56%|█████▋    | 113/200 [00:06<00:01, 47.95it/s][2026-01-17 11:11:17] INFO:     127.0.0.1:34936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35138 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:07<00:01, 49.25it/s][2026-01-17 11:11:17] INFO:     127.0.0.1:36472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:17] INFO:     127.0.0.1:36638 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:07<00:01, 39.86it/s]
 63%|██████▎   | 126/200 [00:07<00:02, 36.64it/s][2026-01-17 11:11:18] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:35408 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:35590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:35204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36192 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:07<00:01, 40.33it/s]
 67%|██████▋   | 134/200 [00:07<00:01, 45.19it/s][2026-01-17 11:11:18] INFO:     127.0.0.1:36666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:35848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36580 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] Decode batch, #running-req: 60, #token: 16640, token usage: 0.04, npu graph: False, gen throughput (token/s): 3613.24, #queue-req: 0,
[2026-01-17 11:11:18] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36674 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:07<00:01, 46.93it/s]
 71%|███████   | 142/200 [00:07<00:01, 50.28it/s][2026-01-17 11:11:18] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:34950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:35804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:35006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:07<00:00, 55.26it/s]
 76%|███████▌  | 152/200 [00:07<00:00, 68.64it/s]
 76%|███████▌  | 152/200 [00:07<00:00, 68.64it/s][2026-01-17 11:11:18] INFO:     127.0.0.1:36152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:35148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:35774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:07<00:00, 48.46it/s]
 80%|████████  | 161/200 [00:07<00:00, 41.38it/s][2026-01-17 11:11:18] INFO:     127.0.0.1:36612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:08<00:00, 42.10it/s][2026-01-17 11:11:18] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:35902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:18] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:08<00:00, 38.30it/s]
 87%|████████▋ | 174/200 [00:08<00:00, 38.12it/s]
 87%|████████▋ | 174/200 [00:08<00:00, 38.12it/s][2026-01-17 11:11:19] INFO:     127.0.0.1:36348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:36680 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:36430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] Decode batch, #running-req: 24, #token: 7936, token usage: 0.02, npu graph: False, gen throughput (token/s): 1779.58, #queue-req: 0,
[2026-01-17 11:11:19] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:34962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:36522 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:08<00:00, 34.26it/s]
 90%|█████████ | 180/200 [00:08<00:00, 33.29it/s][2026-01-17 11:11:19] INFO:     127.0.0.1:35020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:34840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:08<00:00, 21.22it/s][2026-01-17 11:11:19] INFO:     127.0.0.1:36624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:35492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:36290 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:09<00:00, 19.26it/s]
 94%|█████████▍| 189/200 [00:09<00:00, 19.05it/s][2026-01-17 11:11:19] INFO:     127.0.0.1:36024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:35026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] INFO:     127.0.0.1:36404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:19] Decode batch, #running-req: 8, #token: 3840, token usage: 0.01, npu graph: False, gen throughput (token/s): 800.31, #queue-req: 0,
[2026-01-17 11:11:20] INFO:     127.0.0.1:36278 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:09<00:00, 12.11it/s][2026-01-17 11:11:20] INFO:     127.0.0.1:36108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:20] Decode batch, #running-req: 6, #token: 3072, token usage: 0.01, npu graph: False, gen throughput (token/s): 382.01, #queue-req: 0,
[2026-01-17 11:11:20] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:10<00:00,  9.65it/s][2026-01-17 11:11:21] INFO:     127.0.0.1:36506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:21] Decode batch, #running-req: 4, #token: 2688, token usage: 0.01, npu graph: False, gen throughput (token/s): 259.75, #queue-req: 0,
[2026-01-17 11:11:22] Decode batch, #running-req: 4, #token: 2816, token usage: 0.01, npu graph: False, gen throughput (token/s): 202.78, #queue-req: 0,
[2026-01-17 11:11:22] INFO:     127.0.0.1:36294 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:11<00:00,  4.36it/s][2026-01-17 11:11:23] Decode batch, #running-req: 3, #token: 2304, token usage: 0.00, npu graph: False, gen throughput (token/s): 167.66, #queue-req: 0,
[2026-01-17 11:11:23] Decode batch, #running-req: 3, #token: 2560, token usage: 0.01, npu graph: False, gen throughput (token/s): 141.93, #queue-req: 0,
[2026-01-17 11:11:24] Decode batch, #running-req: 3, #token: 2560, token usage: 0.01, npu graph: False, gen throughput (token/s): 138.22, #queue-req: 0,
[2026-01-17 11:11:25] Decode batch, #running-req: 3, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 137.84, #queue-req: 0,
[2026-01-17 11:11:25] INFO:     127.0.0.1:34860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:11:25] INFO:     127.0.0.1:35758 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:15<00:01,  1.78it/s]
100%|█████████▉| 199/200 [00:15<00:00,  1.24it/s][2026-01-17 11:11:26] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 49.09, #queue-req: 0,
[2026-01-17 11:11:27] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 48.10, #queue-req: 0,
[2026-01-17 11:11:27] INFO:     127.0.0.1:36550 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:16<00:00,  1.06it/s]
100%|██████████| 200/200 [00:16<00:00, 11.94it/s]
.
----------------------------------------------------------------------
Ran 1 test in 58.103s

OK
Accuracy: 0.435
Invalid: 0.010
Latency: 16.861 s
Output throughput: 1231.220 token/s
.
.
End (33/62):
filename='ascend/llm_models/test_ascend_pp_single_node.py', elapsed=68, estimated_time=400
.
.

.
.
Begin (34/62):
python3 /data/c30044170/code/newHDK/ascend/llm_models/test_ascend_llava_onevision_qwen2_7b_ov.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 11:11:46] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/lmms-lab/llava-onevision-qwen2-7b-ov', tokenizer_path='/root/.cache/modelscope/hub/models/lmms-lab/llava-onevision-qwen2-7b-ov', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.7, max_running_requests=2048, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=711678592, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/lmms-lab/llava-onevision-qwen2-7b-ov', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=60, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-17 11:11:49] Inferred chat template from model path: chatml-llava
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 11:11:56 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 11:11:57 TP1] Init torch distributed begin.
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2026-01-17 11:11:58 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 11:11:58 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 11:11:58 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 11:11:58 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 11:11:58 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 11:11:59 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 11:11:59 TP1] Load weight begin. avail mem=61.14 GB
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:06<00:20,  6.99s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:13<00:13,  6.54s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:14<00:04,  4.27s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:21<00:00,  5.10s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:21<00:00,  5.29s/it]

[2026-01-17 11:12:29 TP0] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=52.86 GB, mem usage=7.95 GB.
[2026-01-17 11:12:29 TP1] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=53.19 GB, mem usage=7.95 GB.
[2026-01-17 11:12:29 TP0] Using KV cache dtype: torch.float16
[2026-01-17 11:12:29 TP0] The available memory for KV cache is 34.61 GB.
[2026-01-17 11:12:29 TP1] The available memory for KV cache is 34.61 GB.
[2026-01-17 11:12:31 TP0] KV Cache is allocated. #tokens: 1296256, K size: 17.31 GB, V size: 17.31 GB
[2026-01-17 11:12:31 TP0] Memory pool end. avail mem=17.99 GB
[2026-01-17 11:12:31 TP1] KV Cache is allocated. #tokens: 1296256, K size: 17.31 GB, V size: 17.31 GB
[2026-01-17 11:12:31 TP1] Memory pool end. avail mem=18.32 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 11:12:32 TP0] max_total_num_tokens=1296256, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=32768, available_gpu_mem=17.99 GB
[2026-01-17 11:12:32] INFO:     Started server process [297600]
[2026-01-17 11:12:32] INFO:     Waiting for application startup.
[2026-01-17 11:12:32] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 11:12:32] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 11:12:32] INFO:     Application startup complete.
[2026-01-17 11:12:32] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 11:12:33] INFO:     127.0.0.1:38378 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 11:12:58] INFO:     127.0.0.1:32988 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 11:12:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:13:08] INFO:     127.0.0.1:48776 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 11:13:15] INFO:     127.0.0.1:38384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 11:13:15] The server is fired up and ready to roll!
[2026-01-17 11:13:18 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:13:19] INFO:     127.0.0.1:56766 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 11:13:19] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 11:13:19] INFO:     127.0.0.1:56780 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 11:13:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:13:20] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/lmms-lab/llava-onevision-qwen2-7b-ov --trust-remote-code --tp-size 2 --max-running-requests 2048 --mem-fraction-static 0.7 --attention-backend ascend --disable-cuda-graph --mm-per-request-timeout 60 --enable-multimodal --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 11:13:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 11:13:20 TP0] Prefill batch, #new-seq: 12, #new-token: 1792, #cached-token: 9216, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 11:13:20 TP0] Prefill batch, #new-seq: 18, #new-token: 2304, #cached-token: 13824, token usage: 0.00, #running-req: 13, #queue-req: 0,
[2026-01-17 11:13:20 TP0] Prefill batch, #new-seq: 20, #new-token: 2688, #cached-token: 15360, token usage: 0.00, #running-req: 31, #queue-req: 0,
[2026-01-17 11:13:20 TP0] Prefill batch, #new-seq: 24, #new-token: 3328, #cached-token: 18432, token usage: 0.01, #running-req: 51, #queue-req: 0,
[2026-01-17 11:13:20 TP0] Prefill batch, #new-seq: 26, #new-token: 3456, #cached-token: 19968, token usage: 0.01, #running-req: 75, #queue-req: 0,
[2026-01-17 11:13:20 TP0] Prefill batch, #new-seq: 27, #new-token: 3456, #cached-token: 20736, token usage: 0.01, #running-req: 101, #queue-req: 0,
[2026-01-17 11:13:21 TP0] Decode batch, #running-req: 128, #token: 21376, token usage: 0.02, npu graph: False, gen throughput (token/s): 46.40, #queue-req: 0,
[2026-01-17 11:13:21] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:01<04:49,  1.46s/it][2026-01-17 11:13:21] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 11:13:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:21] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:21] INFO:     127.0.0.1:57894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:21] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:01<01:26,  2.28it/s]
  2%|▎         | 5/200 [00:01<00:22,  8.65it/s]
  2%|▎         | 5/200 [00:01<00:22,  8.65it/s][2026-01-17 11:13:21 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 11:13:21] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:21] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:21] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,

  4%|▎         | 7/200 [00:01<00:22,  8.76it/s]
  4%|▍         | 8/200 [00:01<00:19, 10.10it/s][2026-01-17 11:13:21 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:22] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:22] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:22] INFO:     127.0.0.1:57568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:22] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:02<00:21,  8.96it/s]
  6%|▌         | 12/200 [00:02<00:17, 10.71it/s]
  6%|▌         | 12/200 [00:02<00:17, 10.71it/s][2026-01-17 11:13:22 TP0] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.02, #running-req: 124, #queue-req: 0,
[2026-01-17 11:13:22] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:22] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:02<00:20,  9.03it/s][2026-01-17 11:13:22 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 11:13:22] INFO:     127.0.0.1:56990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 11:13:22] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:02<00:18, 10.06it/s][2026-01-17 11:13:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 11:13:22] INFO:     127.0.0.1:56934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:22] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:22] INFO:     127.0.0.1:57682 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:02<00:16, 11.22it/s]
 10%|▉         | 19/200 [00:02<00:12, 13.96it/s][2026-01-17 11:13:22] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:22 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 11:13:22] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:22] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:22 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:22 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 11:13:22] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:02<00:10, 16.16it/s][2026-01-17 11:13:22] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 11:13:23] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 11:13:23] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:03<00:09, 17.59it/s]
 14%|█▍        | 28/200 [00:03<00:08, 20.16it/s][2026-01-17 11:13:23] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 11:13:23 TP0] Decode batch, #running-req: 126, #token: 28800, token usage: 0.02, npu graph: False, gen throughput (token/s): 2921.91, #queue-req: 0,
[2026-01-17 11:13:23] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 11:13:23] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:03<00:07, 22.39it/s][2026-01-17 11:13:23] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 11:13:23] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 11:13:23] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:03<00:07, 21.55it/s][2026-01-17 11:13:23] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:23] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:03<00:07, 20.30it/s]
 20%|██        | 41/200 [00:03<00:07, 21.02it/s][2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 11:13:23] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 11:13:23] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 11:13:23] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:03<00:08, 17.46it/s]
 24%|██▎       | 47/200 [00:03<00:07, 19.39it/s]
 24%|██▎       | 47/200 [00:03<00:07, 19.39it/s]
 24%|██▎       | 47/200 [00:03<00:07, 19.39it/s][2026-01-17 11:13:23] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:23 TP0] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.02, #running-req: 124, #queue-req: 0,
[2026-01-17 11:13:24] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 130, #queue-req: 0,
[2026-01-17 11:13:24] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:04<00:07, 19.24it/s][2026-01-17 11:13:24] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 11:13:24] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:24] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:04<00:07, 20.06it/s]
 28%|██▊       | 56/200 [00:04<00:06, 22.17it/s][2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 11:13:24] INFO:     127.0.0.1:56918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 11:13:24] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:04<00:06, 20.89it/s]
 31%|███       | 62/200 [00:04<00:05, 26.78it/s]
 31%|███       | 62/200 [00:04<00:05, 26.78it/s]
 31%|███       | 62/200 [00:04<00:05, 26.78it/s][2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 3072, token usage: 0.02, #running-req: 124, #queue-req: 0,
[2026-01-17 11:13:24] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.02, #running-req: 130, #queue-req: 0,
[2026-01-17 11:13:24] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:57220 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:04<00:04, 27.20it/s]
 34%|███▍      | 68/200 [00:04<00:04, 29.16it/s][2026-01-17 11:13:24] INFO:     127.0.0.1:58216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:24] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:04<00:04, 25.89it/s][2026-01-17 11:13:24] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 11:13:24] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 11:13:25] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:04<00:04, 26.42it/s][2026-01-17 11:13:25] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25 TP0] Decode batch, #running-req: 122, #token: 26752, token usage: 0.02, npu graph: False, gen throughput (token/s): 2684.08, #queue-req: 0,
[2026-01-17 11:13:25] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:05<00:03, 32.13it/s]
 41%|████      | 82/200 [00:05<00:03, 38.94it/s][2026-01-17 11:13:25] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:05<00:03, 33.89it/s]
 44%|████▍     | 88/200 [00:05<00:03, 32.71it/s][2026-01-17 11:13:25] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:05<00:02, 36.70it/s][2026-01-17 11:13:25] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:58510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57506 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:05<00:01, 48.27it/s][2026-01-17 11:13:25] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:05<00:01, 49.21it/s][2026-01-17 11:13:25] INFO:     127.0.0.1:57602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:25] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:05<00:01, 42.08it/s][2026-01-17 11:13:25] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:06<00:02, 34.37it/s][2026-01-17 11:13:26] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26 TP0] Decode batch, #running-req: 77, #token: 19584, token usage: 0.02, npu graph: False, gen throughput (token/s): 3219.09, #queue-req: 0,
[2026-01-17 11:13:26] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58276 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:06<00:02, 28.85it/s][2026-01-17 11:13:26] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [00:06<00:02, 33.47it/s][2026-01-17 11:13:26] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:06<00:02, 28.00it/s][2026-01-17 11:13:26] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:06<00:02, 28.06it/s]
 70%|███████   | 140/200 [00:06<00:02, 29.95it/s][2026-01-17 11:13:26] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:06<00:01, 31.13it/s][2026-01-17 11:13:26] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:26] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:06<00:01, 32.39it/s][2026-01-17 11:13:27] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:58430 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:07<00:01, 28.80it/s][2026-01-17 11:13:27] INFO:     127.0.0.1:58462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27 TP0] Decode batch, #running-req: 44, #token: 13568, token usage: 0.01, npu graph: False, gen throughput (token/s): 2064.70, #queue-req: 0,
[2026-01-17 11:13:27] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:07<00:01, 24.76it/s][2026-01-17 11:13:27] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:58344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:57852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:58398 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:07<00:01, 23.70it/s]
 81%|████████  | 162/200 [00:07<00:01, 24.60it/s][2026-01-17 11:13:27] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:07<00:01, 24.77it/s][2026-01-17 11:13:27] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:27] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:07<00:01, 23.14it/s][2026-01-17 11:13:27] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:28] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:28] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:08<00:01, 23.01it/s][2026-01-17 11:13:28] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:28] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:28] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:08<00:01, 17.31it/s][2026-01-17 11:13:28 TP0] Decode batch, #running-req: 25, #token: 9216, token usage: 0.01, npu graph: False, gen throughput (token/s): 1210.58, #queue-req: 0,
[2026-01-17 11:13:28] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:28] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:28] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:08<00:01, 11.53it/s]
 89%|████████▉ | 178/200 [00:08<00:02,  9.85it/s][2026-01-17 11:13:28] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:29] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:09<00:02,  8.66it/s][2026-01-17 11:13:29] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:29] INFO:     127.0.0.1:58136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:29] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:09<00:02,  7.25it/s]
 92%|█████████▏| 183/200 [00:09<00:02,  7.23it/s][2026-01-17 11:13:29 TP0] Decode batch, #running-req: 20, #token: 7168, token usage: 0.01, npu graph: False, gen throughput (token/s): 831.37, #queue-req: 0,
[2026-01-17 11:13:29] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:29] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:09<00:01,  8.33it/s][2026-01-17 11:13:29] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:29] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:29] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:09<00:01, 10.67it/s][2026-01-17 11:13:29] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:29] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:09<00:00, 11.99it/s][2026-01-17 11:13:30] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:30] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:10<00:00, 12.78it/s][2026-01-17 11:13:30] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:30] INFO:     127.0.0.1:58142 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:10<00:00,  9.32it/s][2026-01-17 11:13:30] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:30 TP0] Decode batch, #running-req: 5, #token: 2688, token usage: 0.00, npu graph: False, gen throughput (token/s): 356.56, #queue-req: 0,
[2026-01-17 11:13:30] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:10<00:00,  7.59it/s][2026-01-17 11:13:31 TP0] Decode batch, #running-req: 4, #token: 2432, token usage: 0.00, npu graph: False, gen throughput (token/s): 160.31, #queue-req: 0,
[2026-01-17 11:13:32] INFO:     127.0.0.1:58158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 11:13:32 TP0] Decode batch, #running-req: 3, #token: 2304, token usage: 0.00, npu graph: False, gen throughput (token/s): 155.31, #queue-req: 0,
[2026-01-17 11:13:32] INFO:     127.0.0.1:58368 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:12<00:00,  2.68it/s][2026-01-17 11:13:33] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:13<00:00,  2.31it/s][2026-01-17 11:13:33 TP0] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, npu graph: False, gen throughput (token/s): 73.01, #queue-req: 0,
[2026-01-17 11:13:34 TP0] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, npu graph: False, gen throughput (token/s): 40.35, #queue-req: 0,
[2026-01-17 11:13:35 TP0] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 40.20, #queue-req: 0,
[2026-01-17 11:13:36 TP0] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 40.23, #queue-req: 0,
[2026-01-17 11:13:37 TP0] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, npu graph: False, gen throughput (token/s): 38.17, #queue-req: 0,
[2026-01-17 11:13:37] INFO:     127.0.0.1:58354 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:17<00:00,  1.22s/it]
100%|██████████| 200/200 [00:17<00:00, 11.15it/s]
.
----------------------------------------------------------------------
Ran 1 test in 121.020s

OK
Accuracy: 0.740
Invalid: 0.000
Latency: 17.990 s
Output throughput: 1306.916 token/s
.
.
End (34/62):
filename='ascend/llm_models/test_ascend_llava_onevision_qwen2_7b_ov.py', elapsed=131, estimated_time=400
.
.

.
.
Begin (35/62):
python3 /data/c30044170/code/newHDK/ascend/embedding_models/test_ascend_embedding_models.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/embedding_models/test_ascend_embedding_models.py", line 9, in <module>
    from sglang.test.runners import DEFAULT_PROMPTS, HFRunner, SRTRunner
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/runners.py", line 68, in <module>
    with open(os.path.join(dirpath, "long_prompt.txt"), "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/long_prompt.txt'
[ERROR] 2026-01-17-11:13:50 (PID:302838, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (35/62):
filename='ascend/embedding_models/test_ascend_embedding_models.py', elapsed=11, estimated_time=400
.
.


[CI Retry] ascend/embedding_models/test_ascend_embedding_models.py failed with non-retriable error: FileNotFoundError - not retrying


✗ FAILED: ascend/embedding_models/test_ascend_embedding_models.py returned exit code 1

.
.
Begin (36/62):
python3 /data/c30044170/code/newHDK/ascend/embedding_models/test_ascend_embedding_models_e5.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/embedding_models/test_ascend_embedding_models_e5.py", line 23, in <module>
    from sglang.test.runners import DEFAULT_PROMPTS, HFRunner, SRTRunner
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/runners.py", line 68, in <module>
    with open(os.path.join(dirpath, "long_prompt.txt"), "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/long_prompt.txt'
[ERROR] 2026-01-17-11:14:01 (PID:303102, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (36/62):
filename='ascend/embedding_models/test_ascend_embedding_models_e5.py', elapsed=11, estimated_time=400
.
.


[CI Retry] ascend/embedding_models/test_ascend_embedding_models_e5.py failed with non-retriable error: FileNotFoundError - not retrying


✗ FAILED: ascend/embedding_models/test_ascend_embedding_models_e5.py returned exit code 1

.
.
Begin (37/62):
python3 /data/c30044170/code/newHDK/ascend/embedding_models/test_embedding_models_clip_vit_large_patch14_336.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/embedding_models/test_embedding_models_clip_vit_large_patch14_336.py", line 23, in <module>
    from sglang.test.runners import DEFAULT_PROMPTS, HFRunner, SRTRunner
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/runners.py", line 68, in <module>
    with open(os.path.join(dirpath, "long_prompt.txt"), "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/long_prompt.txt'
[ERROR] 2026-01-17-11:14:12 (PID:303366, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (37/62):
filename='ascend/embedding_models/test_embedding_models_clip_vit_large_patch14_336.py', elapsed=12, estimated_time=400
.
.


[CI Retry] ascend/embedding_models/test_embedding_models_clip_vit_large_patch14_336.py failed with non-retriable error: FileNotFoundError - not retrying


✗ FAILED: ascend/embedding_models/test_embedding_models_clip_vit_large_patch14_336.py returned exit code 1

.
.
Begin (38/62):
python3 /data/c30044170/code/newHDK/ascend/embedding_models/test_gme_qwen_models.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/embedding_models/test_gme_qwen_models.py", line 21, in <module>
    from sglang.test.runners import HFRunner, SRTRunner
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/runners.py", line 68, in <module>
    with open(os.path.join(dirpath, "long_prompt.txt"), "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/long_prompt.txt'
[ERROR] 2026-01-17-11:14:24 (PID:303630, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (38/62):
filename='ascend/embedding_models/test_gme_qwen_models.py', elapsed=11, estimated_time=400
.
.


[CI Retry] ascend/embedding_models/test_gme_qwen_models.py failed with non-retriable error: FileNotFoundError - not retrying


✗ FAILED: ascend/embedding_models/test_gme_qwen_models.py returned exit code 1

.
.
Begin (39/62):
python3 /data/c30044170/code/newHDK/ascend/rerank_models/test_ascend_cross_encoder_models.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/rerank_models/test_ascend_cross_encoder_models.py", line 7, in <module>
    from sglang.test.runners import TEST_RERANK_QUERY_DOCS, HFRunner, SRTRunner
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/runners.py", line 68, in <module>
    with open(os.path.join(dirpath, "long_prompt.txt"), "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/long_prompt.txt'
[ERROR] 2026-01-17-11:14:35 (PID:303894, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (39/62):
filename='ascend/rerank_models/test_ascend_cross_encoder_models.py', elapsed=11, estimated_time=400
.
.


[CI Retry] ascend/rerank_models/test_ascend_cross_encoder_models.py failed with non-retriable error: FileNotFoundError - not retrying


✗ FAILED: ascend/rerank_models/test_ascend_cross_encoder_models.py returned exit code 1

.
.
Begin (40/62):
python3 /data/c30044170/code/newHDK/ascend/reward_models/test_ascend_llama_3_1_8b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/reward_models/test_ascend_llama_3_1_8b.py", line 20, in <module>
    from sglang.test.runners import HFRunner, SRTRunner
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/runners.py", line 68, in <module>
    with open(os.path.join(dirpath, "long_prompt.txt"), "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/long_prompt.txt'
[ERROR] 2026-01-17-11:14:46 (PID:304158, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (40/62):
filename='ascend/reward_models/test_ascend_llama_3_1_8b.py', elapsed=11, estimated_time=400
.
.


[CI Retry] ascend/reward_models/test_ascend_llama_3_1_8b.py failed with non-retriable error: FileNotFoundError - not retrying


✗ FAILED: ascend/reward_models/test_ascend_llama_3_1_8b.py returned exit code 1

.
.
Begin (41/62):
python3 /data/c30044170/code/newHDK/ascend/reward_models/test_ascend_Qwen2_5_1_5B_apeach.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/reward_models/test_ascend_Qwen2_5_1_5B_apeach.py", line 6, in <module>
    from sglang.test.runners import SRTRunner
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/runners.py", line 68, in <module>
    with open(os.path.join(dirpath, "long_prompt.txt"), "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/long_prompt.txt'
[ERROR] 2026-01-17-11:14:57 (PID:304422, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (41/62):
filename='ascend/reward_models/test_ascend_Qwen2_5_1_5B_apeach.py', elapsed=11, estimated_time=400
.
.


[CI Retry] ascend/reward_models/test_ascend_Qwen2_5_1_5B_apeach.py failed with non-retriable error: FileNotFoundError - not retrying


✗ FAILED: ascend/reward_models/test_ascend_Qwen2_5_1_5B_apeach.py returned exit code 1

.
.
Begin (42/62):
python3 /data/c30044170/code/newHDK/ascend/reward_models/test_ascend_qwen2_5_math_rm_72b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/reward_models/test_ascend_qwen2_5_math_rm_72b.py", line 6, in <module>
    from sglang.test.runners import SRTRunner
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/runners.py", line 68, in <module>
    with open(os.path.join(dirpath, "long_prompt.txt"), "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/long_prompt.txt'
[ERROR] 2026-01-17-11:15:08 (PID:304686, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (42/62):
filename='ascend/reward_models/test_ascend_qwen2_5_math_rm_72b.py', elapsed=11, estimated_time=400
.
.


[CI Retry] ascend/reward_models/test_ascend_qwen2_5_math_rm_72b.py failed with non-retriable error: FileNotFoundError - not retrying


✗ FAILED: ascend/reward_models/test_ascend_qwen2_5_math_rm_72b.py returned exit code 1

.
.
Begin (43/62):
python3 /data/c30044170/code/newHDK/ascend/reward_models/test_ascend_reward_internlm2_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/reward_models/test_ascend_reward_internlm2_7b.py", line 6, in <module>
    from sglang.test.runners import SRTRunner
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/runners.py", line 68, in <module>
    with open(os.path.join(dirpath, "long_prompt.txt"), "r") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/long_prompt.txt'
[ERROR] 2026-01-17-11:15:19 (PID:304950, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (43/62):
filename='ascend/reward_models/test_ascend_reward_internlm2_7b.py', elapsed=11, estimated_time=400
.
.


[CI Retry] ascend/reward_models/test_ascend_reward_internlm2_7b.py failed with non-retriable error: FileNotFoundError - not retrying


✗ FAILED: ascend/reward_models/test_ascend_reward_internlm2_7b.py returned exit code 1

.
.
Begin (44/62):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_gemma_3_4b_it.py
.
.

