2026-01-05 10:39:50,920 - INFO - Starting Ascend test suite: all
Starting Ascend test suite: all
2026-01-05 10:39:50,920 - INFO - Command args: Namespace(timeout_per_file=3600, suite='all', auto_partition_id=None, auto_partition_size=None, continue_on_error=True, enable_retry=True, max_attempts=2, retry_wait_seconds=60, log_dir='/home/c30044170/newHDK/log/')
Command args: Namespace(timeout_per_file=3600, suite='all', auto_partition_id=None, auto_partition_size=None, continue_on_error=True, enable_retry=True, max_attempts=2, retry_wait_seconds=60, log_dir='/home/c30044170/newHDK/log/')
2026-01-05 10:39:50,920 - INFO - Log directory: /home/c30044170/newHDK/log/all/20260105_103950
Log directory: /home/c30044170/newHDK/log/all/20260105_103950
2026-01-05 10:39:51,235 - INFO - ✅ Suite sanity check passed
✅ Suite sanity check passed
2026-01-05 10:39:51,330 - INFO - Total test files: 44
Total test files: 44
.
.
Begin (0/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_llm_models_grok.py
.
.

usage: launch_server.py [-h] --model-path MODEL_PATH
                        [--tokenizer-path TOKENIZER_PATH]
                        [--tokenizer-mode {auto,slow}]
                        [--tokenizer-worker-num TOKENIZER_WORKER_NUM]
                        [--skip-tokenizer-init]
                        [--load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,flash_rl,remote,remote_instance,private}]
                        [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                        [--trust-remote-code]
                        [--context-length CONTEXT_LENGTH] [--is-embedding]
                        [--enable-multimodal]
                        [--limit-mm-data-per-request LIMIT_MM_DATA_PER_REQUEST]
                        [--revision REVISION] [--model-impl MODEL_IMPL]
                        [--host HOST] [--port PORT]
                        [--fastapi-root-path FASTAPI_ROOT_PATH] [--grpc-mode]
                        [--skip-server-warmup] [--warmups WARMUPS]
                        [--nccl-port NCCL_PORT]
                        [--checkpoint-engine-wait-weights-before-ready]
                        [--dtype {auto,half,float16,bfloat16,float,float32}]
                        [--quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp8,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4,auto-round,compressed-tensors,modelslim}]
                        [--quantization-param-path QUANTIZATION_PARAM_PATH]
                        [--kv-cache-dtype {auto,fp8_e5m2,fp8_e4m3,bf16,bfloat16,fp4_e2m1}]
                        [--enable-fp32-lm-head]
                        [--modelopt-quant MODELOPT_QUANT]
                        [--modelopt-checkpoint-restore-path MODELOPT_CHECKPOINT_RESTORE_PATH]
                        [--modelopt-checkpoint-save-path MODELOPT_CHECKPOINT_SAVE_PATH]
                        [--modelopt-export-path MODELOPT_EXPORT_PATH]
                        [--quantize-and-serve]
                        [--rl-quant-profile RL_QUANT_PROFILE]
                        [--mem-fraction-static MEM_FRACTION_STATIC]
                        [--max-running-requests MAX_RUNNING_REQUESTS]
                        [--max-queued-requests MAX_QUEUED_REQUESTS]
                        [--max-total-tokens MAX_TOTAL_TOKENS]
                        [--chunked-prefill-size CHUNKED_PREFILL_SIZE]
                        [--prefill-max-requests PREFILL_MAX_REQUESTS]
                        [--enable-dynamic-chunking]
                        [--max-prefill-tokens MAX_PREFILL_TOKENS]
                        [--schedule-policy {lpm,random,fcfs,dfs-weight,lof,priority}]
                        [--enable-priority-scheduling]
                        [--abort-on-priority-when-disabled]
                        [--schedule-low-priority-values-first]
                        [--priority-scheduling-preemption-threshold PRIORITY_SCHEDULING_PREEMPTION_THRESHOLD]
                        [--schedule-conservativeness SCHEDULE_CONSERVATIVENESS]
                        [--page-size PAGE_SIZE]
                        [--hybrid-kvcache-ratio [HYBRID_KVCACHE_RATIO]]
                        [--swa-full-tokens-ratio SWA_FULL_TOKENS_RATIO]
                        [--disable-hybrid-swa-memory]
                        [--radix-eviction-policy {lru,lfu}] [--device DEVICE]
                        [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
                        [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]
                        [--pp-max-micro-batch-size PP_MAX_MICRO_BATCH_SIZE]
                        [--pp-async-batch-depth PP_ASYNC_BATCH_DEPTH]
                        [--stream-interval STREAM_INTERVAL] [--stream-output]
                        [--random-seed RANDOM_SEED]
                        [--constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN]
                        [--constrained-json-disable-any-whitespace]
                        [--watchdog-timeout WATCHDOG_TIMEOUT]
                        [--soft-watchdog-timeout SOFT_WATCHDOG_TIMEOUT]
                        [--dist-timeout DIST_TIMEOUT]
                        [--download-dir DOWNLOAD_DIR]
                        [--base-gpu-id BASE_GPU_ID]
                        [--gpu-id-step GPU_ID_STEP] [--sleep-on-idle]
                        [--custom-sigquit-handler CUSTOM_SIGQUIT_HANDLER]
                        [--log-level LOG_LEVEL]
                        [--log-level-http LOG_LEVEL_HTTP] [--log-requests]
                        [--log-requests-level {0,1,2,3}]
                        [--crash-dump-folder CRASH_DUMP_FOLDER]
                        [--show-time-cost] [--enable-metrics]
                        [--enable-metrics-for-all-schedulers]
                        [--tokenizer-metrics-custom-labels-header TOKENIZER_METRICS_CUSTOM_LABELS_HEADER]
                        [--tokenizer-metrics-allowed-custom-labels TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS [TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS ...]]
                        [--bucket-time-to-first-token BUCKET_TIME_TO_FIRST_TOKEN [BUCKET_TIME_TO_FIRST_TOKEN ...]]
                        [--bucket-inter-token-latency BUCKET_INTER_TOKEN_LATENCY [BUCKET_INTER_TOKEN_LATENCY ...]]
                        [--bucket-e2e-request-latency BUCKET_E2E_REQUEST_LATENCY [BUCKET_E2E_REQUEST_LATENCY ...]]
                        [--collect-tokens-histogram]
                        [--prompt-tokens-buckets PROMPT_TOKENS_BUCKETS [PROMPT_TOKENS_BUCKETS ...]]
                        [--generation-tokens-buckets GENERATION_TOKENS_BUCKETS [GENERATION_TOKENS_BUCKETS ...]]
                        [--gc-warning-threshold-secs GC_WARNING_THRESHOLD_SECS]
                        [--decode-log-interval DECODE_LOG_INTERVAL]
                        [--enable-request-time-stats-logging]
                        [--kv-events-config KV_EVENTS_CONFIG] [--enable-trace]
                        [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]
                        [--export-metrics-to-file]
                        [--export-metrics-to-file-dir EXPORT_METRICS_TO_FILE_DIR]
                        [--api-key API_KEY]
                        [--served-model-name SERVED_MODEL_NAME]
                        [--weight-version WEIGHT_VERSION]
                        [--chat-template CHAT_TEMPLATE]
                        [--completion-template COMPLETION_TEMPLATE]
                        [--file-storage-path FILE_STORAGE_PATH]
                        [--enable-cache-report]
                        [--reasoning-parser {deepseek-r1,deepseek-v3,glm45,gpt-oss,kimi,kimi_k2,qwen3,qwen3-thinking,minimax,minimax-append-think,step3,nano_v3,interns1}]
                        [--tool-call-parser {deepseekv3,deepseekv31,deepseekv32,glm,glm45,glm47,gpt-oss,kimi_k2,llama3,mimo,mistral,pythonic,qwen,qwen25,qwen3_coder,step3,minimax-m2,interns1}]
                        [--tool-server TOOL_SERVER]
                        [--sampling-defaults {openai,model}]
                        [--data-parallel-size DATA_PARALLEL_SIZE]
                        [--load-balance-method {round_robin,decode_round_robin,shortest_queue,minimum_tokens}]
                        [--load-watch-interval LOAD_WATCH_INTERVAL]
                        [--prefill-round-robin-balance]
                        [--dist-init-addr DIST_INIT_ADDR] [--nnodes NNODES]
                        [--node-rank NODE_RANK]
                        [--json-model-override-args JSON_MODEL_OVERRIDE_ARGS]
                        [--preferred-sampling-params PREFERRED_SAMPLING_PARAMS]
                        [--enable-lora] [--max-lora-rank MAX_LORA_RANK]
                        [--lora-target-modules [{q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj,qkv_proj,gate_up_proj,embed_tokens,lm_head,all} ...]]
                        [--lora-paths [LORA_PATHS ...]]
                        [--max-loras-per-batch MAX_LORAS_PER_BATCH]
                        [--max-loaded-loras MAX_LOADED_LORAS]
                        [--lora-eviction-policy {lru,fifo}]
                        [--lora-backend {triton,csgmv,ascend,torch_native}]
                        [--max-lora-chunk-size {16,32,64,128}]
                        [--attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend,intel_xpu}]
                        [--prefill-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend,intel_xpu}]
                        [--decode-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend,intel_xpu}]
                        [--sampling-backend {flashinfer,pytorch,ascend}]
                        [--grammar-backend {xgrammar,outlines,llguidance,none}]
                        [--mm-attention-backend {sdpa,fa3,triton_attn,ascend_attn,aiter_attn}]
                        [--nsa-prefill-backend {flashmla_sparse,flashmla_kv,flashmla_auto,fa3,tilelang,aiter}]
                        [--nsa-decode-backend {flashmla_sparse,flashmla_kv,flashmla_auto,fa3,tilelang,aiter}]
                        [--fp8-gemm-backend {auto,deep_gemm,flashinfer_trtllm,cutlass,triton,aiter}]
                        [--disable-flashinfer-autotune]
                        [--speculative-algorithm {EAGLE,EAGLE3,NEXTN,STANDALONE,NGRAM,SUFFIX}]
                        [--speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH]
                        [--speculative-draft-model-revision SPECULATIVE_DRAFT_MODEL_REVISION]
                        [--speculative-draft-load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,flash_rl,remote,remote_instance,private}]
                        [--speculative-num-steps SPECULATIVE_NUM_STEPS]
                        [--speculative-eagle-topk SPECULATIVE_EAGLE_TOPK]
                        [--speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS]
                        [--speculative-accept-threshold-single SPECULATIVE_ACCEPT_THRESHOLD_SINGLE]
                        [--speculative-accept-threshold-acc SPECULATIVE_ACCEPT_THRESHOLD_ACC]
                        [--speculative-token-map SPECULATIVE_TOKEN_MAP]
                        [--speculative-attention-mode {prefill,decode}]
                        [--speculative-draft-attention-backend SPECULATIVE_DRAFT_ATTENTION_BACKEND]
                        [--speculative-moe-runner-backend {auto,deep_gemm,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4,flashinfer_cutedsl,cutlass}]
                        [--speculative-moe-a2a-backend {none,deepep,mooncake,ascend_fuseep}]
                        [--speculative-draft-model-quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp8,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4,auto-round,compressed-tensors,modelslim,unquant}]
                        [--speculative-ngram-min-match-window-size SPECULATIVE_NGRAM_MIN_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-max-match-window-size SPECULATIVE_NGRAM_MAX_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-min-bfs-breadth SPECULATIVE_NGRAM_MIN_BFS_BREADTH]
                        [--speculative-ngram-max-bfs-breadth SPECULATIVE_NGRAM_MAX_BFS_BREADTH]
                        [--speculative-ngram-match-type {BFS,PROB}]
                        [--speculative-ngram-branch-length SPECULATIVE_NGRAM_BRANCH_LENGTH]
                        [--speculative-ngram-capacity SPECULATIVE_NGRAM_CAPACITY]
                        [--speculative-suffix-max-tree-depth SPECULATIVE_SUFFIX_MAX_TREE_DEPTH]
                        [--speculative-suffix-max-cached-requests SPECULATIVE_SUFFIX_MAX_CACHED_REQUESTS]
                        [--speculative-suffix-max-spec-factor SPECULATIVE_SUFFIX_MAX_SPEC_FACTOR]
                        [--speculative-suffix-min-token-prob SPECULATIVE_SUFFIX_MIN_TOKEN_PROB]
                        [--enable-mtp]
                        [--expert-parallel-size EXPERT_PARALLEL_SIZE]
                        [--moe-a2a-backend {none,deepep,mooncake,ascend_fuseep}]
                        [--moe-runner-backend {auto,deep_gemm,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4,flashinfer_cutedsl,cutlass}]
                        [--flashinfer-mxfp4-moe-precision {default,bf16}]
                        [--enable-flashinfer-allreduce-fusion]
                        [--deepep-mode {normal,low_latency,auto}]
                        [--ep-num-redundant-experts EP_NUM_REDUNDANT_EXPERTS]
                        [--ep-dispatch-algorithm EP_DISPATCH_ALGORITHM]
                        [--init-expert-location INIT_EXPERT_LOCATION]
                        [--enable-eplb] [--enable-async-eplb]
                        [--eplb-algorithm EPLB_ALGORITHM]
                        [--eplb-rebalance-num-iterations EPLB_REBALANCE_NUM_ITERATIONS]
                        [--eplb-rebalance-layers-per-chunk EPLB_REBALANCE_LAYERS_PER_CHUNK]
                        [--eplb-min-rebalancing-utilization-threshold EPLB_MIN_REBALANCING_UTILIZATION_THRESHOLD]
                        [--expert-distribution-recorder-mode EXPERT_DISTRIBUTION_RECORDER_MODE]
                        [--expert-distribution-recorder-buffer-size EXPERT_DISTRIBUTION_RECORDER_BUFFER_SIZE]
                        [--enable-expert-distribution-metrics]
                        [--deepep-config DEEPEP_CONFIG]
                        [--moe-dense-tp-size MOE_DENSE_TP_SIZE]
                        [--elastic-ep-backend {none,mooncake}]
                        [--mooncake-ib-device MOONCAKE_IB_DEVICE]
                        [--max-mamba-cache-size MAX_MAMBA_CACHE_SIZE]
                        [--mamba-ssm-dtype {float32,bfloat16}]
                        [--mamba-full-memory-ratio MAMBA_FULL_MEMORY_RATIO]
                        [--mamba-scheduler-strategy {auto,no_buffer,extra_buffer}]
                        [--mamba-track-interval MAMBA_TRACK_INTERVAL]
                        [--enable-hierarchical-cache]
                        [--hicache-ratio HICACHE_RATIO]
                        [--hicache-size HICACHE_SIZE]
                        [--hicache-write-policy {write_back,write_through,write_through_selective}]
                        [--hicache-io-backend {direct,kernel,kernel_ascend}]
                        [--hicache-mem-layout {layer_first,page_first,page_first_direct,page_first_kv_split,page_head}]
                        [--hicache-storage-backend {file,mooncake,hf3fs,nixl,aibrix,dynamic,eic}]
                        [--hicache-storage-prefetch-policy {best_effort,wait_complete,timeout}]
                        [--hicache-storage-backend-extra-config HICACHE_STORAGE_BACKEND_EXTRA_CONFIG]
                        [--enable-lmcache] [--kt-weight-path KT_WEIGHT_PATH]
                        [--kt-method KT_METHOD] [--kt-cpuinfer KT_CPUINFER]
                        [--kt-threadpool-count KT_THREADPOOL_COUNT]
                        [--kt-num-gpu-experts KT_NUM_GPU_EXPERTS]
                        [--kt-max-deferred-experts-per-token KT_MAX_DEFERRED_EXPERTS_PER_TOKEN]
                        [--dllm-algorithm DLLM_ALGORITHM]
                        [--dllm-algorithm-config DLLM_ALGORITHM_CONFIG]
                        [--enable-double-sparsity]
                        [--ds-channel-config-path DS_CHANNEL_CONFIG_PATH]
                        [--ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM]
                        [--ds-heavy-token-num DS_HEAVY_TOKEN_NUM]
                        [--ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE]
                        [--ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD]
                        [--cpu-offload-gb CPU_OFFLOAD_GB]
                        [--offload-group-size OFFLOAD_GROUP_SIZE]
                        [--offload-num-in-group OFFLOAD_NUM_IN_GROUP]
                        [--offload-prefetch-step OFFLOAD_PREFETCH_STEP]
                        [--offload-mode OFFLOAD_MODE]
                        [--multi-item-scoring-delimiter MULTI_ITEM_SCORING_DELIMITER]
                        [--disable-radix-cache]
                        [--cuda-graph-max-bs CUDA_GRAPH_MAX_BS]
                        [--cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]]
                        [--disable-cuda-graph] [--disable-cuda-graph-padding]
                        [--enable-profile-cuda-graph] [--enable-cudagraph-gc]
                        [--enable-layerwise-nvtx-marker] [--enable-nccl-nvls]
                        [--enable-symm-mem]
                        [--disable-flashinfer-cutlass-moe-fp4-allgather]
                        [--enable-tokenizer-batch-encode]
                        [--disable-tokenizer-batch-decode]
                        [--disable-outlines-disk-cache]
                        [--disable-custom-all-reduce] [--enable-mscclpp]
                        [--enable-torch-symm-mem] [--disable-overlap-schedule]
                        [--enable-mixed-chunk] [--enable-dp-attention]
                        [--enable-dp-lm-head] [--enable-two-batch-overlap]
                        [--enable-single-batch-overlap]
                        [--tbo-token-distribution-threshold TBO_TOKEN_DISTRIBUTION_THRESHOLD]
                        [--enable-torch-compile]
                        [--enable-torch-compile-debug-mode]
                        [--enable-piecewise-cuda-graph]
                        [--piecewise-cuda-graph-tokens PIECEWISE_CUDA_GRAPH_TOKENS]
                        [--piecewise-cuda-graph-compiler {eager,inductor}]
                        [--torch-compile-max-bs TORCH_COMPILE_MAX_BS]
                        [--piecewise-cuda-graph-max-tokens PIECEWISE_CUDA_GRAPH_MAX_TOKENS]
                        [--torchao-config TORCHAO_CONFIG]
                        [--enable-nan-detection] [--enable-p2p-check]
                        [--triton-attention-reduce-in-fp32]
                        [--triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS]
                        [--triton-attention-split-tile-size TRITON_ATTENTION_SPLIT_TILE_SIZE]
                        [--num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS]
                        [--delete-ckpt-after-loading] [--enable-memory-saver]
                        [--enable-weights-cpu-backup]
                        [--enable-draft-weights-cpu-backup]
                        [--allow-auto-truncate]
                        [--enable-custom-logit-processor]
                        [--flashinfer-mla-disable-ragged]
                        [--disable-shared-experts-fusion]
                        [--disable-chunked-prefix-cache]
                        [--disable-fast-image-processor]
                        [--keep-mm-feature-on-device]
                        [--enable-return-hidden-states]
                        [--enable-return-routed-experts]
                        [--scheduler-recv-interval SCHEDULER_RECV_INTERVAL]
                        [--numa-node NUMA_NODE [NUMA_NODE ...]]
                        [--enable-deterministic-inference]
                        [--rl-on-policy-target {fsdp}]
                        [--enable-attn-tp-input-scattered]
                        [--enable-nsa-prefill-context-parallel]
                        [--enable-fused-qk-norm-rope]
                        [--enable-dynamic-batch-tokenizer]
                        [--dynamic-batch-tokenizer-batch-size DYNAMIC_BATCH_TOKENIZER_BATCH_SIZE]
                        [--dynamic-batch-tokenizer-batch-timeout DYNAMIC_BATCH_TOKENIZER_BATCH_TIMEOUT]
                        [--debug-tensor-dump-output-folder DEBUG_TENSOR_DUMP_OUTPUT_FOLDER]
                        [--debug-tensor-dump-layers DEBUG_TENSOR_DUMP_LAYERS [DEBUG_TENSOR_DUMP_LAYERS ...]]
                        [--debug-tensor-dump-input-file DEBUG_TENSOR_DUMP_INPUT_FILE]
                        [--debug-tensor-dump-inject DEBUG_TENSOR_DUMP_INJECT]
                        [--disaggregation-mode {null,prefill,decode}]
                        [--disaggregation-transfer-backend {mooncake,nixl,ascend,fake}]
                        [--disaggregation-bootstrap-port DISAGGREGATION_BOOTSTRAP_PORT]
                        [--disaggregation-decode-tp DISAGGREGATION_DECODE_TP]
                        [--disaggregation-decode-dp DISAGGREGATION_DECODE_DP]
                        [--disaggregation-prefill-pp DISAGGREGATION_PREFILL_PP]
                        [--disaggregation-ib-device DISAGGREGATION_IB_DEVICE]
                        [--disaggregation-decode-enable-offload-kvcache]
                        [--num-reserved-decode-tokens NUM_RESERVED_DECODE_TOKENS]
                        [--disaggregation-decode-polling-interval DISAGGREGATION_DECODE_POLLING_INTERVAL]
                        [--encoder-only] [--language-only]
                        [--encoder-transfer-backend {zmq_to_scheduler,zmq_to_tokenizer,mooncake}]
                        [--encoder-urls ENCODER_URLS [ENCODER_URLS ...]]
                        [--custom-weight-loader [CUSTOM_WEIGHT_LOADER ...]]
                        [--weight-loader-disable-mmap]
                        [--remote-instance-weight-loader-seed-instance-ip REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_IP]
                        [--remote-instance-weight-loader-seed-instance-service-port REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_SERVICE_PORT]
                        [--remote-instance-weight-loader-send-weights-group-ports REMOTE_INSTANCE_WEIGHT_LOADER_SEND_WEIGHTS_GROUP_PORTS]
                        [--remote-instance-weight-loader-backend {transfer_engine,nccl}]
                        [--remote-instance-weight-loader-start-seed-via-transfer-engine]
                        [--enable-pdmux]
                        [--pdmux-config-path PDMUX_CONFIG_PATH]
                        [--sm-group-num SM_GROUP_NUM] [--config CONFIG]
                        [--mm-max-concurrent-calls MM_MAX_CONCURRENT_CALLS]
                        [--mm-per-request-timeout MM_PER_REQUEST_TIMEOUT]
                        [--enable-broadcast-mm-inputs-process]
                        [--mm-process-config MM_PROCESS_CONFIG]
                        [--mm-enable-dp-encoder]
                        [--decrypted-config-file DECRYPTED_CONFIG_FILE]
                        [--decrypted-draft-config-file DECRYPTED_DRAFT_CONFIG_FILE]
                        [--enable-prefix-mm-cache]
                        [--forward-hooks FORWARD_HOOKS]
launch_server.py: error: argument --tokenizer-path: expected one argument
E
======================================================================
ERROR: setUpClass (__main__.TestGrok)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/llm_models/test_ascend_llm_models_grok.py", line 56, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 2. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 20.010s

FAILED (errors=1)
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/huihui-ai/grok-2 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-radix-cache --disable-cuda-graph --tokenizer-path --base-gpu-id 10 /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/huihui-ai/grok-2/tokenizer.tok.json --tp-size 16 --device npu --host 127.0.0.1 --port 21000
.
.
End (0/43):
filename='ascend/llm_models/test_ascend_llm_models_grok.py', elapsed=30, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_ascend_llm_models_grok.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/llm_models/test_ascend_llm_models_grok.py

.
.
Begin (0/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_llm_models_grok.py
.
.

usage: launch_server.py [-h] --model-path MODEL_PATH
                        [--tokenizer-path TOKENIZER_PATH]
                        [--tokenizer-mode {auto,slow}]
                        [--tokenizer-worker-num TOKENIZER_WORKER_NUM]
                        [--skip-tokenizer-init]
                        [--load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,flash_rl,remote,remote_instance,private}]
                        [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                        [--trust-remote-code]
                        [--context-length CONTEXT_LENGTH] [--is-embedding]
                        [--enable-multimodal]
                        [--limit-mm-data-per-request LIMIT_MM_DATA_PER_REQUEST]
                        [--revision REVISION] [--model-impl MODEL_IMPL]
                        [--host HOST] [--port PORT]
                        [--fastapi-root-path FASTAPI_ROOT_PATH] [--grpc-mode]
                        [--skip-server-warmup] [--warmups WARMUPS]
                        [--nccl-port NCCL_PORT]
                        [--checkpoint-engine-wait-weights-before-ready]
                        [--dtype {auto,half,float16,bfloat16,float,float32}]
                        [--quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp8,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4,auto-round,compressed-tensors,modelslim}]
                        [--quantization-param-path QUANTIZATION_PARAM_PATH]
                        [--kv-cache-dtype {auto,fp8_e5m2,fp8_e4m3,bf16,bfloat16,fp4_e2m1}]
                        [--enable-fp32-lm-head]
                        [--modelopt-quant MODELOPT_QUANT]
                        [--modelopt-checkpoint-restore-path MODELOPT_CHECKPOINT_RESTORE_PATH]
                        [--modelopt-checkpoint-save-path MODELOPT_CHECKPOINT_SAVE_PATH]
                        [--modelopt-export-path MODELOPT_EXPORT_PATH]
                        [--quantize-and-serve]
                        [--rl-quant-profile RL_QUANT_PROFILE]
                        [--mem-fraction-static MEM_FRACTION_STATIC]
                        [--max-running-requests MAX_RUNNING_REQUESTS]
                        [--max-queued-requests MAX_QUEUED_REQUESTS]
                        [--max-total-tokens MAX_TOTAL_TOKENS]
                        [--chunked-prefill-size CHUNKED_PREFILL_SIZE]
                        [--prefill-max-requests PREFILL_MAX_REQUESTS]
                        [--enable-dynamic-chunking]
                        [--max-prefill-tokens MAX_PREFILL_TOKENS]
                        [--schedule-policy {lpm,random,fcfs,dfs-weight,lof,priority}]
                        [--enable-priority-scheduling]
                        [--abort-on-priority-when-disabled]
                        [--schedule-low-priority-values-first]
                        [--priority-scheduling-preemption-threshold PRIORITY_SCHEDULING_PREEMPTION_THRESHOLD]
                        [--schedule-conservativeness SCHEDULE_CONSERVATIVENESS]
                        [--page-size PAGE_SIZE]
                        [--hybrid-kvcache-ratio [HYBRID_KVCACHE_RATIO]]
                        [--swa-full-tokens-ratio SWA_FULL_TOKENS_RATIO]
                        [--disable-hybrid-swa-memory]
                        [--radix-eviction-policy {lru,lfu}] [--device DEVICE]
                        [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
                        [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]
                        [--pp-max-micro-batch-size PP_MAX_MICRO_BATCH_SIZE]
                        [--pp-async-batch-depth PP_ASYNC_BATCH_DEPTH]
                        [--stream-interval STREAM_INTERVAL] [--stream-output]
                        [--random-seed RANDOM_SEED]
                        [--constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN]
                        [--constrained-json-disable-any-whitespace]
                        [--watchdog-timeout WATCHDOG_TIMEOUT]
                        [--soft-watchdog-timeout SOFT_WATCHDOG_TIMEOUT]
                        [--dist-timeout DIST_TIMEOUT]
                        [--download-dir DOWNLOAD_DIR]
                        [--base-gpu-id BASE_GPU_ID]
                        [--gpu-id-step GPU_ID_STEP] [--sleep-on-idle]
                        [--custom-sigquit-handler CUSTOM_SIGQUIT_HANDLER]
                        [--log-level LOG_LEVEL]
                        [--log-level-http LOG_LEVEL_HTTP] [--log-requests]
                        [--log-requests-level {0,1,2,3}]
                        [--crash-dump-folder CRASH_DUMP_FOLDER]
                        [--show-time-cost] [--enable-metrics]
                        [--enable-metrics-for-all-schedulers]
                        [--tokenizer-metrics-custom-labels-header TOKENIZER_METRICS_CUSTOM_LABELS_HEADER]
                        [--tokenizer-metrics-allowed-custom-labels TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS [TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS ...]]
                        [--bucket-time-to-first-token BUCKET_TIME_TO_FIRST_TOKEN [BUCKET_TIME_TO_FIRST_TOKEN ...]]
                        [--bucket-inter-token-latency BUCKET_INTER_TOKEN_LATENCY [BUCKET_INTER_TOKEN_LATENCY ...]]
                        [--bucket-e2e-request-latency BUCKET_E2E_REQUEST_LATENCY [BUCKET_E2E_REQUEST_LATENCY ...]]
                        [--collect-tokens-histogram]
                        [--prompt-tokens-buckets PROMPT_TOKENS_BUCKETS [PROMPT_TOKENS_BUCKETS ...]]
                        [--generation-tokens-buckets GENERATION_TOKENS_BUCKETS [GENERATION_TOKENS_BUCKETS ...]]
                        [--gc-warning-threshold-secs GC_WARNING_THRESHOLD_SECS]
                        [--decode-log-interval DECODE_LOG_INTERVAL]
                        [--enable-request-time-stats-logging]
                        [--kv-events-config KV_EVENTS_CONFIG] [--enable-trace]
                        [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]
                        [--export-metrics-to-file]
                        [--export-metrics-to-file-dir EXPORT_METRICS_TO_FILE_DIR]
                        [--api-key API_KEY]
                        [--served-model-name SERVED_MODEL_NAME]
                        [--weight-version WEIGHT_VERSION]
                        [--chat-template CHAT_TEMPLATE]
                        [--completion-template COMPLETION_TEMPLATE]
                        [--file-storage-path FILE_STORAGE_PATH]
                        [--enable-cache-report]
                        [--reasoning-parser {deepseek-r1,deepseek-v3,glm45,gpt-oss,kimi,kimi_k2,qwen3,qwen3-thinking,minimax,minimax-append-think,step3,nano_v3,interns1}]
                        [--tool-call-parser {deepseekv3,deepseekv31,deepseekv32,glm,glm45,glm47,gpt-oss,kimi_k2,llama3,mimo,mistral,pythonic,qwen,qwen25,qwen3_coder,step3,minimax-m2,interns1}]
                        [--tool-server TOOL_SERVER]
                        [--sampling-defaults {openai,model}]
                        [--data-parallel-size DATA_PARALLEL_SIZE]
                        [--load-balance-method {round_robin,decode_round_robin,shortest_queue,minimum_tokens}]
                        [--load-watch-interval LOAD_WATCH_INTERVAL]
                        [--prefill-round-robin-balance]
                        [--dist-init-addr DIST_INIT_ADDR] [--nnodes NNODES]
                        [--node-rank NODE_RANK]
                        [--json-model-override-args JSON_MODEL_OVERRIDE_ARGS]
                        [--preferred-sampling-params PREFERRED_SAMPLING_PARAMS]
                        [--enable-lora] [--max-lora-rank MAX_LORA_RANK]
                        [--lora-target-modules [{q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj,qkv_proj,gate_up_proj,embed_tokens,lm_head,all} ...]]
                        [--lora-paths [LORA_PATHS ...]]
                        [--max-loras-per-batch MAX_LORAS_PER_BATCH]
                        [--max-loaded-loras MAX_LOADED_LORAS]
                        [--lora-eviction-policy {lru,fifo}]
                        [--lora-backend {triton,csgmv,ascend,torch_native}]
                        [--max-lora-chunk-size {16,32,64,128}]
                        [--attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend,intel_xpu}]
                        [--prefill-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend,intel_xpu}]
                        [--decode-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend,intel_xpu}]
                        [--sampling-backend {ascend,flashinfer,pytorch}]
                        [--grammar-backend {xgrammar,outlines,llguidance,none}]
                        [--mm-attention-backend {sdpa,fa3,triton_attn,ascend_attn,aiter_attn}]
                        [--nsa-prefill-backend {flashmla_sparse,flashmla_kv,flashmla_auto,fa3,tilelang,aiter}]
                        [--nsa-decode-backend {flashmla_sparse,flashmla_kv,flashmla_auto,fa3,tilelang,aiter}]
                        [--fp8-gemm-backend {auto,deep_gemm,flashinfer_trtllm,cutlass,triton,aiter}]
                        [--disable-flashinfer-autotune]
                        [--speculative-algorithm {EAGLE,EAGLE3,NEXTN,STANDALONE,NGRAM,SUFFIX}]
                        [--speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH]
                        [--speculative-draft-model-revision SPECULATIVE_DRAFT_MODEL_REVISION]
                        [--speculative-draft-load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,flash_rl,remote,remote_instance,private}]
                        [--speculative-num-steps SPECULATIVE_NUM_STEPS]
                        [--speculative-eagle-topk SPECULATIVE_EAGLE_TOPK]
                        [--speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS]
                        [--speculative-accept-threshold-single SPECULATIVE_ACCEPT_THRESHOLD_SINGLE]
                        [--speculative-accept-threshold-acc SPECULATIVE_ACCEPT_THRESHOLD_ACC]
                        [--speculative-token-map SPECULATIVE_TOKEN_MAP]
                        [--speculative-attention-mode {prefill,decode}]
                        [--speculative-draft-attention-backend SPECULATIVE_DRAFT_ATTENTION_BACKEND]
                        [--speculative-moe-runner-backend {auto,deep_gemm,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4,flashinfer_cutedsl,cutlass}]
                        [--speculative-moe-a2a-backend {none,deepep,mooncake,ascend_fuseep}]
                        [--speculative-draft-model-quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp8,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4,auto-round,compressed-tensors,modelslim,unquant}]
                        [--speculative-ngram-min-match-window-size SPECULATIVE_NGRAM_MIN_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-max-match-window-size SPECULATIVE_NGRAM_MAX_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-min-bfs-breadth SPECULATIVE_NGRAM_MIN_BFS_BREADTH]
                        [--speculative-ngram-max-bfs-breadth SPECULATIVE_NGRAM_MAX_BFS_BREADTH]
                        [--speculative-ngram-match-type {BFS,PROB}]
                        [--speculative-ngram-branch-length SPECULATIVE_NGRAM_BRANCH_LENGTH]
                        [--speculative-ngram-capacity SPECULATIVE_NGRAM_CAPACITY]
                        [--speculative-suffix-max-tree-depth SPECULATIVE_SUFFIX_MAX_TREE_DEPTH]
                        [--speculative-suffix-max-cached-requests SPECULATIVE_SUFFIX_MAX_CACHED_REQUESTS]
                        [--speculative-suffix-max-spec-factor SPECULATIVE_SUFFIX_MAX_SPEC_FACTOR]
                        [--speculative-suffix-min-token-prob SPECULATIVE_SUFFIX_MIN_TOKEN_PROB]
                        [--enable-mtp]
                        [--expert-parallel-size EXPERT_PARALLEL_SIZE]
                        [--moe-a2a-backend {none,deepep,mooncake,ascend_fuseep}]
                        [--moe-runner-backend {auto,deep_gemm,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4,flashinfer_cutedsl,cutlass}]
                        [--flashinfer-mxfp4-moe-precision {default,bf16}]
                        [--enable-flashinfer-allreduce-fusion]
                        [--deepep-mode {normal,low_latency,auto}]
                        [--ep-num-redundant-experts EP_NUM_REDUNDANT_EXPERTS]
                        [--ep-dispatch-algorithm EP_DISPATCH_ALGORITHM]
                        [--init-expert-location INIT_EXPERT_LOCATION]
                        [--enable-eplb] [--enable-async-eplb]
                        [--eplb-algorithm EPLB_ALGORITHM]
                        [--eplb-rebalance-num-iterations EPLB_REBALANCE_NUM_ITERATIONS]
                        [--eplb-rebalance-layers-per-chunk EPLB_REBALANCE_LAYERS_PER_CHUNK]
                        [--eplb-min-rebalancing-utilization-threshold EPLB_MIN_REBALANCING_UTILIZATION_THRESHOLD]
                        [--expert-distribution-recorder-mode EXPERT_DISTRIBUTION_RECORDER_MODE]
                        [--expert-distribution-recorder-buffer-size EXPERT_DISTRIBUTION_RECORDER_BUFFER_SIZE]
                        [--enable-expert-distribution-metrics]
                        [--deepep-config DEEPEP_CONFIG]
                        [--moe-dense-tp-size MOE_DENSE_TP_SIZE]
                        [--elastic-ep-backend {none,mooncake}]
                        [--mooncake-ib-device MOONCAKE_IB_DEVICE]
                        [--max-mamba-cache-size MAX_MAMBA_CACHE_SIZE]
                        [--mamba-ssm-dtype {float32,bfloat16}]
                        [--mamba-full-memory-ratio MAMBA_FULL_MEMORY_RATIO]
                        [--mamba-scheduler-strategy {auto,no_buffer,extra_buffer}]
                        [--mamba-track-interval MAMBA_TRACK_INTERVAL]
                        [--enable-hierarchical-cache]
                        [--hicache-ratio HICACHE_RATIO]
                        [--hicache-size HICACHE_SIZE]
                        [--hicache-write-policy {write_back,write_through,write_through_selective}]
                        [--hicache-io-backend {direct,kernel,kernel_ascend}]
                        [--hicache-mem-layout {layer_first,page_first,page_first_direct,page_first_kv_split,page_head}]
                        [--hicache-storage-backend {file,mooncake,hf3fs,nixl,aibrix,dynamic,eic}]
                        [--hicache-storage-prefetch-policy {best_effort,wait_complete,timeout}]
                        [--hicache-storage-backend-extra-config HICACHE_STORAGE_BACKEND_EXTRA_CONFIG]
                        [--enable-lmcache] [--kt-weight-path KT_WEIGHT_PATH]
                        [--kt-method KT_METHOD] [--kt-cpuinfer KT_CPUINFER]
                        [--kt-threadpool-count KT_THREADPOOL_COUNT]
                        [--kt-num-gpu-experts KT_NUM_GPU_EXPERTS]
                        [--kt-max-deferred-experts-per-token KT_MAX_DEFERRED_EXPERTS_PER_TOKEN]
                        [--dllm-algorithm DLLM_ALGORITHM]
                        [--dllm-algorithm-config DLLM_ALGORITHM_CONFIG]
                        [--enable-double-sparsity]
                        [--ds-channel-config-path DS_CHANNEL_CONFIG_PATH]
                        [--ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM]
                        [--ds-heavy-token-num DS_HEAVY_TOKEN_NUM]
                        [--ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE]
                        [--ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD]
                        [--cpu-offload-gb CPU_OFFLOAD_GB]
                        [--offload-group-size OFFLOAD_GROUP_SIZE]
                        [--offload-num-in-group OFFLOAD_NUM_IN_GROUP]
                        [--offload-prefetch-step OFFLOAD_PREFETCH_STEP]
                        [--offload-mode OFFLOAD_MODE]
                        [--multi-item-scoring-delimiter MULTI_ITEM_SCORING_DELIMITER]
                        [--disable-radix-cache]
                        [--cuda-graph-max-bs CUDA_GRAPH_MAX_BS]
                        [--cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]]
                        [--disable-cuda-graph] [--disable-cuda-graph-padding]
                        [--enable-profile-cuda-graph] [--enable-cudagraph-gc]
                        [--enable-layerwise-nvtx-marker] [--enable-nccl-nvls]
                        [--enable-symm-mem]
                        [--disable-flashinfer-cutlass-moe-fp4-allgather]
                        [--enable-tokenizer-batch-encode]
                        [--disable-tokenizer-batch-decode]
                        [--disable-outlines-disk-cache]
                        [--disable-custom-all-reduce] [--enable-mscclpp]
                        [--enable-torch-symm-mem] [--disable-overlap-schedule]
                        [--enable-mixed-chunk] [--enable-dp-attention]
                        [--enable-dp-lm-head] [--enable-two-batch-overlap]
                        [--enable-single-batch-overlap]
                        [--tbo-token-distribution-threshold TBO_TOKEN_DISTRIBUTION_THRESHOLD]
                        [--enable-torch-compile]
                        [--enable-torch-compile-debug-mode]
                        [--enable-piecewise-cuda-graph]
                        [--piecewise-cuda-graph-tokens PIECEWISE_CUDA_GRAPH_TOKENS]
                        [--piecewise-cuda-graph-compiler {eager,inductor}]
                        [--torch-compile-max-bs TORCH_COMPILE_MAX_BS]
                        [--piecewise-cuda-graph-max-tokens PIECEWISE_CUDA_GRAPH_MAX_TOKENS]
                        [--torchao-config TORCHAO_CONFIG]
                        [--enable-nan-detection] [--enable-p2p-check]
                        [--triton-attention-reduce-in-fp32]
                        [--triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS]
                        [--triton-attention-split-tile-size TRITON_ATTENTION_SPLIT_TILE_SIZE]
                        [--num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS]
                        [--delete-ckpt-after-loading] [--enable-memory-saver]
                        [--enable-weights-cpu-backup]
                        [--enable-draft-weights-cpu-backup]
                        [--allow-auto-truncate]
                        [--enable-custom-logit-processor]
                        [--flashinfer-mla-disable-ragged]
                        [--disable-shared-experts-fusion]
                        [--disable-chunked-prefix-cache]
                        [--disable-fast-image-processor]
                        [--keep-mm-feature-on-device]
                        [--enable-return-hidden-states]
                        [--enable-return-routed-experts]
                        [--scheduler-recv-interval SCHEDULER_RECV_INTERVAL]
                        [--numa-node NUMA_NODE [NUMA_NODE ...]]
                        [--enable-deterministic-inference]
                        [--rl-on-policy-target {fsdp}]
                        [--enable-attn-tp-input-scattered]
                        [--enable-nsa-prefill-context-parallel]
                        [--enable-fused-qk-norm-rope]
                        [--enable-dynamic-batch-tokenizer]
                        [--dynamic-batch-tokenizer-batch-size DYNAMIC_BATCH_TOKENIZER_BATCH_SIZE]
                        [--dynamic-batch-tokenizer-batch-timeout DYNAMIC_BATCH_TOKENIZER_BATCH_TIMEOUT]
                        [--debug-tensor-dump-output-folder DEBUG_TENSOR_DUMP_OUTPUT_FOLDER]
                        [--debug-tensor-dump-layers DEBUG_TENSOR_DUMP_LAYERS [DEBUG_TENSOR_DUMP_LAYERS ...]]
                        [--debug-tensor-dump-input-file DEBUG_TENSOR_DUMP_INPUT_FILE]
                        [--debug-tensor-dump-inject DEBUG_TENSOR_DUMP_INJECT]
                        [--disaggregation-mode {null,prefill,decode}]
                        [--disaggregation-transfer-backend {mooncake,nixl,ascend,fake}]
                        [--disaggregation-bootstrap-port DISAGGREGATION_BOOTSTRAP_PORT]
                        [--disaggregation-decode-tp DISAGGREGATION_DECODE_TP]
                        [--disaggregation-decode-dp DISAGGREGATION_DECODE_DP]
                        [--disaggregation-prefill-pp DISAGGREGATION_PREFILL_PP]
                        [--disaggregation-ib-device DISAGGREGATION_IB_DEVICE]
                        [--disaggregation-decode-enable-offload-kvcache]
                        [--num-reserved-decode-tokens NUM_RESERVED_DECODE_TOKENS]
                        [--disaggregation-decode-polling-interval DISAGGREGATION_DECODE_POLLING_INTERVAL]
                        [--encoder-only] [--language-only]
                        [--encoder-transfer-backend {zmq_to_scheduler,zmq_to_tokenizer,mooncake}]
                        [--encoder-urls ENCODER_URLS [ENCODER_URLS ...]]
                        [--custom-weight-loader [CUSTOM_WEIGHT_LOADER ...]]
                        [--weight-loader-disable-mmap]
                        [--remote-instance-weight-loader-seed-instance-ip REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_IP]
                        [--remote-instance-weight-loader-seed-instance-service-port REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_SERVICE_PORT]
                        [--remote-instance-weight-loader-send-weights-group-ports REMOTE_INSTANCE_WEIGHT_LOADER_SEND_WEIGHTS_GROUP_PORTS]
                        [--remote-instance-weight-loader-backend {transfer_engine,nccl}]
                        [--remote-instance-weight-loader-start-seed-via-transfer-engine]
                        [--enable-pdmux]
                        [--pdmux-config-path PDMUX_CONFIG_PATH]
                        [--sm-group-num SM_GROUP_NUM] [--config CONFIG]
                        [--mm-max-concurrent-calls MM_MAX_CONCURRENT_CALLS]
                        [--mm-per-request-timeout MM_PER_REQUEST_TIMEOUT]
                        [--enable-broadcast-mm-inputs-process]
                        [--mm-process-config MM_PROCESS_CONFIG]
                        [--mm-enable-dp-encoder]
                        [--decrypted-config-file DECRYPTED_CONFIG_FILE]
                        [--decrypted-draft-config-file DECRYPTED_DRAFT_CONFIG_FILE]
                        [--enable-prefix-mm-cache]
                        [--forward-hooks FORWARD_HOOKS]
launch_server.py: error: argument --tokenizer-path: expected one argument
E
======================================================================
ERROR: setUpClass (__main__.TestGrok)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/llm_models/test_ascend_llm_models_grok.py", line 56, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 2. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 10.005s

FAILED (errors=1)
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/huihui-ai/grok-2 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-radix-cache --disable-cuda-graph --tokenizer-path --base-gpu-id 10 /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/huihui-ai/grok-2/tokenizer.tok.json --tp-size 16 --device npu --host 127.0.0.1 --port 21000
.
.
End (0/43):
filename='ascend/llm_models/test_ascend_llm_models_grok.py', elapsed=25, estimated_time=400
.
.


✗ FAILED: ascend/llm_models/test_ascend_llm_models_grok.py returned exit code 1

.
.
Begin (1/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_internlm2_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:42:04] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b', tokenizer_path='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=729868951, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 10:42:06] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 10:42:15] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 10:42:16] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:42:16] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:42:17] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:42:17] Load weight begin. avail mem=60.82 GB

Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading pt checkpoint shards:  50% Completed | 1/2 [00:12<00:12, 12.87s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:36<00:00, 19.05s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:36<00:00, 18.13s/it]

[2026-01-05 10:42:54] Load weight end. type=InternLM2ForCausalLM, dtype=torch.bfloat16, avail mem=46.38 GB, mem usage=14.44 GB.
[2026-01-05 10:42:54] Using KV cache dtype: torch.bfloat16
[2026-01-05 10:42:54] The available memory for KV cache is 34.22 GB.
[2026-01-05 10:42:54] KV Cache is allocated. #tokens: 280320, K size: 17.12 GB, V size: 17.12 GB
[2026-01-05 10:42:54] Memory pool end. avail mem=11.64 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 10:42:55] max_total_num_tokens=280320, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.64 GB
[2026-01-05 10:42:55] INFO:     Started server process [101026]
[2026-01-05 10:42:55] INFO:     Waiting for application startup.
[2026-01-05 10:42:55] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:42:55] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:42:55] INFO:     Application startup complete.
[2026-01-05 10:42:55] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 10:42:56] INFO:     127.0.0.1:55198 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 10:42:56] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 10:42:56.540092555 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 10:43:03] INFO:     127.0.0.1:55210 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:43:13] INFO:     127.0.0.1:56080 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:43:15] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:15] The server is fired up and ready to roll!
[2026-01-05 10:43:23] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:43:24] INFO:     127.0.0.1:53246 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 10:43:24] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 10:43:24] INFO:     127.0.0.1:53258 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 10:43:24] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:43:24] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 10:43:24] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:43:24] Prefill batch, #new-seq: 14, #new-token: 2048, #cached-token: 8960, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 10:43:25] Prefill batch, #new-seq: 56, #new-token: 8192, #cached-token: 35840, token usage: 0.01, #running-req: 15, #queue-req: 57,
[2026-01-05 10:43:27] Prefill batch, #new-seq: 57, #new-token: 7936, #cached-token: 36480, token usage: 0.04, #running-req: 71, #queue-req: 0,
[2026-01-05 10:43:28] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:03<12:04,  3.64s/it][2026-01-05 10:43:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-05 10:43:28] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:03<05:24,  1.64s/it][2026-01-05 10:43:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:43:28] Decode batch, #running-req: 127, #token: 24832, token usage: 0.09, cpu graph: False, gen throughput (token/s): 53.15, #queue-req: 0,
[2026-01-05 10:43:28] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:04<03:22,  1.03s/it][2026-01-05 10:43:28] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:43:28] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:28] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:28] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:28] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:28] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:29] Prefill batch, #new-seq: 5, #new-token: 640, #cached-token: 3200, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:43:30] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:05<01:04,  2.97it/s]
  6%|▌         | 11/200 [00:05<00:44,  4.21it/s][2026-01-05 10:43:30] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:43:30] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:30] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:43:30] INFO:     127.0.0.1:53900 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:05<00:28,  6.31it/s][2026-01-05 10:43:30] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:43:30] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:30] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] INFO:     127.0.0.1:53698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] INFO:     127.0.0.1:54200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:30] Prefill batch, #new-seq: 5, #new-token: 1024, #cached-token: 3200, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-05 10:43:31] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:06<00:27,  6.26it/s][2026-01-05 10:43:31] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:43:31] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:31] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:43:31] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:07<00:22,  7.51it/s][2026-01-05 10:43:31] INFO:     127.0.0.1:54400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:43:31] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:53276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:53848 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:07<00:19,  8.53it/s]
 17%|█▋        | 34/200 [00:07<00:12, 13.38it/s]
 17%|█▋        | 34/200 [00:07<00:12, 13.38it/s][2026-01-05 10:43:32] INFO:     127.0.0.1:53596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.10, #running-req: 125, #queue-req: 0,
[2026-01-05 10:43:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 127, #queue-req: 0,

 18%|█▊        | 37/200 [00:07<00:11, 13.79it/s][2026-01-05 10:43:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:53694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.11, #running-req: 126, #queue-req: 0,

 20%|██        | 40/200 [00:07<00:10, 15.27it/s][2026-01-05 10:43:32] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:07<00:08, 18.48it/s][2026-01-05 10:43:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:54292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:07<00:07, 20.56it/s]
 25%|██▌       | 50/200 [00:07<00:06, 24.00it/s][2026-01-05 10:43:32] Decode batch, #running-req: 128, #token: 28544, token usage: 0.10, cpu graph: False, gen throughput (token/s): 1272.19, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:54112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:43:32] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:08<00:06, 23.47it/s][2026-01-05 10:43:32] INFO:     127.0.0.1:54000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:53310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:53926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:43:32] INFO:     127.0.0.1:54010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:32] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:08<00:05, 25.31it/s]
 30%|██▉       | 59/200 [00:08<00:04, 28.37it/s][2026-01-05 10:43:33] INFO:     127.0.0.1:53462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:43:33] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:43:33] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:08<00:05, 25.87it/s][2026-01-05 10:43:33] INFO:     127.0.0.1:53886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:43:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:33] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:53822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:08<00:05, 25.23it/s]
 33%|███▎      | 66/200 [00:08<00:05, 26.78it/s][2026-01-05 10:43:33] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.10, #running-req: 125, #queue-req: 0,
[2026-01-05 10:43:33] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:43:33] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-05 10:43:33] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:08<00:04, 27.67it/s]
 36%|███▌      | 72/200 [00:08<00:04, 30.14it/s][2026-01-05 10:43:33] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:43:33] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:38008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:38208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:53606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:38110 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:08<00:03, 37.38it/s]
 42%|████▏     | 84/200 [00:08<00:01, 66.70it/s]
 42%|████▏     | 84/200 [00:08<00:01, 66.70it/s]
 42%|████▏     | 84/200 [00:08<00:01, 66.70it/s]
 42%|████▏     | 84/200 [00:08<00:01, 66.70it/s][2026-01-05 10:43:33] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:53880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:53954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:08<00:01, 59.39it/s]
 46%|████▌     | 92/200 [00:08<00:01, 56.47it/s][2026-01-05 10:43:33] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:38012 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:09<00:01, 53.11it/s][2026-01-05 10:43:33] INFO:     127.0.0.1:53948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:54160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:53506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:33] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:09<00:01, 51.20it/s]
 52%|█████▎    | 105/200 [00:09<00:01, 51.93it/s][2026-01-05 10:43:34] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:54072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:37950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:37974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:37926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38252 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:09<00:01, 55.73it/s]
 56%|█████▋    | 113/200 [00:09<00:01, 61.25it/s][2026-01-05 10:43:34] INFO:     127.0.0.1:38092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] Decode batch, #running-req: 87, #token: 20736, token usage: 0.07, cpu graph: False, gen throughput (token/s): 3136.55, #queue-req: 0,
[2026-01-05 10:43:34] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:37934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38240 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:09<00:01, 63.45it/s][2026-01-05 10:43:34] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:37958 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:09<00:01, 58.04it/s][2026-01-05 10:43:34] INFO:     127.0.0.1:38126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:09<00:01, 57.86it/s]
 68%|██████▊   | 135/200 [00:09<00:01, 60.16it/s][2026-01-05 10:43:34] INFO:     127.0.0.1:38196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53398 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:09<00:00, 60.35it/s][2026-01-05 10:43:34] INFO:     127.0.0.1:38070 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:53292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38436 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:09<00:00, 62.92it/s]
 76%|███████▌  | 151/200 [00:09<00:00, 67.28it/s][2026-01-05 10:43:34] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:38362 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:10<00:00, 52.03it/s][2026-01-05 10:43:34] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:34] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] Decode batch, #running-req: 40, #token: 11520, token usage: 0.04, cpu graph: False, gen throughput (token/s): 2578.31, #queue-req: 0,
[2026-01-05 10:43:35] INFO:     127.0.0.1:38108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:37912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38330 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:10<00:00, 41.46it/s]
 82%|████████▎ | 165/200 [00:10<00:00, 37.31it/s][2026-01-05 10:43:35] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38312 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:10<00:00, 33.22it/s][2026-01-05 10:43:35] INFO:     127.0.0.1:38390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:53450 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:10<00:00, 39.18it/s][2026-01-05 10:43:35] INFO:     127.0.0.1:38040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:37908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:37988 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:11<00:00, 27.80it/s][2026-01-05 10:43:35] INFO:     127.0.0.1:38162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:38492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:35] Decode batch, #running-req: 16, #token: 5248, token usage: 0.02, cpu graph: False, gen throughput (token/s): 1291.39, #queue-req: 0,
[2026-01-05 10:43:35] INFO:     127.0.0.1:38510 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:11<00:00, 30.28it/s][2026-01-05 10:43:36] INFO:     127.0.0.1:38574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:36] INFO:     127.0.0.1:37994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:36] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:36] INFO:     127.0.0.1:38524 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:11<00:00, 15.83it/s][2026-01-05 10:43:36] Decode batch, #running-req: 9, #token: 4096, token usage: 0.01, cpu graph: False, gen throughput (token/s): 614.28, #queue-req: 0,
[2026-01-05 10:43:36] INFO:     127.0.0.1:38218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:36] INFO:     127.0.0.1:38314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:36] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:11<00:00, 16.40it/s][2026-01-05 10:43:37] INFO:     127.0.0.1:38474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:37] INFO:     127.0.0.1:53806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:37] Decode batch, #running-req: 4, #token: 2176, token usage: 0.01, cpu graph: False, gen throughput (token/s): 286.86, #queue-req: 0,
[2026-01-05 10:43:37] INFO:     127.0.0.1:38076 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:12<00:00,  9.02it/s][2026-01-05 10:43:37] INFO:     127.0.0.1:38376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:43:38] Decode batch, #running-req: 2, #token: 1536, token usage: 0.01, cpu graph: False, gen throughput (token/s): 159.97, #queue-req: 0,
[2026-01-05 10:43:38] INFO:     127.0.0.1:38432 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:13<00:00,  6.15it/s][2026-01-05 10:43:38] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 75.71, #queue-req: 0,
[2026-01-05 10:43:39] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 56.04, #queue-req: 0,
[2026-01-05 10:43:40] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 55.96, #queue-req: 0,
[2026-01-05 10:43:41] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 56.01, #queue-req: 0,
[2026-01-05 10:43:41] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cpu graph: False, gen throughput (token/s): 56.15, #queue-req: 0,
[2026-01-05 10:43:42] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cpu graph: False, gen throughput (token/s): 56.04, #queue-req: 0,
[2026-01-05 10:43:42] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:17<00:00, 11.19it/s]
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/gsm8k_ascend_mixin.py", line 64, in test_gsm8k
    self.assertGreater(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1271, in assertGreater
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: np.float64(0.595) not greater than 0.6 : Accuracy of /root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b is 0.595, is lower than 0.6
E
======================================================================
ERROR: test_gsm8k (__main__.TestMistral7B.test_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: np.float64(0.595) not greater than 0.6 : Accuracy of /root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b is 0.595, is lower than 0.6

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 109.226s

FAILED (errors=1)
Accuracy: 0.595
Invalid: 0.005
Latency: 17.934 s
Output throughput: 1012.455 token/s
.
.
End (1/43):
filename='ascend/llm_models/test_ascend_internlm2_7b.py', elapsed=121, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_ascend_internlm2_7b.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/llm_models/test_ascend_internlm2_7b.py

.
.
Begin (1/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_internlm2_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:45:05] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b', tokenizer_path='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=771943436, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 10:45:06] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 10:45:22] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 10:45:23] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:45:23] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:45:24] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:45:24] Load weight begin. avail mem=60.82 GB

Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading pt checkpoint shards:  50% Completed | 1/2 [00:12<00:12, 12.12s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:32<00:00, 17.26s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:32<00:00, 16.49s/it]

[2026-01-05 10:45:57] Load weight end. type=InternLM2ForCausalLM, dtype=torch.bfloat16, avail mem=46.39 GB, mem usage=14.44 GB.
[2026-01-05 10:45:57] Using KV cache dtype: torch.bfloat16
[2026-01-05 10:45:57] The available memory for KV cache is 34.22 GB.
[2026-01-05 10:45:57] KV Cache is allocated. #tokens: 280320, K size: 17.12 GB, V size: 17.12 GB
[2026-01-05 10:45:57] Memory pool end. avail mem=11.64 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 10:45:58] max_total_num_tokens=280320, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.64 GB
[2026-01-05 10:45:59] INFO:     Started server process [103950]
[2026-01-05 10:45:59] INFO:     Waiting for application startup.
[2026-01-05 10:45:59] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:45:59] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:45:59] INFO:     Application startup complete.
[2026-01-05 10:45:59] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 10:46:00] INFO:     127.0.0.1:36446 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 10:46:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 10:46:00.189952907 compiler_depend.ts:198] Warning: Driver Version: "-" is invalid or not supported yet. (function operator())
[2026-01-05 10:46:04] INFO:     127.0.0.1:36468 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:46:14] INFO:     127.0.0.1:36892 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:46:17] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:17] The server is fired up and ready to roll!
[2026-01-05 10:46:24] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:46:25] INFO:     127.0.0.1:60064 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 10:46:25] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 10:46:25] INFO:     127.0.0.1:60070 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 10:46:25] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:46:25] INFO:     127.0.0.1:60074 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 10:46:25] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:46:25] Prefill batch, #new-seq: 13, #new-token: 1920, #cached-token: 8320, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 10:46:25] Prefill batch, #new-seq: 29, #new-token: 3968, #cached-token: 18560, token usage: 0.01, #running-req: 14, #queue-req: 0,
[2026-01-05 10:46:27] Prefill batch, #new-seq: 57, #new-token: 8192, #cached-token: 36480, token usage: 0.02, #running-req: 43, #queue-req: 28,
[2026-01-05 10:46:27] Prefill batch, #new-seq: 28, #new-token: 4096, #cached-token: 17920, token usage: 0.05, #running-req: 100, #queue-req: 0,
[2026-01-05 10:46:29] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:04<14:40,  4.43s/it][2026-01-05 10:46:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-05 10:46:30] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:04<06:29,  1.97s/it][2026-01-05 10:46:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:46:30] Decode batch, #running-req: 127, #token: 24832, token usage: 0.09, cpu graph: False, gen throughput (token/s): 57.19, #queue-req: 0,
[2026-01-05 10:46:30] INFO:     127.0.0.1:60994 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:04<03:57,  1.20s/it][2026-01-05 10:46:30] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:46:30] INFO:     127.0.0.1:60598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:60788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:30] Prefill batch, #new-seq: 5, #new-token: 640, #cached-token: 3200, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:46:30] INFO:     127.0.0.1:60384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:05<00:45,  4.14it/s]
  6%|▌         | 11/200 [00:05<00:22,  8.45it/s][2026-01-05 10:46:30] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:46:30] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:60618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:32816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:30] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:46:30] INFO:     127.0.0.1:60986 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:05<00:15, 11.79it/s][2026-01-05 10:46:30] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:46:30] INFO:     127.0.0.1:60766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:60090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:30] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:60728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] Prefill batch, #new-seq: 5, #new-token: 1024, #cached-token: 3200, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-05 10:46:30] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:05<00:10, 16.71it/s][2026-01-05 10:46:30] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:60468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:05<00:09, 17.68it/s][2026-01-05 10:46:31] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:60634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] INFO:     127.0.0.1:32894 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:05<00:09, 17.88it/s]
 17%|█▋        | 34/200 [00:05<00:07, 22.15it/s]
 17%|█▋        | 34/200 [00:05<00:07, 22.15it/s][2026-01-05 10:46:31] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.10, #running-req: 125, #queue-req: 0,
[2026-01-05 10:46:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] INFO:     127.0.0.1:60670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 127, #queue-req: 0,

 18%|█▊        | 37/200 [00:06<00:08, 20.08it/s][2026-01-05 10:46:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:60472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] INFO:     127.0.0.1:32924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:06<00:07, 20.68it/s]
 20%|██        | 41/200 [00:06<00:06, 22.98it/s][2026-01-05 10:46:31] INFO:     127.0.0.1:60538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:06<00:06, 23.06it/s][2026-01-05 10:46:31] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:06<00:06, 24.31it/s]
 25%|██▌       | 50/200 [00:06<00:05, 27.16it/s][2026-01-05 10:46:31] Decode batch, #running-req: 128, #token: 28544, token usage: 0.10, cpu graph: False, gen throughput (token/s): 2751.83, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:31] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:46:31] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:32] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:06<00:05, 25.58it/s][2026-01-05 10:46:32] INFO:     127.0.0.1:32856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:46:32] INFO:     127.0.0.1:60482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:32] INFO:     127.0.0.1:32786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:46:32] INFO:     127.0.0.1:60584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:06<00:05, 25.02it/s]
 29%|██▉       | 58/200 [00:06<00:05, 26.51it/s][2026-01-05 10:46:32] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:46:32] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:32] INFO:     127.0.0.1:33032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 127, #queue-req: 0,

 30%|███       | 61/200 [00:07<00:05, 24.05it/s][2026-01-05 10:46:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:32] INFO:     127.0.0.1:60540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:07<00:05, 25.08it/s][2026-01-05 10:46:32] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.10, #running-req: 125, #queue-req: 0,
[2026-01-05 10:46:32] INFO:     127.0.0.1:33018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:32] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-05 10:46:33] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] INFO:     127.0.0.1:33010 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:08<00:16,  7.78it/s]
 35%|███▌      | 70/200 [00:08<00:24,  5.33it/s]
 35%|███▌      | 70/200 [00:08<00:24,  5.33it/s][2026-01-05 10:46:33] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.10, #running-req: 125, #queue-req: 0,
[2026-01-05 10:46:33] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:46:33] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] INFO:     127.0.0.1:50918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] INFO:     127.0.0.1:60844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] INFO:     127.0.0.1:60932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:33] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:08<00:14,  8.52it/s][2026-01-05 10:46:33] INFO:     127.0.0.1:60550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:08<00:07, 14.00it/s][2026-01-05 10:46:34] INFO:     127.0.0.1:60216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:32908 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:08<00:06, 16.72it/s]
 47%|████▋     | 94/200 [00:08<00:05, 20.87it/s][2026-01-05 10:46:34] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60698 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:08<00:04, 23.47it/s]
 50%|█████     | 101/200 [00:08<00:03, 32.48it/s]
 50%|█████     | 101/200 [00:08<00:03, 32.48it/s][2026-01-05 10:46:34] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:32784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:09<00:02, 33.78it/s][2026-01-05 10:46:34] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] Decode batch, #running-req: 90, #token: 21504, token usage: 0.08, cpu graph: False, gen throughput (token/s): 1790.48, #queue-req: 0,
[2026-01-05 10:46:34] INFO:     127.0.0.1:60674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:09<00:02, 36.83it/s]
 57%|█████▊    | 115/200 [00:09<00:01, 53.05it/s]
 57%|█████▊    | 115/200 [00:09<00:01, 53.05it/s]
 57%|█████▊    | 115/200 [00:09<00:01, 53.05it/s][2026-01-05 10:46:34] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:09<00:01, 50.72it/s]
 62%|██████▏   | 124/200 [00:09<00:01, 51.16it/s][2026-01-05 10:46:34] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:50902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:51136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60498 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [00:09<00:01, 52.68it/s][2026-01-05 10:46:34] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:51256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:51164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:34] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:09<00:00, 60.66it/s][2026-01-05 10:46:35] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:50966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:09<00:00, 60.11it/s]
 74%|███████▍  | 149/200 [00:09<00:00, 62.00it/s][2026-01-05 10:46:35] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:32768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:09<00:00, 47.32it/s][2026-01-05 10:46:35] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:33012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] Decode batch, #running-req: 41, #token: 11520, token usage: 0.04, cpu graph: False, gen throughput (token/s): 2608.04, #queue-req: 0,
[2026-01-05 10:46:35] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:10<00:00, 42.73it/s][2026-01-05 10:46:35] INFO:     127.0.0.1:60792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:50940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:10<00:00, 42.99it/s][2026-01-05 10:46:35] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:60156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:60174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:10<00:00, 39.40it/s][2026-01-05 10:46:35] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:35] INFO:     127.0.0.1:60232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:50872 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:10<00:00, 26.32it/s][2026-01-05 10:46:36] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:32840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] Decode batch, #running-req: 22, #token: 7296, token usage: 0.03, cpu graph: False, gen throughput (token/s): 1271.47, #queue-req: 0,
[2026-01-05 10:46:36] INFO:     127.0.0.1:60248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:10<00:00, 24.06it/s]
 91%|█████████ | 182/200 [00:10<00:00, 23.84it/s][2026-01-05 10:46:36] INFO:     127.0.0.1:50736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:11<00:00, 23.76it/s][2026-01-05 10:46:36] INFO:     127.0.0.1:50800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:11<00:00, 20.11it/s][2026-01-05 10:46:36] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:11<00:00, 19.46it/s]
 96%|█████████▌| 192/200 [00:11<00:00, 20.58it/s][2026-01-05 10:46:36] INFO:     127.0.0.1:51270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:36] INFO:     127.0.0.1:32960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:37] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:11<00:00, 22.11it/s][2026-01-05 10:46:37] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:37] Decode batch, #running-req: 4, #token: 2176, token usage: 0.01, cpu graph: False, gen throughput (token/s): 552.34, #queue-req: 0,
[2026-01-05 10:46:37] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:38] Decode batch, #running-req: 3, #token: 1792, token usage: 0.01, cpu graph: False, gen throughput (token/s): 166.13, #queue-req: 0,
[2026-01-05 10:46:38] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:13<00:00,  5.77it/s][2026-01-05 10:46:38] Decode batch, #running-req: 2, #token: 1536, token usage: 0.01, cpu graph: False, gen throughput (token/s): 135.38, #queue-req: 0,
[2026-01-05 10:46:38] INFO:     127.0.0.1:51116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:46:39] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 59.63, #queue-req: 0,
[2026-01-05 10:46:40] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 50.85, #queue-req: 0,
[2026-01-05 10:46:41] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 50.67, #queue-req: 0,
[2026-01-05 10:46:41] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 51.02, #queue-req: 0,
[2026-01-05 10:46:42] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cpu graph: False, gen throughput (token/s): 50.58, #queue-req: 0,
[2026-01-05 10:46:43] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cpu graph: False, gen throughput (token/s): 51.36, #queue-req: 0,
[2026-01-05 10:46:43] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:18<00:00,  1.54it/s]
100%|██████████| 200/200 [00:18<00:00, 10.92it/s]
.
----------------------------------------------------------------------
Ran 1 test in 109.648s

OK
Accuracy: 0.620
Invalid: 0.000
Latency: 18.380 s
Output throughput: 991.651 token/s
.
.
End (1/43):
filename='ascend/llm_models/test_ascend_internlm2_7b.py', elapsed=121, estimated_time=400
.
.


✓ PASSED on retry (attempt 2): ascend/llm_models/test_ascend_internlm2_7b.py

.
.
Begin (2/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_granite_3_1_8b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:47:04] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.1-8b-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.1-8b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=1052626436, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.1-8b-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 10:47:04] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 10:47:13] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 10:47:14] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:47:14] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:47:15] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:47:15] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:06<00:18,  6.10s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:12<00:12,  6.20s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:14<00:04,  4.27s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  5.04s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  5.14s/it]

[2026-01-05 10:47:37] Load weight end. type=GraniteForCausalLM, dtype=torch.bfloat16, avail mem=45.56 GB, mem usage=15.26 GB.
[2026-01-05 10:47:37] Using KV cache dtype: torch.bfloat16
[2026-01-05 10:47:37] The available memory for KV cache is 33.36 GB.
[2026-01-05 10:47:37] KV Cache is allocated. #tokens: 218496, K size: 16.68 GB, V size: 16.68 GB
[2026-01-05 10:47:37] Memory pool end. avail mem=11.17 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 10:47:37] max_total_num_tokens=218496, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=11.17 GB
[2026-01-05 10:47:38] INFO:     Started server process [106802]
[2026-01-05 10:47:38] INFO:     Waiting for application startup.
[2026-01-05 10:47:38] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:47:38] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:47:38] INFO:     Application startup complete.
[2026-01-05 10:47:38] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 10:47:39] INFO:     127.0.0.1:47210 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 10:47:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 10:47:39.828747274 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 10:47:45] INFO:     127.0.0.1:47240 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:47:51] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:47:51] The server is fired up and ready to roll!
[2026-01-05 10:47:55] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:47:56] INFO:     127.0.0.1:47788 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 10:47:56] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 10:47:56] INFO:     127.0.0.1:50396 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 10:47:56] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:47:56] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/ibm-granite/granite-3.1-8b-instruct --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 10:47:56] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:47:56] Prefill batch, #new-seq: 13, #new-token: 3328, #cached-token: 9984, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 10:47:57] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.02, #running-req: 14, #queue-req: 82,
[2026-01-05 10:47:58] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.06, #running-req: 46, #queue-req: 50,
[2026-01-05 10:47:59] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.09, #running-req: 78, #queue-req: 18,
[2026-01-05 10:47:59] Prefill batch, #new-seq: 18, #new-token: 4608, #cached-token: 13824, token usage: 0.13, #running-req: 110, #queue-req: 0,
[2026-01-05 10:48:01] Decode batch, #running-req: 128, #token: 34304, token usage: 0.16, cpu graph: False, gen throughput (token/s): 83.60, #queue-req: 0,
[2026-01-05 10:48:01] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:05<17:17,  5.22s/it][2026-01-05 10:48:01] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:01] INFO:     127.0.0.1:51548 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:05<07:25,  2.25s/it][2026-01-05 10:48:01] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:01] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:01] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:01] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:05<02:54,  1.12it/s][2026-01-05 10:48:01] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:01] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] INFO:     127.0.0.1:51228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.16, #running-req: 125, #queue-req: 0,
[2026-01-05 10:48:02] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] INFO:     127.0.0.1:50912 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:05<01:09,  2.78it/s]
  4%|▍         | 9/200 [00:05<00:37,  5.12it/s][2026-01-05 10:48:02] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.16, #running-req: 126, #queue-req: 0,
[2026-01-05 10:48:02] INFO:     127.0.0.1:51222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.16, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:02] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 130, #queue-req: 0,
[2026-01-05 10:48:02] INFO:     127.0.0.1:50832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:06<00:27,  6.88it/s]
  7%|▋         | 14/200 [00:06<00:19,  9.34it/s][2026-01-05 10:48:02] INFO:     127.0.0.1:50982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.16, #running-req: 126, #queue-req: 0,
[2026-01-05 10:48:02] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.16, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:02] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.17, #running-req: 129, #queue-req: 0,
[2026-01-05 10:48:02] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:06<00:16, 11.04it/s][2026-01-05 10:48:02] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:02] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:02] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:06<00:18,  9.76it/s]
 11%|█         | 22/200 [00:06<00:16, 10.66it/s]
 11%|█         | 22/200 [00:06<00:16, 10.66it/s][2026-01-05 10:48:02] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:02] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.16, #running-req: 125, #queue-req: 0,
[2026-01-05 10:48:04] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.17, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:04] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:07<00:33,  5.20it/s][2026-01-05 10:48:04] Decode batch, #running-req: 128, #token: 37120, token usage: 0.17, cpu graph: False, gen throughput (token/s): 1655.81, #queue-req: 0,
[2026-01-05 10:48:04] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:04] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:08<00:30,  5.75it/s][2026-01-05 10:48:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:04] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.17, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:04] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:08<00:25,  6.66it/s][2026-01-05 10:48:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.18, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:04] INFO:     127.0.0.1:51050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:04] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:04] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:08<00:22,  7.45it/s]
 16%|█▌        | 32/200 [00:08<00:18,  9.29it/s][2026-01-05 10:48:04] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.18, #running-req: 125, #queue-req: 0,
[2026-01-05 10:48:06] INFO:     127.0.0.1:51594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.18, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:06] INFO:     127.0.0.1:50788 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:09<00:39,  4.18it/s][2026-01-05 10:48:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.18, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:06] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:06] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:06] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.18, #running-req: 126, #queue-req: 0,
[2026-01-05 10:48:07] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:07] INFO:     127.0.0.1:50618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:07] INFO:     127.0.0.1:50726 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:11<00:49,  3.32it/s]
 20%|█▉        | 39/200 [00:11<00:45,  3.54it/s]
 20%|█▉        | 39/200 [00:11<00:45,  3.54it/s][2026-01-05 10:48:07] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.18, #running-req: 125, #queue-req: 0,
[2026-01-05 10:48:07] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:07] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:11<00:40,  3.88it/s][2026-01-05 10:48:07] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:07] INFO:     127.0.0.1:50882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:07] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:07] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:07] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.19, #running-req: 129, #queue-req: 0,
[2026-01-05 10:48:07] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:11<00:26,  5.84it/s][2026-01-05 10:48:07] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:07] INFO:     127.0.0.1:51440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 129, #queue-req: 0,
[2026-01-05 10:48:08] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:11<00:21,  6.91it/s][2026-01-05 10:48:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:08] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:08] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:08] INFO:     127.0.0.1:51522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:08] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:08] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:11<00:16,  8.81it/s][2026-01-05 10:48:08] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:08] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 129, #queue-req: 0,
[2026-01-05 10:48:08] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:08] INFO:     127.0.0.1:51176 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:12<00:14,  9.63it/s]
 28%|██▊       | 57/200 [00:12<00:12, 11.30it/s][2026-01-05 10:48:08] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:08] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.19, #running-req: 126, #queue-req: 0,
[2026-01-05 10:48:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:08] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:12<00:12, 11.28it/s][2026-01-05 10:48:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:08] INFO:     127.0.0.1:50816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:08] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:12<00:11, 11.80it/s][2026-01-05 10:48:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:08] Decode batch, #running-req: 128, #token: 42112, token usage: 0.19, cpu graph: False, gen throughput (token/s): 1100.16, #queue-req: 0,
[2026-01-05 10:48:08] INFO:     127.0.0.1:51402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:08] INFO:     127.0.0.1:51618 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:12<00:11, 12.39it/s][2026-01-05 10:48:09] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.19, #running-req: 126, #queue-req: 0,
[2026-01-05 10:48:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:09] INFO:     127.0.0.1:50558 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:12<00:11, 11.51it/s][2026-01-05 10:48:09] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:09] INFO:     127.0.0.1:51634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:51466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.19, #running-req: 128, #queue-req: 0,
[2026-01-05 10:48:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 130, #queue-req: 0,
[2026-01-05 10:48:09] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:13<00:09, 13.68it/s][2026-01-05 10:48:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.19, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:09] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.20, #running-req: 127, #queue-req: 0,
[2026-01-05 10:48:09] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:13<00:09, 13.83it/s]
 36%|███▌      | 72/200 [00:13<00:08, 15.68it/s][2026-01-05 10:48:09] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.19, #running-req: 126, #queue-req: 0,
[2026-01-05 10:48:09] INFO:     127.0.0.1:51740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:13<00:08, 15.22it/s]
 38%|███▊      | 77/200 [00:13<00:05, 24.12it/s]
 38%|███▊      | 77/200 [00:13<00:05, 24.12it/s]
 38%|███▊      | 77/200 [00:13<00:05, 24.12it/s][2026-01-05 10:48:09] INFO:     127.0.0.1:51692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:13<00:05, 23.37it/s]
 41%|████      | 82/200 [00:13<00:04, 28.06it/s]
 41%|████      | 82/200 [00:13<00:04, 28.06it/s][2026-01-05 10:48:09] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:51112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:09] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:13<00:04, 26.33it/s]
 43%|████▎     | 86/200 [00:13<00:04, 27.23it/s][2026-01-05 10:48:10] INFO:     127.0.0.1:50750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:13<00:03, 34.69it/s][2026-01-05 10:48:10] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51098 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:13<00:03, 32.79it/s][2026-01-05 10:48:10] INFO:     127.0.0.1:51478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:50574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [00:13<00:02, 34.25it/s][2026-01-05 10:48:10] INFO:     127.0.0.1:50490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] Decode batch, #running-req: 97, #token: 33152, token usage: 0.15, cpu graph: False, gen throughput (token/s): 2753.94, #queue-req: 0,
[2026-01-05 10:48:10] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51734 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:14<00:04, 22.75it/s]
 54%|█████▍    | 108/200 [00:14<00:04, 19.83it/s]
 54%|█████▍    | 108/200 [00:14<00:04, 19.83it/s]
 54%|█████▍    | 108/200 [00:14<00:04, 19.83it/s][2026-01-05 10:48:10] INFO:     127.0.0.1:51842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:50598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:36062 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:14<00:03, 22.90it/s]
 58%|█████▊    | 117/200 [00:14<00:01, 46.93it/s]
 58%|█████▊    | 117/200 [00:14<00:01, 46.93it/s]
 58%|█████▊    | 117/200 [00:14<00:01, 46.93it/s]
 58%|█████▊    | 117/200 [00:14<00:01, 46.93it/s][2026-01-05 10:48:10] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:51162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:50424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:10] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:14<00:01, 41.64it/s][2026-01-05 10:48:11] INFO:     127.0.0.1:50504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:51218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:14<00:02, 29.28it/s][2026-01-05 10:48:11] INFO:     127.0.0.1:36108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:51708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:35930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:15<00:02, 29.89it/s][2026-01-05 10:48:11] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:36186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:50818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:35932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:36198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:15<00:01, 35.85it/s][2026-01-05 10:48:11] INFO:     127.0.0.1:36008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:35914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:15<00:01, 30.34it/s][2026-01-05 10:48:11] INFO:     127.0.0.1:51770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:36208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] Decode batch, #running-req: 56, #token: 21248, token usage: 0.10, cpu graph: False, gen throughput (token/s): 2384.10, #queue-req: 0,
[2026-01-05 10:48:11] INFO:     127.0.0.1:35988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:11] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:15<00:01, 28.21it/s]
 74%|███████▍  | 149/200 [00:15<00:01, 28.31it/s][2026-01-05 10:48:12] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:51660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:35890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:36056 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:15<00:01, 29.44it/s][2026-01-05 10:48:12] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:50486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:36080 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:15<00:01, 30.73it/s][2026-01-05 10:48:12] INFO:     127.0.0.1:51622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:36140 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:16<00:01, 26.95it/s][2026-01-05 10:48:12] INFO:     127.0.0.1:51796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:51662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:51716 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:16<00:01, 21.01it/s]
 83%|████████▎ | 166/200 [00:16<00:01, 19.09it/s][2026-01-05 10:48:12] INFO:     127.0.0.1:36022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:51830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:12] INFO:     127.0.0.1:36120 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:16<00:01, 17.33it/s][2026-01-05 10:48:13] Decode batch, #running-req: 31, #token: 13440, token usage: 0.06, cpu graph: False, gen throughput (token/s): 1428.56, #queue-req: 0,
[2026-01-05 10:48:13] INFO:     127.0.0.1:51116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:13] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:16<00:01, 17.39it/s][2026-01-05 10:48:13] INFO:     127.0.0.1:51602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:13] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:13] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:16<00:01, 14.53it/s]
 87%|████████▋ | 174/200 [00:16<00:01, 14.18it/s][2026-01-05 10:48:13] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:13] INFO:     127.0.0.1:36220 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:17<00:01, 14.20it/s][2026-01-05 10:48:13] INFO:     127.0.0.1:50800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:13] INFO:     127.0.0.1:36088 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:17<00:02, 10.00it/s][2026-01-05 10:48:13] INFO:     127.0.0.1:51790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:13] INFO:     127.0.0.1:36116 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:17<00:01, 10.88it/s][2026-01-05 10:48:14] INFO:     127.0.0.1:51054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:14] INFO:     127.0.0.1:36200 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:17<00:01, 12.15it/s][2026-01-05 10:48:14] Decode batch, #running-req: 18, #token: 8576, token usage: 0.04, cpu graph: False, gen throughput (token/s): 880.76, #queue-req: 0,
[2026-01-05 10:48:14] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:14] INFO:     127.0.0.1:36070 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:14] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:14] INFO:     127.0.0.1:36236 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:17<00:01, 10.63it/s]
 93%|█████████▎| 186/200 [00:17<00:01, 12.75it/s]
 93%|█████████▎| 186/200 [00:17<00:01, 12.75it/s][2026-01-05 10:48:14] INFO:     127.0.0.1:51756 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:14] INFO:     127.0.0.1:36164 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:18<00:01,  9.12it/s][2026-01-05 10:48:14] INFO:     127.0.0.1:35962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:14] INFO:     127.0.0.1:35978 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:18<00:01,  9.70it/s][2026-01-05 10:48:15] INFO:     127.0.0.1:50670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:15] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:18<00:00,  8.97it/s][2026-01-05 10:48:15] Decode batch, #running-req: 8, #token: 4096, token usage: 0.02, cpu graph: False, gen throughput (token/s): 468.55, #queue-req: 0,
[2026-01-05 10:48:15] INFO:     127.0.0.1:51816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:48:15] INFO:     127.0.0.1:36174 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:19<00:00,  8.75it/s][2026-01-05 10:48:15] INFO:     127.0.0.1:36000 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:19<00:00,  8.22it/s][2026-01-05 10:48:16] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:19<00:00,  5.18it/s][2026-01-05 10:48:16] Decode batch, #running-req: 4, #token: 2816, token usage: 0.01, cpu graph: False, gen throughput (token/s): 203.55, #queue-req: 0,
[2026-01-05 10:48:16] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:20<00:00,  4.04it/s][2026-01-05 10:48:17] INFO:     127.0.0.1:36042 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:20<00:00,  3.03it/s][2026-01-05 10:48:17] Decode batch, #running-req: 2, #token: 1792, token usage: 0.01, cpu graph: False, gen throughput (token/s): 116.78, #queue-req: 0,
[2026-01-05 10:48:18] Decode batch, #running-req: 2, #token: 1792, token usage: 0.01, cpu graph: False, gen throughput (token/s): 76.39, #queue-req: 0,
[2026-01-05 10:48:19] INFO:     127.0.0.1:36104 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:22<00:00,  1.38it/s][2026-01-05 10:48:19] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, cpu graph: False, gen throughput (token/s): 63.46, #queue-req: 0,
[2026-01-05 10:48:20] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, cpu graph: False, gen throughput (token/s): 39.35, #queue-req: 0,
[2026-01-05 10:48:21] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, cpu graph: False, gen throughput (token/s): 41.98, #queue-req: 0,
[2026-01-05 10:48:22] Decode batch, #running-req: 1, #token: 1536, token usage: 0.01, cpu graph: False, gen throughput (token/s): 41.88, #queue-req: 0,
[2026-01-05 10:48:22] INFO:     127.0.0.1:35906 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:26<00:00,  1.44s/it]
100%|██████████| 200/200 [00:26<00:00,  7.66it/s]
.
----------------------------------------------------------------------
Ran 1 test in 87.477s

OK
Accuracy: 0.700
Invalid: 0.000
Latency: 26.194 s
Output throughput: 970.810 token/s
.
.
End (2/43):
filename='ascend/llm_models/test_ascend_granite_3_1_8b.py', elapsed=99, estimated_time=400
.
.

.
.
Begin (3/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_chatglm2_6b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 10:48:43] WARNING model_config.py:1014: Casting torch.float16 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:48:43] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/ZhipuAI/chatglm2-6b', tokenizer_path='/root/.cache/modelscope/hub/models/ZhipuAI/chatglm2-6b', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=932310776, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/ZhipuAI/chatglm2-6b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 10:48:43] Casting torch.float16 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
[2026-01-05 10:48:44] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:48:52] Casting torch.float16 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 10:48:53] Casting torch.float16 to torch.bfloat16.
[2026-01-05 10:48:53] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 10:48:53] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:48:54] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:48:54] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:48:54] Load weight begin. avail mem=60.82 GB

Loading pt checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]

Loading pt checkpoint shards:  14% Completed | 1/7 [00:04<00:26,  4.39s/it]

Loading pt checkpoint shards:  29% Completed | 2/7 [00:09<00:23,  4.65s/it]

Loading pt checkpoint shards:  43% Completed | 3/7 [00:13<00:18,  4.68s/it]

Loading pt checkpoint shards:  57% Completed | 4/7 [00:18<00:13,  4.64s/it]

Loading pt checkpoint shards:  71% Completed | 5/7 [00:23<00:09,  4.67s/it]

Loading pt checkpoint shards:  86% Completed | 6/7 [00:27<00:04,  4.53s/it]

Loading pt checkpoint shards: 100% Completed | 7/7 [00:29<00:00,  3.83s/it]

Loading pt checkpoint shards: 100% Completed | 7/7 [00:29<00:00,  4.27s/it]

[2026-01-05 10:49:24] Load weight end. type=ChatGLMModel, dtype=torch.bfloat16, avail mem=49.18 GB, mem usage=11.65 GB.
[2026-01-05 10:49:24] Using KV cache dtype: torch.bfloat16
[2026-01-05 10:49:24] The available memory for KV cache is 37.01 GB.
[2026-01-05 10:49:25] KV Cache is allocated. #tokens: 1385984, K size: 18.51 GB, V size: 18.51 GB
[2026-01-05 10:49:25] Memory pool end. avail mem=11.66 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 10:49:25] max_total_num_tokens=1385984, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.66 GB
[2026-01-05 10:49:26] INFO:     Started server process [109716]
[2026-01-05 10:49:26] INFO:     Waiting for application startup.
[2026-01-05 10:49:26] INFO:     Application startup complete.
[2026-01-05 10:49:26] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 10:49:27] INFO:     127.0.0.1:52448 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 10:49:27] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 10:49:27.719688905 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 10:49:34] INFO:     127.0.0.1:52456 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:49:39] INFO:     127.0.0.1:52450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:39] The server is fired up and ready to roll!
[2026-01-05 10:49:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:49:45] INFO:     127.0.0.1:59312 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 10:49:45] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 10:49:45] INFO:     127.0.0.1:59320 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 10:49:45] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:49:45] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/ZhipuAI/chatglm2-6b --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --dtype bfloat16 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 10:49:45] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:49:45] Prefill batch, #new-seq: 5, #new-token: 768, #cached-token: 3840, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 10:49:45] Prefill batch, #new-seq: 7, #new-token: 1536, #cached-token: 5376, token usage: 0.00, #running-req: 6, #queue-req: 0,
[2026-01-05 10:49:45] Prefill batch, #new-seq: 11, #new-token: 1920, #cached-token: 8448, token usage: 0.00, #running-req: 13, #queue-req: 0,
[2026-01-05 10:49:46] Prefill batch, #new-seq: 45, #new-token: 8192, #cached-token: 34560, token usage: 0.00, #running-req: 24, #queue-req: 59,
[2026-01-05 10:49:46] Prefill batch, #new-seq: 43, #new-token: 8192, #cached-token: 32256, token usage: 0.01, #running-req: 68, #queue-req: 17,
[2026-01-05 10:49:47] Prefill batch, #new-seq: 17, #new-token: 3200, #cached-token: 13056, token usage: 0.02, #running-req: 111, #queue-req: 0,
[2026-01-05 10:49:48] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<08:58,  2.71s/it][2026-01-05 10:49:48] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:48] Decode batch, #running-req: 128, #token: 33536, token usage: 0.02, cpu graph: False, gen throughput (token/s): 82.76, #queue-req: 0,
[2026-01-05 10:49:48] INFO:     127.0.0.1:60310 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:02<04:10,  1.26s/it][2026-01-05 10:49:48] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:48] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:48] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:03<01:44,  1.87it/s][2026-01-05 10:49:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:48] INFO:     127.0.0.1:59372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:48] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 10:49:48] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:03<00:51,  3.78it/s][2026-01-05 10:49:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:48] INFO:     127.0.0.1:60144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:48] INFO:     127.0.0.1:59348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:03<00:32,  5.81it/s]
  6%|▌         | 11/200 [00:03<00:21,  8.82it/s][2026-01-05 10:49:49] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:59674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:03<00:19,  9.67it/s]
  7%|▋         | 14/200 [00:03<00:15, 11.84it/s][2026-01-05 10:49:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:03<00:12, 14.45it/s][2026-01-05 10:49:49] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] INFO:     127.0.0.1:60534 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:04<00:11, 15.19it/s]
 12%|█▏        | 24/200 [00:04<00:09, 19.52it/s]
 12%|█▏        | 24/200 [00:04<00:09, 19.52it/s][2026-01-05 10:49:49] Prefill batch, #new-seq: 3, #new-token: 640, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] Decode batch, #running-req: 128, #token: 32896, token usage: 0.02, cpu graph: False, gen throughput (token/s): 3812.14, #queue-req: 0,
[2026-01-05 10:49:49] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:04<00:09, 19.03it/s][2026-01-05 10:49:49] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:60044 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:04<00:09, 18.64it/s][2026-01-05 10:49:49] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 10:49:49] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] INFO:     127.0.0.1:59798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:49] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:59380 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:04<00:08, 19.93it/s][2026-01-05 10:49:50] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:60358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:59334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:04<00:08, 19.68it/s][2026-01-05 10:49:50] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:60510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:04<00:08, 20.17it/s]
 20%|█▉        | 39/200 [00:04<00:07, 22.52it/s][2026-01-05 10:49:50] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:60184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:59394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] INFO:     127.0.0.1:60470 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:04<00:05, 26.26it/s]
 22%|██▎       | 45/200 [00:04<00:04, 31.04it/s][2026-01-05 10:49:50] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:05<00:07, 21.41it/s][2026-01-05 10:49:50] INFO:     127.0.0.1:59908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:59464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:05<00:06, 22.54it/s]
 28%|██▊       | 56/200 [00:05<00:04, 33.54it/s]
 28%|██▊       | 56/200 [00:05<00:04, 33.54it/s]
 28%|██▊       | 56/200 [00:05<00:04, 33.54it/s][2026-01-05 10:49:50] Prefill batch, #new-seq: 4, #new-token: 768, #cached-token: 3072, token usage: 0.02, #running-req: 124, #queue-req: 0,
[2026-01-05 10:49:50] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:50] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 10:49:51] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.02, #running-req: 130, #queue-req: 0,
[2026-01-05 10:49:51] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:59990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:05<00:04, 32.28it/s]
 32%|███▏      | 63/200 [00:05<00:03, 35.61it/s]
 32%|███▏      | 63/200 [00:05<00:03, 35.61it/s][2026-01-05 10:49:51] Prefill batch, #new-seq: 3, #new-token: 640, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-05 10:49:51] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-05 10:49:51] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.02, #running-req: 130, #queue-req: 0,
[2026-01-05 10:49:51] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:05<00:03, 34.58it/s]
 34%|███▍      | 69/200 [00:05<00:03, 35.68it/s][2026-01-05 10:49:51] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 10:49:51] INFO:     127.0.0.1:59968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] Prefill batch, #new-seq: 3, #new-token: 640, #cached-token: 2304, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-05 10:49:51] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] Decode batch, #running-req: 126, #token: 33280, token usage: 0.02, cpu graph: False, gen throughput (token/s): 3093.89, #queue-req: 0,
[2026-01-05 10:49:51] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60090 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:05<00:03, 37.06it/s]
 38%|███▊      | 77/200 [00:05<00:02, 44.10it/s]
 38%|███▊      | 77/200 [00:05<00:02, 44.10it/s][2026-01-05 10:49:51] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:06<00:02, 41.78it/s][2026-01-05 10:49:51] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:59558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:06<00:02, 44.43it/s][2026-01-05 10:49:51] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:59894 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:06<00:02, 42.22it/s]
 47%|████▋     | 94/200 [00:06<00:02, 42.83it/s][2026-01-05 10:49:51] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:60056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:51] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:06<00:02, 45.09it/s]
 50%|█████     | 101/200 [00:06<00:02, 49.15it/s][2026-01-05 10:49:51] INFO:     127.0.0.1:59938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:59686 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:06<00:02, 45.66it/s][2026-01-05 10:49:52] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:60342 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:06<00:02, 43.10it/s][2026-01-05 10:49:52] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] Decode batch, #running-req: 86, #token: 26624, token usage: 0.02, cpu graph: False, gen throughput (token/s): 4641.31, #queue-req: 0,
[2026-01-05 10:49:52] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56752 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:06<00:02, 39.75it/s]
 58%|█████▊    | 117/200 [00:06<00:02, 39.70it/s][2026-01-05 10:49:52] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56668 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:07<00:02, 34.88it/s][2026-01-05 10:49:52] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:07<00:01, 36.42it/s][2026-01-05 10:49:52] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:07<00:01, 35.06it/s][2026-01-05 10:49:52] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:60438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [00:07<00:01, 35.93it/s]
 68%|██████▊   | 137/200 [00:07<00:01, 39.05it/s][2026-01-05 10:49:52] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:52] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:59882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:07<00:01, 41.16it/s]
 72%|███████▏  | 143/200 [00:07<00:01, 45.34it/s][2026-01-05 10:49:53] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:60478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] Decode batch, #running-req: 54, #token: 19072, token usage: 0.01, cpu graph: False, gen throughput (token/s): 3327.09, #queue-req: 0,
[2026-01-05 10:49:53] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:07<00:01, 39.42it/s]
 75%|███████▌  | 150/200 [00:07<00:01, 39.40it/s]
 75%|███████▌  | 150/200 [00:07<00:01, 39.40it/s][2026-01-05 10:49:53] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:07<00:01, 38.01it/s]
 78%|███████▊  | 156/200 [00:07<00:01, 39.05it/s][2026-01-05 10:49:53] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:59400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:07<00:01, 38.81it/s]
 82%|████████▏ | 163/200 [00:07<00:00, 51.03it/s]
 82%|████████▏ | 163/200 [00:07<00:00, 51.03it/s]
 82%|████████▏ | 163/200 [00:07<00:00, 51.03it/s][2026-01-05 10:49:53] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:59358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:08<00:00, 42.71it/s][2026-01-05 10:49:53] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:08<00:00, 39.13it/s][2026-01-05 10:49:53] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:53] Decode batch, #running-req: 23, #token: 9472, token usage: 0.01, cpu graph: False, gen throughput (token/s): 1792.00, #queue-req: 0,
[2026-01-05 10:49:54] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:08<00:00, 34.34it/s][2026-01-05 10:49:54] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:54] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:54] INFO:     127.0.0.1:60238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:54] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:08<00:00, 29.87it/s][2026-01-05 10:49:54] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:54] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:54] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:54] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:08<00:00, 28.00it/s][2026-01-05 10:49:54] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:54] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:54] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:09<00:00, 25.08it/s][2026-01-05 10:49:54] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:54] INFO:     127.0.0.1:56932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:54] Decode batch, #running-req: 8, #token: 4096, token usage: 0.00, cpu graph: False, gen throughput (token/s): 774.82, #queue-req: 0,
[2026-01-05 10:49:55] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:09<00:00, 14.12it/s][2026-01-05 10:49:55] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:55] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:55] Decode batch, #running-req: 5, #token: 3072, token usage: 0.00, cpu graph: False, gen throughput (token/s): 386.44, #queue-req: 0,
[2026-01-05 10:49:55] INFO:     127.0.0.1:56388 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:09<00:00, 11.08it/s][2026-01-05 10:49:55] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:55] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:10<00:00,  8.81it/s][2026-01-05 10:49:56] INFO:     127.0.0.1:56820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:49:56] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cpu graph: False, gen throughput (token/s): 187.51, #queue-req: 0,
[2026-01-05 10:49:56] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cpu graph: False, gen throughput (token/s): 59.54, #queue-req: 0,
[2026-01-05 10:49:57] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:11<00:00,  4.32it/s]
100%|██████████| 200/200 [00:11<00:00, 17.08it/s]
.
----------------------------------------------------------------------
Ran 1 test in 83.016s

OK
Accuracy: 0.300
Invalid: 0.000
Latency: 11.777 s
Output throughput: 2062.523 token/s
.
.
End (3/43):
filename='ascend/llm_models/test_ascend_chatglm2_6b.py', elapsed=94, estimated_time=400
.
.

.
.
Begin (4/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_gemma_3_4b_it_llm.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 10:50:26] INFO model_config.py:150: Multimodal is disabled for gemma3. To enable it, set --enable-multimodal.
[2026-01-05 10:50:26] INFO model_config.py:981: For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 10:50:26] INFO model_config.py:1010: Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 10:50:26] WARNING server_args.py:1247: Disable hybrid SWA memory for Gemma3ForConditionalGeneration as it is not yet supported.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:50:26] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', tokenizer_path='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=True, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=535617151, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 10:50:26] Multimodal is disabled for gemma3. To enable it, set --enable-multimodal.
[2026-01-05 10:50:26] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 10:50:26] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 10:50:28] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:50:35] Multimodal is disabled for gemma3. To enable it, set --enable-multimodal.
[2026-01-05 10:50:35] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 10:50:35] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 10:50:36] Multimodal is disabled for gemma3. To enable it, set --enable-multimodal.
[2026-01-05 10:50:36] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 10:50:36] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 10:50:36] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 10:50:37] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:50:37] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:50:38] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:50:39] Load weight begin. avail mem=60.82 GB
[2026-01-05 10:50:39] Multimodal attention backend not set. Use sdpa.
[2026-01-05 10:50:39] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:06<00:06,  6.86s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:12<00:00,  6.03s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:12<00:00,  6.15s/it]

[2026-01-05 10:50:52] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=52.79 GB, mem usage=8.03 GB.
[2026-01-05 10:50:52] Using KV cache dtype: torch.bfloat16
[2026-01-05 10:50:52] The available memory for KV cache is 40.63 GB.
[2026-01-05 10:50:53] KV Cache is allocated. #tokens: 313216, K size: 20.32 GB, V size: 20.32 GB
[2026-01-05 10:50:53] Memory pool end. avail mem=4.16 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 10:50:54] max_total_num_tokens=313216, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=1048576, available_gpu_mem=4.14 GB
[2026-01-05 10:50:56] INFO:     Started server process [112156]
[2026-01-05 10:50:56] INFO:     Waiting for application startup.
[2026-01-05 10:50:56] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 64, 'top_p': 0.95}
[2026-01-05 10:50:56] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 64, 'top_p': 0.95}
[2026-01-05 10:50:56] INFO:     Application startup complete.
[2026-01-05 10:50:56] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 10:50:57] INFO:     127.0.0.1:48630 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 10:50:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 10:50:57.975674357 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 10:51:04] INFO:     127.0.0.1:48656 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:51:14] INFO:     127.0.0.1:35906 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:51:15] INFO:     127.0.0.1:48646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:15] The server is fired up and ready to roll!
[2026-01-05 10:51:24] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:51:25] INFO:     127.0.0.1:49838 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 10:51:25] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 10:51:25] INFO:     127.0.0.1:49844 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 10:51:25] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:51:25] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/google/gemma-3-4b-it --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 10:51:25] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:51:25] Prefill batch, #new-seq: 39, #new-token: 5376, #cached-token: 29952, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 10:51:26] Prefill batch, #new-seq: 58, #new-token: 8192, #cached-token: 44544, token usage: 0.02, #running-req: 40, #queue-req: 30,
[2026-01-05 10:51:28] Prefill batch, #new-seq: 30, #new-token: 3968, #cached-token: 23040, token usage: 0.05, #running-req: 98, #queue-req: 0,
[2026-01-05 10:51:31] Decode batch, #running-req: 128, #token: 23680, token usage: 0.08, cpu graph: False, gen throughput (token/s): 70.68, #queue-req: 0,
[2026-01-05 10:51:32] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:07<23:15,  7.01s/it][2026-01-05 10:51:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:51:32] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:32] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:32] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:32] INFO:     127.0.0.1:50886 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:07<09:51,  2.99s/it]
  2%|▎         | 5/200 [00:07<00:53,  3.61it/s]
  2%|▎         | 5/200 [00:07<00:53,  3.61it/s]
  2%|▎         | 5/200 [00:07<00:53,  3.61it/s][2026-01-05 10:51:33] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.09, #running-req: 124, #queue-req: 0,
[2026-01-05 10:51:33] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:51:33] INFO:     127.0.0.1:50134 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:07<00:54,  3.51it/s][2026-01-05 10:51:33] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:33] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 10:51:34] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:51:35] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:35] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:09<01:09,  2.75it/s]
  6%|▌         | 11/200 [00:09<01:09,  2.72it/s][2026-01-05 10:51:35] INFO:     127.0.0.1:50504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:35] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:35] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:09<00:57,  3.26it/s][2026-01-05 10:51:35] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:51:35] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 129, #queue-req: 0,
[2026-01-05 10:51:35] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:35] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:09<00:57,  3.21it/s]
  8%|▊         | 15/200 [00:09<00:50,  3.69it/s][2026-01-05 10:51:35] INFO:     127.0.0.1:50050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:35] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:35] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:51:36] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:10<00:48,  3.73it/s][2026-01-05 10:51:36] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:36] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:51:36] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:36] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:36] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:36] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:10<00:34,  5.28it/s]
 11%|█         | 22/200 [00:10<00:16, 11.10it/s]
 11%|█         | 22/200 [00:10<00:16, 11.10it/s][2026-01-05 10:51:36] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:51:36] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-05 10:51:36] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:36] INFO:     127.0.0.1:51050 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:11<00:21,  8.07it/s][2026-01-05 10:51:36] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:36] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:37] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:51:37] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:37] INFO:     127.0.0.1:50372 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:11<00:26,  6.50it/s]
 14%|█▎        | 27/200 [00:11<00:27,  6.25it/s][2026-01-05 10:51:37] Decode batch, #running-req: 128, #token: 30336, token usage: 0.10, cpu graph: False, gen throughput (token/s): 896.77, #queue-req: 0,
[2026-01-05 10:51:37] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:37] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:11<00:28,  5.99it/s][2026-01-05 10:51:37] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:51:37] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:37] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:11<00:28,  5.97it/s]
 15%|█▌        | 30/200 [00:11<00:24,  7.04it/s][2026-01-05 10:51:37] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:37] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:12<00:26,  6.48it/s][2026-01-05 10:51:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:51:38] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:38] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:12<00:26,  6.34it/s]
 16%|█▋        | 33/200 [00:12<00:21,  7.65it/s][2026-01-05 10:51:38] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:38] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:12<00:30,  5.39it/s][2026-01-05 10:51:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:51:38] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:12<00:29,  5.55it/s][2026-01-05 10:51:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:51:38] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:38] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:13<00:28,  5.71it/s]
 18%|█▊        | 37/200 [00:13<00:22,  7.37it/s][2026-01-05 10:51:38] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:39] INFO:     127.0.0.1:49866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:39] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:13<00:24,  6.73it/s]
 20%|█▉        | 39/200 [00:13<00:20,  7.82it/s][2026-01-05 10:51:39] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:39] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:39] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:13<00:22,  6.99it/s]
 20%|██        | 41/200 [00:13<00:20,  7.94it/s][2026-01-05 10:51:39] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:40] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:14<01:05,  2.40it/s][2026-01-05 10:51:40] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:40] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:51:40] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:40] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 10:51:41] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:41] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:15<00:43,  3.58it/s]
 23%|██▎       | 46/200 [00:15<00:29,  5.18it/s][2026-01-05 10:51:41] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:41] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:41] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:41] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:41] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:41] INFO:     127.0.0.1:50570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:41] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.09, #running-req: 128, #queue-req: 0,

 25%|██▌       | 50/200 [00:15<00:20,  7.45it/s]
 26%|██▌       | 51/200 [00:15<00:14, 10.63it/s][2026-01-05 10:51:41] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 131, #queue-req: 0,
[2026-01-05 10:51:41] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:41] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:41] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:15<00:16,  8.81it/s]
 27%|██▋       | 54/200 [00:15<00:17,  8.45it/s][2026-01-05 10:51:41] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:41] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.09, #running-req: 125, #queue-req: 0,
[2026-01-05 10:51:41] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 128, #queue-req: 0,

 28%|██▊       | 56/200 [00:16<00:16,  8.66it/s][2026-01-05 10:51:41] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 129, #queue-req: 0,
[2026-01-05 10:51:42] INFO:     127.0.0.1:50992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:42] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:51:42] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:42] INFO:     127.0.0.1:50010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:42] INFO:     127.0.0.1:50478 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:16<00:21,  6.62it/s]
 30%|███       | 60/200 [00:16<00:20,  6.71it/s]
 30%|███       | 60/200 [00:16<00:20,  6.71it/s][2026-01-05 10:51:42] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.09, #running-req: 125, #queue-req: 0,
[2026-01-05 10:51:42] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:42] INFO:     127.0.0.1:50594 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:17<00:25,  5.54it/s]
 31%|███       | 62/200 [00:17<00:25,  5.37it/s][2026-01-05 10:51:42] INFO:     127.0.0.1:50396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:42] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:42] INFO:     127.0.0.1:50608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:42] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:42] INFO:     127.0.0.1:50846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:42] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 128, #queue-req: 0,

 32%|███▎      | 65/200 [00:17<00:19,  7.05it/s]
 33%|███▎      | 66/200 [00:17<00:13,  9.69it/s][2026-01-05 10:51:43] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.09, #running-req: 130, #queue-req: 0,
[2026-01-05 10:51:43] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:43] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:51:43] INFO:     127.0.0.1:49960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:43] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:17<00:19,  6.87it/s]
 34%|███▍      | 69/200 [00:17<00:21,  6.18it/s][2026-01-05 10:51:43] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:43] INFO:     127.0.0.1:42858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:43] INFO:     127.0.0.1:42876 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:17<00:21,  6.09it/s]
 36%|███▌      | 71/200 [00:17<00:18,  6.95it/s][2026-01-05 10:51:43] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 10:51:43] INFO:     127.0.0.1:50870 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:18<00:19,  6.60it/s][2026-01-05 10:51:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:51:44] INFO:     127.0.0.1:50738 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:18<00:19,  6.60it/s][2026-01-05 10:51:44] Decode batch, #running-req: 127, #token: 28288, token usage: 0.09, cpu graph: False, gen throughput (token/s): 747.21, #queue-req: 0,
[2026-01-05 10:51:44] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:44] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:44] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:18<00:18,  6.84it/s]
 38%|███▊      | 76/200 [00:18<00:10, 12.14it/s]
 38%|███▊      | 76/200 [00:18<00:10, 12.14it/s][2026-01-05 10:51:44] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:44] INFO:     127.0.0.1:42794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:44] INFO:     127.0.0.1:42860 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:18<00:10, 11.58it/s]
 40%|███▉      | 79/200 [00:18<00:09, 12.90it/s][2026-01-05 10:51:44] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:44] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:18<00:08, 13.54it/s][2026-01-05 10:51:44] INFO:     127.0.0.1:50120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:44] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:44] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:44] INFO:     127.0.0.1:50748 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:18<00:08, 14.03it/s]
 42%|████▎     | 85/200 [00:18<00:05, 20.45it/s]
 42%|████▎     | 85/200 [00:18<00:05, 20.45it/s][2026-01-05 10:51:44] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:44] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:44] INFO:     127.0.0.1:50524 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:19<00:05, 18.73it/s][2026-01-05 10:51:44] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:44] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:19<00:06, 17.92it/s][2026-01-05 10:51:45] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:19<00:07, 15.24it/s]
 47%|████▋     | 94/200 [00:19<00:06, 17.16it/s]
 47%|████▋     | 94/200 [00:19<00:06, 17.16it/s][2026-01-05 10:51:45] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:42918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:19<00:05, 18.42it/s][2026-01-05 10:51:45] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:19<00:05, 17.75it/s]
 50%|█████     | 101/200 [00:19<00:05, 19.14it/s][2026-01-05 10:51:45] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:19<00:03, 23.52it/s][2026-01-05 10:51:45] INFO:     127.0.0.1:50126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:20<00:04, 20.82it/s][2026-01-05 10:51:45] INFO:     127.0.0.1:50266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:45] INFO:     127.0.0.1:42930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:46] INFO:     127.0.0.1:50488 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:20<00:04, 19.17it/s][2026-01-05 10:51:46] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:46] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:46] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:20<00:04, 20.27it/s][2026-01-05 10:51:46] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:46] INFO:     127.0.0.1:50278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:46] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:20<00:05, 14.08it/s][2026-01-05 10:51:46] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:46] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:46] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:20<00:05, 14.42it/s]
 60%|██████    | 121/200 [00:20<00:04, 16.44it/s][2026-01-05 10:51:46] Decode batch, #running-req: 82, #token: 20480, token usage: 0.07, cpu graph: False, gen throughput (token/s): 1635.71, #queue-req: 0,
[2026-01-05 10:51:46] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:46] INFO:     127.0.0.1:50312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:46] INFO:     127.0.0.1:50372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:46] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:46] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:21<00:03, 19.69it/s][2026-01-05 10:51:47] INFO:     127.0.0.1:50384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:47] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:47] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:21<00:04, 16.81it/s][2026-01-05 10:51:47] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:47] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:47] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:47] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:47] INFO:     127.0.0.1:50618 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:21<00:03, 18.21it/s]
 67%|██████▋   | 134/200 [00:21<00:02, 25.67it/s]
 67%|██████▋   | 134/200 [00:21<00:02, 25.67it/s][2026-01-05 10:51:47] INFO:     127.0.0.1:50434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:47] INFO:     127.0.0.1:42940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:47] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:21<00:04, 15.38it/s][2026-01-05 10:51:47] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:47] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:47] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:22<00:04, 13.32it/s][2026-01-05 10:51:48] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:48] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:22<00:04, 12.71it/s][2026-01-05 10:51:48] INFO:     127.0.0.1:50072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:48] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:22<00:04, 12.23it/s][2026-01-05 10:51:48] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:48] INFO:     127.0.0.1:50520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:48] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:22<00:04, 13.25it/s][2026-01-05 10:51:48] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:48] INFO:     127.0.0.1:50252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:48] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:48] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:23<00:03, 14.23it/s][2026-01-05 10:51:48] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:48] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:23<00:03, 13.30it/s][2026-01-05 10:51:49] Decode batch, #running-req: 47, #token: 14976, token usage: 0.05, cpu graph: False, gen throughput (token/s): 981.17, #queue-req: 0,
[2026-01-05 10:51:49] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:49] INFO:     127.0.0.1:42826 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:23<00:05,  8.72it/s][2026-01-05 10:51:49] INFO:     127.0.0.1:42926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:49] INFO:     127.0.0.1:50788 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:23<00:04,  9.86it/s][2026-01-05 10:51:49] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:49] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:24<00:04,  9.35it/s][2026-01-05 10:51:50] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:50] INFO:     127.0.0.1:49870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:50] INFO:     127.0.0.1:50866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:50] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:24<00:06,  6.21it/s]
 82%|████████▏ | 163/200 [00:24<00:06,  5.99it/s]
 82%|████████▏ | 163/200 [00:24<00:06,  5.99it/s][2026-01-05 10:51:50] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:50] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:24<00:04,  7.00it/s][2026-01-05 10:51:50] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:50] INFO:     127.0.0.1:50354 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:25<00:04,  6.89it/s][2026-01-05 10:51:51] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:51] INFO:     127.0.0.1:50554 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:25<00:05,  5.35it/s]
 84%|████████▍ | 169/200 [00:25<00:06,  5.14it/s][2026-01-05 10:51:51] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:51] INFO:     127.0.0.1:49916 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:25<00:04,  6.10it/s][2026-01-05 10:51:51] Decode batch, #running-req: 29, #token: 10880, token usage: 0.03, cpu graph: False, gen throughput (token/s): 625.04, #queue-req: 0,
[2026-01-05 10:51:51] INFO:     127.0.0.1:50636 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:25<00:04,  5.62it/s][2026-01-05 10:51:52] INFO:     127.0.0.1:50348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:52] INFO:     127.0.0.1:50710 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:26<00:05,  4.92it/s]
 87%|████████▋ | 174/200 [00:26<00:04,  5.41it/s][2026-01-05 10:51:52] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:26<00:04,  5.43it/s][2026-01-05 10:51:52] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:26<00:04,  5.05it/s][2026-01-05 10:51:52] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:52] INFO:     127.0.0.1:42886 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:26<00:05,  4.45it/s]
 89%|████████▉ | 178/200 [00:26<00:04,  5.12it/s][2026-01-05 10:51:52] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:27<00:04,  5.20it/s][2026-01-05 10:51:53] INFO:     127.0.0.1:50650 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:27<00:03,  5.27it/s][2026-01-05 10:51:53] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:27<00:03,  5.32it/s][2026-01-05 10:51:53] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:53] INFO:     127.0.0.1:50750 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:27<00:02,  6.80it/s][2026-01-05 10:51:53] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:27<00:02,  7.08it/s][2026-01-05 10:51:53] INFO:     127.0.0.1:50346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:51:53] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:28<00:02,  6.20it/s][2026-01-05 10:51:54] Decode batch, #running-req: 14, #token: 6400, token usage: 0.02, cpu graph: False, gen throughput (token/s): 356.76, #queue-req: 0,
[2026-01-05 10:51:54] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:28<00:03,  3.93it/s][2026-01-05 10:51:56] Decode batch, #running-req: 13, #token: 6912, token usage: 0.02, cpu graph: False, gen throughput (token/s): 204.59, #queue-req: 0,
[2026-01-05 10:51:58] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:32<00:13,  1.11s/it][2026-01-05 10:51:59] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:33<00:10,  1.01it/s][2026-01-05 10:51:59] Decode batch, #running-req: 11, #token: 6400, token usage: 0.02, cpu graph: False, gen throughput (token/s): 195.72, #queue-req: 0,
[2026-01-05 10:51:59] INFO:     127.0.0.1:50586 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:33<00:08,  1.23it/s][2026-01-05 10:51:59] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:33<00:05,  1.52it/s][2026-01-05 10:52:00] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:35<00:06,  1.15it/s][2026-01-05 10:52:01] Decode batch, #running-req: 8, #token: 4992, token usage: 0.02, cpu graph: False, gen throughput (token/s): 141.40, #queue-req: 0,
[2026-01-05 10:52:04] Decode batch, #running-req: 8, #token: 5760, token usage: 0.02, cpu graph: False, gen throughput (token/s): 127.48, #queue-req: 0,
[2026-01-05 10:52:06] Decode batch, #running-req: 8, #token: 5888, token usage: 0.02, cpu graph: False, gen throughput (token/s): 129.12, #queue-req: 0,
[2026-01-05 10:52:09] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:52:09] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:52:09] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:52:09] Decode batch, #running-req: 8, #token: 2048, token usage: 0.01, cpu graph: False, gen throughput (token/s): 129.89, #queue-req: 0,
[2026-01-05 10:52:09] INFO:     127.0.0.1:50272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:52:09] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:52:09] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:43<00:20,  3.00s/it]
 99%|█████████▉| 198/200 [00:43<00:03,  1.82s/it]
 99%|█████████▉| 198/200 [00:43<00:03,  1.82s/it]
 99%|█████████▉| 198/200 [00:43<00:03,  1.82s/it]
 99%|█████████▉| 198/200 [00:43<00:03,  1.82s/it]
 99%|█████████▉| 198/200 [00:43<00:03,  1.82s/it][2026-01-05 10:52:11] Decode batch, #running-req: 2, #token: 2176, token usage: 0.01, cpu graph: False, gen throughput (token/s): 35.10, #queue-req: 0,
[2026-01-05 10:52:13] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:47<00:02,  2.02s/it][2026-01-05 10:52:13] INFO:     127.0.0.1:50332 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:47<00:00,  1.81s/it]
100%|██████████| 200/200 [00:47<00:00,  4.19it/s]
.
----------------------------------------------------------------------
Ran 1 test in 119.086s

OK
Accuracy: 0.710
Invalid: 0.000
Latency: 47.786 s
Output throughput: 535.933 token/s
.
.
End (4/43):
filename='ascend/llm_models/test_ascend_gemma_3_4b_it_llm.py', elapsed=136, estimated_time=400
.
.

.
.
Begin (5/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_smollm_1_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 10:52:34] INFO model_config.py:1010: Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:52:34] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/HuggingFaceTB/SmolLM-1.7B', tokenizer_path='/root/.cache/modelscope/hub/models/HuggingFaceTB/SmolLM-1.7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=1070887449, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/HuggingFaceTB/SmolLM-1.7B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 10:52:34] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 10:52:34] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:52:43] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:52:43] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 10:52:43] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 10:52:44] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:52:44] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:52:45] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:52:45] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:07<00:07,  7.07s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.43s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.83s/it]

[2026-01-05 10:52:55] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=57.62 GB, mem usage=3.21 GB.
[2026-01-05 10:52:55] Using KV cache dtype: torch.bfloat16
[2026-01-05 10:52:55] The available memory for KV cache is 45.45 GB.
[2026-01-05 10:52:55] KV Cache is allocated. #tokens: 248192, K size: 22.73 GB, V size: 22.73 GB
[2026-01-05 10:52:55] Memory pool end. avail mem=12.13 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 10:52:56] max_total_num_tokens=248192, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=2048, available_gpu_mem=12.11 GB
[2026-01-05 10:52:56] INFO:     Started server process [115006]
[2026-01-05 10:52:56] INFO:     Waiting for application startup.
[2026-01-05 10:52:56] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:52:56] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:52:56] INFO:     Application startup complete.
[2026-01-05 10:52:56] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 10:52:57] INFO:     127.0.0.1:59410 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 10:52:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 10:52:57.093739339 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 10:53:05] INFO:     127.0.0.1:59436 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:53:15] INFO:     127.0.0.1:58320 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:53:16] INFO:     127.0.0.1:59420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:16] The server is fired up and ready to roll!
[2026-01-05 10:53:25] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:53:26] INFO:     127.0.0.1:50970 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 10:53:26] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 10:53:26] INFO:     127.0.0.1:50982 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 10:53:26] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:53:26] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/HuggingFaceTB/SmolLM-1.7B --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --dtype bfloat16 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 10:53:26] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:53:26] Prefill batch, #new-seq: 8, #new-token: 1280, #cached-token: 6144, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 10:53:26] Prefill batch, #new-seq: 12, #new-token: 1664, #cached-token: 9216, token usage: 0.01, #running-req: 9, #queue-req: 0,
[2026-01-05 10:53:26] Prefill batch, #new-seq: 17, #new-token: 2176, #cached-token: 13056, token usage: 0.02, #running-req: 21, #queue-req: 0,
[2026-01-05 10:53:27] Prefill batch, #new-seq: 54, #new-token: 8192, #cached-token: 41472, token usage: 0.02, #running-req: 38, #queue-req: 36,
[2026-01-05 10:53:27] Prefill batch, #new-seq: 37, #new-token: 5632, #cached-token: 27648, token usage: 0.06, #running-req: 91, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<09:49,  2.96s/it][2026-01-05 10:53:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:03<02:50,  1.16it/s][2026-01-05 10:53:29] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] INFO:     127.0.0.1:51042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] INFO:     127.0.0.1:46994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] INFO:     127.0.0.1:47150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:29] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:03<00:42,  4.45it/s]
  5%|▌         | 10/200 [00:03<00:20,  9.11it/s][2026-01-05 10:53:29] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 10:53:29] Decode batch, #running-req: 126, #token: 25344, token usage: 0.10, cpu graph: False, gen throughput (token/s): 86.44, #queue-req: 0,
[2026-01-05 10:53:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:47370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:51258 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:03<00:16, 11.48it/s][2026-01-05 10:53:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] INFO:     127.0.0.1:51222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:47148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] INFO:     127.0.0.1:47166 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:03<00:12, 14.65it/s][2026-01-05 10:53:29] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:47228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:51136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:29] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:51244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:03<00:13, 13.69it/s]
 12%|█▏        | 23/200 [00:03<00:12, 14.09it/s][2026-01-05 10:53:30] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:46878 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:04<00:11, 15.20it/s]
 14%|█▎        | 27/200 [00:04<00:09, 17.60it/s][2026-01-05 10:53:30] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:47314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 131, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:04<00:07, 21.66it/s]
 16%|█▋        | 33/200 [00:04<00:06, 26.91it/s][2026-01-05 10:53:30] INFO:     127.0.0.1:46966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:47316 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:04<00:05, 29.14it/s][2026-01-05 10:53:30] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:04<00:05, 29.57it/s][2026-01-05 10:53:30] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:47174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:46874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:47172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:47358 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:04<00:05, 29.19it/s]
 24%|██▎       | 47/200 [00:04<00:04, 30.78it/s][2026-01-05 10:53:30] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:47354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Decode batch, #running-req: 128, #token: 29824, token usage: 0.12, cpu graph: False, gen throughput (token/s): 3727.68, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:04<00:04, 30.70it/s][2026-01-05 10:53:30] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.12, #running-req: 125, #queue-req: 0,
[2026-01-05 10:53:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:30] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:51282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:47276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:30] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.12, #running-req: 125, #queue-req: 0,

 28%|██▊       | 55/200 [00:04<00:04, 31.94it/s]
 28%|██▊       | 56/200 [00:04<00:04, 35.22it/s][2026-01-05 10:53:30] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:31] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 10:53:31] INFO:     127.0.0.1:47404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,

 30%|███       | 60/200 [00:05<00:04, 33.46it/s]
 30%|███       | 61/200 [00:05<00:04, 34.48it/s][2026-01-05 10:53:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-05 10:53:31] INFO:     127.0.0.1:47526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:31] INFO:     127.0.0.1:51310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-05 10:53:31] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:05<00:04, 27.96it/s][2026-01-05 10:53:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:31] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:53:31] INFO:     127.0.0.1:47880 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:05<00:04, 29.63it/s]
 35%|███▌      | 70/200 [00:05<00:03, 33.20it/s][2026-01-05 10:53:31] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 10:53:31] INFO:     127.0.0.1:47028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47752 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:05<00:03, 33.82it/s][2026-01-05 10:53:31] INFO:     127.0.0.1:51150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 124, #queue-req: 0,
[2026-01-05 10:53:31] INFO:     127.0.0.1:51394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47680 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:05<00:03, 34.49it/s][2026-01-05 10:53:31] INFO:     127.0.0.1:47272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:51290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:51242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:51424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47030 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:05<00:03, 33.65it/s]
 42%|████▎     | 85/200 [00:05<00:02, 38.39it/s]
 42%|████▎     | 85/200 [00:05<00:02, 38.39it/s][2026-01-05 10:53:31] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:51036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:05<00:02, 44.63it/s][2026-01-05 10:53:31] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:31] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] Decode batch, #running-req: 102, #token: 25088, token usage: 0.10, cpu graph: False, gen throughput (token/s): 4042.66, #queue-req: 0,
[2026-01-05 10:53:32] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47924 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [00:05<00:02, 49.19it/s]
 51%|█████     | 102/200 [00:05<00:01, 54.66it/s][2026-01-05 10:53:32] INFO:     127.0.0.1:47584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:46908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47210 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:06<00:01, 49.58it/s][2026-01-05 10:53:32] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:48170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:46978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:46918 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:06<00:02, 42.84it/s][2026-01-05 10:53:32] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:46976 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:06<00:01, 47.64it/s][2026-01-05 10:53:32] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:48200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47884 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:06<00:01, 47.36it/s][2026-01-05 10:53:32] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:48122 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:06<00:01, 43.42it/s][2026-01-05 10:53:32] INFO:     127.0.0.1:46940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:47796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] Decode batch, #running-req: 63, #token: 18432, token usage: 0.07, cpu graph: False, gen throughput (token/s): 3854.16, #queue-req: 0,
[2026-01-05 10:53:32] INFO:     127.0.0.1:47132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:48110 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:06<00:01, 44.88it/s]
 70%|███████   | 141/200 [00:06<00:01, 47.75it/s][2026-01-05 10:53:32] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:32] INFO:     127.0.0.1:48182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:47394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:47722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:47998 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:06<00:01, 49.74it/s][2026-01-05 10:53:33] INFO:     127.0.0.1:47850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:47940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:51044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:47708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:48016 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:07<00:01, 35.54it/s][2026-01-05 10:53:33] INFO:     127.0.0.1:48134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:47512 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:07<00:01, 32.93it/s][2026-01-05 10:53:33] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] Decode batch, #running-req: 41, #token: 14720, token usage: 0.06, cpu graph: False, gen throughput (token/s): 2567.96, #queue-req: 0,
[2026-01-05 10:53:33] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:48068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:33] INFO:     127.0.0.1:48142 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:07<00:01, 22.71it/s][2026-01-05 10:53:34] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:34] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:34] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:08<00:01, 21.55it/s][2026-01-05 10:53:34] Decode batch, #running-req: 35, #token: 13952, token usage: 0.06, cpu graph: False, gen throughput (token/s): 2233.84, #queue-req: 0,
[2026-01-05 10:53:34] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:34] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:34] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:08<00:02, 11.43it/s][2026-01-05 10:53:35] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:35] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:08<00:02, 11.33it/s][2026-01-05 10:53:35] Decode batch, #running-req: 32, #token: 13056, token usage: 0.05, cpu graph: False, gen throughput (token/s): 2027.57, #queue-req: 0,
[2026-01-05 10:53:35] Decode batch, #running-req: 30, #token: 14976, token usage: 0.06, cpu graph: False, gen throughput (token/s): 1824.30, #queue-req: 0,
[2026-01-05 10:53:35] INFO:     127.0.0.1:47760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:36] Decode batch, #running-req: 29, #token: 15232, token usage: 0.06, cpu graph: False, gen throughput (token/s): 1747.76, #queue-req: 0,
[2026-01-05 10:53:37] Decode batch, #running-req: 29, #token: 16256, token usage: 0.07, cpu graph: False, gen throughput (token/s): 1729.03, #queue-req: 0,
[2026-01-05 10:53:37] Decode batch, #running-req: 29, #token: 17920, token usage: 0.07, cpu graph: False, gen throughput (token/s): 1728.38, #queue-req: 0,
[2026-01-05 10:53:38] Decode batch, #running-req: 29, #token: 18816, token usage: 0.08, cpu graph: False, gen throughput (token/s): 1729.86, #queue-req: 0,
[2026-01-05 10:53:38] INFO:     127.0.0.1:47974 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:12<00:12,  2.33it/s][2026-01-05 10:53:39] INFO:     127.0.0.1:51028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] Decode batch, #running-req: 28, #token: 7168, token usage: 0.03, cpu graph: False, gen throughput (token/s): 1674.02, #queue-req: 0,
[2026-01-05 10:53:39] INFO:     127.0.0.1:51270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:12<00:09,  2.66it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s]
 95%|█████████▌| 190/200 [00:12<00:00, 43.47it/s][2026-01-05 10:53:39] INFO:     127.0.0.1:47562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47652 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] Decode batch, #running-req: 7, #token: 5376, token usage: 0.02, cpu graph: False, gen throughput (token/s): 601.23, #queue-req: 0,
[2026-01-05 10:53:39] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:39] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:13<00:00, 23.56it/s][2026-01-05 10:53:40] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:40] INFO:     127.0.0.1:47958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:40] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:53:40] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:14<00:00, 21.00it/s][2026-01-05 10:53:40] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, cpu graph: False, gen throughput (token/s): 314.02, #queue-req: 0,
[2026-01-05 10:53:40] INFO:     127.0.0.1:48096 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:14<00:00, 13.98it/s]
.
----------------------------------------------------------------------
Ran 1 test in 75.617s

OK
Accuracy: 0.105
Invalid: 0.005
Latency: 14.349 s
Output throughput: 2068.447 token/s
.
.
End (5/43):
filename='ascend/llm_models/test_ascend_smollm_1_7b.py', elapsed=87, estimated_time=400
.
.

.
.
Begin (6/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_exaone_3.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 10:54:01] INFO model_config.py:1010: Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:54:01] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=1046042340, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 10:54:01] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 10:54:01] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:54:10] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 10:54:10] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 10:54:10] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 10:54:11] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:54:11] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:54:12] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:54:12] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:07<00:42,  7.08s/it]

Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:13<00:34,  6.83s/it]

Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:20<00:26,  6.66s/it]

Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:26<00:19,  6.55s/it]

Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:28<00:10,  5.03s/it]

Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:35<00:05,  5.56s/it]

Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:41<00:00,  5.87s/it]

Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:41<00:00,  6.00s/it]

[2026-01-05 10:54:55] Load weight end. type=ExaoneForCausalLM, dtype=torch.bfloat16, avail mem=46.23 GB, mem usage=14.59 GB.
[2026-01-05 10:54:55] Using KV cache dtype: torch.bfloat16
[2026-01-05 10:54:55] The available memory for KV cache is 34.06 GB.
[2026-01-05 10:54:55] KV Cache is allocated. #tokens: 279040, K size: 17.04 GB, V size: 17.04 GB
[2026-01-05 10:54:55] Memory pool end. avail mem=11.66 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 10:54:55] max_total_num_tokens=279040, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.66 GB
[2026-01-05 10:54:56] INFO:     Started server process [117906]
[2026-01-05 10:54:56] INFO:     Waiting for application startup.
[2026-01-05 10:54:56] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:54:56] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:54:56] INFO:     Application startup complete.
[2026-01-05 10:54:56] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 10:54:57] INFO:     127.0.0.1:35878 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 10:54:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 10:54:57.108179671 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 10:55:01] INFO:     127.0.0.1:35896 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:55:11] INFO:     127.0.0.1:43984 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:55:16] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:16] The server is fired up and ready to roll!
[2026-01-05 10:55:21] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:55:22] INFO:     127.0.0.1:34522 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 10:55:22] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 10:55:22] INFO:     127.0.0.1:34536 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 10:55:22] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:55:22] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --dtype bfloat16 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 10:55:23] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:55:23] Prefill batch, #new-seq: 9, #new-token: 1408, #cached-token: 6912, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 10:55:25] Prefill batch, #new-seq: 56, #new-token: 8192, #cached-token: 43008, token usage: 0.01, #running-req: 10, #queue-req: 62,
[2026-01-05 10:55:26] Prefill batch, #new-seq: 53, #new-token: 8192, #cached-token: 40704, token usage: 0.04, #running-req: 66, #queue-req: 9,
[2026-01-05 10:55:26] Prefill batch, #new-seq: 9, #new-token: 1408, #cached-token: 6912, token usage: 0.07, #running-req: 119, #queue-req: 0,
[2026-01-05 10:55:28] Decode batch, #running-req: 128, #token: 27520, token usage: 0.10, cpu graph: False, gen throughput (token/s): 51.27, #queue-req: 0,
[2026-01-05 10:55:28] INFO:     127.0.0.1:34774 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:05<17:13,  5.19s/it][2026-01-05 10:55:28] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:28] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:28] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:05<07:49,  2.37s/it]
  2%|▏         | 3/200 [00:05<03:18,  1.01s/it][2026-01-05 10:55:28] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-05 10:55:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:28] INFO:     127.0.0.1:35018 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:05<01:55,  1.69it/s][2026-01-05 10:55:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:28] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:05<01:30,  2.13it/s][2026-01-05 10:55:28] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:28] INFO:     127.0.0.1:34846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:29] INFO:     127.0.0.1:35076 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:06<00:56,  3.39it/s][2026-01-05 10:55:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:29] INFO:     127.0.0.1:35004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] INFO:     127.0.0.1:35156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,

  5%|▌         | 10/200 [00:06<00:42,  4.44it/s][2026-01-05 10:55:29] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] INFO:     127.0.0.1:35314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-05 10:55:29] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:06<00:27,  6.87it/s][2026-01-05 10:55:29] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:29] INFO:     127.0.0.1:35642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:29] Decode batch, #running-req: 128, #token: 31872, token usage: 0.11, cpu graph: False, gen throughput (token/s): 3551.69, #queue-req: 0,
[2026-01-05 10:55:29] INFO:     127.0.0.1:35264 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:06<00:20,  9.05it/s][2026-01-05 10:55:29] INFO:     127.0.0.1:35098 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] INFO:     127.0.0.1:35628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:29] Prefill batch, #new-seq: 3, #new-token: 640, #cached-token: 2304, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:29] INFO:     127.0.0.1:35234 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:06<00:14, 12.51it/s][2026-01-05 10:55:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:29] INFO:     127.0.0.1:35222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:29] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:06<00:14, 12.36it/s][2026-01-05 10:55:29] INFO:     127.0.0.1:35552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:29] INFO:     127.0.0.1:35178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:29] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-05 10:55:30] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:07<00:12, 13.74it/s]
 13%|█▎        | 26/200 [00:07<00:10, 16.39it/s][2026-01-05 10:55:30] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-05 10:55:30] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] INFO:     127.0.0.1:35602 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] INFO:     127.0.0.1:35294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 130, #queue-req: 0,
[2026-01-05 10:55:30] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:07<00:09, 18.42it/s]
 16%|█▌        | 31/200 [00:07<00:07, 21.60it/s][2026-01-05 10:55:30] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-05 10:55:30] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] INFO:     127.0.0.1:35518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-05 10:55:30] INFO:     127.0.0.1:34970 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:07<00:08, 19.07it/s][2026-01-05 10:55:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:30] INFO:     127.0.0.1:34812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] INFO:     127.0.0.1:34978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] INFO:     127.0.0.1:34730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 131, #queue-req: 0,
[2026-01-05 10:55:30] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:07<00:07, 21.56it/s]
 20%|██        | 41/200 [00:07<00:06, 25.00it/s][2026-01-05 10:55:30] INFO:     127.0.0.1:34622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-05 10:55:30] INFO:     127.0.0.1:34876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:30] INFO:     127.0.0.1:34608 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:07<00:06, 24.11it/s][2026-01-05 10:55:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:30] INFO:     127.0.0.1:34808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] INFO:     127.0.0.1:35146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:30] INFO:     127.0.0.1:57140 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:07<00:06, 22.34it/s][2026-01-05 10:55:31] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:35236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 130, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:35042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:08<00:08, 17.56it/s][2026-01-05 10:55:31] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:31] Decode batch, #running-req: 127, #token: 31744, token usage: 0.11, cpu graph: False, gen throughput (token/s): 2869.12, #queue-req: 0,
[2026-01-05 10:55:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:34944 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:08<00:09, 16.22it/s][2026-01-05 10:55:31] INFO:     127.0.0.1:34950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:08<00:08, 16.86it/s][2026-01-05 10:55:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:34668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] INFO:     127.0.0.1:35138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:08<00:09, 15.42it/s]
 30%|██▉       | 59/200 [00:08<00:07, 18.26it/s]
 30%|██▉       | 59/200 [00:08<00:07, 18.26it/s][2026-01-05 10:55:31] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.12, #running-req: 125, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] INFO:     127.0.0.1:34924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 125, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:34984 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:08<00:06, 21.62it/s][2026-01-05 10:55:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:34868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:31] INFO:     127.0.0.1:34906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:31] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:08<00:05, 22.61it/s][2026-01-05 10:55:32] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 10:55:32] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:35512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 10:55:32] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:35536 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:09<00:07, 17.04it/s]
 35%|███▌      | 70/200 [00:09<00:08, 15.80it/s][2026-01-05 10:55:32] INFO:     127.0.0.1:34796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 10:55:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 10:55:32] INFO:     127.0.0.1:35104 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:09<00:08, 15.55it/s][2026-01-05 10:55:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 10:55:32] INFO:     127.0.0.1:35116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:35062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:09<00:07, 17.83it/s][2026-01-05 10:55:32] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:34982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:35598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:35404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:34960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:35506 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:09<00:05, 23.56it/s]
 40%|████      | 81/200 [00:09<00:03, 30.42it/s][2026-01-05 10:55:32] INFO:     127.0.0.1:34858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:35038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:34790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:09<00:04, 28.02it/s]
 43%|████▎     | 86/200 [00:09<00:04, 28.41it/s][2026-01-05 10:55:32] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] Decode batch, #running-req: 113, #token: 31616, token usage: 0.11, cpu graph: False, gen throughput (token/s): 3158.38, #queue-req: 0,
[2026-01-05 10:55:32] INFO:     127.0.0.1:34640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:32] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:09<00:04, 26.91it/s][2026-01-05 10:55:33] INFO:     127.0.0.1:34574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:34962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:35308 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:10<00:04, 21.56it/s]
 47%|████▋     | 94/200 [00:10<00:05, 20.03it/s][2026-01-05 10:55:33] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:10<00:05, 17.81it/s][2026-01-05 10:55:33] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:35318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:57480 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:10<00:05, 19.44it/s]
 51%|█████     | 102/200 [00:10<00:03, 27.73it/s]
 51%|█████     | 102/200 [00:10<00:03, 27.73it/s][2026-01-05 10:55:33] INFO:     127.0.0.1:34822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:35244 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:10<00:03, 25.33it/s][2026-01-05 10:55:33] INFO:     127.0.0.1:35024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:35214 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:10<00:03, 27.95it/s][2026-01-05 10:55:33] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:33] INFO:     127.0.0.1:35300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] Decode batch, #running-req: 89, #token: 27776, token usage: 0.10, cpu graph: False, gen throughput (token/s): 3633.21, #queue-req: 0,
[2026-01-05 10:55:34] INFO:     127.0.0.1:35352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57278 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:11<00:03, 25.55it/s][2026-01-05 10:55:34] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:11<00:02, 32.43it/s][2026-01-05 10:55:34] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:35582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [00:11<00:02, 28.66it/s][2026-01-05 10:55:34] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:35240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:11<00:01, 35.24it/s][2026-01-05 10:55:34] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:35434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:11<00:02, 26.76it/s][2026-01-05 10:55:34] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:35620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:34] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:11<00:02, 21.80it/s]
 70%|██████▉   | 139/200 [00:11<00:02, 20.45it/s][2026-01-05 10:55:35] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] Decode batch, #running-req: 61, #token: 21888, token usage: 0.08, cpu graph: False, gen throughput (token/s): 2804.31, #queue-req: 0,
[2026-01-05 10:55:35] INFO:     127.0.0.1:57760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:12<00:02, 21.16it/s][2026-01-05 10:55:35] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:12<00:02, 21.76it/s][2026-01-05 10:55:35] INFO:     127.0.0.1:35364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] INFO:     127.0.0.1:34988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] INFO:     127.0.0.1:34986 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:12<00:02, 22.49it/s][2026-01-05 10:55:35] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] INFO:     127.0.0.1:34792 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:12<00:02, 18.91it/s][2026-01-05 10:55:35] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] INFO:     127.0.0.1:35494 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:12<00:02, 20.33it/s][2026-01-05 10:55:35] INFO:     127.0.0.1:57234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:35] Decode batch, #running-req: 44, #token: 17792, token usage: 0.06, cpu graph: False, gen throughput (token/s): 2135.51, #queue-req: 0,
[2026-01-05 10:55:36] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:13<00:02, 14.58it/s][2026-01-05 10:55:36] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:36] INFO:     127.0.0.1:35566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:36] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:36] INFO:     127.0.0.1:35424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:36] INFO:     127.0.0.1:57396 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:13<00:01, 19.06it/s][2026-01-05 10:55:36] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:36] INFO:     127.0.0.1:35056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:36] INFO:     127.0.0.1:35190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:36] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:13<00:02, 12.51it/s]
 84%|████████▎ | 167/200 [00:13<00:03, 10.71it/s][2026-01-05 10:55:36] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:36] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:13<00:02, 11.37it/s][2026-01-05 10:55:36] Decode batch, #running-req: 32, #token: 14592, token usage: 0.05, cpu graph: False, gen throughput (token/s): 1658.42, #queue-req: 0,
[2026-01-05 10:55:36] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:36] INFO:     127.0.0.1:34958 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:13<00:02, 11.80it/s][2026-01-05 10:55:37] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:37] INFO:     127.0.0.1:34892 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:14<00:02, 11.53it/s][2026-01-05 10:55:37] INFO:     127.0.0.1:57710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:37] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:37] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:14<00:02, 10.66it/s][2026-01-05 10:55:37] Decode batch, #running-req: 24, #token: 11648, token usage: 0.04, cpu graph: False, gen throughput (token/s): 1316.85, #queue-req: 0,
[2026-01-05 10:55:37] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:37] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:14<00:02,  9.97it/s][2026-01-05 10:55:37] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:37] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:14<00:01, 10.50it/s][2026-01-05 10:55:38] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:38] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:15<00:01, 11.28it/s][2026-01-05 10:55:38] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:38] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:15<00:01,  9.75it/s][2026-01-05 10:55:38] Decode batch, #running-req: 16, #token: 9216, token usage: 0.03, cpu graph: False, gen throughput (token/s): 955.68, #queue-req: 0,
[2026-01-05 10:55:38] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:38] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:15<00:01,  7.12it/s][2026-01-05 10:55:38] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:16<00:01,  6.66it/s][2026-01-05 10:55:39] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:39] INFO:     127.0.0.1:35448 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:16<00:01,  7.14it/s][2026-01-05 10:55:39] Decode batch, #running-req: 11, #token: 7040, token usage: 0.03, cpu graph: False, gen throughput (token/s): 701.20, #queue-req: 0,
[2026-01-05 10:55:40] Decode batch, #running-req: 11, #token: 7296, token usage: 0.03, cpu graph: False, gen throughput (token/s): 567.12, #queue-req: 0,
[2026-01-05 10:55:40] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:17<00:02,  3.50it/s][2026-01-05 10:55:40] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:40] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:40] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:17<00:01,  5.76it/s][2026-01-05 10:55:40] INFO:     127.0.0.1:57374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:40] Decode batch, #running-req: 6, #token: 2944, token usage: 0.01, cpu graph: False, gen throughput (token/s): 371.29, #queue-req: 0,
[2026-01-05 10:55:40] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:40] INFO:     127.0.0.1:34744 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:17<00:01,  4.77it/s]
 98%|█████████▊| 196/200 [00:17<00:00,  4.90it/s][2026-01-05 10:55:41] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:18<00:00,  4.29it/s][2026-01-05 10:55:41] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:55:41] Decode batch, #running-req: 2, #token: 2048, token usage: 0.01, cpu graph: False, gen throughput (token/s): 163.29, #queue-req: 0,
[2026-01-05 10:55:41] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:18<00:00,  3.99it/s][2026-01-05 10:55:42] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, cpu graph: False, gen throughput (token/s): 68.40, #queue-req: 0,
[2026-01-05 10:55:42] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:19<00:00,  3.20it/s]
100%|██████████| 200/200 [00:19<00:00, 10.30it/s]
.
----------------------------------------------------------------------
Ran 1 test in 110.795s

OK
Accuracy: 0.815
Invalid: 0.000
Latency: 19.498 s
Output throughput: 1687.887 token/s
.
.
End (6/43):
filename='ascend/llm_models/test_ascend_exaone_3.py', elapsed=123, estimated_time=400
.
.

.
.
Begin (7/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_phi_4_multimodal.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:56:06] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/LLM-Research/Phi-4-multimodal-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/LLM-Research/Phi-4-multimodal-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=581266756, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/LLM-Research/Phi-4-multimodal-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-05 10:56:09] Inferred chat template from model path: phi-4-mm
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 10:56:16] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 10:56:17] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:56:17] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:56:18] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:56:19] Load weight begin. avail mem=60.82 GB
[2026-01-05 10:56:19] Multimodal attention backend not set. Use sdpa.
[2026-01-05 10:56:19] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.40s/it]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:07<00:04,  4.16s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:13<00:00,  4.78s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:13<00:00,  4.33s/it]

[2026-01-05 10:56:32] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=51.97 GB, mem usage=8.85 GB.
[2026-01-05 10:56:32] Using KV cache dtype: torch.bfloat16
[2026-01-05 10:56:32] The available memory for KV cache is 39.80 GB.
[2026-01-05 10:56:32] KV Cache is allocated. #tokens: 326016, K size: 19.91 GB, V size: 19.91 GB
[2026-01-05 10:56:32] Memory pool end. avail mem=11.15 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 10:56:34] max_total_num_tokens=326016, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=11.15 GB
[2026-01-05 10:56:35] INFO:     Started server process [120720]
[2026-01-05 10:56:35] INFO:     Waiting for application startup.
[2026-01-05 10:56:35] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:56:35] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:56:35] INFO:     Application startup complete.
[2026-01-05 10:56:35] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 10:56:36] INFO:     127.0.0.1:46512 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 10:56:36] INFO:     127.0.0.1:46534 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:56:36] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 10:56:36.965783293 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 10:56:46] INFO:     127.0.0.1:54130 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:56:53] INFO:     127.0.0.1:46520 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 10:56:53] The server is fired up and ready to roll!
[2026-01-05 10:56:56] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:56:57] INFO:     127.0.0.1:51730 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 10:56:57] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 10:56:57] INFO:     127.0.0.1:51746 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 10:56:57] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:56:57] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/LLM-Research/Phi-4-multimodal-instruct --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 10:56:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:56:57] Prefill batch, #new-seq: 14, #new-token: 2048, #cached-token: 8960, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 10:56:57] Prefill batch, #new-seq: 24, #new-token: 3072, #cached-token: 15360, token usage: 0.01, #running-req: 15, #queue-req: 0,
[2026-01-05 10:56:58] Prefill batch, #new-seq: 61, #new-token: 8192, #cached-token: 39040, token usage: 0.02, #running-req: 39, #queue-req: 28,
[2026-01-05 10:56:58] Prefill batch, #new-seq: 28, #new-token: 3712, #cached-token: 17920, token usage: 0.04, #running-req: 100, #queue-req: 0,
[2026-01-05 10:57:00] Decode batch, #running-req: 128, #token: 21248, token usage: 0.07, cpu graph: False, gen throughput (token/s): 89.16, #queue-req: 0,
[2026-01-05 10:57:00] INFO:     127.0.0.1:51790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:00] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:00] INFO:     127.0.0.1:52478 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:03<10:23,  3.13s/it]
  2%|▏         | 3/200 [00:03<04:36,  1.40s/it]
  2%|▏         | 3/200 [00:03<04:36,  1.40s/it][2026-01-05 10:57:00] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-05 10:57:00] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:00] INFO:     127.0.0.1:52014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:00] INFO:     127.0.0.1:52314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:00] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.07, #running-req: 125, #queue-req: 0,
[2026-01-05 10:57:00] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:03<02:08,  1.50it/s][2026-01-05 10:57:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.07, #running-req: 127, #queue-req: 0,
[2026-01-05 10:57:00] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:00] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:00] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:00] INFO:     127.0.0.1:52586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:00] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:03<01:37,  1.95it/s]
  6%|▌         | 12/200 [00:03<00:27,  6.73it/s][2026-01-05 10:57:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,

  6%|▌         | 12/200 [00:03<00:27,  6.73it/s]
  6%|▌         | 12/200 [00:03<00:27,  6.73it/s][2026-01-05 10:57:00] INFO:     127.0.0.1:51776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:00] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-05 10:57:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 132, #queue-req: 0,
[2026-01-05 10:57:01] INFO:     127.0.0.1:52528 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:03<00:25,  7.18it/s][2026-01-05 10:57:01] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-05 10:57:01] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] INFO:     127.0.0.1:52064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-05 10:57:01] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.08, #running-req: 130, #queue-req: 0,
[2026-01-05 10:57:01] INFO:     127.0.0.1:52306 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:03<00:19,  9.23it/s][2026-01-05 10:57:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-05 10:57:01] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-05 10:57:01] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:04<00:18,  9.84it/s]
 10%|█         | 21/200 [00:04<00:15, 11.64it/s][2026-01-05 10:57:01] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.08, #running-req: 126, #queue-req: 0,
[2026-01-05 10:57:01] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:57:01] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:04<00:14, 11.96it/s][2026-01-05 10:57:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:57:01] INFO:     127.0.0.1:52334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,

 12%|█▎        | 25/200 [00:04<00:14, 11.75it/s][2026-01-05 10:57:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 10:57:01] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] INFO:     127.0.0.1:52042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:01] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:04<00:15, 11.53it/s]
 16%|█▌        | 32/200 [00:04<00:06, 27.43it/s]
 16%|█▌        | 32/200 [00:04<00:06, 27.43it/s]
 16%|█▌        | 32/200 [00:04<00:06, 27.43it/s]
 16%|█▌        | 32/200 [00:04<00:06, 27.43it/s]
 16%|█▌        | 32/200 [00:04<00:06, 27.43it/s][2026-01-05 10:57:01] Prefill batch, #new-seq: 7, #new-token: 1024, #cached-token: 4480, token usage: 0.09, #running-req: 121, #queue-req: 0,
[2026-01-05 10:57:02] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:02] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:57:02] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 10:57:02] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:04<00:07, 22.14it/s]
 18%|█▊        | 36/200 [00:04<00:08, 20.31it/s][2026-01-05 10:57:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.09, #running-req: 129, #queue-req: 0,
[2026-01-05 10:57:02] INFO:     127.0.0.1:52664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:02] Decode batch, #running-req: 128, #token: 28288, token usage: 0.09, cpu graph: False, gen throughput (token/s): 2274.61, #queue-req: 0,
[2026-01-05 10:57:02] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 10:57:02] INFO:     127.0.0.1:52504 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:05<00:08, 18.26it/s][2026-01-05 10:57:02] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:02] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:02] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 10:57:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.09, #running-req: 130, #queue-req: 0,
[2026-01-05 10:57:03] INFO:     127.0.0.1:52756 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:03] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:06<00:23,  6.67it/s]
 22%|██▏       | 43/200 [00:06<00:32,  4.82it/s][2026-01-05 10:57:03] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 10:57:03] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:03] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 127, #queue-req: 0,

 22%|██▎       | 45/200 [00:06<00:29,  5.28it/s][2026-01-05 10:57:03] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:52682 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:06<00:25,  6.05it/s][2026-01-05 10:57:04] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:52062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:52146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.09, #running-req: 129, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:06<00:16,  8.80it/s][2026-01-05 10:57:04] INFO:     127.0.0.1:52340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-05 10:57:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.08, #running-req: 130, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:52196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:07<00:13, 10.59it/s]
 28%|██▊       | 57/200 [00:07<00:10, 13.17it/s][2026-01-05 10:57:04] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.08, #running-req: 126, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.08, #running-req: 130, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:52686 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:07<00:08, 16.44it/s]
 32%|███▎      | 65/200 [00:07<00:05, 24.42it/s]
 32%|███▎      | 65/200 [00:07<00:05, 24.42it/s][2026-01-05 10:57:04] INFO:     127.0.0.1:52958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.08, #running-req: 125, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.08, #running-req: 129, #queue-req: 0,
[2026-01-05 10:57:04] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:52236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:52492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:04] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:07<00:05, 22.53it/s]
 36%|███▋      | 73/200 [00:07<00:04, 28.51it/s]
 36%|███▋      | 73/200 [00:07<00:04, 28.51it/s]
 36%|███▋      | 73/200 [00:07<00:04, 28.51it/s]
 36%|███▋      | 73/200 [00:07<00:04, 28.51it/s][2026-01-05 10:57:05] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.08, #running-req: 123, #queue-req: 0,
[2026-01-05 10:57:05] INFO:     127.0.0.1:52750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:07<00:03, 30.87it/s]
 40%|████      | 80/200 [00:07<00:03, 34.44it/s][2026-01-05 10:57:05] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:07<00:03, 32.20it/s]
 43%|████▎     | 86/200 [00:07<00:03, 33.98it/s]
 43%|████▎     | 86/200 [00:07<00:03, 33.98it/s][2026-01-05 10:57:05] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:51898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:51946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:08<00:03, 32.05it/s]
 46%|████▌     | 92/200 [00:08<00:03, 34.77it/s]
 46%|████▌     | 92/200 [00:08<00:03, 34.77it/s][2026-01-05 10:57:05] INFO:     127.0.0.1:52048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:08<00:03, 32.42it/s][2026-01-05 10:57:05] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:08<00:03, 32.87it/s]
 52%|█████▏    | 103/200 [00:08<00:02, 45.34it/s]
 52%|█████▏    | 103/200 [00:08<00:02, 45.34it/s]
 52%|█████▏    | 103/200 [00:08<00:02, 45.34it/s][2026-01-05 10:57:05] Decode batch, #running-req: 102, #token: 20992, token usage: 0.06, cpu graph: False, gen throughput (token/s): 1446.06, #queue-req: 0,
[2026-01-05 10:57:05] INFO:     127.0.0.1:52596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:08<00:02, 41.15it/s]
 55%|█████▍    | 109/200 [00:08<00:02, 40.64it/s][2026-01-05 10:57:05] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:05] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:52266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:08<00:02, 38.72it/s]
 57%|█████▊    | 115/200 [00:08<00:02, 39.51it/s][2026-01-05 10:57:06] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:52426 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:08<00:02, 31.15it/s][2026-01-05 10:57:06] INFO:     127.0.0.1:52004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:52622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:51912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:09<00:02, 30.11it/s]
 62%|██████▎   | 125/200 [00:09<00:02, 34.08it/s]
 62%|██████▎   | 125/200 [00:09<00:02, 34.08it/s][2026-01-05 10:57:06] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:09<00:02, 32.02it/s]
 66%|██████▌   | 131/200 [00:09<00:01, 34.91it/s]
 66%|██████▌   | 131/200 [00:09<00:01, 34.91it/s][2026-01-05 10:57:06] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:52652 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:09<00:01, 35.24it/s][2026-01-05 10:57:06] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:09<00:01, 32.72it/s][2026-01-05 10:57:06] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:06] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:09<00:02, 24.01it/s][2026-01-05 10:57:07] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:09<00:02, 24.69it/s][2026-01-05 10:57:07] Decode batch, #running-req: 55, #token: 14080, token usage: 0.04, cpu graph: False, gen throughput (token/s): 2030.36, #queue-req: 0,
[2026-01-05 10:57:07] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:51962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53570 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:09<00:02, 23.54it/s]
 75%|███████▌  | 150/200 [00:09<00:02, 24.63it/s][2026-01-05 10:57:07] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:10<00:01, 26.00it/s][2026-01-05 10:57:07] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:10<00:01, 26.48it/s][2026-01-05 10:57:07] INFO:     127.0.0.1:53346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:53450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:07] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:10<00:01, 26.72it/s][2026-01-05 10:57:07] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:10<00:01, 16.85it/s]
 84%|████████▍ | 169/200 [00:10<00:02, 13.64it/s]
 84%|████████▍ | 169/200 [00:10<00:02, 13.64it/s][2026-01-05 10:57:08] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:10<00:01, 15.48it/s][2026-01-05 10:57:08] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:11<00:01, 15.92it/s][2026-01-05 10:57:08] INFO:     127.0.0.1:51900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:53372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:11<00:01, 16.38it/s]
 89%|████████▉ | 178/200 [00:11<00:00, 22.89it/s]
 89%|████████▉ | 178/200 [00:11<00:00, 22.89it/s][2026-01-05 10:57:08] Decode batch, #running-req: 22, #token: 6528, token usage: 0.02, cpu graph: False, gen throughput (token/s): 1037.96, #queue-req: 0,
[2026-01-05 10:57:08] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:11<00:00, 20.90it/s][2026-01-05 10:57:08] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:08] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:09] INFO:     127.0.0.1:53600 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:11<00:01, 12.41it/s][2026-01-05 10:57:09] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:09] INFO:     127.0.0.1:53238 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:11<00:01, 13.39it/s][2026-01-05 10:57:09] INFO:     127.0.0.1:53308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:09] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:09] INFO:     127.0.0.1:53388 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:12<00:00, 16.06it/s][2026-01-05 10:57:09] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:09] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:09] INFO:     127.0.0.1:53586 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:12<00:00, 14.73it/s][2026-01-05 10:57:09] INFO:     127.0.0.1:53008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:09] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:12<00:00, 13.18it/s][2026-01-05 10:57:10] Decode batch, #running-req: 6, #token: 2688, token usage: 0.01, cpu graph: False, gen throughput (token/s): 384.37, #queue-req: 0,
[2026-01-05 10:57:10] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:10] INFO:     127.0.0.1:53536 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:12<00:00,  9.71it/s][2026-01-05 10:57:10] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:11] Decode batch, #running-req: 3, #token: 1664, token usage: 0.01, cpu graph: False, gen throughput (token/s): 109.93, #queue-req: 0,
[2026-01-05 10:57:11] INFO:     127.0.0.1:53192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:57:11] INFO:     127.0.0.1:53488 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:14<00:00,  4.30it/s]
100%|█████████▉| 199/200 [00:14<00:00,  3.41it/s][2026-01-05 10:57:12] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:14<00:00,  2.86it/s]
100%|██████████| 200/200 [00:14<00:00, 13.55it/s]
.
----------------------------------------------------------------------
Ran 1 test in 76.110s

OK
Accuracy: 0.830
Invalid: 0.000
Latency: 14.812 s
Output throughput: 1288.392 token/s
.
.
End (7/43):
filename='ascend/llm_models/test_ascend_phi_4_multimodal.py', elapsed=90, estimated_time=400
.
.

.
.
Begin (8/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_c4ai_command_r_v01.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 10:57:33] WARNING model_config.py:1014: Casting torch.float16 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:57:34] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/CohereForAI/c4ai-command-r-v01', tokenizer_path='/root/.cache/modelscope/hub/models/CohereForAI/c4ai-command-r-v01', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=64101919, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/CohereForAI/c4ai-command-r-v01', weight_version='default', chat_template='/data/l30079981/run_sglang/1224/ascend/llm_models/tool_chat_template_c4ai_command_r_v01.jinja', completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 10:57:34] Casting torch.float16 to torch.bfloat16.
[2026-01-05 10:57:34] Loading chat template from argument: /data/l30079981/run_sglang/1224/ascend/llm_models/tool_chat_template_c4ai_command_r_v01.jinja
[2026-01-05 10:57:34] Detected user specified Jinja chat template with content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:57:42 TP0] Casting torch.float16 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:57:43 TP0] Casting torch.float16 to torch.bfloat16.
[2026-01-05 10:57:43 TP0] Init torch distributed begin.
[2026-01-05 10:57:43 TP1] Casting torch.float16 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 10:57:44 TP1] Casting torch.float16 to torch.bfloat16.
[2026-01-05 10:57:44 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2026-01-05 10:57:45 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 10:57:45 TP1] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:57:45 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:57:45 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 10:57:45 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 10:57:45 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 10:57:45 TP1] Load weight begin. avail mem=61.13 GB

Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:01<00:14,  1.04s/it]

Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:05<00:40,  3.15s/it]

Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:10<00:46,  3.87s/it]

Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:15<00:46,  4.20s/it]

Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:19<00:43,  4.37s/it]

Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:24<00:41,  4.63s/it]

Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:29<00:37,  4.66s/it]

Loading safetensors checkpoint shards:  53% Completed | 8/15 [00:34<00:32,  4.70s/it]

Loading safetensors checkpoint shards:  60% Completed | 9/15 [00:39<00:28,  4.78s/it]

Loading safetensors checkpoint shards:  67% Completed | 10/15 [00:44<00:24,  4.81s/it]

Loading safetensors checkpoint shards:  73% Completed | 11/15 [00:48<00:19,  4.79s/it]

Loading safetensors checkpoint shards:  80% Completed | 12/15 [00:53<00:14,  4.78s/it]

Loading safetensors checkpoint shards:  87% Completed | 13/15 [00:58<00:09,  4.78s/it]

Loading safetensors checkpoint shards:  93% Completed | 14/15 [01:01<00:04,  4.39s/it]

Loading safetensors checkpoint shards: 100% Completed | 15/15 [01:06<00:00,  4.46s/it]

Loading safetensors checkpoint shards: 100% Completed | 15/15 [01:06<00:00,  4.44s/it]

[2026-01-05 10:58:53 TP0] Load weight end. type=CohereForCausalLM, dtype=torch.bfloat16, avail mem=28.20 GB, mem usage=32.62 GB.
[2026-01-05 10:58:53 TP1] Load weight end. type=CohereForCausalLM, dtype=torch.bfloat16, avail mem=28.51 GB, mem usage=32.62 GB.
[2026-01-05 10:58:53 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 10:58:53 TP1] The available memory for KV cache is 16.01 GB.
[2026-01-05 10:58:53 TP0] The available memory for KV cache is 16.01 GB.
[2026-01-05 10:58:53 TP1] KV Cache is allocated. #tokens: 26112, K size: 8.01 GB, V size: 8.01 GB
[2026-01-05 10:58:53 TP0] KV Cache is allocated. #tokens: 26112, K size: 8.01 GB, V size: 8.01 GB
[2026-01-05 10:58:53 TP1] Memory pool end. avail mem=11.48 GB
[2026-01-05 10:58:53 TP0] Memory pool end. avail mem=11.17 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 10:58:54 TP0] max_total_num_tokens=26112, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=11.17 GB
[2026-01-05 10:58:55] INFO:     Started server process [123898]
[2026-01-05 10:58:55] INFO:     Waiting for application startup.
[2026-01-05 10:58:55] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:58:55] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 10:58:55] INFO:     Application startup complete.
[2026-01-05 10:58:55] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 10:58:56] INFO:     127.0.0.1:38392 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 10:58:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank1]:[W105 10:58:56.186218957 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank0]:[W105 10:58:56.186443355 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 10:59:04] INFO:     127.0.0.1:38404 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 10:59:11] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:11] The server is fired up and ready to roll!
[2026-01-05 10:59:14 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:59:15] INFO:     127.0.0.1:50374 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 10:59:15] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 10:59:15] INFO:     127.0.0.1:50382 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 10:59:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 10:59:16] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/CohereForAI/c4ai-command-r-v01 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --chat-template /data/l30079981/run_sglang/1224/ascend/llm_models/tool_chat_template_c4ai_command_r_v01.jinja --tp-size 2 --dtype bfloat16 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestC4AI.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 10:59:16 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 0, #queue-req: 0,
[2026-01-05 10:59:16 TP0] Prefill batch, #new-seq: 20, #new-token: 2944, #cached-token: 15360, token usage: 0.03, #running-req: 1, #queue-req: 0,
[2026-01-05 10:59:16 TP0] Prefill batch, #new-seq: 22, #new-token: 2944, #cached-token: 16896, token usage: 0.15, #running-req: 21, #queue-req: 8,
[2026-01-05 10:59:18 TP0] Decode batch, #running-req: 43, #token: 8832, token usage: 0.34, cpu graph: False, gen throughput (token/s): 13.99, #queue-req: 85,
[2026-01-05 10:59:19] INFO:     127.0.0.1:48144 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<09:14,  2.79s/it][2026-01-05 10:59:19 TP0] Prefill batch, #new-seq: 4, #new-token: 768, #cached-token: 3072, token usage: 0.38, #running-req: 42, #queue-req: 82,
[2026-01-05 10:59:19] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:19] INFO:     127.0.0.1:48012 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:03<04:38,  1.41s/it]
  2%|▏         | 3/200 [00:03<02:10,  1.51it/s][2026-01-05 10:59:19 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.40, #running-req: 44, #queue-req: 81,
[2026-01-05 10:59:19] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:03<01:36,  2.03it/s][2026-01-05 10:59:19 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.41, #running-req: 46, #queue-req: 81,
[2026-01-05 10:59:19] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:03<01:20,  2.43it/s][2026-01-05 10:59:19 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.43, #running-req: 46, #queue-req: 81,
[2026-01-05 10:59:20] INFO:     127.0.0.1:48112 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:04<01:27,  2.21it/s][2026-01-05 10:59:20 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.44, #running-req: 46, #queue-req: 81,
[2026-01-05 10:59:20] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:04<01:13,  2.61it/s][2026-01-05 10:59:20] INFO:     127.0.0.1:47866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:20 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.43, #running-req: 46, #queue-req: 80,
[2026-01-05 10:59:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.45, #running-req: 47, #queue-req: 80,
[2026-01-05 10:59:21] INFO:     127.0.0.1:48028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:21] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:04<01:04,  2.98it/s]
  5%|▌         | 10/200 [00:04<00:49,  3.81it/s][2026-01-05 10:59:21 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.44, #running-req: 46, #queue-req: 80,
[2026-01-05 10:59:21] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:05<00:44,  4.29it/s][2026-01-05 10:59:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.44, #running-req: 47, #queue-req: 80,
[2026-01-05 10:59:21 TP0] Decode batch, #running-req: 48, #token: 11648, token usage: 0.45, cpu graph: False, gen throughput (token/s): 662.43, #queue-req: 80,
[2026-01-05 10:59:21] INFO:     127.0.0.1:47980 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:05<00:52,  3.58it/s][2026-01-05 10:59:21] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:21 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.43, #running-req: 47, #queue-req: 79,
[2026-01-05 10:59:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.44, #running-req: 48, #queue-req: 79,
[2026-01-05 10:59:22] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:05<00:40,  4.61it/s][2026-01-05 10:59:22 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.43, #running-req: 48, #queue-req: 78,
[2026-01-05 10:59:22] INFO:     127.0.0.1:47944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:22] INFO:     127.0.0.1:48096 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:05<00:40,  4.52it/s]
  8%|▊         | 16/200 [00:05<00:33,  5.47it/s][2026-01-05 10:59:22 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.42, #running-req: 48, #queue-req: 78,
[2026-01-05 10:59:22] INFO:     127.0.0.1:47972 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:06<00:30,  5.97it/s][2026-01-05 10:59:22 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.42, #running-req: 49, #queue-req: 77,
[2026-01-05 10:59:22] INFO:     127.0.0.1:48160 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:06<00:33,  5.36it/s][2026-01-05 10:59:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.44, #running-req: 50, #queue-req: 77,
[2026-01-05 10:59:22] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:06<00:30,  5.97it/s][2026-01-05 10:59:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.43, #running-req: 50, #queue-req: 77,
[2026-01-05 10:59:22] INFO:     127.0.0.1:47852 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:06<00:27,  6.58it/s][2026-01-05 10:59:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.43, #running-req: 50, #queue-req: 77,
[2026-01-05 10:59:23] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:06<00:33,  5.38it/s][2026-01-05 10:59:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.45, #running-req: 50, #queue-req: 77,
[2026-01-05 10:59:23] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:23] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:06<00:29,  6.06it/s]
 12%|█▏        | 23/200 [00:06<00:20,  8.58it/s][2026-01-05 10:59:23 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.43, #running-req: 49, #queue-req: 78,
[2026-01-05 10:59:23] INFO:     127.0.0.1:48092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:23] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:07<00:23,  7.40it/s][2026-01-05 10:59:23 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.44, #running-req: 49, #queue-req: 78,
[2026-01-05 10:59:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.45, #running-req: 49, #queue-req: 78,
[2026-01-05 10:59:23] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:07<00:28,  6.01it/s][2026-01-05 10:59:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.45, #running-req: 49, #queue-req: 78,
[2026-01-05 10:59:24] INFO:     127.0.0.1:47954 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:07<00:30,  5.65it/s][2026-01-05 10:59:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.44, #running-req: 49, #queue-req: 77,
[2026-01-05 10:59:24] INFO:     127.0.0.1:48078 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:08<00:35,  4.84it/s][2026-01-05 10:59:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.44, #running-req: 50, #queue-req: 77,
[2026-01-05 10:59:24] INFO:     127.0.0.1:48188 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:08<00:35,  4.79it/s][2026-01-05 10:59:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.44, #running-req: 50, #queue-req: 76,
[2026-01-05 10:59:24 TP0] Decode batch, #running-req: 50, #token: 11776, token usage: 0.45, cpu graph: False, gen throughput (token/s): 640.79, #queue-req: 76,
[2026-01-05 10:59:24] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:08<00:39,  4.35it/s][2026-01-05 10:59:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.46, #running-req: 51, #queue-req: 76,
[2026-01-05 10:59:25] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:08<00:41,  4.08it/s][2026-01-05 10:59:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.47, #running-req: 51, #queue-req: 76,
[2026-01-05 10:59:25] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:25] INFO:     127.0.0.1:48256 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:09<00:39,  4.22it/s]
 16%|█▋        | 33/200 [00:09<00:29,  5.59it/s][2026-01-05 10:59:25] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:25 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.45, #running-req: 50, #queue-req: 76,
[2026-01-05 10:59:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.46, #running-req: 51, #queue-req: 76,
[2026-01-05 10:59:25] INFO:     127.0.0.1:47884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:25] INFO:     127.0.0.1:48412 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:09<00:30,  5.42it/s]
 18%|█▊        | 36/200 [00:09<00:26,  6.29it/s][2026-01-05 10:59:25 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.46, #running-req: 50, #queue-req: 76,
[2026-01-05 10:59:26] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:09<00:31,  5.21it/s][2026-01-05 10:59:26] INFO:     127.0.0.1:48288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:26 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.46, #running-req: 51, #queue-req: 75,
[2026-01-05 10:59:26 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.47, #running-req: 52, #queue-req: 75,
[2026-01-05 10:59:26] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:09<00:27,  5.91it/s][2026-01-05 10:59:26 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.46, #running-req: 52, #queue-req: 75,
[2026-01-05 10:59:26] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:26] INFO:     127.0.0.1:48424 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:10<00:25,  6.36it/s]
 20%|██        | 41/200 [00:10<00:19,  8.33it/s][2026-01-05 10:59:26 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.45, #running-req: 51, #queue-req: 75,
[2026-01-05 10:59:26] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:26] INFO:     127.0.0.1:48298 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:10<00:18,  8.47it/s][2026-01-05 10:59:26 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.44, #running-req: 51, #queue-req: 75,
[2026-01-05 10:59:26] INFO:     127.0.0.1:47900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:26 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.44, #running-req: 52, #queue-req: 74,
[2026-01-05 10:59:26] INFO:     127.0.0.1:47828 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:10<00:18,  8.48it/s][2026-01-05 10:59:26 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.45, #running-req: 53, #queue-req: 74,
[2026-01-05 10:59:27] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:10<00:18,  8.39it/s][2026-01-05 10:59:27 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.44, #running-req: 53, #queue-req: 74,
[2026-01-05 10:59:27] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:10<00:23,  6.43it/s][2026-01-05 10:59:27 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.45, #running-req: 53, #queue-req: 74,
[2026-01-05 10:59:27] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:11<00:22,  6.86it/s][2026-01-05 10:59:27 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.45, #running-req: 53, #queue-req: 74,
[2026-01-05 10:59:27] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:11<00:31,  4.84it/s][2026-01-05 10:59:27 TP0] Decode batch, #running-req: 54, #token: 12544, token usage: 0.48, cpu graph: False, gen throughput (token/s): 653.77, #queue-req: 74,
[2026-01-05 10:59:27] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:27] INFO:     127.0.0.1:48318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:27] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:11<00:21,  6.94it/s]
 26%|██▌       | 52/200 [00:11<00:13, 10.65it/s][2026-01-05 10:59:27 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.46, #running-req: 52, #queue-req: 75,
[2026-01-05 10:59:28] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:28 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.47, #running-req: 50, #queue-req: 76,
[2026-01-05 10:59:28] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:11<00:13, 10.89it/s][2026-01-05 10:59:28 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.47, #running-req: 51, #queue-req: 75,
[2026-01-05 10:59:28] INFO:     127.0.0.1:47916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.49, #running-req: 52, #queue-req: 75,
[2026-01-05 10:59:28] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:12<00:20,  6.92it/s][2026-01-05 10:59:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.49, #running-req: 52, #queue-req: 75,
[2026-01-05 10:59:28] INFO:     127.0.0.1:48370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:28] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:12<00:20,  7.04it/s][2026-01-05 10:59:28 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.47, #running-req: 52, #queue-req: 75,
[2026-01-05 10:59:29] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:29] INFO:     127.0.0.1:48516 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:12<00:20,  6.76it/s]
 30%|███       | 60/200 [00:12<00:17,  7.85it/s][2026-01-05 10:59:29 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.48, #running-req: 50, #queue-req: 76,
[2026-01-05 10:59:29] INFO:     127.0.0.1:47872 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:12<00:20,  6.70it/s][2026-01-05 10:59:29 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.48, #running-req: 51, #queue-req: 76,
[2026-01-05 10:59:29] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:29] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:13<00:22,  6.03it/s]
 32%|███▏      | 63/200 [00:13<00:19,  6.88it/s][2026-01-05 10:59:29] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:29] INFO:     127.0.0.1:48544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:29 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.46, #running-req: 50, #queue-req: 75,
[2026-01-05 10:59:29 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.48, #running-req: 51, #queue-req: 75,
[2026-01-05 10:59:29] INFO:     127.0.0.1:48240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:29] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:13<00:17,  7.45it/s]
 34%|███▎      | 67/200 [00:13<00:15,  8.77it/s][2026-01-05 10:59:29 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.48, #running-req: 51, #queue-req: 75,
[2026-01-05 10:59:30] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:13<00:18,  7.11it/s][2026-01-05 10:59:30 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.48, #running-req: 52, #queue-req: 74,
[2026-01-05 10:59:30] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:14<00:20,  6.39it/s][2026-01-05 10:59:30] INFO:     127.0.0.1:48358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:30 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.48, #running-req: 53, #queue-req: 74,
[2026-01-05 10:59:30 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.48, #running-req: 53, #queue-req: 74,
[2026-01-05 10:59:30] INFO:     127.0.0.1:48566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:30] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:14<00:22,  5.67it/s]
 36%|███▌      | 72/200 [00:14<00:20,  6.10it/s][2026-01-05 10:59:30 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.48, #running-req: 52, #queue-req: 74,
[2026-01-05 10:59:31 TP0] Decode batch, #running-req: 52, #token: 12416, token usage: 0.48, cpu graph: False, gen throughput (token/s): 672.68, #queue-req: 74,
[2026-01-05 10:59:31] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:14<00:19,  6.44it/s][2026-01-05 10:59:31 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.48, #running-req: 53, #queue-req: 72,
[2026-01-05 10:59:31] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:14<00:22,  5.70it/s][2026-01-05 10:59:31] INFO:     127.0.0.1:48768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:31 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.48, #running-req: 54, #queue-req: 70,
[2026-01-05 10:59:31] INFO:     127.0.0.1:48386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:31] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [00:15<00:16,  7.45it/s]
 38%|███▊      | 77/200 [00:15<00:11, 10.71it/s][2026-01-05 10:59:31 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.47, #running-req: 53, #queue-req: 68,
[2026-01-05 10:59:31] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:31] INFO:     127.0.0.1:48574 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:15<00:14,  8.44it/s][2026-01-05 10:59:31 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.48, #running-req: 53, #queue-req: 66,
[2026-01-05 10:59:31] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.49, #running-req: 54, #queue-req: 65,
[2026-01-05 10:59:32] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:15<00:16,  7.05it/s][2026-01-05 10:59:32 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.49, #running-req: 54, #queue-req: 63,
[2026-01-05 10:59:32] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:16<00:20,  5.90it/s][2026-01-05 10:59:32] INFO:     127.0.0.1:48128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.49, #running-req: 55, #queue-req: 62,
[2026-01-05 10:59:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.49, #running-req: 55, #queue-req: 61,
[2026-01-05 10:59:32] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:16<00:17,  6.65it/s][2026-01-05 10:59:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.49, #running-req: 55, #queue-req: 60,
[2026-01-05 10:59:32] INFO:     127.0.0.1:48182 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:16<00:19,  5.77it/s][2026-01-05 10:59:33 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.50, #running-req: 55, #queue-req: 59,
[2026-01-05 10:59:33] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:17<00:26,  4.27it/s][2026-01-05 10:59:33 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.50, #running-req: 55, #queue-req: 58,
[2026-01-05 10:59:33] INFO:     127.0.0.1:48238 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:17<00:23,  4.85it/s][2026-01-05 10:59:33 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.50, #running-req: 55, #queue-req: 56,
[2026-01-05 10:59:33] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:17<00:20,  5.51it/s][2026-01-05 10:59:33 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.50, #running-req: 56, #queue-req: 55,
[2026-01-05 10:59:33] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:17<00:18,  6.03it/s][2026-01-05 10:59:33 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.50, #running-req: 56, #queue-req: 54,
[2026-01-05 10:59:33] INFO:     127.0.0.1:48846 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:17<00:16,  6.52it/s][2026-01-05 10:59:33 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.51, #running-req: 56, #queue-req: 53,
[2026-01-05 10:59:33] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:17<00:15,  7.10it/s][2026-01-05 10:59:34 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.51, #running-req: 56, #queue-req: 52,
[2026-01-05 10:59:34] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:17<00:17,  6.02it/s][2026-01-05 10:59:34 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.51, #running-req: 56, #queue-req: 51,
[2026-01-05 10:59:34 TP0] Decode batch, #running-req: 56, #token: 13440, token usage: 0.51, cpu graph: False, gen throughput (token/s): 665.14, #queue-req: 51,
[2026-01-05 10:59:34] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:18<00:20,  5.30it/s][2026-01-05 10:59:34 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.51, #running-req: 56, #queue-req: 50,
[2026-01-05 10:59:34] INFO:     127.0.0.1:48646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:34] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:18<00:17,  5.96it/s]
 48%|████▊     | 95/200 [00:18<00:12,  8.47it/s][2026-01-05 10:59:34 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.50, #running-req: 55, #queue-req: 49,
[2026-01-05 10:59:35] INFO:     127.0.0.1:48940 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:18<00:22,  4.67it/s][2026-01-05 10:59:35] INFO:     127.0.0.1:48170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:35] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:35 TP0] Prefill batch, #new-seq: 3, #new-token: 640, #cached-token: 2304, token usage: 0.49, #running-req: 55, #queue-req: 46,
[2026-01-05 10:59:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.51, #running-req: 56, #queue-req: 45,
[2026-01-05 10:59:35] INFO:     127.0.0.1:48630 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:19<00:17,  5.91it/s][2026-01-05 10:59:35 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.51, #running-req: 56, #queue-req: 43,
[2026-01-05 10:59:35] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:19<00:15,  6.29it/s][2026-01-05 10:59:35 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.51, #running-req: 57, #queue-req: 42,
[2026-01-05 10:59:35] INFO:     127.0.0.1:48688 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [00:19<00:17,  5.61it/s][2026-01-05 10:59:35] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:35 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.50, #running-req: 57, #queue-req: 40,
[2026-01-05 10:59:35] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:19<00:12,  7.54it/s][2026-01-05 10:59:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.52, #running-req: 57, #queue-req: 39,
[2026-01-05 10:59:36] INFO:     127.0.0.1:49046 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:19<00:16,  5.98it/s][2026-01-05 10:59:36] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:36 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.50, #running-req: 57, #queue-req: 37,
[2026-01-05 10:59:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.52, #running-req: 58, #queue-req: 36,
[2026-01-05 10:59:36] INFO:     127.0.0.1:48978 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:20<00:14,  6.51it/s][2026-01-05 10:59:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.51, #running-req: 58, #queue-req: 35,
[2026-01-05 10:59:36] INFO:     127.0.0.1:48678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:36] INFO:     127.0.0.1:48810 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:20<00:15,  5.96it/s]
 54%|█████▍    | 108/200 [00:20<00:13,  6.85it/s][2026-01-05 10:59:36 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.50, #running-req: 57, #queue-req: 33,
[2026-01-05 10:59:36] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:36] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:36] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:36] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:20<00:12,  7.10it/s]
 56%|█████▌    | 112/200 [00:20<00:05, 17.60it/s]
 56%|█████▌    | 112/200 [00:20<00:05, 17.60it/s]
 56%|█████▌    | 112/200 [00:20<00:05, 17.60it/s][2026-01-05 10:59:36 TP0] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.48, #running-req: 55, #queue-req: 29,
[2026-01-05 10:59:37] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:37] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:37 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.50, #running-req: 58, #queue-req: 27,
[2026-01-05 10:59:37] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:20<00:07, 11.37it/s][2026-01-05 10:59:37 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.51, #running-req: 58, #queue-req: 26,
[2026-01-05 10:59:37] INFO:     127.0.0.1:49090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:37 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.51, #running-req: 58, #queue-req: 25,
[2026-01-05 10:59:37 TP0] Decode batch, #running-req: 59, #token: 13440, token usage: 0.51, cpu graph: False, gen throughput (token/s): 670.98, #queue-req: 25,
[2026-01-05 10:59:37] INFO:     127.0.0.1:49114 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:21<00:10,  8.18it/s][2026-01-05 10:59:37 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.52, #running-req: 58, #queue-req: 24,
[2026-01-05 10:59:38] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:38] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:21<00:11,  7.00it/s][2026-01-05 10:59:38 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.51, #running-req: 58, #queue-req: 22,
[2026-01-05 10:59:38] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:38 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.52, #running-req: 58, #queue-req: 20,
[2026-01-05 10:59:38] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:22<00:14,  5.64it/s][2026-01-05 10:59:38] INFO:     127.0.0.1:49162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:38 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.53, #running-req: 59, #queue-req: 19,
[2026-01-05 10:59:38] INFO:     127.0.0.1:48872 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:22<00:11,  6.91it/s][2026-01-05 10:59:38 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.53, #running-req: 58, #queue-req: 18,
[2026-01-05 10:59:38] INFO:     127.0.0.1:49030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:38] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:22<00:08,  8.38it/s][2026-01-05 10:59:38 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.52, #running-req: 57, #queue-req: 16,
[2026-01-05 10:59:39] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:39 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.52, #running-req: 58, #queue-req: 14,
[2026-01-05 10:59:39] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:39] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:23<00:11,  6.43it/s]
 64%|██████▍   | 128/200 [00:23<00:11,  6.31it/s][2026-01-05 10:59:39] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:39 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.52, #running-req: 58, #queue-req: 12,
[2026-01-05 10:59:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.54, #running-req: 59, #queue-req: 11,
[2026-01-05 10:59:39] INFO:     127.0.0.1:49206 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:23<00:11,  5.86it/s][2026-01-05 10:59:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.54, #running-req: 59, #queue-req: 10,
[2026-01-05 10:59:40] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [00:23<00:12,  5.57it/s][2026-01-05 10:59:40 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.54, #running-req: 59, #queue-req: 8,
[2026-01-05 10:59:40] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:23<00:12,  5.24it/s][2026-01-05 10:59:40] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:40 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.53, #running-req: 60, #queue-req: 6,
[2026-01-05 10:59:40] INFO:     127.0.0.1:48964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:40 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.55, #running-req: 60, #queue-req: 5,
[2026-01-05 10:59:40] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:40] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:24<00:11,  5.90it/s]
 68%|██████▊   | 136/200 [00:24<00:09,  7.09it/s][2026-01-05 10:59:40 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.53, #running-req: 59, #queue-req: 3,
[2026-01-05 10:59:40 TP0] Decode batch, #running-req: 61, #token: 14208, token usage: 0.54, cpu graph: False, gen throughput (token/s): 740.27, #queue-req: 3,
[2026-01-05 10:59:40] INFO:     127.0.0.1:49178 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:24<00:09,  6.43it/s][2026-01-05 10:59:41 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.54, #running-req: 60, #queue-req: 1,
[2026-01-05 10:59:41] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:41] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:24<00:09,  6.68it/s]
 70%|██████▉   | 139/200 [00:24<00:07,  8.28it/s][2026-01-05 10:59:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.53, #running-req: 60, #queue-req: 0,
[2026-01-05 10:59:41] INFO:     127.0.0.1:47138 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:24<00:07,  8.24it/s][2026-01-05 10:59:41] INFO:     127.0.0.1:48598 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:25<00:15,  3.92it/s][2026-01-05 10:59:42] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:25<00:13,  4.22it/s][2026-01-05 10:59:42] INFO:     127.0.0.1:49346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:42] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:26<00:13,  4.14it/s]
 72%|███████▏  | 144/200 [00:26<00:10,  5.15it/s][2026-01-05 10:59:42] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:42] INFO:     127.0.0.1:49354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:42] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:26<00:07,  6.96it/s]
 74%|███████▎  | 147/200 [00:26<00:05, 10.29it/s][2026-01-05 10:59:42] INFO:     127.0.0.1:49382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:42] INFO:     127.0.0.1:49026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:42] INFO:     127.0.0.1:49220 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:26<00:04, 10.69it/s]
 75%|███████▌  | 150/200 [00:26<00:03, 12.63it/s][2026-01-05 10:59:42] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:43] INFO:     127.0.0.1:49112 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:26<00:05,  9.11it/s][2026-01-05 10:59:43] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:43] INFO:     127.0.0.1:47262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:43] INFO:     127.0.0.1:49060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:43] INFO:     127.0.0.1:49376 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:26<00:03, 11.99it/s]
 78%|███████▊  | 156/200 [00:26<00:02, 16.47it/s][2026-01-05 10:59:43] INFO:     127.0.0.1:49190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:43] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:43 TP0] Decode batch, #running-req: 44, #token: 11520, token usage: 0.44, cpu graph: False, gen throughput (token/s): 902.24, #queue-req: 0,
[2026-01-05 10:59:43] INFO:     127.0.0.1:47158 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:27<00:03, 12.99it/s][2026-01-05 10:59:43] INFO:     127.0.0.1:47106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:43] INFO:     127.0.0.1:47306 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:27<00:03, 10.34it/s][2026-01-05 10:59:43] INFO:     127.0.0.1:48906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:44] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:27<00:04,  8.88it/s][2026-01-05 10:59:44] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:44] INFO:     127.0.0.1:47130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:44] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:27<00:03, 10.22it/s][2026-01-05 10:59:44] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:28<00:02, 11.22it/s][2026-01-05 10:59:44] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:44] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:28<00:02, 12.63it/s][2026-01-05 10:59:44] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:44] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:28<00:02, 14.01it/s][2026-01-05 10:59:44] INFO:     127.0.0.1:47222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:44] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:44] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:28<00:01, 15.86it/s][2026-01-05 10:59:45] INFO:     127.0.0.1:47354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:45] INFO:     127.0.0.1:47372 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:28<00:02, 11.83it/s][2026-01-05 10:59:45] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:45 TP0] Decode batch, #running-req: 24, #token: 7424, token usage: 0.28, cpu graph: False, gen throughput (token/s): 613.46, #queue-req: 0,
[2026-01-05 10:59:45] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:29<00:03,  6.35it/s][2026-01-05 10:59:45] INFO:     127.0.0.1:49260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:45] INFO:     127.0.0.1:47164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:46] INFO:     127.0.0.1:47288 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:29<00:02,  7.46it/s][2026-01-05 10:59:46] INFO:     127.0.0.1:49320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:46] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:29<00:02,  7.70it/s][2026-01-05 10:59:46] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:46] INFO:     127.0.0.1:49366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:46] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:30<00:01,  8.44it/s]
 93%|█████████▎| 186/200 [00:30<00:01, 10.36it/s][2026-01-05 10:59:47] INFO:     127.0.0.1:49290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:47] INFO:     127.0.0.1:47186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:47] INFO:     127.0.0.1:47250 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:31<00:02,  5.15it/s]
 94%|█████████▍| 189/200 [00:31<00:02,  4.21it/s][2026-01-05 10:59:47 TP0] Decode batch, #running-req: 11, #token: 4608, token usage: 0.18, cpu graph: False, gen throughput (token/s): 339.15, #queue-req: 0,
[2026-01-05 10:59:47] INFO:     127.0.0.1:47236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:47] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:31<00:02,  4.28it/s]
 96%|█████████▌| 191/200 [00:31<00:01,  5.09it/s][2026-01-05 10:59:48] INFO:     127.0.0.1:47264 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:31<00:01,  4.35it/s][2026-01-05 10:59:48] INFO:     127.0.0.1:47246 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:31<00:01,  4.67it/s][2026-01-05 10:59:48] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:32<00:01,  3.70it/s][2026-01-05 10:59:49 TP0] Decode batch, #running-req: 6, #token: 3200, token usage: 0.12, cpu graph: False, gen throughput (token/s): 145.83, #queue-req: 0,
[2026-01-05 10:59:49] INFO:     127.0.0.1:49282 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:33<00:02,  2.13it/s][2026-01-05 10:59:50] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:33<00:01,  2.28it/s][2026-01-05 10:59:50] INFO:     127.0.0.1:48888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:50] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:33<00:00,  3.27it/s][2026-01-05 10:59:50] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 10:59:51 TP0] Decode batch, #running-req: 1, #token: 1152, token usage: 0.04, cpu graph: False, gen throughput (token/s): 56.87, #queue-req: 0,
[2026-01-05 10:59:51] INFO:     127.0.0.1:47404 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:35<00:00,  2.19it/s]
100%|██████████| 200/200 [00:35<00:00,  5.65it/s]
.
----------------------------------------------------------------------
Ran 1 test in 147.521s

OK
Accuracy: 0.580
Invalid: 0.005
Latency: 36.193 s
Output throughput: 576.462 token/s
.
.
End (8/43):
filename='ascend/llm_models/test_ascend_c4ai_command_r_v01.py', elapsed=159, estimated_time=400
.
.

.
.
Begin (9/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_llm_models_XVERSE.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:00:21] server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/xverse/XVERSE-MoE-A36B', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/xverse/XVERSE-MoE-A36B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=2048, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=200263299, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/xverse/XVERSE-MoE-A36B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:00:22] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2026-01-05 11:00:32 TP9] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:00:33 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 11:00:33 TP4] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:00:33 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:00:34 TP13] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-05 11:00:34 TP5] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:00:34 TP11] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 11:00:34 TP8] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:00:34 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:00:34 TP10] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:00:34 TP7] Init torch distributed begin.
[2026-01-05 11:00:34 TP14] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:00:35 TP15] Init torch distributed begin.
[2026-01-05 11:00:35 TP12] Init torch distributed begin.
[2026-01-05 11:00:35 TP6] Init torch distributed begin.
[2026-01-05 11:00:35 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[2026-01-05 11:00:38 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP14] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP15] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP12] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP8] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP13] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP11] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP9] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-05 11:00:38 TP10] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:00:38 TP3] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:00:38 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:00:39 TP15] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP10] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP14] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP13] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP8] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP11] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP12] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:00:39 TP9] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:00:39 TP10] Load weight begin. avail mem=60.90 GB
[2026-01-05 11:00:39 TP2] Load weight begin. avail mem=60.89 GB
[2026-01-05 11:00:39 TP4] Load weight begin. avail mem=60.90 GB
[2026-01-05 11:00:39 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 11:00:39 TP6] Load weight begin. avail mem=60.90 GB
[2026-01-05 11:00:39 TP14] Load weight begin. avail mem=60.90 GB
[2026-01-05 11:00:39 TP12] Load weight begin. avail mem=60.90 GB
[2026-01-05 11:00:39 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 11:00:39 TP8] Load weight begin. avail mem=60.89 GB
[2026-01-05 11:00:39 TP13] Load weight begin. avail mem=61.12 GB
[2026-01-05 11:00:39 TP7] Load weight begin. avail mem=61.12 GB
[2026-01-05 11:00:39 TP9] Load weight begin. avail mem=61.13 GB
[2026-01-05 11:00:39 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 11:00:39 TP5] Load weight begin. avail mem=61.12 GB
[2026-01-05 11:00:39 TP11] Load weight begin. avail mem=61.12 GB
[2026-01-05 11:00:39 TP15] Load weight begin. avail mem=61.12 GB

Loading safetensors checkpoint shards:   0% Completed | 0/51 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   2% Completed | 1/51 [00:07<06:31,  7.83s/it]

Loading safetensors checkpoint shards:   4% Completed | 2/51 [00:16<06:35,  8.06s/it]

Loading safetensors checkpoint shards:   6% Completed | 3/51 [00:24<06:43,  8.42s/it]

Loading safetensors checkpoint shards:   8% Completed | 4/51 [00:33<06:40,  8.52s/it]

Loading safetensors checkpoint shards:  10% Completed | 5/51 [00:43<06:52,  8.97s/it]

Loading safetensors checkpoint shards:  12% Completed | 6/51 [00:52<06:53,  9.19s/it]

Loading safetensors checkpoint shards:  14% Completed | 7/51 [01:01<06:40,  9.10s/it]

Loading safetensors checkpoint shards:  16% Completed | 8/51 [01:10<06:28,  9.02s/it]

Loading safetensors checkpoint shards:  18% Completed | 9/51 [01:19<06:16,  8.97s/it]

Loading safetensors checkpoint shards:  20% Completed | 10/51 [01:29<06:13,  9.11s/it]

Loading safetensors checkpoint shards:  22% Completed | 11/51 [01:38<06:08,  9.22s/it]

Loading safetensors checkpoint shards:  24% Completed | 12/51 [01:48<06:08,  9.44s/it]

Loading safetensors checkpoint shards:  25% Completed | 13/51 [01:57<05:57,  9.40s/it]

Loading safetensors checkpoint shards:  27% Completed | 14/51 [02:06<05:43,  9.28s/it]

Loading safetensors checkpoint shards:  29% Completed | 15/51 [02:15<05:31,  9.20s/it]

Loading safetensors checkpoint shards:  31% Completed | 16/51 [02:25<05:26,  9.33s/it]

Loading safetensors checkpoint shards:  33% Completed | 17/51 [02:34<05:13,  9.21s/it]

Loading safetensors checkpoint shards:  35% Completed | 18/51 [02:43<05:00,  9.12s/it]

Loading safetensors checkpoint shards:  37% Completed | 19/51 [02:52<04:53,  9.17s/it]

Loading safetensors checkpoint shards:  39% Completed | 20/51 [03:01<04:42,  9.10s/it]

Loading safetensors checkpoint shards:  41% Completed | 21/51 [03:10<04:31,  9.04s/it]

Loading safetensors checkpoint shards:  43% Completed | 22/51 [03:19<04:24,  9.12s/it]

Loading safetensors checkpoint shards:  45% Completed | 23/51 [03:27<04:01,  8.62s/it]

Loading safetensors checkpoint shards:  47% Completed | 24/51 [03:35<03:54,  8.69s/it]

Loading safetensors checkpoint shards:  49% Completed | 25/51 [03:45<03:54,  9.02s/it]

Loading safetensors checkpoint shards:  51% Completed | 26/51 [03:54<03:46,  9.06s/it]

Loading safetensors checkpoint shards:  53% Completed | 27/51 [04:03<03:35,  8.98s/it]

Loading safetensors checkpoint shards:  55% Completed | 28/51 [04:13<03:32,  9.22s/it]

Loading safetensors checkpoint shards:  57% Completed | 29/51 [04:22<03:23,  9.24s/it]

Loading safetensors checkpoint shards:  59% Completed | 30/51 [04:32<03:15,  9.32s/it]

Loading safetensors checkpoint shards:  61% Completed | 31/51 [04:41<03:06,  9.30s/it]

Loading safetensors checkpoint shards:  63% Completed | 32/51 [04:51<03:01,  9.56s/it]

Loading safetensors checkpoint shards:  65% Completed | 33/51 [05:00<02:48,  9.38s/it]

Loading safetensors checkpoint shards:  67% Completed | 34/51 [05:10<02:42,  9.53s/it]

Loading safetensors checkpoint shards:  69% Completed | 35/51 [05:20<02:35,  9.69s/it]

Loading safetensors checkpoint shards:  71% Completed | 36/51 [05:29<02:23,  9.59s/it]

Loading safetensors checkpoint shards:  73% Completed | 37/51 [05:39<02:11,  9.42s/it]

Loading safetensors checkpoint shards:  75% Completed | 38/51 [05:48<02:01,  9.37s/it]

Loading safetensors checkpoint shards:  76% Completed | 39/51 [05:57<01:52,  9.39s/it]

Loading safetensors checkpoint shards:  78% Completed | 40/51 [06:06<01:42,  9.34s/it]

Loading safetensors checkpoint shards:  80% Completed | 41/51 [06:16<01:33,  9.35s/it]

Loading safetensors checkpoint shards:  82% Completed | 42/51 [06:26<01:26,  9.63s/it]

Loading safetensors checkpoint shards:  84% Completed | 43/51 [06:35<01:15,  9.45s/it]

Loading safetensors checkpoint shards:  86% Completed | 44/51 [06:44<01:05,  9.38s/it]

Loading safetensors checkpoint shards:  88% Completed | 45/51 [06:54<00:56,  9.45s/it]

Loading safetensors checkpoint shards:  90% Completed | 46/51 [07:03<00:46,  9.29s/it]

Loading safetensors checkpoint shards:  92% Completed | 47/51 [07:13<00:38,  9.67s/it]

Loading safetensors checkpoint shards:  94% Completed | 48/51 [07:32<00:37, 12.41s/it]

Loading safetensors checkpoint shards:  96% Completed | 49/51 [08:05<00:36, 18.39s/it]

Loading safetensors checkpoint shards:  98% Completed | 50/51 [08:23<00:18, 18.40s/it]

Loading safetensors checkpoint shards: 100% Completed | 51/51 [08:50<00:00, 20.92s/it]

Loading safetensors checkpoint shards: 100% Completed | 51/51 [08:50<00:00, 10.40s/it]

[2026-01-05 11:09:33 TP5] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.30 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP0] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=30.99 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP10] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.07 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP4] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.07 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP2] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.07 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP1] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.30 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP9] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.30 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP14] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.07 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP11] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.30 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP12] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.07 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP7] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.30 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP15] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.30 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP13] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.29 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP6] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.07 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP3] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.30 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP8] Load weight end. type=XverseMoeForCausalLM, dtype=torch.bfloat16, avail mem=31.07 GB, mem usage=29.83 GB.
[2026-01-05 11:09:34 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:09:34 TP15] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP13] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP14] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP12] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP11] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP9] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP0] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP8] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP10] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP7] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP6] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP5] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP4] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP2] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP3] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP1] The available memory for KV cache is 18.83 GB.
[2026-01-05 11:09:34 TP13] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP7] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP14] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP3] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP9] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP1] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP11] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP6] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP12] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP15] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP8] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP2] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP10] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP0] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP14] Memory pool end. avail mem=12.20 GB
[2026-01-05 11:09:34 TP3] Memory pool end. avail mem=12.43 GB
[2026-01-05 11:09:34 TP13] Memory pool end. avail mem=12.43 GB
[2026-01-05 11:09:34 TP1] Memory pool end. avail mem=12.44 GB
[2026-01-05 11:09:34 TP9] Memory pool end. avail mem=12.43 GB
[2026-01-05 11:09:34 TP7] Memory pool end. avail mem=12.43 GB
[2026-01-05 11:09:34 TP11] Memory pool end. avail mem=12.43 GB
[2026-01-05 11:09:34 TP5] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP6] Memory pool end. avail mem=12.21 GB
[2026-01-05 11:09:34 TP12] Memory pool end. avail mem=12.21 GB
[2026-01-05 11:09:34 TP15] Memory pool end. avail mem=12.43 GB
[2026-01-05 11:09:34 TP8] Memory pool end. avail mem=12.20 GB
[2026-01-05 11:09:34 TP2] Memory pool end. avail mem=12.20 GB
[2026-01-05 11:09:34 TP10] Memory pool end. avail mem=12.21 GB
[2026-01-05 11:09:34 TP0] Memory pool end. avail mem=12.13 GB
[2026-01-05 11:09:34 TP5] Memory pool end. avail mem=12.43 GB
[2026-01-05 11:09:34 TP4] KV Cache is allocated. #tokens: 789632, K size: 9.41 GB, V size: 9.41 GB
[2026-01-05 11:09:34 TP4] Memory pool end. avail mem=12.21 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:09:35 TP0] max_total_num_tokens=789632, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=2048, available_gpu_mem=12.13 GB
[2026-01-05 11:09:35] INFO:     Started server process [128879]
[2026-01-05 11:09:35] INFO:     Waiting for application startup.
[2026-01-05 11:09:35] INFO:     Application startup complete.
[2026-01-05 11:09:35] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:09:36] INFO:     127.0.0.1:53454 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:09:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank12]:[W105 11:09:36.133859562 compiler_depend.ts:198] Warning: Driver Version: "k" is invalid or not supported yet. (function operator())
[rank9]:[W105 11:09:36.133871242 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank15]:[W105 11:09:36.133885482 compiler_depend.ts:198] Warning: Driver Version: "Q" is invalid or not supported yet. (function operator())
[rank11]:[W105 11:09:36.133885282 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank8]:[W105 11:09:36.133882892 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank13]:[W105 11:09:36.133887343 compiler_depend.ts:198] Warning: Driver Version: "۱" is invalid or not supported yet. (function operator())
[rank7]:[W105 11:09:36.133893973 compiler_depend.ts:198] Warning: Driver Version: "[" is invalid or not supported yet. (function operator())
[rank1]:[W105 11:09:36.133905283 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank6]:[W105 11:09:36.133916444 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank0]:[W105 11:09:36.133922164 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank2]:[W105 11:09:36.133924404 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank14]:[W105 11:09:36.133929044 compiler_depend.ts:198] Warning: Driver Version: "k" is invalid or not supported yet. (function operator())
[rank10]:[W105 11:09:36.133929024 compiler_depend.ts:198] Warning: Driver Version: "6" is invalid or not supported yet. (function operator())
[rank3]:[W105 11:09:36.133935064 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank5]:[W105 11:09:36.133939434 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank4]:[W105 11:09:36.133938264 compiler_depend.ts:198] Warning: Driver Version: "k" is invalid or not supported yet. (function operator())
[2026-01-05 11:09:45] INFO:     127.0.0.1:53468 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:09:55] INFO:     127.0.0.1:60058 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:10:05] INFO:     127.0.0.1:44460 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[rank3]:[W105 11:10:13.747629147 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank5]:[W105 11:10:13.747635087 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank10]:[W105 11:10:13.747633547 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank6]:[W105 11:10:13.757074968 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank9]:[W105 11:10:13.758496741 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank4]:[W105 11:10:13.759780318 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank13]:[W105 11:10:13.770717535 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank15]:[W105 11:10:13.772377266 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank14]:[W105 11:10:13.773372443 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank12]:[W105 11:10:13.775639898 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank2]:[W105 11:10:13.775639828 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank1]:[W105 11:10:13.786749390 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank11]:[W105 11:10:13.795580138 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank0]:[W105 11:10:13.815625293 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank8]:[W105 11:10:13.837517396 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank7]:[W105 11:10:13.844483455 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[2026-01-05 11:10:14] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:14] The server is fired up and ready to roll!
[2026-01-05 11:10:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:10:16] INFO:     127.0.0.1:38414 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:10:16] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:10:16] INFO:     127.0.0.1:38430 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:10:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:10:18] INFO:     127.0.0.1:38444 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/xverse/XVERSE-MoE-A36B --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 16 --context-length 2048 --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestXVERSE.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:10:18 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:10:18 TP0] Prefill batch, #new-seq: 29, #new-token: 4480, #cached-token: 29696, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 11:10:19 TP0] Prefill batch, #new-seq: 40, #new-token: 7168, #cached-token: 40960, token usage: 0.01, #running-req: 30, #queue-req: 0,
[2026-01-05 11:10:19 TP0] Prefill batch, #new-seq: 48, #new-token: 8192, #cached-token: 49152, token usage: 0.02, #running-req: 70, #queue-req: 10,
[2026-01-05 11:10:19 TP0] Prefill batch, #new-seq: 10, #new-token: 1792, #cached-token: 10240, token usage: 0.03, #running-req: 118, #queue-req: 0,
[2026-01-05 11:10:22 TP0] Decode batch, #running-req: 128, #token: 29568, token usage: 0.04, cpu graph: False, gen throughput (token/s): 6.77, #queue-req: 0,
[2026-01-05 11:10:23] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:04<15:13,  4.59s/it][2026-01-05 11:10:23] INFO:     127.0.0.1:33730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:23 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:10:23] INFO:     127.0.0.1:33136 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:05<04:31,  1.38s/it][2026-01-05 11:10:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:24] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:05<03:35,  1.10s/it][2026-01-05 11:10:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:24] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:05<02:34,  1.26it/s][2026-01-05 11:10:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:25] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:06<02:40,  1.21it/s][2026-01-05 11:10:25] INFO:     127.0.0.1:33084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:25] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:25] INFO:     127.0.0.1:33836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,

  4%|▍         | 8/200 [00:06<01:30,  2.11it/s]
  4%|▍         | 9/200 [00:06<00:50,  3.80it/s][2026-01-05 11:10:25 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 2048, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-05 11:10:26] INFO:     127.0.0.1:33572 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:07<00:54,  3.51it/s][2026-01-05 11:10:26] INFO:     127.0.0.1:33510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:26] INFO:     127.0.0.1:33912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:26 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:26] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:26] INFO:     127.0.0.1:33544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:26] INFO:     127.0.0.1:33584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:26] INFO:     127.0.0.1:33852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:26] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:07<00:34,  5.47it/s]
  8%|▊         | 17/200 [00:07<00:08, 22.15it/s]
  8%|▊         | 17/200 [00:07<00:08, 22.15it/s]
  8%|▊         | 17/200 [00:07<00:08, 22.15it/s]
  8%|▊         | 17/200 [00:07<00:08, 22.15it/s][2026-01-05 11:10:26 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 2048, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:10:26 TP0] Prefill batch, #new-seq: 5, #new-token: 768, #cached-token: 5120, token usage: 0.04, #running-req: 130, #queue-req: 0,
[2026-01-05 11:10:26] INFO:     127.0.0.1:33504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:26] INFO:     127.0.0.1:33664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:26] INFO:     127.0.0.1:33550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:26] INFO:     127.0.0.1:33802 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:07<00:12, 14.96it/s]
 10%|█         | 21/200 [00:07<00:14, 12.52it/s][2026-01-05 11:10:26 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:10:26 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 2048, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:10:27] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:27] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:27] INFO:     127.0.0.1:33436 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:08<00:17,  9.88it/s]
 12%|█▏        | 24/200 [00:08<00:19,  9.01it/s][2026-01-05 11:10:27 TP0] Decode batch, #running-req: 128, #token: 32128, token usage: 0.04, cpu graph: False, gen throughput (token/s): 1113.05, #queue-req: 0,
[2026-01-05 11:10:27] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:27 TP0] Prefill batch, #new-seq: 3, #new-token: 640, #cached-token: 3072, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-05 11:10:27 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:10:27] INFO:     127.0.0.1:33594 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:09<00:27,  6.40it/s][2026-01-05 11:10:27] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:27] INFO:     127.0.0.1:34018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:27 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:28] INFO:     127.0.0.1:33166 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:28] INFO:     127.0.0.1:33192 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:09<00:21,  7.89it/s]
 15%|█▌        | 30/200 [00:09<00:16, 10.37it/s][2026-01-05 11:10:28 TP0] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 2048, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:10:28 TP0] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 2048, token usage: 0.04, #running-req: 130, #queue-req: 0,
[2026-01-05 11:10:28] INFO:     127.0.0.1:33188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:28] INFO:     127.0.0.1:33532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:28] INFO:     127.0.0.1:33902 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:09<00:18,  8.87it/s]
 16%|█▋        | 33/200 [00:09<00:19,  8.76it/s][2026-01-05 11:10:28] INFO:     127.0.0.1:33264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:28] INFO:     127.0.0.1:33276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:28] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:28] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:28 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 3072, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-05 11:10:28 TP0] Prefill batch, #new-seq: 4, #new-token: 768, #cached-token: 4096, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:10:28] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:09<00:16,  9.85it/s][2026-01-05 11:10:28 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:29] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:29] INFO:     127.0.0.1:33408 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:29] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:10<00:19,  8.19it/s]
 20%|██        | 41/200 [00:10<00:20,  7.89it/s][2026-01-05 11:10:29 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:29] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:29] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:29 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 2048, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:10:29 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 2048, token usage: 0.04, #running-req: 130, #queue-req: 0,
[2026-01-05 11:10:29] INFO:     127.0.0.1:33032 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:10<00:21,  7.18it/s][2026-01-05 11:10:29] INFO:     127.0.0.1:32854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:29] INFO:     127.0.0.1:33898 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:11<00:19,  7.85it/s][2026-01-05 11:10:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:10:29 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-05 11:10:30] INFO:     127.0.0.1:33238 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:11<00:27,  5.54it/s][2026-01-05 11:10:30 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:30] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:12<00:33,  4.53it/s][2026-01-05 11:10:30] INFO:     127.0.0.1:33628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:30 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:10:31] INFO:     127.0.0.1:33930 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:12<00:36,  4.06it/s][2026-01-05 11:10:31 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:31] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:12<00:34,  4.28it/s][2026-01-05 11:10:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:31] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:31] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:31] INFO:     127.0.0.1:33718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:31] INFO:     127.0.0.1:33914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:31] INFO:     127.0.0.1:33950 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:12<00:32,  4.54it/s][2026-01-05 11:10:31 TP0] Prefill batch, #new-seq: 5, #new-token: 896, #cached-token: 5120, token usage: 0.04, #running-req: 123, #queue-req: 0,
[2026-01-05 11:10:31] INFO:     127.0.0.1:33670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:31] INFO:     127.0.0.1:33768 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:13<00:15,  9.12it/s]
 29%|██▉       | 58/200 [00:13<00:09, 14.47it/s][2026-01-05 11:10:32 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:10:32] INFO:     127.0.0.1:33456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:32] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:13<00:12, 11.01it/s][2026-01-05 11:10:32 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:10:32] INFO:     127.0.0.1:33606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:32] INFO:     127.0.0.1:33784 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:13<00:12, 11.07it/s][2026-01-05 11:10:32 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:10:32] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:32] INFO:     127.0.0.1:34084 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:13<00:12, 11.12it/s][2026-01-05 11:10:32 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:10:33 TP0] Decode batch, #running-req: 128, #token: 34432, token usage: 0.04, cpu graph: False, gen throughput (token/s): 888.52, #queue-req: 0,
[2026-01-05 11:10:33] INFO:     127.0.0.1:33578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:33] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:33] INFO:     127.0.0.1:33760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:33] INFO:     127.0.0.1:33944 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:14<00:16,  8.32it/s]
 34%|███▍      | 68/200 [00:14<00:15,  8.45it/s]
 34%|███▍      | 68/200 [00:14<00:15,  8.45it/s][2026-01-05 11:10:33] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:33 TP0] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 4096, token usage: 0.04, #running-req: 124, #queue-req: 0,
[2026-01-05 11:10:33 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:10:33] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:14<00:19,  6.62it/s][2026-01-05 11:10:33 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1024, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:10:33] INFO:     127.0.0.1:33868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:33] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 71/200 [00:14<00:19,  6.50it/s]
 36%|███▌      | 72/200 [00:14<00:17,  7.41it/s][2026-01-05 11:10:33 TP0] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 2048, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:10:33] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:33] INFO:     127.0.0.1:33922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:33] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:15<00:17,  7.10it/s]
 38%|███▊      | 75/200 [00:15<00:11, 10.43it/s]
 38%|███▊      | 75/200 [00:15<00:11, 10.43it/s][2026-01-05 11:10:34] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:34] INFO:     127.0.0.1:33560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:34] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:15<00:11, 10.76it/s]
 39%|███▉      | 78/200 [00:15<00:09, 12.58it/s][2026-01-05 11:10:34] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:34] INFO:     127.0.0.1:33058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:34] INFO:     127.0.0.1:33862 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:15<00:09, 12.20it/s][2026-01-05 11:10:34] INFO:     127.0.0.1:32968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:34] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:34] INFO:     127.0.0.1:33360 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:15<00:07, 15.12it/s][2026-01-05 11:10:34] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:34] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:34] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:15<00:08, 14.12it/s][2026-01-05 11:10:34] INFO:     127.0.0.1:32952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:35] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:35] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:16<00:09, 11.78it/s][2026-01-05 11:10:35] INFO:     127.0.0.1:33042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:35] INFO:     127.0.0.1:33312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:35] INFO:     127.0.0.1:33364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:35] INFO:     127.0.0.1:34096 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:16<00:10, 10.57it/s]
 47%|████▋     | 94/200 [00:16<00:07, 14.83it/s]
 47%|████▋     | 94/200 [00:16<00:07, 14.83it/s]
 47%|████▋     | 94/200 [00:16<00:07, 14.83it/s][2026-01-05 11:10:35] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:35] INFO:     127.0.0.1:33698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:35] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:35] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:16<00:08, 12.44it/s][2026-01-05 11:10:35] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:36] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:36] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:17<00:09, 10.41it/s]
 50%|█████     | 101/200 [00:17<00:09, 10.13it/s][2026-01-05 11:10:36] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:36] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:36] INFO:     127.0.0.1:58402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:36] INFO:     127.0.0.1:33886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:36] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:17<00:08, 11.40it/s]
 53%|█████▎    | 106/200 [00:17<00:07, 13.40it/s][2026-01-05 11:10:36] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:36] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:36] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:17<00:08, 10.80it/s][2026-01-05 11:10:36 TP0] Decode batch, #running-req: 91, #token: 28800, token usage: 0.04, cpu graph: False, gen throughput (token/s): 1169.41, #queue-req: 0,
[2026-01-05 11:10:36] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:36] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:18<00:08, 10.15it/s]
 56%|█████▌    | 111/200 [00:18<00:08, 10.69it/s][2026-01-05 11:10:37] INFO:     127.0.0.1:33104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:37] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:18<00:08,  9.94it/s][2026-01-05 11:10:37] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:37] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:18<00:09,  9.20it/s][2026-01-05 11:10:37] INFO:     127.0.0.1:33640 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:18<00:09,  8.47it/s][2026-01-05 11:10:37] INFO:     127.0.0.1:33204 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:18<00:10,  7.88it/s][2026-01-05 11:10:37] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:37] INFO:     127.0.0.1:33986 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:19<00:11,  7.38it/s]
 60%|█████▉    | 119/200 [00:19<00:09,  8.53it/s][2026-01-05 11:10:38] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:38] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:19<00:11,  6.81it/s]
 60%|██████    | 121/200 [00:19<00:11,  7.09it/s][2026-01-05 11:10:38] INFO:     127.0.0.1:33860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:38] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:19<00:09,  8.05it/s][2026-01-05 11:10:38] INFO:     127.0.0.1:33654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:38] INFO:     127.0.0.1:33998 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:19<00:08,  8.94it/s][2026-01-05 11:10:38] INFO:     127.0.0.1:58368 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:19<00:09,  8.12it/s][2026-01-05 11:10:38] INFO:     127.0.0.1:33792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:39] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:20<00:13,  5.44it/s][2026-01-05 11:10:39] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:39] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:20<00:12,  5.51it/s]
 65%|██████▌   | 130/200 [00:20<00:10,  6.82it/s][2026-01-05 11:10:39] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:39] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:20<00:08,  8.01it/s][2026-01-05 11:10:39] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:21<00:10,  6.57it/s][2026-01-05 11:10:40] INFO:     127.0.0.1:33616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:40] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:21<00:10,  6.37it/s]
 68%|██████▊   | 135/200 [00:21<00:08,  7.70it/s][2026-01-05 11:10:40 TP0] Decode batch, #running-req: 65, #token: 22400, token usage: 0.03, cpu graph: False, gen throughput (token/s): 902.50, #queue-req: 0,
[2026-01-05 11:10:40] INFO:     127.0.0.1:33020 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [00:21<00:08,  7.14it/s][2026-01-05 11:10:40] INFO:     127.0.0.1:33688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:40] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:21<00:07,  8.14it/s][2026-01-05 11:10:40] INFO:     127.0.0.1:33746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:40] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:21<00:06,  9.12it/s][2026-01-05 11:10:40] INFO:     127.0.0.1:33272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:40] INFO:     127.0.0.1:34034 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:21<00:07,  8.23it/s]
 71%|███████   | 142/200 [00:21<00:06,  9.25it/s][2026-01-05 11:10:40] INFO:     127.0.0.1:58446 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:22<00:06,  8.34it/s][2026-01-05 11:10:41] INFO:     127.0.0.1:33878 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:22<00:07,  7.70it/s][2026-01-05 11:10:41] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▎  | 145/200 [00:22<00:07,  7.19it/s][2026-01-05 11:10:41] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:42] INFO:     127.0.0.1:34058 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:23<00:13,  4.01it/s][2026-01-05 11:10:42] INFO:     127.0.0.1:58294 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:23<00:15,  3.27it/s][2026-01-05 11:10:42] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:42] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:42] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:23<00:13,  3.68it/s]
 76%|███████▌  | 151/200 [00:23<00:06,  7.75it/s]
 76%|███████▌  | 151/200 [00:23<00:06,  7.75it/s][2026-01-05 11:10:42] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:42] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:24<00:05,  8.70it/s][2026-01-05 11:10:43] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:43 TP0] Decode batch, #running-req: 46, #token: 17792, token usage: 0.02, cpu graph: False, gen throughput (token/s): 648.53, #queue-req: 0,
[2026-01-05 11:10:43] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:24<00:08,  5.52it/s][2026-01-05 11:10:43] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:43] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:25<00:07,  6.10it/s][2026-01-05 11:10:44] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:44] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:25<00:08,  5.18it/s]
 80%|███████▉  | 159/200 [00:25<00:07,  5.47it/s][2026-01-05 11:10:44] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:25<00:09,  4.39it/s][2026-01-05 11:10:44] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:44] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:44] INFO:     127.0.0.1:33776 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:26<00:06,  5.69it/s][2026-01-05 11:10:45] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:45] INFO:     127.0.0.1:58216 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:26<00:06,  5.33it/s][2026-01-05 11:10:46] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:27<00:10,  3.16it/s][2026-01-05 11:10:46] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:27<00:10,  3.27it/s][2026-01-05 11:10:46] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:27<00:08,  3.64it/s][2026-01-05 11:10:46 TP0] Decode batch, #running-req: 32, #token: 14592, token usage: 0.02, cpu graph: False, gen throughput (token/s): 449.94, #queue-req: 0,
[2026-01-05 11:10:47] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:28<00:08,  3.47it/s][2026-01-05 11:10:47] INFO:     127.0.0.1:58194 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:28<00:12,  2.47it/s][2026-01-05 11:10:47] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:29<00:09,  2.95it/s][2026-01-05 11:10:48] INFO:     127.0.0.1:34128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:10:48] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:29<00:08,  3.21it/s]
 86%|████████▋ | 173/200 [00:29<00:06,  4.40it/s][2026-01-05 11:10:49] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:31<00:15,  1.71it/s][2026-01-05 11:10:50 TP0] Decode batch, #running-req: 26, #token: 13312, token usage: 0.02, cpu graph: False, gen throughput (token/s): 347.14, #queue-req: 0,
[2026-01-05 11:10:51] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:32<00:19,  1.30it/s][2026-01-05 11:10:51] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:32<00:17,  1.39it/s][2026-01-05 11:10:52] INFO:     127.0.0.1:33444 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:33<00:17,  1.34it/s][2026-01-05 11:10:53 TP0] Decode batch, #running-req: 23, #token: 12288, token usage: 0.02, cpu graph: False, gen throughput (token/s): 302.24, #queue-req: 0,
[2026-01-05 11:10:56 TP0] Decode batch, #running-req: 23, #token: 13312, token usage: 0.02, cpu graph: False, gen throughput (token/s): 284.22, #queue-req: 0,
[2026-01-05 11:10:57] INFO:     127.0.0.1:58306 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:38<00:40,  1.83s/it][2026-01-05 11:10:58] INFO:     127.0.0.1:58398 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:40<00:37,  1.79s/it][2026-01-05 11:10:59 TP0] Decode batch, #running-req: 21, #token: 13696, token usage: 0.02, cpu graph: False, gen throughput (token/s): 269.64, #queue-req: 0,
[2026-01-05 11:11:03 TP0] Decode batch, #running-req: 21, #token: 13952, token usage: 0.02, cpu graph: False, gen throughput (token/s): 257.99, #queue-req: 0,
[2026-01-05 11:11:06] INFO:     127.0.0.1:32924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06 TP0] Decode batch, #running-req: 21, #token: 5760, token usage: 0.01, cpu graph: False, gen throughput (token/s): 249.78, #queue-req: 0,
[2026-01-05 11:11:06] INFO:     127.0.0.1:33074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06] INFO:     127.0.0.1:33178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06] INFO:     127.0.0.1:33224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06] INFO:     127.0.0.1:33758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06] INFO:     127.0.0.1:33790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06] INFO:     127.0.0.1:33808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:06] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:47<01:10,  3.51s/it]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s]
 96%|█████████▌| 192/200 [00:47<00:05,  1.40it/s][2026-01-05 11:11:09 TP0] Decode batch, #running-req: 8, #token: 6400, token usage: 0.01, cpu graph: False, gen throughput (token/s): 102.33, #queue-req: 0,
[2026-01-05 11:11:11] INFO:     127.0.0.1:57988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:11] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:53<00:06,  1.12it/s]
 97%|█████████▋| 194/200 [00:53<00:06,  1.07s/it][2026-01-05 11:11:13 TP0] Decode batch, #running-req: 6, #token: 5248, token usage: 0.01, cpu graph: False, gen throughput (token/s): 91.10, #queue-req: 0,
[2026-01-05 11:11:14] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:14] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:14] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:56<00:06,  1.20s/it]
 98%|█████████▊| 197/200 [00:56<00:03,  1.19s/it]
 98%|█████████▊| 197/200 [00:56<00:03,  1.19s/it][2026-01-05 11:11:15] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:15] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:11:15] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:56<00:00,  1.01it/s]
100%|██████████| 200/200 [00:56<00:00,  1.37it/s]
100%|██████████| 200/200 [00:56<00:00,  3.54it/s]
.
----------------------------------------------------------------------
Ran 1 test in 672.207s

OK
Accuracy: 0.255
Invalid: 0.005
Latency: 59.236 s
Output throughput: 536.820 token/s
.
.
End (9/43):
filename='ascend/llm_models/test_ascend_llm_models_XVERSE.py', elapsed=696, estimated_time=400
.
.

.
.
Begin (10/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_pp_single_node.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:11:55] server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.837, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=256, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=1004036575, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:11:55] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:12:04] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:12:05] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:12:05] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:12:06] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:12:06] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.76s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.76s/it]

[2026-01-05 11:12:10] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=59.57 GB, mem usage=1.25 GB.
[2026-01-05 11:12:10] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:12:10] The available memory for KV cache is 49.65 GB.
[2026-01-05 11:12:10] KV Cache is allocated. #tokens: 464768, K size: 24.83 GB, V size: 24.83 GB
[2026-01-05 11:12:10] Memory pool end. avail mem=9.28 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:12:10] max_total_num_tokens=464768, chunked_prefill_size=256, max_prefill_tokens=16384, max_running_requests=4096, context_len=40960, available_gpu_mem=9.26 GB
[2026-01-05 11:12:11] INFO:     Started server process [151061]
[2026-01-05 11:12:11] INFO:     Waiting for application startup.
[2026-01-05 11:12:11] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-05 11:12:11] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-05 11:12:11] INFO:     Application startup complete.
[2026-01-05 11:12:11] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:12:12] INFO:     127.0.0.1:50854 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:12:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:12:12.104932337 compiler_depend.ts:198] Warning: Driver Version: "(" is invalid or not supported yet. (function operator())
[2026-01-05 11:12:14] INFO:     127.0.0.1:50858 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:12:24] INFO:     127.0.0.1:35358 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:12:34] INFO:     127.0.0.1:53754 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:12:35] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:35] The server is fired up and ready to roll!
[2026-01-05 11:12:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:12:45] INFO:     127.0.0.1:60966 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:12:45] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:12:45] INFO:     127.0.0.1:60968 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:12:45] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:12:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:12:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:12:47] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:12:48] INFO:     127.0.0.1:60972 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-0___6B --chunked-prefill-size 256 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestQwenPPTieWeightsAccuracy.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:12:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:12:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 1, #queue-req: 11,
[2026-01-05 11:12:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 3, #queue-req: 21,
[2026-01-05 11:12:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 4, #queue-req: 33,
[2026-01-05 11:12:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 6, #queue-req: 44,
[2026-01-05 11:12:48] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 8, #queue-req: 51,
[2026-01-05 11:12:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.00, #running-req: 9, #queue-req: 49,
[2026-01-05 11:12:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 11, #queue-req: 47,
[2026-01-05 11:12:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 13, #queue-req: 45,
[2026-01-05 11:12:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 15, #queue-req: 43,
[2026-01-05 11:12:48] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 17, #queue-req: 50,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 19, #queue-req: 58,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 21, #queue-req: 67,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 23, #queue-req: 76,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 25, #queue-req: 84,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 27, #queue-req: 93,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 29, #queue-req: 97,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 31, #queue-req: 95,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 33, #queue-req: 93,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 35, #queue-req: 91,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 37, #queue-req: 89,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 39, #queue-req: 87,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 41, #queue-req: 85,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.01, #running-req: 42, #queue-req: 84,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.01, #running-req: 44, #queue-req: 82,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 46, #queue-req: 80,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 48, #queue-req: 78,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 50, #queue-req: 76,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 52, #queue-req: 74,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 54, #queue-req: 72,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 56, #queue-req: 71,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 57, #queue-req: 69,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 59, #queue-req: 67,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 61, #queue-req: 65,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 63, #queue-req: 63,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 65, #queue-req: 61,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.02, #running-req: 66, #queue-req: 60,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 68, #queue-req: 58,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 70, #queue-req: 56,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 72, #queue-req: 54,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 74, #queue-req: 52,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 76, #queue-req: 50,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.02, #running-req: 78, #queue-req: 48,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 80, #queue-req: 46,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 82, #queue-req: 44,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.03, #running-req: 84, #queue-req: 43,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 85, #queue-req: 41,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 87, #queue-req: 39,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 89, #queue-req: 37,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 91, #queue-req: 35,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 93, #queue-req: 33,
[2026-01-05 11:12:49] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 95, #queue-req: 31,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 97, #queue-req: 29,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 99, #queue-req: 27,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 101, #queue-req: 25,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 103, #queue-req: 23,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 105, #queue-req: 21,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 107, #queue-req: 19,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 109, #queue-req: 17,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 111, #queue-req: 15,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 113, #queue-req: 13,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.03, #running-req: 115, #queue-req: 11,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 117, #queue-req: 9,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 119, #queue-req: 7,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 121, #queue-req: 5,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 123, #queue-req: 3,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 125, #queue-req: 1,
[2026-01-05 11:12:50] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:50] INFO:     127.0.0.1:46354 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<07:26,  2.24s/it][2026-01-05 11:12:50] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:51] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:51] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:51] INFO:     127.0.0.1:46444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:51] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:02<02:12,  1.48it/s]
  2%|▏         | 4/200 [00:02<01:01,  3.20it/s][2026-01-05 11:12:51] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:12:51] INFO:     127.0.0.1:46830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:51] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:51] Decode batch, #running-req: 127, #token: 21120, token usage: 0.05, cpu graph: False, gen throughput (token/s): 83.98, #queue-req: 0,
[2026-01-05 11:12:51] INFO:     127.0.0.1:46234 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:02<00:45,  4.26it/s][2026-01-05 11:12:51] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:51] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:51] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:52] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:03<01:11,  2.69it/s][2026-01-05 11:12:52] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:52] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:52] INFO:     127.0.0.1:47054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:52] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:12:52] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-05 11:12:52] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:04<00:44,  4.27it/s][2026-01-05 11:12:52] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:52] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:52] INFO:     127.0.0.1:46200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:52] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:12:52] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-05 11:12:52] INFO:     127.0.0.1:46176 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:04<00:30,  6.11it/s][2026-01-05 11:12:52] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:52] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:52] INFO:     127.0.0.1:46586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:45976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:12:53] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 130, #queue-req: 1,
[2026-01-05 11:12:53] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 131, #queue-req: 0,
[2026-01-05 11:12:53] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:04<00:18,  9.70it/s][2026-01-05 11:12:53] INFO:     127.0.0.1:46460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:53] INFO:     127.0.0.1:46600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:12:53] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-05 11:12:53] INFO:     127.0.0.1:46530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:46750 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:04<00:15, 11.56it/s]
 12%|█▏        | 23/200 [00:04<00:12, 14.72it/s][2026-01-05 11:12:53] INFO:     127.0.0.1:46218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-05 11:12:53] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:46346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:46510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:46514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:12:53] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.05, #running-req: 129, #queue-req: 4,
[2026-01-05 11:12:53] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 131, #queue-req: 2,
[2026-01-05 11:12:53] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 133, #queue-req: 0,
[2026-01-05 11:12:53] INFO:     127.0.0.1:46134 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:04<00:07, 21.68it/s][2026-01-05 11:12:53] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:53] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:46692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-05 11:12:53] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:04<00:08, 20.61it/s]
 18%|█▊        | 35/200 [00:04<00:07, 21.29it/s][2026-01-05 11:12:53] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-05 11:12:53] INFO:     127.0.0.1:46966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:53] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:46948 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:05<00:07, 21.20it/s]
 20%|█▉        | 39/200 [00:05<00:07, 22.89it/s][2026-01-05 11:12:53] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:47020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:53] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 125, #queue-req: 1,
[2026-01-05 11:12:53] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 126, #queue-req: 3,
[2026-01-05 11:12:53] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 1,
[2026-01-05 11:12:53] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 130, #queue-req: 0,
[2026-01-05 11:12:53] INFO:     127.0.0.1:47046 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:05<00:06, 22.75it/s][2026-01-05 11:12:53] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:47068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-05 11:12:54] Decode batch, #running-req: 128, #token: 28288, token usage: 0.06, cpu graph: False, gen throughput (token/s): 1821.76, #queue-req: 0,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:05<00:06, 22.22it/s][2026-01-05 11:12:54] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 125, #queue-req: 1,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,

 25%|██▌       | 50/200 [00:05<00:05, 25.91it/s][2026-01-05 11:12:54] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:46450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:47196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,

 26%|██▋       | 53/200 [00:05<00:06, 24.21it/s]
 27%|██▋       | 54/200 [00:05<00:05, 25.08it/s][2026-01-05 11:12:54] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46918 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:05<00:06, 23.72it/s]
 29%|██▉       | 58/200 [00:05<00:05, 24.81it/s][2026-01-05 11:12:54] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 126, #queue-req: 1,

 30%|███       | 61/200 [00:05<00:05, 24.99it/s][2026-01-05 11:12:54] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:12:54] INFO:     127.0.0.1:46244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:46412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:47152 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:06<00:06, 21.44it/s]
 32%|███▎      | 65/200 [00:06<00:06, 21.18it/s][2026-01-05 11:12:54] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:46304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:46754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:46886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:47094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:54] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 125, #queue-req: 1,
[2026-01-05 11:12:54] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 127, #queue-req: 6,
[2026-01-05 11:12:54] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 5,
[2026-01-05 11:12:54] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 130, #queue-req: 3,
[2026-01-05 11:12:54] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.06, #running-req: 132, #queue-req: 1,
[2026-01-05 11:12:54] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.06, #running-req: 134, #queue-req: 0,
[2026-01-05 11:12:55] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:06<00:04, 27.32it/s][2026-01-05 11:12:55] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46408 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:06<00:04, 30.14it/s]
 40%|████      | 80/200 [00:06<00:03, 34.43it/s][2026-01-05 11:12:55] INFO:     127.0.0.1:46060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47166 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46846 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:06<00:03, 34.56it/s][2026-01-05 11:12:55] INFO:     127.0.0.1:46652 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46972 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:06<00:03, 33.94it/s][2026-01-05 11:12:55] INFO:     127.0.0.1:45984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] Decode batch, #running-req: 109, #token: 24320, token usage: 0.05, cpu graph: False, gen throughput (token/s): 3382.32, #queue-req: 0,
[2026-01-05 11:12:55] INFO:     127.0.0.1:46464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:06<00:02, 39.50it/s][2026-01-05 11:12:55] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47216 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 101/200 [00:06<00:02, 41.96it/s]
 51%|█████     | 102/200 [00:06<00:02, 46.48it/s][2026-01-05 11:12:55] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:07<00:02, 44.78it/s][2026-01-05 11:12:55] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47408 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:07<00:01, 48.51it/s][2026-01-05 11:12:55] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:55] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47432 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:07<00:01, 46.08it/s][2026-01-05 11:12:56] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:46252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:46892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:46856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47512 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:07<00:01, 38.27it/s]
 63%|██████▎   | 126/200 [00:07<00:02, 35.78it/s][2026-01-05 11:12:56] INFO:     127.0.0.1:46700 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:46870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47306 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:07<00:01, 45.36it/s][2026-01-05 11:12:56] INFO:     127.0.0.1:47604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47712 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:46848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:07<00:01, 48.65it/s][2026-01-05 11:12:56] Decode batch, #running-req: 59, #token: 16384, token usage: 0.04, cpu graph: False, gen throughput (token/s): 3436.24, #queue-req: 0,
[2026-01-05 11:12:56] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47700 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:46466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47544 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:07<00:01, 46.95it/s]
 74%|███████▍  | 149/200 [00:07<00:01, 47.89it/s][2026-01-05 11:12:56] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:08<00:01, 32.10it/s]
 78%|███████▊  | 155/200 [00:08<00:01, 26.68it/s][2026-01-05 11:12:56] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:56] INFO:     127.0.0.1:47630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:46160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:46334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47276 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:08<00:01, 29.48it/s]
 80%|████████  | 161/200 [00:08<00:01, 33.80it/s][2026-01-05 11:12:57] INFO:     127.0.0.1:46710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47346 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:46020 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:08<00:00, 36.25it/s][2026-01-05 11:12:57] INFO:     127.0.0.1:47292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:46438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47660 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:08<00:00, 36.37it/s][2026-01-05 11:12:57] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] Decode batch, #running-req: 27, #token: 9344, token usage: 0.02, cpu graph: False, gen throughput (token/s): 1890.38, #queue-req: 0,
[2026-01-05 11:12:57] INFO:     127.0.0.1:46092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:09<00:01, 22.70it/s][2026-01-05 11:12:57] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:45968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:57] INFO:     127.0.0.1:47358 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:09<00:00, 24.10it/s]
 90%|█████████ | 181/200 [00:09<00:00, 27.00it/s][2026-01-05 11:12:57] INFO:     127.0.0.1:47632 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:58] INFO:     127.0.0.1:47250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:58] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:58] INFO:     127.0.0.1:46210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:58] INFO:     127.0.0.1:47454 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:09<00:00, 18.94it/s]
 93%|█████████▎| 186/200 [00:09<00:00, 16.45it/s][2026-01-05 11:12:58] Decode batch, #running-req: 14, #token: 5760, token usage: 0.01, cpu graph: False, gen throughput (token/s): 964.53, #queue-req: 0,
[2026-01-05 11:12:58] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:58] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:59] INFO:     127.0.0.1:47202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:12:59] INFO:     127.0.0.1:47320 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:10<00:01,  9.78it/s]
 95%|█████████▌| 190/200 [00:10<00:01,  7.82it/s][2026-01-05 11:12:59] Decode batch, #running-req: 10, #token: 4608, token usage: 0.01, cpu graph: False, gen throughput (token/s): 596.87, #queue-req: 0,
[2026-01-05 11:12:59] Decode batch, #running-req: 10, #token: 5376, token usage: 0.01, cpu graph: False, gen throughput (token/s): 477.64, #queue-req: 0,
[2026-01-05 11:13:00] INFO:     127.0.0.1:47522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:13:00] INFO:     127.0.0.1:47386 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:12<00:02,  3.90it/s][2026-01-05 11:13:00] Decode batch, #running-req: 8, #token: 4608, token usage: 0.01, cpu graph: False, gen throughput (token/s): 435.10, #queue-req: 0,
[2026-01-05 11:13:01] Decode batch, #running-req: 8, #token: 4864, token usage: 0.01, cpu graph: False, gen throughput (token/s): 384.46, #queue-req: 0,
[2026-01-05 11:13:02] Decode batch, #running-req: 8, #token: 5504, token usage: 0.01, cpu graph: False, gen throughput (token/s): 383.14, #queue-req: 0,
[2026-01-05 11:13:03] Decode batch, #running-req: 8, #token: 5632, token usage: 0.01, cpu graph: False, gen throughput (token/s): 384.33, #queue-req: 0,
[2026-01-05 11:13:04] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:13:04] Decode batch, #running-req: 8, #token: 2688, token usage: 0.01, cpu graph: False, gen throughput (token/s): 383.20, #queue-req: 0,
[2026-01-05 11:13:04] INFO:     127.0.0.1:45992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:13:04] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:13:04] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:13:04] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:15<00:04,  1.67it/s]
 98%|█████████▊| 197/200 [00:15<00:02,  1.23it/s]
 98%|█████████▊| 197/200 [00:15<00:02,  1.23it/s]
 98%|█████████▊| 197/200 [00:15<00:02,  1.23it/s]
 98%|█████████▊| 197/200 [00:15<00:02,  1.23it/s][2026-01-05 11:13:04] Decode batch, #running-req: 3, #token: 2688, token usage: 0.01, cpu graph: False, gen throughput (token/s): 150.40, #queue-req: 0,
[2026-01-05 11:13:05] INFO:     127.0.0.1:47238 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:16<00:01,  1.20it/s][2026-01-05 11:13:05] INFO:     127.0.0.1:47508 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:16<00:00,  1.26it/s][2026-01-05 11:13:05] INFO:     127.0.0.1:47558 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:17<00:00, 11.72it/s]
.
----------------------------------------------------------------------
Ran 1 test in 81.426s

OK
Accuracy: 0.430
Invalid: 0.010
Latency: 20.135 s
Output throughput: 1124.874 token/s
.
.
End (10/43):
filename='ascend/llm_models/test_ascend_pp_single_node.py', elapsed=97, estimated_time=400
.
.

.
.
Begin (11/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_glm4_9b_chat.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:13:26] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat', tokenizer_path='/root/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=814573829, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
[2026-01-05 11:13:26] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:13:35] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:13:36] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:13:36] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:13:37] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:13:37] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  10% Completed | 1/10 [00:02<00:19,  2.17s/it]

Loading safetensors checkpoint shards:  20% Completed | 2/10 [00:04<00:19,  2.44s/it]

Loading safetensors checkpoint shards:  30% Completed | 3/10 [00:07<00:17,  2.45s/it]

Loading safetensors checkpoint shards:  40% Completed | 4/10 [00:09<00:14,  2.47s/it]

Loading safetensors checkpoint shards:  50% Completed | 5/10 [00:12<00:12,  2.46s/it]

Loading safetensors checkpoint shards:  60% Completed | 6/10 [00:14<00:09,  2.46s/it]

Loading safetensors checkpoint shards:  70% Completed | 7/10 [00:17<00:07,  2.46s/it]

Loading safetensors checkpoint shards:  80% Completed | 8/10 [00:19<00:04,  2.41s/it]

Loading safetensors checkpoint shards:  90% Completed | 9/10 [00:21<00:02,  2.41s/it]

Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:24<00:00,  2.38s/it]

Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:24<00:00,  2.41s/it]

[2026-01-05 11:14:02] Load weight end. type=ChatGLMModel, dtype=torch.bfloat16, avail mem=43.28 GB, mem usage=17.54 GB.
[2026-01-05 11:14:02] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:14:02] The available memory for KV cache is 31.10 GB.
[2026-01-05 11:14:02] KV Cache is allocated. #tokens: 815104, K size: 15.55 GB, V size: 15.55 GB
[2026-01-05 11:14:02] Memory pool end. avail mem=10.62 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:14:03] max_total_num_tokens=815104, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=3184, context_len=131072, available_gpu_mem=10.62 GB
[2026-01-05 11:14:03] INFO:     Started server process [154032]
[2026-01-05 11:14:03] INFO:     Waiting for application startup.
[2026-01-05 11:14:03] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.8, 'top_k': 50, 'top_p': 0.8}
[2026-01-05 11:14:03] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.8, 'top_k': 50, 'top_p': 0.8}
[2026-01-05 11:14:03] INFO:     Application startup complete.
[2026-01-05 11:14:03] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:14:04] INFO:     127.0.0.1:38916 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:14:04] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:14:04.992104468 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 11:14:06] INFO:     127.0.0.1:43784 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:14:16] INFO:     127.0.0.1:53174 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:14:17] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:17] The server is fired up and ready to roll!
[2026-01-05 11:14:26] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:14:27] INFO:     127.0.0.1:59510 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:14:27] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:14:27] INFO:     127.0.0.1:59516 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:14:27] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:14:27] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/ZhipuAI/glm-4-9b-chat --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestGLM49BChat.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:14:27] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:14:27] Prefill batch, #new-seq: 12, #new-token: 1792, #cached-token: 7680, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 11:14:27] Prefill batch, #new-seq: 29, #new-token: 3840, #cached-token: 18560, token usage: 0.00, #running-req: 13, #queue-req: 0,
[2026-01-05 11:14:27] Prefill batch, #new-seq: 24, #new-token: 3072, #cached-token: 15360, token usage: 0.01, #running-req: 42, #queue-req: 0,
[2026-01-05 11:14:28] Prefill batch, #new-seq: 60, #new-token: 8192, #cached-token: 38400, token usage: 0.01, #running-req: 66, #queue-req: 2,
[2026-01-05 11:14:28] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-05 11:14:29] Decode batch, #running-req: 128, #token: 23168, token usage: 0.03, cpu graph: False, gen throughput (token/s): 72.89, #queue-req: 0,
[2026-01-05 11:14:29] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<07:19,  2.21s/it][2026-01-05 11:14:29] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-05 11:14:30] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:02<03:15,  1.01it/s][2026-01-05 11:14:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-05 11:14:30] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-05 11:14:30] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:02<01:27,  2.23it/s][2026-01-05 11:14:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-05 11:14:30] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] INFO:     127.0.0.1:60146 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:02<01:08,  2.86it/s]
  3%|▎         | 6/200 [00:02<00:41,  4.62it/s][2026-01-05 11:14:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-05 11:14:30] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] INFO:     127.0.0.1:59948 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:02<00:39,  4.83it/s]
  4%|▍         | 8/200 [00:02<00:30,  6.34it/s][2026-01-05 11:14:30] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-05 11:14:30] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:14:30] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] INFO:     127.0.0.1:60478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-05 11:14:30] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 13/200 [00:03<00:16, 11.55it/s][2026-01-05 11:14:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:14:30] INFO:     127.0.0.1:59540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] INFO:     127.0.0.1:59678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:14:30] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] INFO:     127.0.0.1:60244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:30] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.04, #running-req: 130, #queue-req: 0,
[2026-01-05 11:14:31] INFO:     127.0.0.1:60206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:60616 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:03<00:10, 16.68it/s]
 10%|█         | 20/200 [00:03<00:08, 21.81it/s][2026-01-05 11:14:31] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:14:31] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:59990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:14:31] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:03<00:08, 19.75it/s]
 12%|█▏        | 24/200 [00:03<00:08, 19.74it/s][2026-01-05 11:14:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:14:31] INFO:     127.0.0.1:60046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:14:31] INFO:     127.0.0.1:59914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:03<00:09, 18.52it/s][2026-01-05 11:14:31] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:14:31] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:03<00:10, 16.84it/s][2026-01-05 11:14:31] Decode batch, #running-req: 128, #token: 29440, token usage: 0.04, cpu graph: False, gen throughput (token/s): 2758.54, #queue-req: 0,
[2026-01-05 11:14:31] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:14:31] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:14:31] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 131, #queue-req: 0,
[2026-01-05 11:14:31] INFO:     127.0.0.1:59654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:59746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:04<00:08, 19.09it/s]
 18%|█▊        | 36/200 [00:04<00:06, 25.58it/s]
 18%|█▊        | 36/200 [00:04<00:06, 25.58it/s][2026-01-05 11:14:31] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-05 11:14:31] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:14:31] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:60408 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:31] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:04<00:06, 24.60it/s]
 20%|██        | 41/200 [00:04<00:05, 28.11it/s]
 20%|██        | 41/200 [00:04<00:05, 28.11it/s][2026-01-05 11:14:31] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 2560, token usage: 0.04, #running-req: 124, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:04<00:06, 25.01it/s][2026-01-05 11:14:32] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:59740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] INFO:     127.0.0.1:59932 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:04<00:06, 22.03it/s]
 24%|██▍       | 48/200 [00:04<00:06, 21.79it/s][2026-01-05 11:14:32] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:59878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 129, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:60218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:04<00:06, 21.25it/s]
 26%|██▋       | 53/200 [00:04<00:06, 22.48it/s][2026-01-05 11:14:32] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:59700 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] INFO:     127.0.0.1:60592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.03, #running-req: 129, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:04<00:06, 21.84it/s]
 29%|██▉       | 58/200 [00:04<00:06, 22.96it/s][2026-01-05 11:14:32] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 126, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:59842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:32] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.03, #running-req: 130, #queue-req: 0,
[2026-01-05 11:14:32] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:05<00:05, 23.53it/s][2026-01-05 11:14:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-05 11:14:33] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:05<00:05, 23.30it/s][2026-01-05 11:14:33] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.03, #running-req: 125, #queue-req: 0,
[2026-01-05 11:14:33] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:59610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:59964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.03, #running-req: 128, #queue-req: 0,
[2026-01-05 11:14:33] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 132, #queue-req: 0,
[2026-01-05 11:14:33] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:05<00:07, 18.22it/s][2026-01-05 11:14:33] INFO:     127.0.0.1:60230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60764 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 121, #queue-req: 0,
[2026-01-05 11:14:33] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:05<00:04, 25.13it/s][2026-01-05 11:14:33] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:05<00:03, 29.28it/s][2026-01-05 11:14:33] Decode batch, #running-req: 117, #token: 26624, token usage: 0.03, cpu graph: False, gen throughput (token/s): 2403.53, #queue-req: 0,
[2026-01-05 11:14:33] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:60598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:33] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [00:06<00:03, 29.60it/s]
 44%|████▍     | 89/200 [00:06<00:03, 31.80it/s][2026-01-05 11:14:33] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60108 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:06<00:04, 26.46it/s][2026-01-05 11:14:34] INFO:     127.0.0.1:60840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:06<00:03, 29.75it/s][2026-01-05 11:14:34] INFO:     127.0.0.1:60898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:32840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:32980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:33016 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:06<00:02, 33.30it/s]
 55%|█████▌    | 110/200 [00:06<00:01, 63.09it/s]
 55%|█████▌    | 110/200 [00:06<00:01, 63.09it/s]
 55%|█████▌    | 110/200 [00:06<00:01, 63.09it/s]
 55%|█████▌    | 110/200 [00:06<00:01, 63.09it/s]
 55%|█████▌    | 110/200 [00:06<00:01, 63.09it/s][2026-01-05 11:14:34] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:06<00:01, 55.06it/s][2026-01-05 11:14:34] INFO:     127.0.0.1:33032 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:59962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60968 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:06<00:01, 44.66it/s]
 62%|██████▏   | 124/200 [00:06<00:01, 40.30it/s][2026-01-05 11:14:34] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] INFO:     127.0.0.1:60532 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [00:07<00:01, 44.13it/s]
 66%|██████▋   | 133/200 [00:07<00:01, 55.66it/s]
 66%|██████▋   | 133/200 [00:07<00:01, 55.66it/s][2026-01-05 11:14:34] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:34] Decode batch, #running-req: 66, #token: 18304, token usage: 0.02, cpu graph: False, gen throughput (token/s): 2993.35, #queue-req: 0,
[2026-01-05 11:14:34] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:60618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:60718 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:07<00:01, 43.54it/s][2026-01-05 11:14:35] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:32994 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:07<00:01, 33.81it/s][2026-01-05 11:14:35] INFO:     127.0.0.1:60662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:32786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:60786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:32888 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:07<00:01, 37.36it/s][2026-01-05 11:14:35] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:60856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:35] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 155/200 [00:08<00:01, 22.75it/s]
 78%|███████▊  | 156/200 [00:08<00:02, 18.28it/s][2026-01-05 11:14:35] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:36] INFO:     127.0.0.1:32798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:36] INFO:     127.0.0.1:60810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:36] INFO:     127.0.0.1:33038 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:08<00:02, 19.02it/s]
 80%|████████  | 160/200 [00:08<00:01, 21.11it/s][2026-01-05 11:14:36] Decode batch, #running-req: 40, #token: 12416, token usage: 0.02, cpu graph: False, gen throughput (token/s): 1830.90, #queue-req: 0,
[2026-01-05 11:14:36] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:36] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:36] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:08<00:01, 20.47it/s][2026-01-05 11:14:36] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:36] INFO:     127.0.0.1:60774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:36] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:08<00:01, 20.08it/s][2026-01-05 11:14:36] INFO:     127.0.0.1:32790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:36] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:36] INFO:     127.0.0.1:32774 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:08<00:01, 19.49it/s][2026-01-05 11:14:36] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:36] INFO:     127.0.0.1:32824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:36] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:09<00:01, 15.03it/s][2026-01-05 11:14:36] INFO:     127.0.0.1:60886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:37] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:09<00:01, 13.35it/s][2026-01-05 11:14:37] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:37] Decode batch, #running-req: 26, #token: 9472, token usage: 0.01, cpu graph: False, gen throughput (token/s): 1150.43, #queue-req: 0,
[2026-01-05 11:14:37] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:09<00:01, 13.60it/s][2026-01-05 11:14:37] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:37] INFO:     127.0.0.1:60848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:37] INFO:     127.0.0.1:60712 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:09<00:01, 14.84it/s][2026-01-05 11:14:37] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:37] INFO:     127.0.0.1:60742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:37] INFO:     127.0.0.1:32904 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:09<00:01, 12.56it/s][2026-01-05 11:14:37] INFO:     127.0.0.1:32962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:38] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:10<00:01, 10.55it/s][2026-01-05 11:14:38] INFO:     127.0.0.1:60990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:38] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:10<00:01,  9.94it/s][2026-01-05 11:14:38] Decode batch, #running-req: 15, #token: 6272, token usage: 0.01, cpu graph: False, gen throughput (token/s): 710.91, #queue-req: 0,
[2026-01-05 11:14:38] INFO:     127.0.0.1:60612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:38] INFO:     127.0.0.1:32850 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:11<00:01,  7.04it/s][2026-01-05 11:14:39] Decode batch, #running-req: 12, #token: 6016, token usage: 0.01, cpu graph: False, gen throughput (token/s): 500.33, #queue-req: 0,
[2026-01-05 11:14:39] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:11<00:02,  4.61it/s][2026-01-05 11:14:40] INFO:     127.0.0.1:60874 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:12<00:03,  3.24it/s][2026-01-05 11:14:40] Decode batch, #running-req: 10, #token: 5376, token usage: 0.01, cpu graph: False, gen throughput (token/s): 425.15, #queue-req: 0,
[2026-01-05 11:14:41] Decode batch, #running-req: 10, #token: 5632, token usage: 0.01, cpu graph: False, gen throughput (token/s): 393.29, #queue-req: 0,
[2026-01-05 11:14:41] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:14<00:05,  1.64it/s][2026-01-05 11:14:42] Decode batch, #running-req: 9, #token: 5888, token usage: 0.01, cpu graph: False, gen throughput (token/s): 372.95, #queue-req: 0,
[2026-01-05 11:14:43] Decode batch, #running-req: 9, #token: 6144, token usage: 0.01, cpu graph: False, gen throughput (token/s): 355.24, #queue-req: 0,
[2026-01-05 11:14:44] Decode batch, #running-req: 9, #token: 2432, token usage: 0.00, cpu graph: False, gen throughput (token/s): 353.22, #queue-req: 0,
[2026-01-05 11:14:44] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:44] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:44] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:44] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:44] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:14:44] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:16<00:08,  1.06s/it]
 98%|█████████▊| 197/200 [00:16<00:01,  1.74it/s]
 98%|█████████▊| 197/200 [00:16<00:01,  1.74it/s]
 98%|█████████▊| 197/200 [00:16<00:01,  1.74it/s]
 98%|█████████▊| 197/200 [00:16<00:01,  1.74it/s]
 98%|█████████▊| 197/200 [00:16<00:01,  1.74it/s][2026-01-05 11:14:45] Decode batch, #running-req: 3, #token: 2560, token usage: 0.00, cpu graph: False, gen throughput (token/s): 124.75, #queue-req: 0,
[2026-01-05 11:14:46] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:18<00:01,  1.45it/s][2026-01-05 11:14:46] Decode batch, #running-req: 2, #token: 2048, token usage: 0.00, cpu graph: False, gen throughput (token/s): 112.42, #queue-req: 0,
[2026-01-05 11:14:46] INFO:     127.0.0.1:32942 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:18<00:00,  1.51it/s][2026-01-05 11:14:46] INFO:     127.0.0.1:33036 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:19<00:00,  1.70it/s]
100%|██████████| 200/200 [00:19<00:00, 10.48it/s]
.
----------------------------------------------------------------------
Ran 1 test in 90.445s

OK
Accuracy: 0.780
Invalid: 0.000
Latency: 19.167 s
Output throughput: 1287.750 token/s
.
.
End (11/43):
filename='ascend/llm_models/test_ascend_glm4_9b_chat.py', elapsed=101, estimated_time=400
.
.

.
.
Begin (12/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_llm_models_stablelm-2-1_6b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:15:10] server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/stabilityai/stablelm-2-1_6b', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/stabilityai/stablelm-2-1_6b', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=41413867, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/stabilityai/stablelm-2-1_6b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:15:10] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:15:24] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:15:25] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:15:25] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:15:26] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:15:26] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.08s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.08s/it]

[2026-01-05 11:15:32] Load weight end. type=StableLmForCausalLM, dtype=torch.float16, avail mem=57.75 GB, mem usage=3.07 GB.
[2026-01-05 11:15:32] Using KV cache dtype: torch.float16
[2026-01-05 11:15:32] The available memory for KV cache is 45.59 GB.
[2026-01-05 11:15:32] KV Cache is allocated. #tokens: 248960, K size: 22.80 GB, V size: 22.80 GB
[2026-01-05 11:15:32] Memory pool end. avail mem=12.07 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:15:33] max_total_num_tokens=248960, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4096, available_gpu_mem=12.07 GB
[2026-01-05 11:15:33] INFO:     Started server process [156433]
[2026-01-05 11:15:33] INFO:     Waiting for application startup.
[2026-01-05 11:15:33] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 11:15:33] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 11:15:33] INFO:     Application startup complete.
[2026-01-05 11:15:33] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:15:34] INFO:     127.0.0.1:52432 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:15:34] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:15:34.445248020 compiler_depend.ts:198] Warning: Driver Version: "޳" is invalid or not supported yet. (function operator())
[2026-01-05 11:15:37] INFO:     127.0.0.1:38188 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:15:47] INFO:     127.0.0.1:59666 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:15:52] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:15:52] The server is fired up and ready to roll!
[2026-01-05 11:15:57] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:15:58] INFO:     127.0.0.1:45058 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:15:58] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:15:58] INFO:     127.0.0.1:45072 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:15:58] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:15:58] INFO:     127.0.0.1:45080 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/stabilityai/stablelm-2-1_6b --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --tp-size 1 --enable-torch-compile --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestStablelm.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:15:58] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:15:58] Prefill batch, #new-seq: 12, #new-token: 1792, #cached-token: 9216, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 11:15:58] Prefill batch, #new-seq: 24, #new-token: 3072, #cached-token: 18432, token usage: 0.01, #running-req: 13, #queue-req: 0,
[2026-01-05 11:15:58] Prefill batch, #new-seq: 22, #new-token: 3200, #cached-token: 16896, token usage: 0.02, #running-req: 37, #queue-req: 0,
[2026-01-05 11:15:58] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.04, #running-req: 59, #queue-req: 0,
[2026-01-05 11:15:58] Prefill batch, #new-seq: 12, #new-token: 1536, #cached-token: 9216, token usage: 0.04, #running-req: 60, #queue-req: 0,
[2026-01-05 11:15:58] Prefill batch, #new-seq: 12, #new-token: 1664, #cached-token: 9216, token usage: 0.04, #running-req: 72, #queue-req: 0,
[2026-01-05 11:15:58] Prefill batch, #new-seq: 16, #new-token: 2048, #cached-token: 12288, token usage: 0.05, #running-req: 84, #queue-req: 0,
[2026-01-05 11:15:58] Prefill batch, #new-seq: 20, #new-token: 2560, #cached-token: 15360, token usage: 0.06, #running-req: 100, #queue-req: 0,
[2026-01-05 11:15:58] Prefill batch, #new-seq: 8, #new-token: 1024, #cached-token: 6144, token usage: 0.07, #running-req: 120, #queue-req: 0,
[2026-01-05 11:15:59] INFO:     127.0.0.1:46060 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:01<03:44,  1.13s/it][2026-01-05 11:15:59] INFO:     127.0.0.1:45518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:15:59] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-05 11:15:59] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.08, #running-req: 128, #queue-req: 0,
[2026-01-05 11:15:59] Decode batch, #running-req: 128, #token: 21376, token usage: 0.09, cpu graph: False, gen throughput (token/s): 111.69, #queue-req: 0,
[2026-01-05 11:15:59] INFO:     127.0.0.1:46000 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:01<01:14,  2.66it/s][2026-01-05 11:15:59] INFO:     127.0.0.1:46136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:15:59] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:15:59] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 11:15:59] INFO:     127.0.0.1:45248 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:01<00:45,  4.31it/s][2026-01-05 11:15:59] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:16:00] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:01<00:48,  4.02it/s]
  4%|▎         | 7/200 [00:01<00:39,  4.89it/s][2026-01-05 11:16:00] INFO:     127.0.0.1:45748 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] INFO:     127.0.0.1:46096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 11:16:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 11:16:00] INFO:     127.0.0.1:45600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] INFO:     127.0.0.1:46044 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:02<00:27,  6.81it/s]
  6%|▌         | 11/200 [00:02<00:20,  9.33it/s][2026-01-05 11:16:00] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-05 11:16:00] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] INFO:     127.0.0.1:45972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 11:16:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-05 11:16:00] INFO:     127.0.0.1:45344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] INFO:     127.0.0.1:45380 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:02<00:15, 11.97it/s]
  8%|▊         | 16/200 [00:02<00:11, 15.41it/s][2026-01-05 11:16:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-05 11:16:00] INFO:     127.0.0.1:45088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] INFO:     127.0.0.1:45724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-05 11:16:00] INFO:     127.0.0.1:45316 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:02<00:10, 17.36it/s][2026-01-05 11:16:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:16:00] INFO:     127.0.0.1:45728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 11:16:00] INFO:     127.0.0.1:45650 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:02<00:11, 15.10it/s][2026-01-05 11:16:00] INFO:     127.0.0.1:46014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:16:00] INFO:     127.0.0.1:46188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] INFO:     127.0.0.1:45712 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] INFO:     127.0.0.1:45832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] INFO:     127.0.0.1:46152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:00] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:16:01] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-05 11:16:01] Decode batch, #running-req: 128, #token: 30336, token usage: 0.12, cpu graph: False, gen throughput (token/s): 3350.73, #queue-req: 0,
[2026-01-05 11:16:01] INFO:     127.0.0.1:46114 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:02<00:08, 19.87it/s][2026-01-05 11:16:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:16:01] INFO:     127.0.0.1:45586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 11:16:01] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:03<00:08, 19.52it/s][2026-01-05 11:16:01] INFO:     127.0.0.1:45752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:16:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:16:01] INFO:     127.0.0.1:45812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] INFO:     127.0.0.1:45936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:03<00:08, 19.18it/s]
 18%|█▊        | 36/200 [00:03<00:07, 23.37it/s]
 18%|█▊        | 36/200 [00:03<00:07, 23.37it/s][2026-01-05 11:16:01] INFO:     127.0.0.1:45676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.11, #running-req: 124, #queue-req: 0,
[2026-01-05 11:16:01] INFO:     127.0.0.1:45738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 11:16:01] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 131, #queue-req: 0,
[2026-01-05 11:16:01] INFO:     127.0.0.1:45868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] INFO:     127.0.0.1:46210 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:03<00:05, 26.71it/s]
 22%|██▏       | 43/200 [00:03<00:05, 30.77it/s][2026-01-05 11:16:01] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 11:16:01] INFO:     127.0.0.1:45294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:16:01] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] INFO:     127.0.0.1:45566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:16:01] INFO:     127.0.0.1:45578 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:03<00:05, 26.49it/s][2026-01-05 11:16:01] INFO:     127.0.0.1:45438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:16:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-05 11:16:01] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] INFO:     127.0.0.1:45614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:01] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,

 25%|██▌       | 50/200 [00:03<00:06, 24.32it/s][2026-01-05 11:16:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:16:02] INFO:     127.0.0.1:45816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:45896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:03<00:05, 24.90it/s][2026-01-05 11:16:02] INFO:     127.0.0.1:46012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-05 11:16:02] INFO:     127.0.0.1:45364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:45490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 11:16:02] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-05 11:16:02] INFO:     127.0.0.1:45590 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:04<00:06, 23.36it/s][2026-01-05 11:16:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 11:16:02] INFO:     127.0.0.1:45430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:45646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-05 11:16:02] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:04<00:06, 22.27it/s][2026-01-05 11:16:02] INFO:     127.0.0.1:45308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 11:16:02] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:45998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 11:16:02] INFO:     127.0.0.1:45328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:46076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:04<00:05, 24.14it/s]
 33%|███▎      | 66/200 [00:04<00:04, 32.19it/s]
 33%|███▎      | 66/200 [00:04<00:04, 32.19it/s][2026-01-05 11:16:02] INFO:     127.0.0.1:45320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-05 11:16:02] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 11:16:02] INFO:     127.0.0.1:46260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 11:16:02] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:45658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:46380 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:04<00:04, 30.03it/s]
 36%|███▋      | 73/200 [00:04<00:03, 36.35it/s]
 36%|███▋      | 73/200 [00:04<00:03, 36.35it/s]
 36%|███▋      | 73/200 [00:04<00:03, 36.35it/s][2026-01-05 11:16:02] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:45604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 11:16:02] Decode batch, #running-req: 125, #token: 27008, token usage: 0.11, cpu graph: False, gen throughput (token/s): 2890.33, #queue-req: 0,
[2026-01-05 11:16:02] INFO:     127.0.0.1:45284 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:45322 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:04<00:03, 35.04it/s][2026-01-05 11:16:02] INFO:     127.0.0.1:45132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:45158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:02] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:46172 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:04<00:03, 32.48it/s]
 42%|████▏     | 84/200 [00:04<00:03, 34.60it/s]
 42%|████▏     | 84/200 [00:04<00:03, 34.60it/s][2026-01-05 11:16:03] INFO:     127.0.0.1:45354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:04<00:02, 39.26it/s][2026-01-05 11:16:03] INFO:     127.0.0.1:45196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:46252 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:05<00:02, 38.99it/s]
 48%|████▊     | 97/200 [00:05<00:02, 40.86it/s][2026-01-05 11:16:03] INFO:     127.0.0.1:45190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:46112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:46232 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:05<00:02, 40.39it/s][2026-01-05 11:16:03] INFO:     127.0.0.1:45900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:05<00:02, 32.81it/s][2026-01-05 11:16:03] INFO:     127.0.0.1:45174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:05<00:02, 34.19it/s][2026-01-05 11:16:03] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:45796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] Decode batch, #running-req: 87, #token: 22400, token usage: 0.09, cpu graph: False, gen throughput (token/s): 4046.38, #queue-req: 0,
[2026-01-05 11:16:03] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:03] INFO:     127.0.0.1:46534 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:05<00:02, 31.77it/s][2026-01-05 11:16:03] INFO:     127.0.0.1:46248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:45506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:05<00:02, 31.78it/s][2026-01-05 11:16:04] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:45210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:05<00:01, 40.49it/s][2026-01-05 11:16:04] INFO:     127.0.0.1:45198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46822 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:06<00:02, 32.91it/s]
 68%|██████▊   | 135/200 [00:06<00:02, 32.29it/s]
 68%|██████▊   | 135/200 [00:06<00:02, 32.29it/s]
 68%|██████▊   | 135/200 [00:06<00:02, 32.29it/s][2026-01-05 11:16:04] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:45562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:45556 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:06<00:01, 31.12it/s]
 70%|███████   | 140/200 [00:06<00:01, 31.88it/s][2026-01-05 11:16:04] INFO:     127.0.0.1:45982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:06<00:01, 33.44it/s]
 72%|███████▎  | 145/200 [00:06<00:01, 37.01it/s][2026-01-05 11:16:04] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] Decode batch, #running-req: 54, #token: 16128, token usage: 0.06, cpu graph: False, gen throughput (token/s): 2744.68, #queue-req: 0,
[2026-01-05 11:16:04] INFO:     127.0.0.1:46732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46778 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:06<00:01, 27.18it/s][2026-01-05 11:16:04] INFO:     127.0.0.1:45304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:04] INFO:     127.0.0.1:46804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:06<00:01, 30.35it/s]
 78%|███████▊  | 155/200 [00:06<00:01, 34.98it/s][2026-01-05 11:16:05] INFO:     127.0.0.1:46270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:45618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:46408 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 159/200 [00:06<00:01, 32.61it/s]
 80%|████████  | 160/200 [00:06<00:01, 32.97it/s][2026-01-05 11:16:05] INFO:     127.0.0.1:45300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:45726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:07<00:01, 23.60it/s][2026-01-05 11:16:05] INFO:     127.0.0.1:45730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:46198 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:07<00:01, 21.42it/s][2026-01-05 11:16:05] INFO:     127.0.0.1:45142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:46594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:45086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] INFO:     127.0.0.1:46320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:05] Decode batch, #running-req: 29, #token: 10112, token usage: 0.04, cpu graph: False, gen throughput (token/s): 1649.05, #queue-req: 0,
[2026-01-05 11:16:05] INFO:     127.0.0.1:46830 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:07<00:01, 22.85it/s][2026-01-05 11:16:05] INFO:     127.0.0.1:46520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:06] INFO:     127.0.0.1:46844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:06] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:06] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:07<00:01, 20.11it/s]
 88%|████████▊ | 176/200 [00:07<00:01, 19.76it/s][2026-01-05 11:16:06] INFO:     127.0.0.1:46572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:06] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:06] Decode batch, #running-req: 22, #token: 8960, token usage: 0.04, cpu graph: False, gen throughput (token/s): 1059.66, #queue-req: 0,
[2026-01-05 11:16:06] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:08<00:02, 10.31it/s][2026-01-05 11:16:06] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:07] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:08<00:02,  9.04it/s][2026-01-05 11:16:07] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:07] INFO:     127.0.0.1:46842 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:09<00:01,  9.40it/s][2026-01-05 11:16:07] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:07] INFO:     127.0.0.1:45468 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:09<00:01, 10.50it/s][2026-01-05 11:16:07] INFO:     127.0.0.1:46276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:07] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:09<00:01, 10.97it/s][2026-01-05 11:16:07] Decode batch, #running-req: 13, #token: 6528, token usage: 0.03, cpu graph: False, gen throughput (token/s): 805.16, #queue-req: 0,
[2026-01-05 11:16:08] Decode batch, #running-req: 13, #token: 6784, token usage: 0.03, cpu graph: False, gen throughput (token/s): 590.25, #queue-req: 0,
[2026-01-05 11:16:08] INFO:     127.0.0.1:46358 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:08] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:10<00:02,  4.19it/s][2026-01-05 11:16:09] Decode batch, #running-req: 11, #token: 6144, token usage: 0.02, cpu graph: False, gen throughput (token/s): 523.17, #queue-req: 0,
[2026-01-05 11:16:09] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:11<00:03,  3.27it/s][2026-01-05 11:16:10] INFO:     127.0.0.1:45768 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:12<00:03,  2.63it/s][2026-01-05 11:16:10] Decode batch, #running-req: 9, #token: 5760, token usage: 0.02, cpu graph: False, gen throughput (token/s): 458.32, #queue-req: 0,
[2026-01-05 11:16:11] Decode batch, #running-req: 9, #token: 5888, token usage: 0.02, cpu graph: False, gen throughput (token/s): 410.01, #queue-req: 0,
[2026-01-05 11:16:12] Decode batch, #running-req: 9, #token: 3840, token usage: 0.02, cpu graph: False, gen throughput (token/s): 411.59, #queue-req: 0,
[2026-01-05 11:16:12] INFO:     127.0.0.1:45546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:12] INFO:     127.0.0.1:45808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:12] INFO:     127.0.0.1:45914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:12] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:13<00:05,  1.48it/s]
 98%|█████████▊| 195/200 [00:13<00:02,  1.72it/s]
 98%|█████████▊| 195/200 [00:13<00:02,  1.72it/s]
 98%|█████████▊| 195/200 [00:13<00:02,  1.72it/s][2026-01-05 11:16:12] Decode batch, #running-req: 5, #token: 3968, token usage: 0.02, cpu graph: False, gen throughput (token/s): 233.10, #queue-req: 0,
[2026-01-05 11:16:13] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:15<00:02,  1.43it/s][2026-01-05 11:16:13] Decode batch, #running-req: 4, #token: 3328, token usage: 0.01, cpu graph: False, gen throughput (token/s): 213.66, #queue-req: 0,
[2026-01-05 11:16:14] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:15<00:02,  1.50it/s][2026-01-05 11:16:14] INFO:     127.0.0.1:46718 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:15<00:01,  1.71it/s][2026-01-05 11:16:14] INFO:     127.0.0.1:46792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:16:14] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:16<00:00,  2.05it/s]
100%|██████████| 200/200 [00:16<00:00,  3.05it/s]
100%|██████████| 200/200 [00:16<00:00, 12.39it/s]
.
----------------------------------------------------------------------
Ran 1 test in 77.476s

OK
Accuracy: 0.195
Invalid: 0.000
Latency: 16.188 s
Output throughput: 1662.255 token/s
.
.
End (12/43):
filename='ascend/llm_models/test_ascend_llm_models_stablelm-2-1_6b.py', elapsed=89, estimated_time=400
.
.

.
.
Begin (13/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_persimmon_8b_chat.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:16:36] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Howeee/persimmon-8b-chat', tokenizer_path='/root/.cache/modelscope/hub/models/Howeee/persimmon-8b-chat', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=765162728, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Howeee/persimmon-8b-chat', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-05 11:17:08] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:17:17] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:17:18] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:17:18] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:17:19] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:17:19] Load weight begin. avail mem=60.82 GB

Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading pt checkpoint shards:  50% Completed | 1/2 [00:22<00:22, 22.36s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:47<00:00, 23.89s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:47<00:00, 23.66s/it]

[2026-01-05 11:18:07] Load weight end. type=PersimmonForCausalLM, dtype=torch.bfloat16, avail mem=43.30 GB, mem usage=17.53 GB.
[2026-01-05 11:18:07] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:18:07] The available memory for KV cache is 31.13 GB.
[2026-01-05 11:18:07] KV Cache is allocated. #tokens: 56576, K size: 15.57 GB, V size: 15.57 GB
[2026-01-05 11:18:07] Memory pool end. avail mem=12.03 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:18:39] max_total_num_tokens=56576, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=16384, available_gpu_mem=12.03 GB
[2026-01-05 11:18:40] INFO:     Started server process [159204]
[2026-01-05 11:18:40] INFO:     Waiting for application startup.
[2026-01-05 11:18:40] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 11:18:40] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 11:18:40] INFO:     Application startup complete.
[2026-01-05 11:18:40] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:18:41] INFO:     127.0.0.1:45130 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:18:41] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:18:41.540608214 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 11:18:46] INFO:     127.0.0.1:54800 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:18:55] INFO:     127.0.0.1:45132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:18:55] The server is fired up and ready to roll!
[2026-01-05 11:18:56] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:18:57] INFO:     127.0.0.1:57698 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:18:57] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:18:57] INFO:     127.0.0.1:57714 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:18:57] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:18:57] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Howeee/persimmon-8b-chat --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:18:57] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-05 11:18:57] Prefill batch, #new-seq: 14, #new-token: 3584, #cached-token: 8960, token usage: 0.02, #running-req: 1, #queue-req: 0,
[2026-01-05 11:18:57] Prefill batch, #new-seq: 27, #new-token: 6912, #cached-token: 17280, token usage: 0.08, #running-req: 15, #queue-req: 0,
[2026-01-05 11:18:58] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 20480, token usage: 0.20, #running-req: 42, #queue-req: 16,
[2026-01-05 11:18:58] Prefill batch, #new-seq: 13, #new-token: 3328, #cached-token: 8320, token usage: 0.35, #running-req: 74, #queue-req: 41,
[2026-01-05 11:18:59] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<07:03,  2.13s/it][2026-01-05 11:18:59] Prefill batch, #new-seq: 7, #new-token: 1792, #cached-token: 4480, token usage: 0.40, #running-req: 86, #queue-req: 35,
[2026-01-05 11:19:01] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:03<05:45,  1.74s/it][2026-01-05 11:19:01] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.43, #running-req: 92, #queue-req: 33,
[2026-01-05 11:19:01] Decode batch, #running-req: 95, #token: 24704, token usage: 0.44, cpu graph: False, gen throughput (token/s): 20.22, #queue-req: 33,
[2026-01-05 11:19:01] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:03<03:29,  1.06s/it][2026-01-05 11:19:01] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.44, #running-req: 94, #queue-req: 32,
[2026-01-05 11:19:01] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:01] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.44, #running-req: 95, #queue-req: 32,
[2026-01-05 11:19:01] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:04<01:40,  1.95it/s][2026-01-05 11:19:01] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.44, #running-req: 95, #queue-req: 32,
[2026-01-05 11:19:02] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:04<01:40,  1.93it/s][2026-01-05 11:19:02] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.44, #running-req: 95, #queue-req: 30,
[2026-01-05 11:19:02] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:02] INFO:     127.0.0.1:58216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:02] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.45, #running-req: 96, #queue-req: 30,
[2026-01-05 11:19:02] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:02] INFO:     127.0.0.1:58416 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:05<01:01,  3.11it/s]
  5%|▌         | 10/200 [00:05<00:41,  4.58it/s][2026-01-05 11:19:02] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:02] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:02] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.44, #running-req: 96, #queue-req: 29,
[2026-01-05 11:19:02] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:02] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:02] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 2560, token usage: 0.44, #running-req: 95, #queue-req: 29,
[2026-01-05 11:19:03] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:05<00:24,  7.53it/s][2026-01-05 11:19:03] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.46, #running-req: 98, #queue-req: 29,
[2026-01-05 11:19:03] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:03] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:03] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:03] INFO:     127.0.0.1:58006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:03] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:03] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.44, #running-req: 98, #queue-req: 27,

  8%|▊         | 17/200 [00:05<00:24,  7.38it/s]
 10%|█         | 20/200 [00:05<00:15, 11.64it/s]
 10%|█         | 20/200 [00:05<00:15, 11.64it/s]
 10%|█         | 20/200 [00:05<00:15, 11.64it/s][2026-01-05 11:19:03] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.46, #running-req: 97, #queue-req: 29,
[2026-01-05 11:19:03] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:03] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.46, #running-req: 98, #queue-req: 29,
[2026-01-05 11:19:03] INFO:     127.0.0.1:58214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:03] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:05<00:16, 10.77it/s]
 12%|█▏        | 23/200 [00:05<00:15, 11.13it/s][2026-01-05 11:19:03] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.46, #running-req: 97, #queue-req: 29,
[2026-01-05 11:19:03] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:03] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.47, #running-req: 98, #queue-req: 29,
[2026-01-05 11:19:03] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:03] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK

 12%|█▎        | 25/200 [00:06<00:19,  9.07it/s]
 13%|█▎        | 26/200 [00:06<00:19,  8.74it/s][2026-01-05 11:19:03] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.47, #running-req: 97, #queue-req: 29,
[2026-01-05 11:19:04] INFO:     127.0.0.1:58402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.47, #running-req: 98, #queue-req: 29,
[2026-01-05 11:19:04] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:06<00:18,  9.27it/s][2026-01-05 11:19:04] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.47, #running-req: 98, #queue-req: 28,
[2026-01-05 11:19:04] Decode batch, #running-req: 98, #token: 27008, token usage: 0.48, cpu graph: False, gen throughput (token/s): 1513.99, #queue-req: 28,
[2026-01-05 11:19:04] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.48, #running-req: 99, #queue-req: 28,
[2026-01-05 11:19:04] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:06<00:19,  8.81it/s][2026-01-05 11:19:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.48, #running-req: 99, #queue-req: 28,
[2026-01-05 11:19:04] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:06<00:18,  8.96it/s][2026-01-05 11:19:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.48, #running-req: 99, #queue-req: 28,
[2026-01-05 11:19:04] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:04] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.48, #running-req: 99, #queue-req: 28,
[2026-01-05 11:19:04] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:04] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:06<00:19,  8.68it/s]
 17%|█▋        | 34/200 [00:06<00:17,  9.76it/s][2026-01-05 11:19:04] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.48, #running-req: 98, #queue-req: 28,
[2026-01-05 11:19:04] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:04] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:04] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:07<00:16,  9.78it/s]
 18%|█▊        | 37/200 [00:07<00:10, 15.33it/s]
 18%|█▊        | 37/200 [00:07<00:10, 15.33it/s][2026-01-05 11:19:04] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.48, #running-req: 97, #queue-req: 28,
[2026-01-05 11:19:04] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:04] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:07<00:11, 13.54it/s][2026-01-05 11:19:05] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:05] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.48, #running-req: 98, #queue-req: 28,
[2026-01-05 11:19:05] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:07<00:10, 14.90it/s][2026-01-05 11:19:05] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.49, #running-req: 98, #queue-req: 29,
[2026-01-05 11:19:05] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:05] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.49, #running-req: 98, #queue-req: 28,
[2026-01-05 11:19:05] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:07<00:13, 11.60it/s][2026-01-05 11:19:05] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:05] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.50, #running-req: 99, #queue-req: 28,
[2026-01-05 11:19:05] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.50, #running-req: 99, #queue-req: 28,
[2026-01-05 11:19:05] INFO:     127.0.0.1:58540 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:07<00:15, 10.01it/s][2026-01-05 11:19:05] INFO:     127.0.0.1:58512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:05] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:05] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:05] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.50, #running-req: 98, #queue-req: 28,

 24%|██▎       | 47/200 [00:08<00:14, 10.80it/s]
 24%|██▍       | 48/200 [00:08<00:11, 13.13it/s][2026-01-05 11:19:05] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.51, #running-req: 98, #queue-req: 29,
[2026-01-05 11:19:05] INFO:     127.0.0.1:58344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:05] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.52, #running-req: 98, #queue-req: 29,
[2026-01-05 11:19:06] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:08<00:12, 11.55it/s][2026-01-05 11:19:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.52, #running-req: 98, #queue-req: 29,
[2026-01-05 11:19:06] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:06] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:06] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.51, #running-req: 98, #queue-req: 28,
[2026-01-05 11:19:06] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:06] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:08<00:13, 10.77it/s]
 27%|██▋       | 54/200 [00:08<00:10, 13.77it/s]
 27%|██▋       | 54/200 [00:08<00:10, 13.77it/s][2026-01-05 11:19:06] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:06] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.51, #running-req: 96, #queue-req: 30,
[2026-01-05 11:19:06] INFO:     127.0.0.1:58044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:06] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:08<00:10, 13.80it/s]
 28%|██▊       | 57/200 [00:08<00:09, 15.54it/s][2026-01-05 11:19:06] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 96, #queue-req: 30,
[2026-01-05 11:19:06] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:06] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 97, #queue-req: 30,

 30%|██▉       | 59/200 [00:08<00:11, 12.09it/s][2026-01-05 11:19:06] INFO:     127.0.0.1:58738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 96, #queue-req: 31,
[2026-01-05 11:19:06] INFO:     127.0.0.1:58142 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:09<00:10, 12.67it/s][2026-01-05 11:19:06] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 96, #queue-req: 31,
[2026-01-05 11:19:06] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:06] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:06] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:06] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.52, #running-req: 94, #queue-req: 31,
[2026-01-05 11:19:06] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:09<00:08, 15.85it/s][2026-01-05 11:19:07] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 96, #queue-req: 30,
[2026-01-05 11:19:07] Decode batch, #running-req: 96, #token: 28800, token usage: 0.51, cpu graph: False, gen throughput (token/s): 1365.56, #queue-req: 30,
[2026-01-05 11:19:07] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:07] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:07] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:07] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:09<00:08, 16.53it/s]
 34%|███▍      | 69/200 [00:09<00:05, 23.90it/s]
 34%|███▍      | 69/200 [00:09<00:05, 23.90it/s][2026-01-05 11:19:07] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 2560, token usage: 0.51, #running-req: 94, #queue-req: 30,
[2026-01-05 11:19:07] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.52, #running-req: 97, #queue-req: 30,
[2026-01-05 11:19:07] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:07] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:09<00:06, 20.15it/s][2026-01-05 11:19:07] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 96, #queue-req: 30,
[2026-01-05 11:19:07] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:07] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 97, #queue-req: 28,
[2026-01-05 11:19:07] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:07] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 98, #queue-req: 26,
[2026-01-05 11:19:07] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:09<00:10, 12.42it/s][2026-01-05 11:19:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 99, #queue-req: 25,
[2026-01-05 11:19:07] INFO:     127.0.0.1:58864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 99, #queue-req: 24,
[2026-01-05 11:19:07] INFO:     127.0.0.1:58306 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:10<00:10, 12.14it/s][2026-01-05 11:19:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 99, #queue-req: 23,
[2026-01-05 11:19:07] INFO:     127.0.0.1:57966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:07] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:08] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.52, #running-req: 98, #queue-req: 21,
[2026-01-05 11:19:08] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:10<00:08, 13.52it/s][2026-01-05 11:19:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 99, #queue-req: 20,
[2026-01-05 11:19:08] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 99, #queue-req: 19,
[2026-01-05 11:19:08] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:10<00:10, 11.19it/s][2026-01-05 11:19:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.53, #running-req: 99, #queue-req: 18,
[2026-01-05 11:19:08] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:08] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:08] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.53, #running-req: 99, #queue-req: 16,

 42%|████▏     | 84/200 [00:11<00:13,  8.35it/s][2026-01-05 11:19:08] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:08] INFO:     127.0.0.1:58758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:08] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.53, #running-req: 98, #queue-req: 14,
[2026-01-05 11:19:08] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:11<00:10, 10.51it/s][2026-01-05 11:19:08] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.54, #running-req: 99, #queue-req: 13,
[2026-01-05 11:19:09] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.54, #running-req: 99, #queue-req: 12,
[2026-01-05 11:19:09] INFO:     127.0.0.1:58800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:09] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:11<00:10, 10.68it/s]
 45%|████▌     | 90/200 [00:11<00:08, 12.24it/s][2026-01-05 11:19:09] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.54, #running-req: 98, #queue-req: 10,
[2026-01-05 11:19:09] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:09] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:09] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.54, #running-req: 98, #queue-req: 8,
[2026-01-05 11:19:09] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:11<00:09, 11.83it/s][2026-01-05 11:19:09] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:09] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.53, #running-req: 99, #queue-req: 6,
[2026-01-05 11:19:09] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:09] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [00:11<00:08, 13.02it/s]
 48%|████▊     | 96/200 [00:11<00:06, 15.99it/s][2026-01-05 11:19:09] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 1920, token usage: 0.53, #running-req: 98, #queue-req: 3,
[2026-01-05 11:19:09] INFO:     127.0.0.1:58300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:09] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.54, #running-req: 100, #queue-req: 2,
[2026-01-05 11:19:09] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:11<00:07, 13.96it/s][2026-01-05 11:19:09] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1280, token usage: 0.54, #running-req: 100, #queue-req: 0,
[2026-01-05 11:19:09] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:09] INFO:     127.0.0.1:57908 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:12<00:07, 13.12it/s][2026-01-05 11:19:09] INFO:     127.0.0.1:58206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:09] INFO:     127.0.0.1:58322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:09] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:09] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:12<00:05, 16.30it/s]
 52%|█████▏    | 104/200 [00:12<00:04, 21.27it/s][2026-01-05 11:19:10] Decode batch, #running-req: 98, #token: 28800, token usage: 0.51, cpu graph: False, gen throughput (token/s): 1340.72, #queue-req: 0,
[2026-01-05 11:19:10] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:10] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:10] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:12<00:06, 13.90it/s][2026-01-05 11:19:10] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:10] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:10] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:12<00:05, 15.35it/s][2026-01-05 11:19:10] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:10] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:10] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:10] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:10] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:10] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:12<00:04, 19.95it/s]
 58%|█████▊    | 116/200 [00:12<00:03, 25.41it/s][2026-01-05 11:19:10] INFO:     127.0.0.1:59234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:10] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:58584 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:13<00:04, 17.06it/s]
 60%|██████    | 120/200 [00:13<00:05, 14.51it/s][2026-01-05 11:19:11] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:58938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:13<00:04, 15.82it/s][2026-01-05 11:19:11] INFO:     127.0.0.1:59100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:13<00:04, 16.04it/s]
 64%|██████▎   | 127/200 [00:13<00:04, 17.67it/s][2026-01-05 11:19:11] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:58772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:48640 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:13<00:05, 13.90it/s][2026-01-05 11:19:11] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:59052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:11] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:14<00:04, 15.52it/s]
 68%|██████▊   | 135/200 [00:14<00:02, 22.96it/s]
 68%|██████▊   | 135/200 [00:14<00:02, 22.96it/s][2026-01-05 11:19:11] Decode batch, #running-req: 68, #token: 21888, token usage: 0.39, cpu graph: False, gen throughput (token/s): 1786.19, #queue-req: 0,
[2026-01-05 11:19:11] INFO:     127.0.0.1:58994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:14<00:03, 18.73it/s][2026-01-05 11:19:12] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:48744 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:14<00:03, 18.69it/s][2026-01-05 11:19:12] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:48606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:48656 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:14<00:03, 18.66it/s][2026-01-05 11:19:12] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:59296 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:14<00:02, 20.04it/s][2026-01-05 11:19:12] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:14<00:02, 21.27it/s][2026-01-05 11:19:12] INFO:     127.0.0.1:58886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:58574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:12] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:15<00:03, 15.22it/s]
 77%|███████▋  | 154/200 [00:15<00:03, 13.87it/s][2026-01-05 11:19:12] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:13] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:13] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:15<00:03, 13.82it/s]
 78%|███████▊  | 157/200 [00:15<00:02, 15.32it/s][2026-01-05 11:19:13] INFO:     127.0.0.1:58790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:13] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:13] INFO:     127.0.0.1:59250 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:15<00:02, 17.72it/s][2026-01-05 11:19:13] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:13] INFO:     127.0.0.1:48698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:13] Decode batch, #running-req: 38, #token: 14976, token usage: 0.26, cpu graph: False, gen throughput (token/s): 1360.38, #queue-req: 0,
[2026-01-05 11:19:13] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:13] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:15<00:02, 14.92it/s]
 82%|████████▏ | 164/200 [00:15<00:02, 14.79it/s][2026-01-05 11:19:13] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:13] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:13] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:13] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:15<00:02, 14.79it/s]
 84%|████████▍ | 168/200 [00:15<00:01, 19.68it/s]
 84%|████████▍ | 168/200 [00:15<00:01, 19.68it/s][2026-01-05 11:19:13] INFO:     127.0.0.1:59092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:13] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:14] INFO:     127.0.0.1:48668 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:16<00:02, 12.42it/s][2026-01-05 11:19:14] INFO:     127.0.0.1:48780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:14] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:16<00:02, 11.97it/s][2026-01-05 11:19:14] INFO:     127.0.0.1:58620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:14] INFO:     127.0.0.1:48712 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:16<00:02, 10.41it/s][2026-01-05 11:19:14] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:14] Decode batch, #running-req: 25, #token: 10624, token usage: 0.19, cpu graph: False, gen throughput (token/s): 967.28, #queue-req: 0,
[2026-01-05 11:19:14] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:14] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:17<00:01, 11.09it/s][2026-01-05 11:19:14] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:14] INFO:     127.0.0.1:59110 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:17<00:01, 11.16it/s][2026-01-05 11:19:15] INFO:     127.0.0.1:48622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:15] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:17<00:01, 11.73it/s][2026-01-05 11:19:15] INFO:     127.0.0.1:48770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:15] Decode batch, #running-req: 17, #token: 8576, token usage: 0.15, cpu graph: False, gen throughput (token/s): 686.57, #queue-req: 0,
[2026-01-05 11:19:16] INFO:     127.0.0.1:59300 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:18<00:03,  4.40it/s][2026-01-05 11:19:16] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:18<00:03,  4.50it/s][2026-01-05 11:19:16] Decode batch, #running-req: 15, #token: 8320, token usage: 0.15, cpu graph: False, gen throughput (token/s): 584.60, #queue-req: 0,
[2026-01-05 11:19:16] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:19<00:03,  3.77it/s][2026-01-05 11:19:17] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:17] Decode batch, #running-req: 13, #token: 8320, token usage: 0.15, cpu graph: False, gen throughput (token/s): 500.70, #queue-req: 0,
[2026-01-05 11:19:19] Decode batch, #running-req: 13, #token: 8448, token usage: 0.15, cpu graph: False, gen throughput (token/s): 490.77, #queue-req: 0,
[2026-01-05 11:19:20] Decode batch, #running-req: 13, #token: 8960, token usage: 0.16, cpu graph: False, gen throughput (token/s): 491.23, #queue-req: 0,
[2026-01-05 11:19:21] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:21] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:21] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:21] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:21] Decode batch, #running-req: 13, #token: 5888, token usage: 0.10, cpu graph: False, gen throughput (token/s): 484.34, #queue-req: 0,
[2026-01-05 11:19:21] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:23<00:10,  1.12it/s]
 96%|█████████▌| 192/200 [00:23<00:06,  1.18it/s]
 96%|█████████▌| 192/200 [00:23<00:06,  1.18it/s]
 96%|█████████▌| 192/200 [00:23<00:06,  1.18it/s]
 96%|█████████▌| 192/200 [00:23<00:06,  1.18it/s][2026-01-05 11:19:21] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:23<00:05,  1.21it/s][2026-01-05 11:19:21] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:24<00:04,  1.33it/s][2026-01-05 11:19:22] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:22] Decode batch, #running-req: 5, #token: 3968, token usage: 0.07, cpu graph: False, gen throughput (token/s): 294.53, #queue-req: 0,
[2026-01-05 11:19:23] Decode batch, #running-req: 5, #token: 4096, token usage: 0.07, cpu graph: False, gen throughput (token/s): 198.93, #queue-req: 0,
[2026-01-05 11:19:23] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:25<00:03,  1.31it/s][2026-01-05 11:19:23] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:26<00:02,  1.41it/s][2026-01-05 11:19:24] INFO:     127.0.0.1:59230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:19:24] Decode batch, #running-req: 2, #token: 2048, token usage: 0.04, cpu graph: False, gen throughput (token/s): 159.48, #queue-req: 0,
[2026-01-05 11:19:24] INFO:     127.0.0.1:48608 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:26<00:00,  1.79it/s][2026-01-05 11:19:24] INFO:     127.0.0.1:48636 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:26<00:00,  2.01it/s]
100%|██████████| 200/200 [00:26<00:00,  7.45it/s]
.
----------------------------------------------------------------------
Ran 1 test in 178.187s

OK
Accuracy: 0.190
Invalid: 0.005
Latency: 26.916 s
Output throughput: 942.624 token/s
.
.
End (13/43):
filename='ascend/llm_models/test_ascend_persimmon_8b_chat.py', elapsed=191, estimated_time=400
.
.

.
.
Begin (14/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_llama_2_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:19:46] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/LLM-Research/Llama-2-7B', tokenizer_path='/root/.cache/modelscope/hub/models/LLM-Research/Llama-2-7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=15349695, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/LLM-Research/Llama-2-7B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:19:46] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:19:55] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:19:56] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:19:56] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:19:57] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:19:57] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:05<00:10,  5.43s/it]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:13<00:06,  6.92s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:20<00:00,  7.21s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:20<00:00,  6.98s/it]

[2026-01-05 11:20:19] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=48.26 GB, mem usage=12.56 GB.
[2026-01-05 11:20:19] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:20:19] The available memory for KV cache is 36.09 GB.
[2026-01-05 11:20:19] KV Cache is allocated. #tokens: 73856, K size: 18.06 GB, V size: 18.06 GB
[2026-01-05 11:20:19] Memory pool end. avail mem=12.07 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:20:19] max_total_num_tokens=73856, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4096, available_gpu_mem=12.05 GB
[2026-01-05 11:20:20] INFO:     Started server process [162003]
[2026-01-05 11:20:20] INFO:     Waiting for application startup.
[2026-01-05 11:20:20] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 11:20:20] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2026-01-05 11:20:20] INFO:     Application startup complete.
[2026-01-05 11:20:20] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:20:21] INFO:     127.0.0.1:45222 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:20:21] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:20:21.751442988 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 11:20:27] INFO:     127.0.0.1:59160 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:20:37] INFO:     127.0.0.1:52396 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:20:46] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:46] The server is fired up and ready to roll!
[2026-01-05 11:20:47] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:20:48] INFO:     127.0.0.1:33000 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:20:48] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:20:48] INFO:     127.0.0.1:33014 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:20:48] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:20:48] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/LLM-Research/Llama-2-7B --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:20:48] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-05 11:20:48] Prefill batch, #new-seq: 14, #new-token: 3584, #cached-token: 10752, token usage: 0.01, #running-req: 1, #queue-req: 0,
[2026-01-05 11:20:48] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.06, #running-req: 15, #queue-req: 4,
[2026-01-05 11:20:48] Prefill batch, #new-seq: 33, #new-token: 8192, #cached-token: 24576, token usage: 0.17, #running-req: 46, #queue-req: 49,
[2026-01-05 11:20:50] Prefill batch, #new-seq: 32, #new-token: 8064, #cached-token: 23808, token usage: 0.28, #running-req: 78, #queue-req: 18,
[2026-01-05 11:20:52] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:03<13:12,  3.98s/it][2026-01-05 11:20:52] Prefill batch, #new-seq: 12, #new-token: 3072, #cached-token: 9216, token usage: 0.40, #running-req: 109, #queue-req: 7,
[2026-01-05 11:20:52] Decode batch, #running-req: 121, #token: 32512, token usage: 0.44, cpu graph: False, gen throughput (token/s): 59.82, #queue-req: 7,
[2026-01-05 11:20:52] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:52] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:52] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:04<06:13,  1.89s/it]
  2%|▏         | 4/200 [00:04<01:25,  2.29it/s]
  2%|▏         | 4/200 [00:04<01:25,  2.29it/s][2026-01-05 11:20:52] Prefill batch, #new-seq: 5, #new-token: 1280, #cached-token: 3840, token usage: 0.43, #running-req: 118, #queue-req: 5,
[2026-01-05 11:20:52] INFO:     127.0.0.1:33328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:53] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.45, #running-req: 122, #queue-req: 4,
[2026-01-05 11:20:53] INFO:     127.0.0.1:34050 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:04<01:02,  3.11it/s][2026-01-05 11:20:53] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.45, #running-req: 123, #queue-req: 4,
[2026-01-05 11:20:53] INFO:     127.0.0.1:33306 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:53] INFO:     127.0.0.1:33956 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:04<00:56,  3.43it/s]
  4%|▍         | 8/200 [00:04<00:41,  4.64it/s][2026-01-05 11:20:53] INFO:     127.0.0.1:33162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:53] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.45, #running-req: 122, #queue-req: 3,
[2026-01-05 11:20:53] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.46, #running-req: 124, #queue-req: 3,
[2026-01-05 11:20:53] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:04<00:33,  5.68it/s][2026-01-05 11:20:53] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.46, #running-req: 124, #queue-req: 3,
[2026-01-05 11:20:53] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:05<00:34,  5.48it/s][2026-01-05 11:20:53] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.47, #running-req: 124, #queue-req: 3,
[2026-01-05 11:20:53] INFO:     127.0.0.1:33846 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:05<00:38,  4.85it/s][2026-01-05 11:20:53] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.46, #running-req: 124, #queue-req: 1,
[2026-01-05 11:20:53] INFO:     127.0.0.1:33064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:53] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:53] INFO:     127.0.0.1:33930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:54] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.47, #running-req: 124, #queue-req: 2,
[2026-01-05 11:20:54] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:05<00:21,  8.39it/s][2026-01-05 11:20:54] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.48, #running-req: 125, #queue-req: 2,
[2026-01-05 11:20:54] INFO:     127.0.0.1:33286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:54] INFO:     127.0.0.1:33624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:54] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:54] INFO:     127.0.0.1:33894 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:05<00:21,  8.58it/s]
 10%|█         | 20/200 [00:05<00:14, 12.30it/s]
 10%|█         | 20/200 [00:05<00:14, 12.30it/s][2026-01-05 11:20:54] Prefill batch, #new-seq: 3, #new-token: 896, #cached-token: 2304, token usage: 0.47, #running-req: 122, #queue-req: 3,
[2026-01-05 11:20:54] INFO:     127.0.0.1:33918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:54] INFO:     127.0.0.1:33928 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:06<00:17, 10.18it/s][2026-01-05 11:20:54] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.48, #running-req: 124, #queue-req: 2,
[2026-01-05 11:20:54] INFO:     127.0.0.1:33938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:54] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.48, #running-req: 124, #queue-req: 2,
[2026-01-05 11:20:54] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:06<00:16, 10.91it/s][2026-01-05 11:20:54] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.49, #running-req: 125, #queue-req: 2,
[2026-01-05 11:20:54] INFO:     127.0.0.1:33926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:54] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.49, #running-req: 125, #queue-req: 2,
[2026-01-05 11:20:54] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:06<00:15, 11.01it/s][2026-01-05 11:20:55] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.49, #running-req: 125, #queue-req: 1,
[2026-01-05 11:20:55] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:55] INFO:     127.0.0.1:33036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:55] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.49, #running-req: 126, #queue-req: 1,

 14%|█▍        | 28/200 [00:06<00:17,  9.63it/s][2026-01-05 11:20:55] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.50, #running-req: 126, #queue-req: 1,
[2026-01-05 11:20:55] Decode batch, #running-req: 127, #token: 37248, token usage: 0.50, cpu graph: False, gen throughput (token/s): 1817.04, #queue-req: 1,
[2026-01-05 11:20:55] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:55] INFO:     127.0.0.1:33682 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:07<00:17,  9.47it/s][2026-01-05 11:20:55] INFO:     127.0.0.1:33392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:55] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.51, #running-req: 124, #queue-req: 3,
[2026-01-05 11:20:55] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:55] INFO:     127.0.0.1:33338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:55] INFO:     127.0.0.1:33760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:55] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:07<00:17,  9.48it/s]
 18%|█▊        | 35/200 [00:07<00:10, 16.33it/s]
 18%|█▊        | 35/200 [00:07<00:10, 16.33it/s]
 18%|█▊        | 35/200 [00:07<00:10, 16.33it/s][2026-01-05 11:20:55] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.50, #running-req: 121, #queue-req: 4,
[2026-01-05 11:20:55] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:55] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:55] INFO:     127.0.0.1:33672 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:55] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.50, #running-req: 121, #queue-req: 4,
[2026-01-05 11:20:55] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:55] INFO:     127.0.0.1:33786 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:07<00:09, 17.63it/s]
 20%|██        | 40/200 [00:07<00:07, 20.08it/s][2026-01-05 11:20:55] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.51, #running-req: 122, #queue-req: 4,
[2026-01-05 11:20:56] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:56] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:56] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.52, #running-req: 122, #queue-req: 5,
[2026-01-05 11:20:56] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:07<00:09, 16.27it/s][2026-01-05 11:20:56] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.52, #running-req: 122, #queue-req: 4,
[2026-01-05 11:20:56] INFO:     127.0.0.1:33444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:56] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:56] INFO:     127.0.0.1:34008 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:07<00:10, 15.07it/s]
 23%|██▎       | 46/200 [00:07<00:09, 15.59it/s][2026-01-05 11:20:56] INFO:     127.0.0.1:33644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:56] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.52, #running-req: 121, #queue-req: 5,
[2026-01-05 11:20:56] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:08<00:09, 16.29it/s][2026-01-05 11:20:56] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.52, #running-req: 121, #queue-req: 5,
[2026-01-05 11:20:56] INFO:     127.0.0.1:33238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:56] INFO:     127.0.0.1:34060 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:08<00:11, 13.64it/s][2026-01-05 11:20:56] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:56] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.53, #running-req: 121, #queue-req: 5,
[2026-01-05 11:20:56] INFO:     127.0.0.1:34212 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:08<00:10, 14.80it/s][2026-01-05 11:20:56] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.53, #running-req: 121, #queue-req: 5,
[2026-01-05 11:20:56] INFO:     127.0.0.1:33110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:56] INFO:     127.0.0.1:34228 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:08<00:10, 13.49it/s][2026-01-05 11:20:57] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:57] INFO:     127.0.0.1:33554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:57] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.53, #running-req: 121, #queue-req: 4,
[2026-01-05 11:20:57] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:08<00:08, 16.72it/s][2026-01-05 11:20:57] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.53, #running-req: 121, #queue-req: 5,
[2026-01-05 11:20:57] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:57] INFO:     127.0.0.1:34340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:57] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.54, #running-req: 122, #queue-req: 4,

 30%|██▉       | 59/200 [00:08<00:10, 13.19it/s][2026-01-05 11:20:57] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.54, #running-req: 123, #queue-req: 4,
[2026-01-05 11:20:57] INFO:     127.0.0.1:34432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:57] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:57] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:57] INFO:     127.0.0.1:33606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:57] INFO:     127.0.0.1:33946 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:09<00:13, 10.06it/s]
 32%|███▏      | 64/200 [00:09<00:11, 12.15it/s]
 32%|███▏      | 64/200 [00:09<00:11, 12.15it/s]
 32%|███▏      | 64/200 [00:09<00:11, 12.15it/s][2026-01-05 11:20:57] Prefill batch, #new-seq: 4, #new-token: 1024, #cached-token: 3072, token usage: 0.53, #running-req: 119, #queue-req: 5,
[2026-01-05 11:20:57] INFO:     127.0.0.1:33880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:57] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.54, #running-req: 122, #queue-req: 4,
[2026-01-05 11:20:57] INFO:     127.0.0.1:33316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:57] INFO:     127.0.0.1:33578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:57] INFO:     127.0.0.1:34200 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:09<00:13, 10.16it/s]
 34%|███▍      | 68/200 [00:09<00:12, 10.53it/s]
 34%|███▍      | 68/200 [00:09<00:12, 10.53it/s][2026-01-05 11:20:58] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.54, #running-req: 121, #queue-req: 4,
[2026-01-05 11:20:58] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.54, #running-req: 123, #queue-req: 4,
[2026-01-05 11:20:58] INFO:     127.0.0.1:33566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:09<00:12, 10.54it/s]
 36%|███▌      | 71/200 [00:09<00:10, 11.74it/s][2026-01-05 11:20:58] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.54, #running-req: 122, #queue-req: 4,
[2026-01-05 11:20:58] Decode batch, #running-req: 124, #token: 39936, token usage: 0.54, cpu graph: False, gen throughput (token/s): 1719.59, #queue-req: 4,
[2026-01-05 11:20:58] INFO:     127.0.0.1:33538 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:33968 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:09<00:10, 11.68it/s][2026-01-05 11:20:58] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.54, #running-req: 122, #queue-req: 3,
[2026-01-05 11:20:58] INFO:     127.0.0.1:33668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:33860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:33908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:34324 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 75/200 [00:10<00:09, 12.91it/s]
 38%|███▊      | 77/200 [00:10<00:05, 20.69it/s]
 38%|███▊      | 77/200 [00:10<00:05, 20.69it/s][2026-01-05 11:20:58] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.53, #running-req: 120, #queue-req: 0,
[2026-01-05 11:20:58] INFO:     127.0.0.1:34172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:33756 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:34548 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:10<00:06, 18.82it/s][2026-01-05 11:20:58] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:33716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:34308 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:10<00:06, 17.49it/s][2026-01-05 11:20:58] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:34388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:34224 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:10<00:05, 19.90it/s]
 44%|████▎     | 87/200 [00:10<00:04, 24.23it/s][2026-01-05 11:20:58] INFO:     127.0.0.1:33100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:33146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:33364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:58] INFO:     127.0.0.1:33512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [00:10<00:03, 32.58it/s][2026-01-05 11:20:59] INFO:     127.0.0.1:33872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:33074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:34116 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:10<00:02, 34.43it/s]
 49%|████▉     | 98/200 [00:10<00:02, 38.51it/s][2026-01-05 11:20:59] INFO:     127.0.0.1:34136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:34152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:34262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:33974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:33136 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:10<00:03, 30.50it/s][2026-01-05 11:20:59] INFO:     127.0.0.1:33336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:33740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:34534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:37972 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:11<00:03, 23.62it/s]
 55%|█████▍    | 109/200 [00:11<00:04, 21.82it/s]
 55%|█████▍    | 109/200 [00:11<00:04, 21.82it/s][2026-01-05 11:20:59] INFO:     127.0.0.1:33610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:34086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:20:59] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:11<00:04, 20.58it/s][2026-01-05 11:20:59] INFO:     127.0.0.1:34384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:34656 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:33570 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:11<00:04, 18.44it/s][2026-01-05 11:21:00] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:38030 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:11<00:04, 17.74it/s][2026-01-05 11:21:00] INFO:     127.0.0.1:34416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:38020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] Decode batch, #running-req: 82, #token: 28416, token usage: 0.38, cpu graph: False, gen throughput (token/s): 2051.52, #queue-req: 0,

 60%|█████▉    | 119/200 [00:11<00:04, 17.24it/s][2026-01-05 11:21:00] INFO:     127.0.0.1:34220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:34588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:33486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:34620 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 124/200 [00:11<00:03, 22.32it/s]
 62%|██████▎   | 125/200 [00:11<00:02, 28.33it/s][2026-01-05 11:21:00] INFO:     127.0.0.1:34070 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:34646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:33152 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:12<00:02, 29.36it/s][2026-01-05 11:21:00] INFO:     127.0.0.1:33912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:37952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:33192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:12<00:02, 24.49it/s]
 68%|██████▊   | 135/200 [00:12<00:02, 23.34it/s][2026-01-05 11:21:00] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:33180 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:00] INFO:     127.0.0.1:34630 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:12<00:02, 23.96it/s][2026-01-05 11:21:00] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:37884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:34504 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:12<00:02, 22.97it/s][2026-01-05 11:21:01] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:34400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:34508 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:12<00:02, 22.22it/s][2026-01-05 11:21:01] INFO:     127.0.0.1:37954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:34526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:12<00:02, 21.94it/s][2026-01-05 11:21:01] INFO:     127.0.0.1:37868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:37844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:37942 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:13<00:02, 17.37it/s][2026-01-05 11:21:01] INFO:     127.0.0.1:38016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] Decode batch, #running-req: 49, #token: 18816, token usage: 0.25, cpu graph: False, gen throughput (token/s): 1681.81, #queue-req: 0,
[2026-01-05 11:21:01] INFO:     127.0.0.1:37834 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:13<00:02, 16.89it/s][2026-01-05 11:21:01] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:34244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:34536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:37818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:01] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 157/200 [00:13<00:02, 20.98it/s][2026-01-05 11:21:01] INFO:     127.0.0.1:33816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:37898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:13<00:01, 21.10it/s][2026-01-05 11:21:02] INFO:     127.0.0.1:37964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:34458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:34604 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:13<00:01, 20.50it/s][2026-01-05 11:21:02] INFO:     127.0.0.1:34258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:37778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:14<00:01, 22.88it/s][2026-01-05 11:21:02] INFO:     127.0.0.1:34298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:14<00:01, 23.83it/s][2026-01-05 11:21:02] INFO:     127.0.0.1:37860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:33594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:37926 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:14<00:01, 19.41it/s][2026-01-05 11:21:02] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] INFO:     127.0.0.1:38048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:02] Decode batch, #running-req: 24, #token: 11136, token usage: 0.15, cpu graph: False, gen throughput (token/s): 1184.48, #queue-req: 0,
[2026-01-05 11:21:02] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:14<00:01, 18.54it/s][2026-01-05 11:21:03] INFO:     127.0.0.1:34096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:03] INFO:     127.0.0.1:37806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:03] INFO:     127.0.0.1:37794 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:15<00:01, 11.17it/s][2026-01-05 11:21:03] INFO:     127.0.0.1:38042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:03] Decode batch, #running-req: 18, #token: 9728, token usage: 0.13, cpu graph: False, gen throughput (token/s): 796.14, #queue-req: 0,
[2026-01-05 11:21:04] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:04] INFO:     127.0.0.1:37892 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:15<00:02,  7.52it/s]
 92%|█████████▏| 184/200 [00:15<00:02,  6.53it/s][2026-01-05 11:21:04] Decode batch, #running-req: 16, #token: 9088, token usage: 0.12, cpu graph: False, gen throughput (token/s): 675.08, #queue-req: 0,
[2026-01-05 11:21:05] INFO:     127.0.0.1:37852 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:17<00:05,  2.81it/s][2026-01-05 11:21:05] Decode batch, #running-req: 16, #token: 9728, token usage: 0.13, cpu graph: False, gen throughput (token/s): 655.04, #queue-req: 0,
[2026-01-05 11:21:06] Decode batch, #running-req: 15, #token: 9984, token usage: 0.14, cpu graph: False, gen throughput (token/s): 626.14, #queue-req: 0,
[2026-01-05 11:21:07] INFO:     127.0.0.1:33088 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:18<00:06,  2.01it/s][2026-01-05 11:21:07] Decode batch, #running-req: 14, #token: 9600, token usage: 0.13, cpu graph: False, gen throughput (token/s): 597.26, #queue-req: 0,
[2026-01-05 11:21:08] Decode batch, #running-req: 14, #token: 10496, token usage: 0.14, cpu graph: False, gen throughput (token/s): 581.80, #queue-req: 0,
[2026-01-05 11:21:09] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:09] INFO:     127.0.0.1:33228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:09] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:09] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:09] Decode batch, #running-req: 14, #token: 4992, token usage: 0.07, cpu graph: False, gen throughput (token/s): 580.80, #queue-req: 0,
[2026-01-05 11:21:09] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:09] INFO:     127.0.0.1:33428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:09] INFO:     127.0.0.1:33598 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:09] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:21<00:11,  1.14it/s]
 97%|█████████▋| 194/200 [00:21<00:02,  2.36it/s]
 97%|█████████▋| 194/200 [00:21<00:02,  2.36it/s]
 97%|█████████▋| 194/200 [00:21<00:02,  2.36it/s]
 97%|█████████▋| 194/200 [00:21<00:02,  2.36it/s]
 97%|█████████▋| 194/200 [00:21<00:02,  2.36it/s]
 97%|█████████▋| 194/200 [00:21<00:02,  2.36it/s]
 97%|█████████▋| 194/200 [00:21<00:02,  2.36it/s][2026-01-05 11:21:10] INFO:     127.0.0.1:34122 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:21<00:02,  2.25it/s][2026-01-05 11:21:10] Decode batch, #running-req: 5, #token: 4480, token usage: 0.06, cpu graph: False, gen throughput (token/s): 251.28, #queue-req: 0,
[2026-01-05 11:21:10] INFO:     127.0.0.1:34344 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:22<00:01,  2.23it/s][2026-01-05 11:21:11] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:23<00:01,  2.13it/s][2026-01-05 11:21:11] Decode batch, #running-req: 3, #token: 3072, token usage: 0.04, cpu graph: False, gen throughput (token/s): 180.76, #queue-req: 0,
[2026-01-05 11:21:11] INFO:     127.0.0.1:37810 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:23<00:00,  2.24it/s][2026-01-05 11:21:12] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:21:12] INFO:     127.0.0.1:37948 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:23<00:00,  2.35it/s]
100%|██████████| 200/200 [00:23<00:00,  2.95it/s]
100%|██████████| 200/200 [00:23<00:00,  8.42it/s]
.
----------------------------------------------------------------------
Ran 1 test in 95.131s

OK
Accuracy: 0.200
Invalid: 0.005
Latency: 23.851 s
Output throughput: 1093.281 token/s
.
.
End (14/43):
filename='ascend/llm_models/test_ascend_llama_2_7b.py', elapsed=108, estimated_time=400
.
.

.
.
Begin (15/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py
.
.

Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py", line 3, in <module>
    from test_ascend_llm_models import TestMistral
ModuleNotFoundError: No module named 'test_ascend_llm_models'
.
.
End (15/43):
filename='ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py', elapsed=0, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py failed with non-retriable error: ModuleNotFoundError - not retrying


✗ FAILED: ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py returned exit code 1

.
.
Begin (16/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_ling_lite.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:21:36] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/inclusionAI/Ling-lite', tokenizer_path='/root/.cache/modelscope/hub/models/inclusionAI/Ling-lite', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=897134408, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/inclusionAI/Ling-lite', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:21:36] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:21:45] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:21:46] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:21:47] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:21:47] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/66 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   2% Completed | 1/66 [00:00<00:51,  1.26it/s]

Loading safetensors checkpoint shards:   3% Completed | 2/66 [00:01<00:58,  1.09it/s]

Loading safetensors checkpoint shards:   5% Completed | 3/66 [00:02<00:46,  1.36it/s]

Loading safetensors checkpoint shards:   6% Completed | 4/66 [00:03<00:51,  1.21it/s]

Loading safetensors checkpoint shards:   8% Completed | 5/66 [00:03<00:43,  1.41it/s]

Loading safetensors checkpoint shards:   9% Completed | 6/66 [00:04<00:37,  1.58it/s]

Loading safetensors checkpoint shards:  11% Completed | 7/66 [00:04<00:34,  1.73it/s]

Loading safetensors checkpoint shards:  12% Completed | 8/66 [00:05<00:40,  1.43it/s]

Loading safetensors checkpoint shards:  14% Completed | 9/66 [00:06<00:44,  1.28it/s]

Loading safetensors checkpoint shards:  15% Completed | 10/66 [00:07<00:46,  1.20it/s]

Loading safetensors checkpoint shards:  17% Completed | 11/66 [00:07<00:37,  1.46it/s]

Loading safetensors checkpoint shards:  18% Completed | 12/66 [00:08<00:36,  1.48it/s]

Loading safetensors checkpoint shards:  20% Completed | 13/66 [00:09<00:40,  1.31it/s]

Loading safetensors checkpoint shards:  21% Completed | 14/66 [00:10<00:43,  1.21it/s]

Loading safetensors checkpoint shards:  23% Completed | 15/66 [00:11<00:45,  1.13it/s]

Loading safetensors checkpoint shards:  24% Completed | 16/66 [00:12<00:38,  1.30it/s]

Loading safetensors checkpoint shards:  26% Completed | 17/66 [00:12<00:33,  1.46it/s]

Loading safetensors checkpoint shards:  27% Completed | 18/66 [00:13<00:36,  1.33it/s]

Loading safetensors checkpoint shards:  29% Completed | 19/66 [00:14<00:39,  1.20it/s]

Loading safetensors checkpoint shards:  30% Completed | 20/66 [00:15<00:33,  1.36it/s]

Loading safetensors checkpoint shards:  32% Completed | 21/66 [00:15<00:36,  1.23it/s]

Loading safetensors checkpoint shards:  33% Completed | 22/66 [00:16<00:31,  1.38it/s]

Loading safetensors checkpoint shards:  35% Completed | 23/66 [00:17<00:30,  1.40it/s]

Loading safetensors checkpoint shards:  36% Completed | 24/66 [00:17<00:27,  1.54it/s]

Loading safetensors checkpoint shards:  38% Completed | 25/66 [00:18<00:30,  1.34it/s]

Loading safetensors checkpoint shards:  39% Completed | 26/66 [00:19<00:32,  1.23it/s]

Loading safetensors checkpoint shards:  41% Completed | 27/66 [00:20<00:34,  1.14it/s]

Loading safetensors checkpoint shards:  42% Completed | 28/66 [00:21<00:31,  1.23it/s]

Loading safetensors checkpoint shards:  44% Completed | 29/66 [00:22<00:31,  1.17it/s]

Loading safetensors checkpoint shards:  45% Completed | 30/66 [00:22<00:27,  1.33it/s]

Loading safetensors checkpoint shards:  48% Completed | 32/66 [00:23<00:21,  1.59it/s]

Loading safetensors checkpoint shards:  50% Completed | 33/66 [00:24<00:19,  1.66it/s]

Loading safetensors checkpoint shards:  52% Completed | 34/66 [00:24<00:17,  1.80it/s]

Loading safetensors checkpoint shards:  53% Completed | 35/66 [00:25<00:20,  1.53it/s]

Loading safetensors checkpoint shards:  55% Completed | 36/66 [00:26<00:18,  1.63it/s]

Loading safetensors checkpoint shards:  56% Completed | 37/66 [00:26<00:16,  1.73it/s]

Loading safetensors checkpoint shards:  58% Completed | 38/66 [00:27<00:19,  1.45it/s]

Loading safetensors checkpoint shards:  59% Completed | 39/66 [00:28<00:20,  1.30it/s]

Loading safetensors checkpoint shards:  61% Completed | 40/66 [00:29<00:18,  1.44it/s]

Loading safetensors checkpoint shards:  62% Completed | 41/66 [00:29<00:15,  1.56it/s]

Loading safetensors checkpoint shards:  64% Completed | 42/66 [00:30<00:17,  1.37it/s]

Loading safetensors checkpoint shards:  65% Completed | 43/66 [00:31<00:18,  1.27it/s]

Loading safetensors checkpoint shards:  67% Completed | 44/66 [00:32<00:18,  1.18it/s]

Loading safetensors checkpoint shards:  68% Completed | 45/66 [00:33<00:18,  1.14it/s]

Loading safetensors checkpoint shards:  70% Completed | 46/66 [00:34<00:18,  1.11it/s]

Loading safetensors checkpoint shards:  71% Completed | 47/66 [00:34<00:14,  1.27it/s]

Loading safetensors checkpoint shards:  73% Completed | 48/66 [00:35<00:12,  1.44it/s]

Loading safetensors checkpoint shards:  74% Completed | 49/66 [00:36<00:14,  1.21it/s]

Loading safetensors checkpoint shards:  76% Completed | 50/66 [00:36<00:11,  1.37it/s]

Loading safetensors checkpoint shards:  77% Completed | 51/66 [00:37<00:09,  1.53it/s]

Loading safetensors checkpoint shards:  79% Completed | 52/66 [00:38<00:10,  1.36it/s]

Loading safetensors checkpoint shards:  80% Completed | 53/66 [00:38<00:08,  1.52it/s]

Loading safetensors checkpoint shards:  82% Completed | 54/66 [00:39<00:07,  1.64it/s]

Loading safetensors checkpoint shards:  83% Completed | 55/66 [00:40<00:06,  1.57it/s]

Loading safetensors checkpoint shards:  85% Completed | 56/66 [00:40<00:05,  1.69it/s]

Loading safetensors checkpoint shards:  86% Completed | 57/66 [00:41<00:05,  1.78it/s]

Loading safetensors checkpoint shards:  88% Completed | 58/66 [00:41<00:04,  1.85it/s]

Loading safetensors checkpoint shards:  89% Completed | 59/66 [00:42<00:04,  1.52it/s]

Loading safetensors checkpoint shards:  92% Completed | 61/66 [00:42<00:02,  2.16it/s]

Loading safetensors checkpoint shards:  94% Completed | 62/66 [00:43<00:01,  2.15it/s]

Loading safetensors checkpoint shards:  95% Completed | 63/66 [00:45<00:02,  1.13it/s]

Loading safetensors checkpoint shards:  98% Completed | 65/66 [00:46<00:00,  1.42it/s]

Loading safetensors checkpoint shards: 100% Completed | 66/66 [00:46<00:00,  1.42it/s]

[2026-01-05 11:22:34] Load weight end. type=BailingMoeForCausalLM, dtype=torch.bfloat16, avail mem=29.50 GB, mem usage=31.32 GB.
[2026-01-05 11:22:34] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:22:34] The available memory for KV cache is 17.33 GB.
[2026-01-05 11:22:34] KV Cache is allocated. #tokens: 324480, K size: 8.67 GB, V size: 8.67 GB
[2026-01-05 11:22:34] Memory pool end. avail mem=11.65 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:22:35] max_total_num_tokens=324480, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.65 GB
[2026-01-05 11:22:35] INFO:     Started server process [164842]
[2026-01-05 11:22:35] INFO:     Waiting for application startup.
[2026-01-05 11:22:35] INFO:     Application startup complete.
[2026-01-05 11:22:35] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:22:36] INFO:     127.0.0.1:54400 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:22:36] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:22:36.668372265 compiler_depend.ts:198] Warning: Driver Version: "B" is invalid or not supported yet. (function operator())
[2026-01-05 11:22:45] INFO:     127.0.0.1:54424 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[rank0]:[W105 11:22:52.439328983 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[2026-01-05 11:22:55] INFO:     127.0.0.1:55726 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:22:55] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:22:55] The server is fired up and ready to roll!
[2026-01-05 11:23:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:23:06] INFO:     127.0.0.1:53828 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:23:06] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:23:06] INFO:     127.0.0.1:44698 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:23:06] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:23:07] INFO:     127.0.0.1:44714 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/inclusionAI/Ling-lite --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:23:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:23:07] Prefill batch, #new-seq: 20, #new-token: 2944, #cached-token: 15360, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 11:23:07] Prefill batch, #new-seq: 32, #new-token: 4480, #cached-token: 24576, token usage: 0.01, #running-req: 21, #queue-req: 0,
[2026-01-05 11:23:07] Prefill batch, #new-seq: 7, #new-token: 1280, #cached-token: 5376, token usage: 0.03, #running-req: 53, #queue-req: 0,
[2026-01-05 11:23:08] Prefill batch, #new-seq: 41, #new-token: 5376, #cached-token: 31488, token usage: 0.03, #running-req: 60, #queue-req: 0,
[2026-01-05 11:23:08] Prefill batch, #new-seq: 27, #new-token: 3968, #cached-token: 20736, token usage: 0.05, #running-req: 101, #queue-req: 0,
[2026-01-05 11:23:09] Decode batch, #running-req: 128, #token: 25728, token usage: 0.08, cpu graph: False, gen throughput (token/s): 54.24, #queue-req: 0,
[2026-01-05 11:23:10] INFO:     127.0.0.1:44726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:10] INFO:     127.0.0.1:44870 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<08:44,  2.63s/it]
  1%|          | 2/200 [00:02<05:28,  1.66s/it][2026-01-05 11:23:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 11:23:10] INFO:     127.0.0.1:44744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:10] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:02<03:41,  1.12s/it]
  2%|▏         | 4/200 [00:02<01:56,  1.69it/s][2026-01-05 11:23:10] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 11:23:10] INFO:     127.0.0.1:45572 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:03<01:40,  1.95it/s][2026-01-05 11:23:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:10] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:11] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:03<01:09,  2.78it/s][2026-01-05 11:23:11] INFO:     127.0.0.1:44904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:11] INFO:     127.0.0.1:45202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 11:23:11] INFO:     127.0.0.1:44718 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:03<00:43,  4.34it/s][2026-01-05 11:23:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:11] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:03<00:41,  4.53it/s][2026-01-05 11:23:11] INFO:     127.0.0.1:45544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:11] INFO:     127.0.0.1:45776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:11] INFO:     127.0.0.1:44976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 11:23:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-05 11:23:11] INFO:     127.0.0.1:45248 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:04<00:26,  7.07it/s][2026-01-05 11:23:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:11] INFO:     127.0.0.1:45762 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:04<00:26,  6.84it/s][2026-01-05 11:23:12] INFO:     127.0.0.1:44944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] INFO:     127.0.0.1:45020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] INFO:     127.0.0.1:45462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] INFO:     127.0.0.1:45738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:12] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] INFO:     127.0.0.1:45674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] Prefill batch, #new-seq: 4, #new-token: 768, #cached-token: 3072, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 11:23:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 132, #queue-req: 0,
[2026-01-05 11:23:12] Decode batch, #running-req: 128, #token: 31104, token usage: 0.10, cpu graph: False, gen throughput (token/s): 2140.40, #queue-req: 0,
[2026-01-05 11:23:12] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] INFO:     127.0.0.1:44920 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:04<00:15, 11.26it/s]
 12%|█▏        | 24/200 [00:04<00:11, 14.90it/s][2026-01-05 11:23:12] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 11:23:12] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] INFO:     127.0.0.1:45328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 11:23:12] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 129, #queue-req: 0,
[2026-01-05 11:23:12] INFO:     127.0.0.1:45158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] INFO:     127.0.0.1:45360 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:04<00:12, 13.87it/s]
 14%|█▍        | 28/200 [00:04<00:12, 14.09it/s][2026-01-05 11:23:12] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 11:23:12] INFO:     127.0.0.1:45096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 11:23:12] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 129, #queue-req: 0,
[2026-01-05 11:23:12] INFO:     127.0.0.1:44850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:12] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:05<00:12, 13.58it/s]
 16%|█▌        | 32/200 [00:05<00:11, 14.29it/s][2026-01-05 11:23:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 11:23:13] INFO:     127.0.0.1:44860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:13] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:13] INFO:     127.0.0.1:45146 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:05<00:14, 11.44it/s]
 18%|█▊        | 35/200 [00:05<00:15, 10.64it/s][2026-01-05 11:23:13] INFO:     127.0.0.1:45006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:13] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.09, #running-req: 125, #queue-req: 0,
[2026-01-05 11:23:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 11:23:13] INFO:     127.0.0.1:45400 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 37/200 [00:05<00:16,  9.92it/s][2026-01-05 11:23:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:13] INFO:     127.0.0.1:44990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:13] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:13] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:13] INFO:     127.0.0.1:45666 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:06<00:18,  8.88it/s]
 20%|██        | 40/200 [00:06<00:17,  9.18it/s][2026-01-05 11:23:13] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 11:23:13] INFO:     127.0.0.1:45524 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:13] INFO:     127.0.0.1:45884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:13] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 11:23:13] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:06<00:14, 11.04it/s][2026-01-05 11:23:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:13] INFO:     127.0.0.1:45456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:14] INFO:     127.0.0.1:44902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:06<00:13, 11.25it/s]
 23%|██▎       | 46/200 [00:06<00:11, 12.87it/s][2026-01-05 11:23:14] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 11:23:14] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] INFO:     127.0.0.1:45376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] INFO:     127.0.0.1:45818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.09, #running-req: 125, #queue-req: 0,
[2026-01-05 11:23:14] INFO:     127.0.0.1:44750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] INFO:     127.0.0.1:45028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] INFO:     127.0.0.1:45756 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:06<00:13, 10.84it/s]
 26%|██▌       | 52/200 [00:06<00:12, 11.66it/s]
 26%|██▌       | 52/200 [00:06<00:12, 11.66it/s][2026-01-05 11:23:14] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.09, #running-req: 125, #queue-req: 0,
[2026-01-05 11:23:14] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:14] INFO:     127.0.0.1:45800 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:06<00:12, 11.68it/s][2026-01-05 11:23:14] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:14] Decode batch, #running-req: 127, #token: 30464, token usage: 0.09, cpu graph: False, gen throughput (token/s): 2045.92, #queue-req: 0,
[2026-01-05 11:23:14] INFO:     127.0.0.1:45382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:14] INFO:     127.0.0.1:44956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] INFO:     127.0.0.1:45214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] INFO:     127.0.0.1:45628 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 56/200 [00:07<00:12, 11.70it/s]
 29%|██▉       | 58/200 [00:07<00:09, 15.41it/s]
 29%|██▉       | 58/200 [00:07<00:09, 15.41it/s][2026-01-05 11:23:14] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.09, #running-req: 125, #queue-req: 0,
[2026-01-05 11:23:14] INFO:     127.0.0.1:45486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:14] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:15] INFO:     127.0.0.1:45106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:15] INFO:     127.0.0.1:45740 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:07<00:09, 14.50it/s]
 30%|███       | 61/200 [00:07<00:09, 15.33it/s][2026-01-05 11:23:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 11:23:15] INFO:     127.0.0.1:45260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:15] INFO:     127.0.0.1:45910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 126, #queue-req: 0,
[2026-01-05 11:23:15] INFO:     127.0.0.1:45918 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:07<00:09, 14.21it/s][2026-01-05 11:23:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:15] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:15] INFO:     127.0.0.1:45344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:15] INFO:     127.0.0.1:45656 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:15] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 11:23:15] INFO:     127.0.0.1:44978 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:07<00:09, 13.99it/s][2026-01-05 11:23:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:15] INFO:     127.0.0.1:45198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-05 11:23:15] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:15] INFO:     127.0.0.1:45732 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:08<00:10, 12.81it/s]
 36%|███▌      | 71/200 [00:08<00:09, 13.30it/s][2026-01-05 11:23:15] INFO:     127.0.0.1:44794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:15] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:15] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.09, #running-req: 128, #queue-req: 0,
[2026-01-05 11:23:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.09, #running-req: 130, #queue-req: 0,
[2026-01-05 11:23:15] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:08<00:09, 13.63it/s][2026-01-05 11:23:16] INFO:     127.0.0.1:44980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:44798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45390 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:08<00:07, 17.23it/s]
 40%|███▉      | 79/200 [00:08<00:05, 22.18it/s][2026-01-05 11:23:16] INFO:     127.0.0.1:45554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45916 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:08<00:06, 19.52it/s][2026-01-05 11:23:16] INFO:     127.0.0.1:45686 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK

 42%|████▎     | 85/200 [00:08<00:07, 15.75it/s]
 44%|████▍     | 88/200 [00:08<00:06, 17.31it/s]
 44%|████▍     | 88/200 [00:08<00:06, 17.31it/s]
 44%|████▍     | 88/200 [00:08<00:06, 17.31it/s][2026-01-05 11:23:16] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:46066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:09<00:07, 14.74it/s][2026-01-05 11:23:16] INFO:     127.0.0.1:45690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] Decode batch, #running-req: 108, #token: 28928, token usage: 0.09, cpu graph: False, gen throughput (token/s): 2249.00, #queue-req: 0,
[2026-01-05 11:23:16] INFO:     127.0.0.1:46244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:16] INFO:     127.0.0.1:45996 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 95/200 [00:09<00:05, 19.05it/s]
 48%|████▊     | 96/200 [00:09<00:04, 24.70it/s][2026-01-05 11:23:17] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:46054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:45412 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:46160 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:09<00:04, 20.53it/s][2026-01-05 11:23:17] INFO:     127.0.0.1:45634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:46172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:45874 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:09<00:05, 18.91it/s][2026-01-05 11:23:17] INFO:     127.0.0.1:44898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:09<00:04, 19.96it/s][2026-01-05 11:23:17] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:44790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:46178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:45084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:09<00:03, 22.72it/s]
 56%|█████▌    | 111/200 [00:09<00:03, 27.00it/s][2026-01-05 11:23:17] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:45812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▊    | 115/200 [00:10<00:02, 28.40it/s][2026-01-05 11:23:17] INFO:     127.0.0.1:45586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:17] INFO:     127.0.0.1:45496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:46010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 119/200 [00:10<00:03, 25.17it/s][2026-01-05 11:23:18] INFO:     127.0.0.1:45292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:46220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:46530 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:10<00:03, 21.61it/s]
 62%|██████▏   | 123/200 [00:10<00:03, 21.04it/s][2026-01-05 11:23:18] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:44930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:45228 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:10<00:03, 19.19it/s][2026-01-05 11:23:18] Decode batch, #running-req: 74, #token: 23296, token usage: 0.07, cpu graph: False, gen throughput (token/s): 2229.08, #queue-req: 0,
[2026-01-05 11:23:18] INFO:     127.0.0.1:46034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:44838 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:11<00:04, 15.16it/s][2026-01-05 11:23:18] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:45370 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 131/200 [00:11<00:04, 15.42it/s][2026-01-05 11:23:18] INFO:     127.0.0.1:45704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:46090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:18] INFO:     127.0.0.1:46366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:46036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:11<00:03, 17.96it/s]
 68%|██████▊   | 136/200 [00:11<00:02, 21.63it/s][2026-01-05 11:23:19] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:44768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:44926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:46450 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:11<00:02, 22.47it/s]
 70%|███████   | 140/200 [00:11<00:02, 25.22it/s][2026-01-05 11:23:19] INFO:     127.0.0.1:45394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:45976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:45844 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:11<00:02, 25.22it/s][2026-01-05 11:23:19] INFO:     127.0.0.1:45442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:46046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:44966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:11<00:03, 15.00it/s]
 74%|███████▎  | 147/200 [00:11<00:04, 12.55it/s][2026-01-05 11:23:19] INFO:     127.0.0.1:44812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:45900 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:12<00:03, 13.21it/s][2026-01-05 11:23:19] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:19] INFO:     127.0.0.1:46134 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:12<00:03, 13.86it/s][2026-01-05 11:23:19] INFO:     127.0.0.1:46478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:20] INFO:     127.0.0.1:46122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:20] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:12<00:03, 13.59it/s]
 77%|███████▋  | 154/200 [00:12<00:03, 15.07it/s][2026-01-05 11:23:20] Decode batch, #running-req: 48, #token: 15872, token usage: 0.05, cpu graph: False, gen throughput (token/s): 1534.08, #queue-req: 0,
[2026-01-05 11:23:20] INFO:     127.0.0.1:46232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:20] INFO:     127.0.0.1:45004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:20] INFO:     127.0.0.1:45426 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:12<00:03, 13.55it/s][2026-01-05 11:23:20] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:12<00:04, 10.03it/s][2026-01-05 11:23:20] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:20] INFO:     127.0.0.1:45174 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:13<00:03, 11.09it/s][2026-01-05 11:23:20] INFO:     127.0.0.1:45348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:20] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:13<00:03, 11.27it/s][2026-01-05 11:23:21] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:21] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:13<00:03, 10.07it/s][2026-01-05 11:23:21] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:21] INFO:     127.0.0.1:44802 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:13<00:04,  7.33it/s][2026-01-05 11:23:21] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:21] Decode batch, #running-req: 33, #token: 12800, token usage: 0.04, cpu graph: False, gen throughput (token/s): 965.37, #queue-req: 0,
[2026-01-05 11:23:21] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:14<00:03,  8.66it/s][2026-01-05 11:23:21] INFO:     127.0.0.1:46328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:21] INFO:     127.0.0.1:45034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:21] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:14<00:03,  9.94it/s]
 86%|████████▌ | 171/200 [00:14<00:02, 12.74it/s][2026-01-05 11:23:22] INFO:     127.0.0.1:44820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:22] INFO:     127.0.0.1:46082 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 173/200 [00:14<00:03,  8.89it/s][2026-01-05 11:23:22] INFO:     127.0.0.1:46026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:22] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:22] INFO:     127.0.0.1:46304 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:14<00:02,  8.61it/s]
 88%|████████▊ | 176/200 [00:14<00:02,  9.60it/s][2026-01-05 11:23:22] INFO:     127.0.0.1:46512 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:22] INFO:     127.0.0.1:45946 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:15<00:02,  9.12it/s][2026-01-05 11:23:22] INFO:     127.0.0.1:46288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:22] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:15<00:01, 10.27it/s][2026-01-05 11:23:23] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:23] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:15<00:02,  8.82it/s][2026-01-05 11:23:23] Decode batch, #running-req: 18, #token: 7552, token usage: 0.02, cpu graph: False, gen throughput (token/s): 604.88, #queue-req: 0,
[2026-01-05 11:23:23] INFO:     127.0.0.1:46200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:23] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:15<00:01,  8.84it/s][2026-01-05 11:23:23] INFO:     127.0.0.1:45860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:23] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:15<00:01,  8.47it/s][2026-01-05 11:23:23] INFO:     127.0.0.1:46116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:23] INFO:     127.0.0.1:46216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:23] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:16<00:01, 10.25it/s][2026-01-05 11:23:24] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:23:24] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:16<00:01,  7.65it/s][2026-01-05 11:23:24] INFO:     127.0.0.1:46464 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:16<00:01,  6.63it/s][2026-01-05 11:23:24] Decode batch, #running-req: 8, #token: 4352, token usage: 0.01, cpu graph: False, gen throughput (token/s): 306.45, #queue-req: 0,
[2026-01-05 11:23:25] INFO:     127.0.0.1:46248 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:17<00:01,  4.61it/s][2026-01-05 11:23:25] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:17<00:01,  3.50it/s][2026-01-05 11:23:26] INFO:     127.0.0.1:46400 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:18<00:01,  2.83it/s][2026-01-05 11:23:26] Decode batch, #running-req: 5, #token: 3200, token usage: 0.01, cpu graph: False, gen throughput (token/s): 186.60, #queue-req: 0,
[2026-01-05 11:23:27] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:19<00:02,  1.68it/s][2026-01-05 11:23:27] Decode batch, #running-req: 4, #token: 3072, token usage: 0.01, cpu graph: False, gen throughput (token/s): 136.21, #queue-req: 0,
[2026-01-05 11:23:28] INFO:     127.0.0.1:46566 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:20<00:02,  1.41it/s][2026-01-05 11:23:29] Decode batch, #running-req: 3, #token: 2560, token usage: 0.01, cpu graph: False, gen throughput (token/s): 98.25, #queue-req: 0,
[2026-01-05 11:23:29] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:22<00:01,  1.14it/s][2026-01-05 11:23:30] INFO:     127.0.0.1:45600 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:22<00:00,  1.33it/s][2026-01-05 11:23:30] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, cpu graph: False, gen throughput (token/s): 62.86, #queue-req: 0,
[2026-01-05 11:23:32] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, cpu graph: False, gen throughput (token/s): 28.17, #queue-req: 0,
[2026-01-05 11:23:33] Decode batch, #running-req: 1, #token: 1408, token usage: 0.00, cpu graph: False, gen throughput (token/s): 28.36, #queue-req: 0,
[2026-01-05 11:23:34] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:26<00:00,  1.78s/it]
100%|██████████| 200/200 [00:26<00:00,  7.41it/s]
.
----------------------------------------------------------------------
Ran 1 test in 129.518s

OK
Accuracy: 0.885
Invalid: 0.000
Latency: 28.223 s
Output throughput: 1047.356 token/s
.
.
End (16/43):
filename='ascend/llm_models/test_ascend_ling_lite.py', elapsed=146, estimated_time=400
.
.

.
.
Begin (17/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_qwen3_30B_models.py
.
.

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 1348, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 1303, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 1349, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 1298, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 1058, in _send_output
    self.send(msg)
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 996, in send
    self.connect()
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 962, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/socket.py", line 863, in create_connection
    raise exceptions[0]
  File "/usr/local/python3.11.13/lib/python3.11/socket.py", line 848, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/llm_models/test_qwen3_30B_models.py", line 62, in test_a_gsm8k
    metrics = run_eval_few_shot_gsm8k(args)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/few_shot_gsm8k.py", line 49, in run_eval
    set_default_backend(RuntimeEndpoint(f"{args.host}:{args.port}"))
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/lang/backend/runtime_endpoint.py", line 40, in __init__
    res = http_request(
          ^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/utils.py", line 170, in http_request
    resp = urllib.request.urlopen(req, data=data, cafile=verify)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 216, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 519, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 536, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 496, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 1377, in http_open
    return self.do_open(http.client.HTTPConnection, req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 1351, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 111] Connection refused>
E
======================================================================
ERROR: test_a_gsm8k (__main__.TestAscendTp4Bf16.test_a_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 1348, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 1303, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 1349, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 1298, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 1058, in _send_output
    self.send(msg)
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 996, in send
    self.connect()
  File "/usr/local/python3.11.13/lib/python3.11/http/client.py", line 962, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/socket.py", line 863, in create_connection
    raise exceptions[0]
  File "/usr/local/python3.11.13/lib/python3.11/socket.py", line 848, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/llm_models/test_qwen3_30B_models.py", line 62, in test_a_gsm8k
    metrics = run_eval_few_shot_gsm8k(args)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/few_shot_gsm8k.py", line 49, in run_eval
    set_default_backend(RuntimeEndpoint(f"{args.host}:{args.port}"))
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/lang/backend/runtime_endpoint.py", line 40, in __init__
    res = http_request(
          ^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/utils.py", line 170, in http_request
    resp = urllib.request.urlopen(req, data=data, cafile=verify)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 216, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 519, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 536, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 496, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 1377, in http_open
    return self.do_open(http.client.HTTPConnection, req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/urllib/request.py", line 1351, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 111] Connection refused>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 0.013s

FAILED (errors=1)
[CI Test Method] TestAscendTp4Bf16.test_a_gsm8k
.
.
End (17/43):
filename='ascend/llm_models/test_qwen3_30B_models.py', elapsed=10, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_qwen3_30B_models.py failed with non-retriable error: ConnectionRefusedError - not retrying


✗ FAILED: ascend/llm_models/test_qwen3_30B_models.py returned exit code 1

.
.
Begin (18/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_baichuan2_13b_chat.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:24:11] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat', tokenizer_path='/root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=556698809, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
[2026-01-05 11:24:11] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:24:20] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:24:21] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:24:21] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:24:22] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:24:22] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:04<00:20,  4.04s/it]

Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:10<00:20,  5.21s/it]

Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:16<00:16,  5.61s/it]

Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:22<00:11,  5.88s/it]

Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:29<00:06,  6.14s/it]

Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:35<00:00,  6.14s/it]

Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:35<00:00,  5.86s/it]

[2026-01-05 11:24:58] Load weight end. type=BaichuanForCausalLM, dtype=torch.bfloat16, avail mem=34.92 GB, mem usage=25.90 GB.
[2026-01-05 11:24:58] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:24:58] The available memory for KV cache is 22.76 GB.
[2026-01-05 11:24:58] KV Cache is allocated. #tokens: 29824, K size: 11.43 GB, V size: 11.43 GB
[2026-01-05 11:24:58] Memory pool end. avail mem=12.01 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:24:59] max_total_num_tokens=29824, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=3728, context_len=4096, available_gpu_mem=12.01 GB
[2026-01-05 11:24:59] INFO:     Started server process [167969]
[2026-01-05 11:24:59] INFO:     Waiting for application startup.
[2026-01-05 11:24:59] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.3, 'top_k': 5, 'top_p': 0.85}
[2026-01-05 11:24:59] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.3, 'top_k': 5, 'top_p': 0.85}
[2026-01-05 11:24:59] INFO:     Application startup complete.
[2026-01-05 11:24:59] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:25:00] INFO:     127.0.0.1:60838 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:25:00] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:25:00.214259230 compiler_depend.ts:198] Warning: Driver Version: "+" is invalid or not supported yet. (function operator())
[2026-01-05 11:25:01] INFO:     127.0.0.1:60866 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:25:11] INFO:     127.0.0.1:42858 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:25:21] INFO:     127.0.0.1:48532 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:25:22] INFO:     127.0.0.1:60852 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:25:22] The server is fired up and ready to roll!
[2026-01-05 11:25:31] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:25:32] INFO:     127.0.0.1:57276 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:25:32] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:25:32] INFO:     127.0.0.1:57286 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:25:32] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:25:32] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:25:32] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 0, #queue-req: 0,
[2026-01-05 11:25:32] Prefill batch, #new-seq: 10, #new-token: 1536, #cached-token: 7680, token usage: 0.03, #running-req: 1, #queue-req: 0,
[2026-01-05 11:25:32] Prefill batch, #new-seq: 18, #new-token: 2432, #cached-token: 13824, token usage: 0.08, #running-req: 11, #queue-req: 0,
[2026-01-05 11:25:32] Prefill batch, #new-seq: 21, #new-token: 3328, #cached-token: 16128, token usage: 0.16, #running-req: 29, #queue-req: 10,
[2026-01-05 11:25:34] Decode batch, #running-req: 50, #token: 12288, token usage: 0.41, cpu graph: False, gen throughput (token/s): 20.89, #queue-req: 78,
[2026-01-05 11:25:36] Decode batch, #running-req: 50, #token: 13696, token usage: 0.46, cpu graph: False, gen throughput (token/s): 963.02, #queue-req: 78,
[2026-01-05 11:25:38] Decode batch, #running-req: 50, #token: 14336, token usage: 0.48, cpu graph: False, gen throughput (token/s): 938.79, #queue-req: 78,
[2026-01-05 11:25:41] Decode batch, #running-req: 50, #token: 17792, token usage: 0.60, cpu graph: False, gen throughput (token/s): 900.09, #queue-req: 78,
[2026-01-05 11:25:43] Decode batch, #running-req: 50, #token: 19968, token usage: 0.67, cpu graph: False, gen throughput (token/s): 860.13, #queue-req: 78,
[2026-01-05 11:25:45] Decode batch, #running-req: 50, #token: 20352, token usage: 0.68, cpu graph: False, gen throughput (token/s): 832.94, #queue-req: 78,
[2026-01-05 11:25:48] Decode batch, #running-req: 50, #token: 23296, token usage: 0.78, cpu graph: False, gen throughput (token/s): 812.83, #queue-req: 78,
[2026-01-05 11:25:50] Decode batch, #running-req: 50, #token: 26368, token usage: 0.88, cpu graph: False, gen throughput (token/s): 793.47, #queue-req: 78,
[2026-01-05 11:25:53] Decode batch, #running-req: 50, #token: 26624, token usage: 0.89, cpu graph: False, gen throughput (token/s): 776.44, #queue-req: 78,
[2026-01-05 11:25:56] Decode batch, #running-req: 50, #token: 28288, token usage: 0.95, cpu graph: False, gen throughput (token/s): 755.18, #queue-req: 78,
[2026-01-05 11:25:56] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.2967 -> 0.8242
[2026-01-05 11:25:57] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8202 -> 0.8340
[2026-01-05 11:25:57] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8279 -> 0.8476
[2026-01-05 11:25:57] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8456 -> 0.8535
[2026-01-05 11:25:58] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8434 -> 0.8750
[2026-01-05 11:25:58] Decode batch, #running-req: 45, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 716.47, #queue-req: 83,
[2026-01-05 11:26:01] Decode batch, #running-req: 45, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 671.79, #queue-req: 83,
[2026-01-05 11:26:03] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.7957 -> 1.0000
[2026-01-05 11:26:04] Decode batch, #running-req: 44, #token: 0, token usage: 0.00, cpu graph: False, gen throughput (token/s): 654.15, #queue-req: 84,
[2026-01-05 11:26:04] Prefill batch, #new-seq: 42, #new-token: 7168, #cached-token: 32256, token usage: 0.03, #running-req: 0, #queue-req: 42,
[2026-01-05 11:26:04] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:04] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:31<1:45:44, 31.88s/it]
 16%|█▌        | 32/200 [00:31<03:56,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it]
 22%|██▏       | 44/200 [00:31<03:39,  1.41s/it][2026-01-05 11:26:06] Decode batch, #running-req: 42, #token: 11264, token usage: 0.38, cpu graph: False, gen throughput (token/s): 696.39, #queue-req: 86,
[2026-01-05 11:26:08] Decode batch, #running-req: 42, #token: 11648, token usage: 0.39, cpu graph: False, gen throughput (token/s): 890.21, #queue-req: 86,
[2026-01-05 11:26:10] Decode batch, #running-req: 42, #token: 13056, token usage: 0.44, cpu graph: False, gen throughput (token/s): 866.49, #queue-req: 86,
[2026-01-05 11:26:12] Decode batch, #running-req: 42, #token: 16000, token usage: 0.54, cpu graph: False, gen throughput (token/s): 838.31, #queue-req: 86,
[2026-01-05 11:26:14] Decode batch, #running-req: 42, #token: 16896, token usage: 0.57, cpu graph: False, gen throughput (token/s): 807.62, #queue-req: 86,
[2026-01-05 11:26:16] Decode batch, #running-req: 42, #token: 17792, token usage: 0.60, cpu graph: False, gen throughput (token/s): 784.48, #queue-req: 86,
[2026-01-05 11:26:18] Decode batch, #running-req: 42, #token: 20608, token usage: 0.69, cpu graph: False, gen throughput (token/s): 764.05, #queue-req: 86,
[2026-01-05 11:26:21] Decode batch, #running-req: 42, #token: 22272, token usage: 0.75, cpu graph: False, gen throughput (token/s): 744.72, #queue-req: 86,
[2026-01-05 11:26:23] Decode batch, #running-req: 42, #token: 22784, token usage: 0.76, cpu graph: False, gen throughput (token/s): 727.51, #queue-req: 86,
[2026-01-05 11:26:25] Decode batch, #running-req: 42, #token: 25344, token usage: 0.85, cpu graph: False, gen throughput (token/s): 708.65, #queue-req: 86,
[2026-01-05 11:26:28] Decode batch, #running-req: 42, #token: 27648, token usage: 0.93, cpu graph: False, gen throughput (token/s): 692.71, #queue-req: 86,
[2026-01-05 11:26:30] Decode batch, #running-req: 42, #token: 28032, token usage: 0.94, cpu graph: False, gen throughput (token/s): 679.74, #queue-req: 86,
[2026-01-05 11:26:32] Prefill batch, #new-seq: 40, #new-token: 8192, #cached-token: 30720, token usage: 0.03, #running-req: 0, #queue-req: 46,
[2026-01-05 11:26:32] Prefill batch, #new-seq: 16, #new-token: 3328, #cached-token: 11520, token usage: 0.30, #running-req: 39, #queue-req: 31,
[2026-01-05 11:26:32] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:26:32] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [01:00<04:55,  1.91s/it]
 40%|████      | 81/200 [01:00<04:46,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it]
 43%|████▎     | 86/200 [01:00<04:34,  2.41s/it][2026-01-05 11:26:33] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [01:01<04:27,  2.36s/it][2026-01-05 11:26:33] Prefill batch, #new-seq: 8, #new-token: 1664, #cached-token: 6144, token usage: 0.39, #running-req: 54, #queue-req: 51,
[2026-01-05 11:26:36] Decode batch, #running-req: 62, #token: 13568, token usage: 0.45, cpu graph: False, gen throughput (token/s): 313.16, #queue-req: 51,
[2026-01-05 11:26:38] Decode batch, #running-req: 62, #token: 18560, token usage: 0.62, cpu graph: False, gen throughput (token/s): 1030.53, #queue-req: 51,
[2026-01-05 11:26:41] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [01:08<04:50,  2.59s/it][2026-01-05 11:26:41] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.63, #running-req: 61, #queue-req: 49,
[2026-01-05 11:26:41] Decode batch, #running-req: 63, #token: 18944, token usage: 0.64, cpu graph: False, gen throughput (token/s): 973.32, #queue-req: 49,
[2026-01-05 11:26:41] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [01:09<04:34,  2.47s/it][2026-01-05 11:26:41] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.61, #running-req: 62, #queue-req: 47,
[2026-01-05 11:26:42] INFO:     127.0.0.1:57374 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [01:09<04:11,  2.29s/it][2026-01-05 11:26:42] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.61, #running-req: 63, #queue-req: 44,
[2026-01-05 11:26:42] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [01:10<03:48,  2.10s/it][2026-01-05 11:26:42] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.61, #running-req: 65, #queue-req: 42,
[2026-01-05 11:26:42] INFO:     127.0.0.1:57682 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [01:10<03:21,  1.87s/it][2026-01-05 11:26:43] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.61, #running-req: 66, #queue-req: 40,
[2026-01-05 11:26:44] Decode batch, #running-req: 68, #token: 19840, token usage: 0.67, cpu graph: False, gen throughput (token/s): 938.37, #queue-req: 40,
[2026-01-05 11:26:46] Decode batch, #running-req: 68, #token: 24448, token usage: 0.82, cpu graph: False, gen throughput (token/s): 993.96, #queue-req: 40,
[2026-01-05 11:26:49] Decode batch, #running-req: 68, #token: 26112, token usage: 0.88, cpu graph: False, gen throughput (token/s): 961.25, #queue-req: 40,
[2026-01-05 11:26:52] Decode batch, #running-req: 68, #token: 27904, token usage: 0.94, cpu graph: False, gen throughput (token/s): 931.63, #queue-req: 40,
[2026-01-05 11:26:54] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.2094 -> 0.5425
[2026-01-05 11:26:54] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5394 -> 0.5531
[2026-01-05 11:26:55] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5521 -> 0.5598
[2026-01-05 11:26:55] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5578 -> 0.5685
[2026-01-05 11:26:55] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5685 -> 0.5732
[2026-01-05 11:26:55] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5712 -> 0.5819
[2026-01-05 11:26:55] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5819 -> 0.5867
[2026-01-05 11:26:55] Decode batch, #running-req: 62, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 905.00, #queue-req: 47,
[2026-01-05 11:26:55] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5837 -> 0.5974
[2026-01-05 11:26:56] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5954 -> 0.6063
[2026-01-05 11:26:56] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.6053 -> 0.6130
[2026-01-05 11:26:56] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.6099 -> 0.6236
[2026-01-05 11:26:56] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.6186 -> 0.6355
[2026-01-05 11:26:58] Decode batch, #running-req: 56, #token: 29440, token usage: 0.99, cpu graph: False, gen throughput (token/s): 813.38, #queue-req: 52,
[2026-01-05 11:27:01] Decode batch, #running-req: 56, #token: 29824, token usage: 1.00, cpu graph: False, gen throughput (token/s): 770.20, #queue-req: 52,
[2026-01-05 11:27:01] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5743 -> 0.7567
[2026-01-05 11:27:01] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.7537 -> 0.7647
[2026-01-05 11:27:02] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.7547 -> 0.7863
[2026-01-05 11:27:03] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.7753 -> 0.8100
[2026-01-05 11:27:03] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8029 -> 0.8258
[2026-01-05 11:27:04] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8238 -> 0.8318
[2026-01-05 11:27:04] Decode batch, #running-req: 50, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 766.39, #queue-req: 58,
[2026-01-05 11:27:04] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8288 -> 0.8398
[2026-01-05 11:27:04] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8388 -> 0.8437
[2026-01-05 11:27:04] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8407 -> 0.8515
[2026-01-05 11:27:05] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8445 -> 0.8672
[2026-01-05 11:27:05] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8641 -> 0.8750
[2026-01-05 11:27:06] Decode batch, #running-req: 45, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 697.69, #queue-req: 63,
[2026-01-05 11:27:09] Decode batch, #running-req: 45, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 668.63, #queue-req: 63,
[2026-01-05 11:27:10] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8027 -> 1.0000
[2026-01-05 11:27:11] Prefill batch, #new-seq: 42, #new-token: 7168, #cached-token: 32256, token usage: 0.03, #running-req: 0, #queue-req: 22,
[2026-01-05 11:27:11] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 768, token usage: 0.27, #running-req: 42, #queue-req: 21,
[2026-01-05 11:27:11] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:42678 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:11] INFO:     127.0.0.1:58212 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [01:38<10:48,  6.06s/it]
 67%|██████▋   | 134/200 [01:38<01:51,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it]
 68%|██████▊   | 136/200 [01:38<01:48,  1.70s/it][2026-01-05 11:27:13] Decode batch, #running-req: 43, #token: 9344, token usage: 0.31, cpu graph: False, gen throughput (token/s): 492.53, #queue-req: 21,
[2026-01-05 11:27:15] Decode batch, #running-req: 43, #token: 12288, token usage: 0.41, cpu graph: False, gen throughput (token/s): 891.39, #queue-req: 21,
[2026-01-05 11:27:16] Decode batch, #running-req: 43, #token: 12800, token usage: 0.43, cpu graph: False, gen throughput (token/s): 867.89, #queue-req: 21,
[2026-01-05 11:27:19] Decode batch, #running-req: 43, #token: 14336, token usage: 0.48, cpu graph: False, gen throughput (token/s): 841.00, #queue-req: 21,
[2026-01-05 11:27:21] Decode batch, #running-req: 43, #token: 17664, token usage: 0.59, cpu graph: False, gen throughput (token/s): 810.21, #queue-req: 21,
[2026-01-05 11:27:23] Decode batch, #running-req: 43, #token: 18304, token usage: 0.61, cpu graph: False, gen throughput (token/s): 778.23, #queue-req: 21,
[2026-01-05 11:27:25] Decode batch, #running-req: 43, #token: 19328, token usage: 0.65, cpu graph: False, gen throughput (token/s): 757.92, #queue-req: 21,
[2026-01-05 11:27:27] Decode batch, #running-req: 43, #token: 22656, token usage: 0.76, cpu graph: False, gen throughput (token/s): 739.12, #queue-req: 21,
[2026-01-05 11:27:30] Decode batch, #running-req: 43, #token: 23552, token usage: 0.79, cpu graph: False, gen throughput (token/s): 720.70, #queue-req: 21,
[2026-01-05 11:27:30] INFO:     127.0.0.1:42860 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [01:58<02:14,  2.14s/it][2026-01-05 11:27:30] Prefill batch, #new-seq: 3, #new-token: 1152, #cached-token: 2304, token usage: 0.77, #running-req: 42, #queue-req: 18,
[2026-01-05 11:27:31] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [01:58<02:09,  2.08s/it][2026-01-05 11:27:31] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 768, token usage: 0.79, #running-req: 44, #queue-req: 17,
[2026-01-05 11:27:31] Prefill batch, #new-seq: 2, #new-token: 768, #cached-token: 1536, token usage: 0.78, #running-req: 44, #queue-req: 15,
[2026-01-05 11:27:31] INFO:     127.0.0.1:42868 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [01:59<02:02,  2.00s/it][2026-01-05 11:27:34] Decode batch, #running-req: 46, #token: 24576, token usage: 0.82, cpu graph: False, gen throughput (token/s): 477.97, #queue-req: 15,
[2026-01-05 11:27:36] Decode batch, #running-req: 46, #token: 27392, token usage: 0.92, cpu graph: False, gen throughput (token/s): 725.98, #queue-req: 15,
[2026-01-05 11:27:39] Decode batch, #running-req: 46, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 705.34, #queue-req: 15,
[2026-01-05 11:27:40] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5204 -> 0.9194
[2026-01-05 11:27:41] Decode batch, #running-req: 45, #token: 29824, token usage: 1.00, cpu graph: False, gen throughput (token/s): 679.15, #queue-req: 16,
[2026-01-05 11:27:42] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8852 -> 0.9940
[2026-01-05 11:27:43] Prefill batch, #new-seq: 15, #new-token: 8192, #cached-token: 11520, token usage: 0.10, #running-req: 4, #queue-req: 2,
[2026-01-05 11:27:43] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:42934 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:42948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:42880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:42972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:42900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:42872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:42886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:42916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:40884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:43] INFO:     127.0.0.1:41074 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [02:10<02:36,  2.61s/it]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s]
 90%|████████▉ | 179/200 [02:10<00:16,  1.26it/s][2026-01-05 11:27:44] Prefill batch, #new-seq: 3, #new-token: 1536, #cached-token: 1536, token usage: 0.37, #running-req: 18, #queue-req: 0,
[2026-01-05 11:27:45] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [02:13<00:17,  1.17it/s][2026-01-05 11:27:46] Decode batch, #running-req: 20, #token: 12544, token usage: 0.42, cpu graph: False, gen throughput (token/s): 270.29, #queue-req: 0,
[2026-01-05 11:27:48] Decode batch, #running-req: 20, #token: 13056, token usage: 0.44, cpu graph: False, gen throughput (token/s): 503.65, #queue-req: 0,
[2026-01-05 11:27:48] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [02:16<00:17,  1.07it/s][2026-01-05 11:27:49] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [02:16<00:16,  1.12it/s][2026-01-05 11:27:49] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [02:16<00:14,  1.18it/s][2026-01-05 11:27:49] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [02:16<00:12,  1.27it/s][2026-01-05 11:27:49] Decode batch, #running-req: 16, #token: 10880, token usage: 0.36, cpu graph: False, gen throughput (token/s): 483.29, #queue-req: 0,
[2026-01-05 11:27:49] INFO:     127.0.0.1:42758 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [02:17<00:10,  1.38it/s][2026-01-05 11:27:49] INFO:     127.0.0.1:42750 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [02:17<00:09,  1.55it/s][2026-01-05 11:27:49] INFO:     127.0.0.1:42718 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [02:17<00:07,  1.81it/s][2026-01-05 11:27:50] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [02:17<00:06,  2.00it/s][2026-01-05 11:27:50] INFO:     127.0.0.1:42734 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [02:18<00:05,  2.08it/s][2026-01-05 11:27:50] Decode batch, #running-req: 11, #token: 8064, token usage: 0.27, cpu graph: False, gen throughput (token/s): 375.37, #queue-req: 0,
[2026-01-05 11:27:51] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [02:18<00:04,  2.21it/s][2026-01-05 11:27:51] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [02:18<00:03,  2.70it/s][2026-01-05 11:27:51] INFO:     127.0.0.1:42822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:51] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:51] INFO:     127.0.0.1:42836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:51] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [02:18<00:01,  4.75it/s]
 98%|█████████▊| 195/200 [02:18<00:00,  7.77it/s][2026-01-05 11:27:51] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:52] Decode batch, #running-req: 4, #token: 3328, token usage: 0.11, cpu graph: False, gen throughput (token/s): 206.87, #queue-req: 0,
[2026-01-05 11:27:52] INFO:     127.0.0.1:42852 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [02:20<00:00,  3.43it/s][2026-01-05 11:27:52] INFO:     127.0.0.1:42760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:27:53] Decode batch, #running-req: 2, #token: 2048, token usage: 0.07, cpu graph: False, gen throughput (token/s): 122.56, #queue-req: 0,
[2026-01-05 11:27:54] Decode batch, #running-req: 2, #token: 2048, token usage: 0.07, cpu graph: False, gen throughput (token/s): 78.34, #queue-req: 0,
[2026-01-05 11:27:55] INFO:     127.0.0.1:42786 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [02:22<00:00,  1.91it/s][2026-01-05 11:27:55] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [02:22<00:00,  1.40it/s]
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/gsm8k_ascend_mixin.py", line 64, in test_gsm8k
    self.assertGreater(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1271, in assertGreater
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: np.float64(0.0) not greater than 0.0 : Accuracy of /root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat is 0.0, is lower than 0.0
E
======================================================================
ERROR: test_gsm8k (__main__.TestMistral7B.test_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: np.float64(0.0) not greater than 0.0 : Accuracy of /root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat is 0.0, is lower than 0.0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 234.163s

FAILED (errors=1)
Accuracy: 0.000
Invalid: 0.000
Latency: 142.875 s
Output throughput: 716.712 token/s
.
.
End (18/43):
filename='ascend/llm_models/test_ascend_baichuan2_13b_chat.py', elapsed=246, estimated_time=400
.
.


[CI Retry] ascend/llm_models/test_ascend_baichuan2_13b_chat.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/llm_models/test_ascend_baichuan2_13b_chat.py

.
.
Begin (18/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_baichuan2_13b_chat.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:29:18] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat', tokenizer_path='/root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=472423059, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
[2026-01-05 11:29:19] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
[2026-01-05 11:29:28] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:29:29] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:29:29] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:29:30] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:29:30] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:03<00:19,  3.95s/it]

Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:10<00:21,  5.28s/it]

Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:16<00:16,  5.54s/it]

Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:21<00:11,  5.65s/it]

Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:27<00:05,  5.75s/it]

Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:34<00:00,  5.93s/it]

Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:34<00:00,  5.67s/it]

[2026-01-05 11:30:05] Load weight end. type=BaichuanForCausalLM, dtype=torch.bfloat16, avail mem=34.92 GB, mem usage=25.90 GB.
[2026-01-05 11:30:05] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:30:05] The available memory for KV cache is 22.76 GB.
[2026-01-05 11:30:05] KV Cache is allocated. #tokens: 29824, K size: 11.43 GB, V size: 11.43 GB
[2026-01-05 11:30:05] Memory pool end. avail mem=12.01 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:30:06] max_total_num_tokens=29824, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=3728, context_len=4096, available_gpu_mem=12.01 GB
[2026-01-05 11:30:06] INFO:     Started server process [170595]
[2026-01-05 11:30:06] INFO:     Waiting for application startup.
[2026-01-05 11:30:06] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.3, 'top_k': 5, 'top_p': 0.85}
[2026-01-05 11:30:06] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.3, 'top_k': 5, 'top_p': 0.85}
[2026-01-05 11:30:06] INFO:     Application startup complete.
[2026-01-05 11:30:06] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:30:07] INFO:     127.0.0.1:46978 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:30:07] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:30:07.210238458 compiler_depend.ts:198] Warning: Driver Version: "D" is invalid or not supported yet. (function operator())
[2026-01-05 11:30:08] INFO:     127.0.0.1:47004 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:30:18] INFO:     127.0.0.1:42930 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:30:28] INFO:     127.0.0.1:47288 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:30:29] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:30:29] The server is fired up and ready to roll!
[2026-01-05 11:30:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:30:39] INFO:     127.0.0.1:48018 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:30:39] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:30:39] INFO:     127.0.0.1:48026 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:30:39] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:30:39] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:30:39] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.03, #running-req: 0, #queue-req: 0,
[2026-01-05 11:30:39] Prefill batch, #new-seq: 9, #new-token: 1408, #cached-token: 6912, token usage: 0.03, #running-req: 1, #queue-req: 0,
[2026-01-05 11:30:39] Prefill batch, #new-seq: 17, #new-token: 2304, #cached-token: 13056, token usage: 0.08, #running-req: 10, #queue-req: 0,
[2026-01-05 11:30:39] Prefill batch, #new-seq: 23, #new-token: 3584, #cached-token: 17664, token usage: 0.15, #running-req: 27, #queue-req: 10,
[2026-01-05 11:30:41] Decode batch, #running-req: 50, #token: 12288, token usage: 0.41, cpu graph: False, gen throughput (token/s): 21.03, #queue-req: 78,
[2026-01-05 11:30:43] Decode batch, #running-req: 50, #token: 13696, token usage: 0.46, cpu graph: False, gen throughput (token/s): 963.16, #queue-req: 78,
[2026-01-05 11:30:45] Decode batch, #running-req: 50, #token: 14336, token usage: 0.48, cpu graph: False, gen throughput (token/s): 938.86, #queue-req: 78,
[2026-01-05 11:30:48] Decode batch, #running-req: 50, #token: 17792, token usage: 0.60, cpu graph: False, gen throughput (token/s): 900.37, #queue-req: 78,
[2026-01-05 11:30:50] Decode batch, #running-req: 50, #token: 19968, token usage: 0.67, cpu graph: False, gen throughput (token/s): 860.47, #queue-req: 78,
[2026-01-05 11:30:52] Decode batch, #running-req: 50, #token: 20352, token usage: 0.68, cpu graph: False, gen throughput (token/s): 833.26, #queue-req: 78,
[2026-01-05 11:30:55] Decode batch, #running-req: 50, #token: 23296, token usage: 0.78, cpu graph: False, gen throughput (token/s): 813.49, #queue-req: 78,
[2026-01-05 11:30:57] Decode batch, #running-req: 50, #token: 26368, token usage: 0.88, cpu graph: False, gen throughput (token/s): 793.87, #queue-req: 78,
[2026-01-05 11:31:00] Decode batch, #running-req: 50, #token: 26624, token usage: 0.89, cpu graph: False, gen throughput (token/s): 776.65, #queue-req: 78,
[2026-01-05 11:31:03] Decode batch, #running-req: 50, #token: 28288, token usage: 0.95, cpu graph: False, gen throughput (token/s): 755.59, #queue-req: 78,
[2026-01-05 11:31:03] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.2967 -> 0.8242
[2026-01-05 11:31:04] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8202 -> 0.8340
[2026-01-05 11:31:04] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8279 -> 0.8476
[2026-01-05 11:31:04] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8456 -> 0.8535
[2026-01-05 11:31:05] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8434 -> 0.8750
[2026-01-05 11:31:05] Decode batch, #running-req: 45, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 716.53, #queue-req: 83,
[2026-01-05 11:31:08] Decode batch, #running-req: 45, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 671.39, #queue-req: 83,
[2026-01-05 11:31:10] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.7957 -> 1.0000
[2026-01-05 11:31:11] Decode batch, #running-req: 44, #token: 0, token usage: 0.00, cpu graph: False, gen throughput (token/s): 653.95, #queue-req: 84,
[2026-01-05 11:31:11] Prefill batch, #new-seq: 42, #new-token: 7040, #cached-token: 32256, token usage: 0.03, #running-req: 0, #queue-req: 42,
[2026-01-05 11:31:11] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48152 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:11] INFO:     127.0.0.1:48416 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:31<1:45:43, 31.88s/it]
 20%|██        | 40/200 [00:31<03:35,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it]
 22%|██▏       | 44/200 [00:31<03:30,  1.35s/it][2026-01-05 11:31:13] Decode batch, #running-req: 42, #token: 11008, token usage: 0.37, cpu graph: False, gen throughput (token/s): 699.85, #queue-req: 86,
[2026-01-05 11:31:15] Decode batch, #running-req: 42, #token: 11648, token usage: 0.39, cpu graph: False, gen throughput (token/s): 890.24, #queue-req: 86,
[2026-01-05 11:31:17] Decode batch, #running-req: 42, #token: 12928, token usage: 0.43, cpu graph: False, gen throughput (token/s): 867.39, #queue-req: 86,
[2026-01-05 11:31:19] Decode batch, #running-req: 42, #token: 15616, token usage: 0.52, cpu graph: False, gen throughput (token/s): 838.77, #queue-req: 86,
[2026-01-05 11:31:21] Decode batch, #running-req: 42, #token: 16896, token usage: 0.57, cpu graph: False, gen throughput (token/s): 809.63, #queue-req: 86,
[2026-01-05 11:31:23] Decode batch, #running-req: 42, #token: 17664, token usage: 0.59, cpu graph: False, gen throughput (token/s): 785.26, #queue-req: 86,
[2026-01-05 11:31:25] Decode batch, #running-req: 42, #token: 20480, token usage: 0.69, cpu graph: False, gen throughput (token/s): 764.62, #queue-req: 86,
[2026-01-05 11:31:28] Decode batch, #running-req: 42, #token: 22272, token usage: 0.75, cpu graph: False, gen throughput (token/s): 745.03, #queue-req: 86,
[2026-01-05 11:31:30] Decode batch, #running-req: 42, #token: 22784, token usage: 0.76, cpu graph: False, gen throughput (token/s): 727.39, #queue-req: 86,
[2026-01-05 11:31:32] Decode batch, #running-req: 42, #token: 24960, token usage: 0.84, cpu graph: False, gen throughput (token/s): 709.31, #queue-req: 86,
[2026-01-05 11:31:35] Decode batch, #running-req: 42, #token: 27648, token usage: 0.93, cpu graph: False, gen throughput (token/s): 693.11, #queue-req: 86,
[2026-01-05 11:31:37] Decode batch, #running-req: 42, #token: 28032, token usage: 0.94, cpu graph: False, gen throughput (token/s): 680.43, #queue-req: 86,
[2026-01-05 11:31:39] Prefill batch, #new-seq: 40, #new-token: 8192, #cached-token: 30720, token usage: 0.03, #running-req: 0, #queue-req: 46,
[2026-01-05 11:31:39] Prefill batch, #new-seq: 16, #new-token: 3584, #cached-token: 11520, token usage: 0.30, #running-req: 39, #queue-req: 31,
[2026-01-05 11:31:39] INFO:     127.0.0.1:48492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48526 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48552 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48558 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48720 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:31:39] INFO:     127.0.0.1:48854 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [01:00<04:43,  1.83s/it]
 40%|████      | 81/200 [01:00<02:52,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it]
 43%|████▎     | 86/200 [01:00<02:45,  1.45s/it][2026-01-05 11:31:40] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [01:01<02:42,  1.44s/it][2026-01-05 11:31:40] Prefill batch, #new-seq: 7, #new-token: 1280, #cached-token: 5376, token usage: 0.41, #running-req: 54, #queue-req: 52,
[2026-01-05 11:31:41] Decode batch, #running-req: 61, #token: 13568, token usage: 0.45, cpu graph: False, gen throughput (token/s): 503.19, #queue-req: 52,
[2026-01-05 11:31:43] Decode batch, #running-req: 61, #token: 18304, token usage: 0.61, cpu graph: False, gen throughput (token/s): 1021.87, #queue-req: 52,
[2026-01-05 11:31:45] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 88/200 [01:06<02:52,  1.54s/it][2026-01-05 11:31:45] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.62, #running-req: 60, #queue-req: 50,
[2026-01-05 11:31:46] Decode batch, #running-req: 62, #token: 18688, token usage: 0.63, cpu graph: False, gen throughput (token/s): 964.38, #queue-req: 50,
[2026-01-05 11:31:46] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [01:07<02:47,  1.51s/it][2026-01-05 11:31:46] Prefill batch, #new-seq: 3, #new-token: 640, #cached-token: 2304, token usage: 0.61, #running-req: 61, #queue-req: 47,
[2026-01-05 11:31:46] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [01:07<02:38,  1.44s/it][2026-01-05 11:31:46] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.60, #running-req: 63, #queue-req: 45,
[2026-01-05 11:31:47] INFO:     127.0.0.1:48072 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [01:07<02:29,  1.37s/it][2026-01-05 11:31:47] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.60, #running-req: 64, #queue-req: 43,
[2026-01-05 11:31:47] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [01:08<02:18,  1.28s/it][2026-01-05 11:31:47] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.60, #running-req: 65, #queue-req: 41,
[2026-01-05 11:31:48] Decode batch, #running-req: 67, #token: 19584, token usage: 0.66, cpu graph: False, gen throughput (token/s): 948.81, #queue-req: 41,
[2026-01-05 11:31:51] Decode batch, #running-req: 67, #token: 24320, token usage: 0.82, cpu graph: False, gen throughput (token/s): 1011.05, #queue-req: 41,
[2026-01-05 11:31:54] Decode batch, #running-req: 67, #token: 25856, token usage: 0.87, cpu graph: False, gen throughput (token/s): 977.25, #queue-req: 41,
[2026-01-05 11:31:57] Decode batch, #running-req: 67, #token: 27648, token usage: 0.93, cpu graph: False, gen throughput (token/s): 952.72, #queue-req: 41,
[2026-01-05 11:31:59] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.2064 -> 0.5481
[2026-01-05 11:31:59] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5461 -> 0.5568
[2026-01-05 11:31:59] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5558 -> 0.5636
[2026-01-05 11:31:59] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5625 -> 0.5704
[2026-01-05 11:31:59] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5704 -> 0.5751
[2026-01-05 11:31:59] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5741 -> 0.5819
[2026-01-05 11:31:59] Decode batch, #running-req: 61, #token: 29824, token usage: 1.00, cpu graph: False, gen throughput (token/s): 921.54, #queue-req: 47,
[2026-01-05 11:31:59] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5798 -> 0.5906
[2026-01-05 11:32:00] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5886 -> 0.5994
[2026-01-05 11:32:00] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5964 -> 0.6103
[2026-01-05 11:32:00] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.6063 -> 0.6228
[2026-01-05 11:32:02] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.5987 -> 0.6745
[2026-01-05 11:32:02] Decode batch, #running-req: 56, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 825.01, #queue-req: 52,
[2026-01-05 11:32:05] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.6364 -> 0.7509
[2026-01-05 11:32:05] Decode batch, #running-req: 55, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 769.52, #queue-req: 53,
[2026-01-05 11:32:05] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.7458 -> 0.7627
[2026-01-05 11:32:06] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.7537 -> 0.7824
[2026-01-05 11:32:06] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.7764 -> 0.7963
[2026-01-05 11:32:07] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.7862 -> 0.8179
[2026-01-05 11:32:08] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8139 -> 0.8279
[2026-01-05 11:32:08] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8259 -> 0.8340
[2026-01-05 11:32:08] Decode batch, #running-req: 50, #token: 29440, token usage: 0.99, cpu graph: False, gen throughput (token/s): 764.88, #queue-req: 59,
[2026-01-05 11:32:08] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8309 -> 0.8418
[2026-01-05 11:32:08] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8408 -> 0.8457
[2026-01-05 11:32:09] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8407 -> 0.8574
[2026-01-05 11:32:09] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8494 -> 0.8750
[2026-01-05 11:32:11] Decode batch, #running-req: 45, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 694.51, #queue-req: 63,
[2026-01-05 11:32:13] Decode batch, #running-req: 45, #token: 29568, token usage: 0.99, cpu graph: False, gen throughput (token/s): 668.22, #queue-req: 63,
[2026-01-05 11:32:14] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.8047 -> 1.0000
[2026-01-05 11:32:15] Prefill batch, #new-seq: 43, #new-token: 7296, #cached-token: 33024, token usage: 0.03, #running-req: 0, #queue-req: 21,
[2026-01-05 11:32:15] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48998 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49146 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:15] INFO:     127.0.0.1:48972 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 93/200 [01:36<07:53,  4.42s/it]
 66%|██████▌   | 131/200 [01:36<03:35,  3.12s/it]
 68%|██████▊   | 136/200 [01:36<03:19,  3.12s/it]
 68%|██████▊   | 136/200 [01:36<03:19,  3.12s/it]
 68%|██████▊   | 136/200 [01:36<03:19,  3.12s/it]
 68%|██████▊   | 136/200 [01:36<03:19,  3.12s/it]
 68%|██████▊   | 136/200 [01:36<03:19,  3.12s/it]
 68%|██████▊   | 136/200 [01:36<03:19,  3.12s/it]
 68%|██████▊   | 136/200 [01:36<03:19,  3.12s/it]
 68%|██████▊   | 136/200 [01:36<03:19,  3.12s/it]
 68%|██████▊   | 136/200 [01:36<03:19,  3.12s/it][2026-01-05 11:32:16] Decode batch, #running-req: 43, #token: 9216, token usage: 0.31, cpu graph: False, gen throughput (token/s): 585.53, #queue-req: 21,
[2026-01-05 11:32:18] Decode batch, #running-req: 43, #token: 12032, token usage: 0.40, cpu graph: False, gen throughput (token/s): 892.37, #queue-req: 21,
[2026-01-05 11:32:20] Decode batch, #running-req: 43, #token: 12672, token usage: 0.42, cpu graph: False, gen throughput (token/s): 871.64, #queue-req: 21,
[2026-01-05 11:32:22] Decode batch, #running-req: 43, #token: 14080, token usage: 0.47, cpu graph: False, gen throughput (token/s): 844.06, #queue-req: 21,
[2026-01-05 11:32:24] Decode batch, #running-req: 43, #token: 17536, token usage: 0.59, cpu graph: False, gen throughput (token/s): 812.09, #queue-req: 21,
[2026-01-05 11:32:26] Decode batch, #running-req: 43, #token: 18048, token usage: 0.61, cpu graph: False, gen throughput (token/s): 779.85, #queue-req: 21,
[2026-01-05 11:32:29] Decode batch, #running-req: 43, #token: 19072, token usage: 0.64, cpu graph: False, gen throughput (token/s): 761.65, #queue-req: 21,
[2026-01-05 11:32:31] Decode batch, #running-req: 43, #token: 22528, token usage: 0.76, cpu graph: False, gen throughput (token/s): 742.41, #queue-req: 21,
[2026-01-05 11:32:33] Decode batch, #running-req: 43, #token: 23296, token usage: 0.78, cpu graph: False, gen throughput (token/s): 723.50, #queue-req: 21,
[2026-01-05 11:32:34] INFO:     127.0.0.1:51234 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [01:55<04:03,  3.86s/it][2026-01-05 11:32:34] Prefill batch, #new-seq: 3, #new-token: 1152, #cached-token: 2304, token usage: 0.77, #running-req: 42, #queue-req: 18,
[2026-01-05 11:32:34] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [01:55<03:45,  3.64s/it][2026-01-05 11:32:34] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 768, token usage: 0.79, #running-req: 44, #queue-req: 17,
[2026-01-05 11:32:36] Decode batch, #running-req: 45, #token: 24320, token usage: 0.82, cpu graph: False, gen throughput (token/s): 673.32, #queue-req: 17,
[2026-01-05 11:32:39] Decode batch, #running-req: 45, #token: 26880, token usage: 0.90, cpu graph: False, gen throughput (token/s): 708.66, #queue-req: 17,
[2026-01-05 11:32:41] Decode batch, #running-req: 45, #token: 29184, token usage: 0.98, cpu graph: False, gen throughput (token/s): 687.49, #queue-req: 17,
[2026-01-05 11:32:44] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.4933 -> 0.9802
[2026-01-05 11:32:44] Decode batch, #running-req: 44, #token: 29440, token usage: 0.99, cpu graph: False, gen throughput (token/s): 670.99, #queue-req: 18,
[2026-01-05 11:32:45] KV cache pool is full. Retract requests. #retracted_reqs: 1, #new_token_ratio: 0.9652 -> 1.0000
[2026-01-05 11:32:45] Prefill batch, #new-seq: 16, #new-token: 8192, #cached-token: 12288, token usage: 0.06, #running-req: 2, #queue-req: 3,
[2026-01-05 11:32:45] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53430 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] Prefill batch, #new-seq: 4, #new-token: 2048, #cached-token: 2304, token usage: 0.34, #running-req: 17, #queue-req: 0,
[2026-01-05 11:32:45] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53494 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:45] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [02:06<04:18,  4.23s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it]
 90%|████████▉ | 179/200 [02:06<00:21,  1.02s/it][2026-01-05 11:32:47] INFO:     127.0.0.1:48984 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [02:07<00:20,  1.04s/it][2026-01-05 11:32:47] Decode batch, #running-req: 20, #token: 11776, token usage: 0.39, cpu graph: False, gen throughput (token/s): 396.38, #queue-req: 0,
[2026-01-05 11:32:49] Decode batch, #running-req: 20, #token: 12800, token usage: 0.43, cpu graph: False, gen throughput (token/s): 507.50, #queue-req: 0,
[2026-01-05 11:32:49] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [02:10<00:21,  1.13s/it][2026-01-05 11:32:50] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [02:10<00:19,  1.08s/it][2026-01-05 11:32:50] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [02:11<00:16,  1.00it/s][2026-01-05 11:32:50] INFO:     127.0.0.1:49064 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:50] Decode batch, #running-req: 16, #token: 10624, token usage: 0.36, cpu graph: False, gen throughput (token/s): 490.45, #queue-req: 0,
[2026-01-05 11:32:50] INFO:     127.0.0.1:51112 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [02:11<00:12,  1.19it/s][2026-01-05 11:32:50] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [02:11<00:10,  1.34it/s][2026-01-05 11:32:51] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [02:11<00:08,  1.52it/s][2026-01-05 11:32:51] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [02:12<00:07,  1.65it/s][2026-01-05 11:32:51] INFO:     127.0.0.1:51146 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [02:12<00:05,  1.88it/s][2026-01-05 11:32:51] Decode batch, #running-req: 11, #token: 7552, token usage: 0.25, cpu graph: False, gen throughput (token/s): 385.33, #queue-req: 0,
[2026-01-05 11:32:52] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [02:12<00:04,  2.06it/s][2026-01-05 11:32:52] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [02:12<00:03,  2.42it/s][2026-01-05 11:32:52] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [02:13<00:02,  2.75it/s][2026-01-05 11:32:52] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:32:52] INFO:     127.0.0.1:51228 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [02:13<00:01,  3.53it/s][2026-01-05 11:32:53] Decode batch, #running-req: 6, #token: 4608, token usage: 0.15, cpu graph: False, gen throughput (token/s): 264.02, #queue-req: 0,
[2026-01-05 11:32:53] INFO:     127.0.0.1:51222 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [02:13<00:01,  2.96it/s][2026-01-05 11:32:54] Decode batch, #running-req: 5, #token: 3968, token usage: 0.13, cpu graph: False, gen throughput (token/s): 183.93, #queue-req: 0,
[2026-01-05 11:32:55] Decode batch, #running-req: 5, #token: 3968, token usage: 0.13, cpu graph: False, gen throughput (token/s): 177.86, #queue-req: 0,
[2026-01-05 11:32:55] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [02:16<00:03,  1.21it/s][2026-01-05 11:32:56] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [02:16<00:02,  1.26it/s][2026-01-05 11:32:56] Decode batch, #running-req: 3, #token: 2944, token usage: 0.10, cpu graph: False, gen throughput (token/s): 145.15, #queue-req: 0,
[2026-01-05 11:32:56] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [02:17<00:01,  1.43it/s][2026-01-05 11:32:56] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [02:17<00:00,  1.88it/s][2026-01-05 11:32:56] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [02:17<00:00,  1.45it/s]
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/gsm8k_ascend_mixin.py", line 64, in test_gsm8k
    self.assertGreater(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1271, in assertGreater
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: np.float64(0.0) not greater than 0.0 : Accuracy of /root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat is 0.0, is lower than 0.0
E
======================================================================
ERROR: test_gsm8k (__main__.TestMistral7B.test_gsm8k)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: np.float64(0.0) not greater than 0.0 : Accuracy of /root/.cache/modelscope/hub/models/baichuan-inc/Baichuan2-13B-Chat is 0.0, is lower than 0.0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 228.977s

FAILED (errors=1)
Accuracy: 0.000
Invalid: 0.000
Latency: 137.696 s
Output throughput: 743.667 token/s
.
.
End (18/43):
filename='ascend/llm_models/test_ascend_baichuan2_13b_chat.py', elapsed=241, estimated_time=400
.
.


✗ FAILED: ascend/llm_models/test_ascend_baichuan2_13b_chat.py returned exit code 1

.
.
Begin (19/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_afm_4_5b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
The explicitly set RoPE scaling factor (config.rope_scaling['factor'] = 20.0) does not match the ratio implicitly set by other parameters (implicit factor = post-yarn context length / pre-yarn context length = config.max_position_embeddings / config.rope_scaling['original_max_position_embeddings'] = 16.0). Using the explicit factor (20.0) in YaRN. This may cause unexpected behaviour in model usage, please correct the 'max_position_embeddings' fields in the model config.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:33:18] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/arcee-ai/AFM-4.5B-Base', tokenizer_path='/root/.cache/modelscope/hub/models/arcee-ai/AFM-4.5B-Base', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=542733969, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/arcee-ai/AFM-4.5B-Base', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:33:19] No chat template found, defaulting to 'string' content format
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
The explicitly set RoPE scaling factor (config.rope_scaling['factor'] = 20.0) does not match the ratio implicitly set by other parameters (implicit factor = post-yarn context length / pre-yarn context length = config.max_position_embeddings / config.rope_scaling['original_max_position_embeddings'] = 16.0). Using the explicit factor (20.0) in YaRN. This may cause unexpected behaviour in model usage, please correct the 'max_position_embeddings' fields in the model config.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:33:28] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:33:29] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:33:29] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:33:30] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:33:30] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:06<00:06,  6.61s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:12<00:00,  6.15s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:12<00:00,  6.22s/it]

[2026-01-05 11:33:43] Load weight end. type=ArceeForCausalLM, dtype=torch.bfloat16, avail mem=52.18 GB, mem usage=8.64 GB.
[2026-01-05 11:33:43] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:33:43] The available memory for KV cache is 40.02 GB.
[2026-01-05 11:33:43] KV Cache is allocated. #tokens: 582784, K size: 20.01 GB, V size: 20.01 GB
[2026-01-05 11:33:43] Memory pool end. avail mem=11.15 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:33:44] max_total_num_tokens=582784, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=65536, available_gpu_mem=11.15 GB
[2026-01-05 11:33:45] INFO:     Started server process [173008]
[2026-01-05 11:33:45] INFO:     Waiting for application startup.
[2026-01-05 11:33:45] INFO:     Application startup complete.
[2026-01-05 11:33:45] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:33:46] INFO:     127.0.0.1:54850 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:33:46] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:33:46.872020090 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 11:33:48] INFO:     127.0.0.1:54870 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:33:58] INFO:     127.0.0.1:46848 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:34:01] INFO:     127.0.0.1:54866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:01] The server is fired up and ready to roll!
[2026-01-05 11:34:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:34:09] INFO:     127.0.0.1:57346 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:34:09] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:34:09] INFO:     127.0.0.1:57360 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:34:09] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:34:09] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/arcee-ai/AFM-4.5B-Base --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:34:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:34:09] Prefill batch, #new-seq: 13, #new-token: 1920, #cached-token: 8320, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 11:34:09] Prefill batch, #new-seq: 21, #new-token: 2688, #cached-token: 13440, token usage: 0.00, #running-req: 14, #queue-req: 0,
[2026-01-05 11:34:09] Prefill batch, #new-seq: 25, #new-token: 3328, #cached-token: 16000, token usage: 0.01, #running-req: 35, #queue-req: 0,
[2026-01-05 11:34:09] Prefill batch, #new-seq: 8, #new-token: 1152, #cached-token: 5120, token usage: 0.01, #running-req: 60, #queue-req: 0,
[2026-01-05 11:34:10] Prefill batch, #new-seq: 49, #new-token: 6528, #cached-token: 31360, token usage: 0.02, #running-req: 68, #queue-req: 0,
[2026-01-05 11:34:10] Prefill batch, #new-seq: 11, #new-token: 1408, #cached-token: 7040, token usage: 0.03, #running-req: 117, #queue-req: 0,
[2026-01-05 11:34:10] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:00<03:16,  1.01it/s][2026-01-05 11:34:10] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.03, #running-req: 127, #queue-req: 0,
[2026-01-05 11:34:11] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:01<02:06,  1.57it/s][2026-01-05 11:34:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:34:11] Decode batch, #running-req: 127, #token: 21376, token usage: 0.04, cpu graph: False, gen throughput (token/s): 91.74, #queue-req: 0,
[2026-01-05 11:34:11] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:34:11] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:01<00:53,  3.68it/s][2026-01-05 11:34:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:34:11] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:34:11] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:01<00:35,  5.50it/s][2026-01-05 11:34:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:34:11] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58322 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:01<00:24,  7.70it/s]
  4%|▍         | 9/200 [00:01<00:16, 11.61it/s][2026-01-05 11:34:11] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:57924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58068 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-05 11:34:11] Prefill batch, #new-seq: 8, #new-token: 1152, #cached-token: 5120, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:34:11] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:01<00:07, 24.39it/s][2026-01-05 11:34:11] INFO:     127.0.0.1:58136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:34:11] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:34:11] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.04, #running-req: 130, #queue-req: 0,
[2026-01-05 11:34:11] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58354 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:02<00:06, 27.75it/s]
 12%|█▎        | 25/200 [00:02<00:05, 31.87it/s][2026-01-05 11:34:11] INFO:     127.0.0.1:57568 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 126, #queue-req: 0,
[2026-01-05 11:34:11] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:34:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-05 11:34:11] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:02<00:05, 29.39it/s]
 15%|█▌        | 30/200 [00:02<00:05, 29.43it/s][2026-01-05 11:34:11] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:11] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.04, #running-req: 125, #queue-req: 0,
[2026-01-05 11:34:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:34:12] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.05, #running-req: 126, #queue-req: 0,

 17%|█▋        | 34/200 [00:02<00:05, 27.95it/s]
 18%|█▊        | 35/200 [00:02<00:05, 28.70it/s][2026-01-05 11:34:12] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58284 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:34:12] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 2560, token usage: 0.05, #running-req: 130, #queue-req: 0,
[2026-01-05 11:34:12] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:02<00:05, 29.02it/s][2026-01-05 11:34:12] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:34:12] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:57992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:34:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.05, #running-req: 131, #queue-req: 0,
[2026-01-05 11:34:12] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:02<00:04, 31.12it/s][2026-01-05 11:34:12] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:34:12] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:34:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-05 11:34:12] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:02<00:05, 29.37it/s][2026-01-05 11:34:12] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-05 11:34:12] INFO:     127.0.0.1:58300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:34:12] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.05, #running-req: 129, #queue-req: 0,
[2026-01-05 11:34:12] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:03<00:04, 29.84it/s]
 29%|██▉       | 58/200 [00:03<00:03, 39.60it/s]
 29%|██▉       | 58/200 [00:03<00:03, 39.60it/s]
 29%|██▉       | 58/200 [00:03<00:03, 39.60it/s][2026-01-05 11:34:12] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 2560, token usage: 0.04, #running-req: 124, #queue-req: 0,
[2026-01-05 11:34:12] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 1920, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:34:12] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.05, #running-req: 131, #queue-req: 0,
[2026-01-05 11:34:12] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:03<00:03, 38.52it/s]
 34%|███▎      | 67/200 [00:03<00:02, 45.74it/s]
 34%|███▎      | 67/200 [00:03<00:02, 45.74it/s]
 34%|███▎      | 67/200 [00:03<00:02, 45.74it/s][2026-01-05 11:34:12] INFO:     127.0.0.1:57506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:12] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 2560, token usage: 0.04, #running-req: 124, #queue-req: 0,
[2026-01-05 11:34:13] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:34:13] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.04, #running-req: 129, #queue-req: 0,
[2026-01-05 11:34:13] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 127, #queue-req: 0,
[2026-01-05 11:34:13] INFO:     127.0.0.1:58000 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:03<00:03, 39.43it/s]
 37%|███▋      | 74/200 [00:03<00:03, 37.03it/s]
 37%|███▋      | 74/200 [00:03<00:03, 37.03it/s][2026-01-05 11:34:13] Decode batch, #running-req: 127, #token: 24960, token usage: 0.04, cpu graph: False, gen throughput (token/s): 2509.17, #queue-req: 0,
[2026-01-05 11:34:13] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.04, #running-req: 128, #queue-req: 0,
[2026-01-05 11:34:13] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:03<00:03, 39.01it/s][2026-01-05 11:34:13] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:03<00:03, 37.90it/s][2026-01-05 11:34:13] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:03<00:02, 45.73it/s][2026-01-05 11:34:13] INFO:     127.0.0.1:58586 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58722 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:03<00:02, 43.14it/s][2026-01-05 11:34:13] INFO:     127.0.0.1:58402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:04<00:02, 43.92it/s]
 52%|█████▏    | 104/200 [00:04<00:01, 51.79it/s]
 52%|█████▏    | 104/200 [00:04<00:01, 51.79it/s][2026-01-05 11:34:13] INFO:     127.0.0.1:57504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 110/200 [00:04<00:01, 48.95it/s][2026-01-05 11:34:13] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:13] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:04<00:01, 50.03it/s]
 58%|█████▊    | 117/200 [00:04<00:01, 53.32it/s][2026-01-05 11:34:14] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 126/200 [00:04<00:01, 62.14it/s][2026-01-05 11:34:14] INFO:     127.0.0.1:58506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:04<00:01, 62.85it/s][2026-01-05 11:34:14] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] Decode batch, #running-req: 64, #token: 14848, token usage: 0.03, cpu graph: False, gen throughput (token/s): 3330.42, #queue-req: 0,
[2026-01-05 11:34:14] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:04<00:01, 58.89it/s]
 72%|███████▏  | 143/200 [00:04<00:00, 64.84it/s]
 72%|███████▏  | 143/200 [00:04<00:00, 64.84it/s]
 72%|███████▏  | 143/200 [00:04<00:00, 64.84it/s][2026-01-05 11:34:14] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:57710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:04<00:00, 64.86it/s]
 76%|███████▌  | 151/200 [00:04<00:00, 67.38it/s][2026-01-05 11:34:14] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58306 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:04<00:00, 50.81it/s][2026-01-05 11:34:14] INFO:     127.0.0.1:57582 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:57764 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:05<00:00, 44.82it/s][2026-01-05 11:34:14] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58812 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:14] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:05<00:00, 45.56it/s][2026-01-05 11:34:15] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:15] INFO:     127.0.0.1:58938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:15] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:15] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:15] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:05<00:00, 36.31it/s][2026-01-05 11:34:15] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:15] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:15] Decode batch, #running-req: 25, #token: 7168, token usage: 0.01, cpu graph: False, gen throughput (token/s): 1520.70, #queue-req: 0,
[2026-01-05 11:34:15] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:15] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:15] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:05<00:00, 23.98it/s][2026-01-05 11:34:15] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:15] INFO:     127.0.0.1:59098 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:15] INFO:     127.0.0.1:58910 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:15] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:06<00:00, 18.94it/s][2026-01-05 11:34:16] INFO:     127.0.0.1:58666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:16] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:16] Decode batch, #running-req: 15, #token: 5120, token usage: 0.01, cpu graph: False, gen throughput (token/s): 824.19, #queue-req: 0,
[2026-01-05 11:34:16] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:06<00:00, 15.56it/s][2026-01-05 11:34:16] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:16] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:16] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:06<00:00, 15.13it/s][2026-01-05 11:34:16] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:16] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:07<00:00, 12.34it/s][2026-01-05 11:34:16] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:17] Decode batch, #running-req: 8, #token: 3584, token usage: 0.01, cpu graph: False, gen throughput (token/s): 440.25, #queue-req: 0,
[2026-01-05 11:34:17] INFO:     127.0.0.1:58996 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:08<00:01,  6.48it/s][2026-01-05 11:34:17] INFO:     127.0.0.1:58858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:17] INFO:     127.0.0.1:58930 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:08<00:00,  7.50it/s][2026-01-05 11:34:18] Decode batch, #running-req: 5, #token: 2688, token usage: 0.00, cpu graph: False, gen throughput (token/s): 294.61, #queue-req: 0,
[2026-01-05 11:34:19] Decode batch, #running-req: 5, #token: 3072, token usage: 0.01, cpu graph: False, gen throughput (token/s): 219.78, #queue-req: 0,
[2026-01-05 11:34:19] Decode batch, #running-req: 5, #token: 3200, token usage: 0.01, cpu graph: False, gen throughput (token/s): 219.86, #queue-req: 0,
[2026-01-05 11:34:20] Decode batch, #running-req: 5, #token: 3200, token usage: 0.01, cpu graph: False, gen throughput (token/s): 234.45, #queue-req: 0,
[2026-01-05 11:34:21] Decode batch, #running-req: 5, #token: 3584, token usage: 0.01, cpu graph: False, gen throughput (token/s): 236.80, #queue-req: 0,
[2026-01-05 11:34:22] Decode batch, #running-req: 5, #token: 3840, token usage: 0.01, cpu graph: False, gen throughput (token/s): 237.36, #queue-req: 0,
[2026-01-05 11:34:23] Decode batch, #running-req: 5, #token: 1920, token usage: 0.00, cpu graph: False, gen throughput (token/s): 236.62, #queue-req: 0,
[2026-01-05 11:34:23] INFO:     127.0.0.1:57544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:23] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:34:23] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:13<00:02,  1.31it/s]
 99%|█████████▉| 198/200 [00:13<00:02,  1.13s/it][2026-01-05 11:34:24] Decode batch, #running-req: 2, #token: 1280, token usage: 0.00, cpu graph: False, gen throughput (token/s): 98.02, #queue-req: 0,
[2026-01-05 11:34:24] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:14<00:01,  1.09s/it][2026-01-05 11:34:24] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:14<00:00,  1.07it/s]
100%|██████████| 200/200 [00:14<00:00, 13.66it/s]
.
----------------------------------------------------------------------
Ran 1 test in 75.961s

OK
Accuracy: 0.560
Invalid: 0.005
Latency: 14.688 s
Output throughput: 1173.470 token/s
.
.
End (19/43):
filename='ascend/llm_models/test_ascend_afm_4_5b.py', elapsed=89, estimated_time=400
.
.

.
.
Begin (20/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_granite_3_0_3b_a800m.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:34:47] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.0-3b-a800m-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.0-3b-a800m-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=797494277, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/ibm-granite/granite-3.0-3b-a800m-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:34:47] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:34:57] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:34:57] Init torch distributed ends. mem usage=-0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:34:58] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:34:58] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.19it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.67it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.33it/s]

[2026-01-05 11:35:06] Parameter lm_head.weight not found in params_dict
[2026-01-05 11:35:09] Load weight end. type=GraniteMoeForCausalLM, dtype=torch.bfloat16, avail mem=54.66 GB, mem usage=6.16 GB.
[2026-01-05 11:35:09] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:35:09] The available memory for KV cache is 42.50 GB.
[2026-01-05 11:35:09] KV Cache is allocated. #tokens: 696192, K size: 21.25 GB, V size: 21.25 GB
[2026-01-05 11:35:09] Memory pool end. avail mem=12.10 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:35:09] max_total_num_tokens=696192, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4096, available_gpu_mem=12.08 GB
[2026-01-05 11:35:09] INFO:     Started server process [175780]
[2026-01-05 11:35:09] INFO:     Waiting for application startup.
[2026-01-05 11:35:09] INFO:     Application startup complete.
[2026-01-05 11:35:09] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:35:10] INFO:     127.0.0.1:58614 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:35:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:35:11.679244420 compiler_depend.ts:198] Warning: Driver Version: "," is invalid or not supported yet. (function operator())
[2026-01-05 11:35:17] INFO:     127.0.0.1:33470 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:35:28] INFO:     127.0.0.1:58472 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[rank0]:[W105 11:35:35.199604850 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[2026-01-05 11:35:37] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:37] The server is fired up and ready to roll!
[2026-01-05 11:35:38] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:35:39] INFO:     127.0.0.1:38412 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:35:39] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:35:39] INFO:     127.0.0.1:38424 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:35:39] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:35:39] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/ibm-granite/granite-3.0-3b-a800m-instruct --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:35:39] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:35:39] Prefill batch, #new-seq: 23, #new-token: 5888, #cached-token: 17664, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 11:35:39] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.01, #running-req: 24, #queue-req: 4,
[2026-01-05 11:35:39] Prefill batch, #new-seq: 17, #new-token: 4352, #cached-token: 13056, token usage: 0.02, #running-req: 56, #queue-req: 0,
[2026-01-05 11:35:39] Prefill batch, #new-seq: 32, #new-token: 8192, #cached-token: 24576, token usage: 0.03, #running-req: 73, #queue-req: 15,
[2026-01-05 11:35:39] Prefill batch, #new-seq: 23, #new-token: 5888, #cached-token: 17664, token usage: 0.04, #running-req: 105, #queue-req: 0,
[2026-01-05 11:35:41] INFO:     127.0.0.1:38940 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<08:17,  2.50s/it][2026-01-05 11:35:41] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:42] Decode batch, #running-req: 128, #token: 34304, token usage: 0.05, cpu graph: False, gen throughput (token/s): 87.59, #queue-req: 0,
[2026-01-05 11:35:42] INFO:     127.0.0.1:38654 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:03<04:49,  1.46s/it][2026-01-05 11:35:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:42] INFO:     127.0.0.1:38714 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:03<03:03,  1.07it/s][2026-01-05 11:35:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:42] INFO:     127.0.0.1:39564 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:03<02:07,  1.54it/s][2026-01-05 11:35:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:42] INFO:     127.0.0.1:38732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:42] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:43] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:04<01:13,  2.64it/s][2026-01-05 11:35:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:43] INFO:     127.0.0.1:38446 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:43] INFO:     127.0.0.1:39004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:43] INFO:     127.0.0.1:39488 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:04<00:53,  3.57it/s]
  4%|▍         | 9/200 [00:04<00:36,  5.18it/s][2026-01-05 11:35:43] INFO:     127.0.0.1:38676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:43] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-05 11:35:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:43] INFO:     127.0.0.1:39328 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:04<00:31,  5.94it/s][2026-01-05 11:35:43] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:43] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:43] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:04<00:34,  5.39it/s]
  6%|▋         | 13/200 [00:04<00:31,  6.00it/s][2026-01-05 11:35:43] INFO:     127.0.0.1:39472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:43] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-05 11:35:44] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:44] INFO:     127.0.0.1:38920 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 15/200 [00:05<00:27,  6.75it/s][2026-01-05 11:35:44] INFO:     127.0.0.1:39020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:44] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:44] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:44] INFO:     127.0.0.1:38436 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:44] INFO:     127.0.0.1:38976 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:05<00:26,  6.98it/s]
  9%|▉         | 18/200 [00:05<00:22,  8.27it/s][2026-01-05 11:35:44] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-05 11:35:44] Decode batch, #running-req: 126, #token: 37632, token usage: 0.05, cpu graph: False, gen throughput (token/s): 2188.11, #queue-req: 0,
[2026-01-05 11:35:44] INFO:     127.0.0.1:38670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:44] INFO:     127.0.0.1:38742 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:05<00:23,  7.70it/s]
 10%|█         | 20/200 [00:05<00:21,  8.56it/s][2026-01-05 11:35:44] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.05, #running-req: 126, #queue-req: 0,
[2026-01-05 11:35:44] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:05<00:22,  7.80it/s][2026-01-05 11:35:44] INFO:     127.0.0.1:38730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:44] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.05, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:44] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:44] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.05, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:44] INFO:     127.0.0.1:38860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:44] INFO:     127.0.0.1:38996 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:44] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 130, #queue-req: 0,
[2026-01-05 11:35:45] INFO:     127.0.0.1:38782 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:45] INFO:     127.0.0.1:39076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:45] INFO:     127.0.0.1:39172 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:05<00:14, 11.61it/s]
 14%|█▍        | 28/200 [00:05<00:09, 18.65it/s]
 14%|█▍        | 28/200 [00:05<00:09, 18.65it/s][2026-01-05 11:35:45] INFO:     127.0.0.1:38640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:45] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.06, #running-req: 125, #queue-req: 0,
[2026-01-05 11:35:45] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:45] INFO:     127.0.0.1:39580 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:06<00:11, 14.98it/s][2026-01-05 11:35:45] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:45] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:45] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:45] INFO:     127.0.0.1:39356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:45] INFO:     127.0.0.1:39524 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:06<00:12, 13.37it/s]
 16%|█▋        | 33/200 [00:06<00:12, 13.40it/s][2026-01-05 11:35:45] INFO:     127.0.0.1:39372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:45] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-05 11:35:45] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:45] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:45] INFO:     127.0.0.1:39192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:45] INFO:     127.0.0.1:39312 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:06<00:15, 10.94it/s]
 18%|█▊        | 37/200 [00:06<00:14, 11.32it/s]
 18%|█▊        | 37/200 [00:06<00:14, 11.32it/s][2026-01-05 11:35:45] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:45] Prefill batch, #new-seq: 3, #new-token: 768, #cached-token: 2304, token usage: 0.06, #running-req: 125, #queue-req: 0,
[2026-01-05 11:35:45] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:46] INFO:     127.0.0.1:39540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:46] INFO:     127.0.0.1:39550 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:06<00:15, 10.70it/s]
 20%|██        | 40/200 [00:06<00:14, 11.39it/s][2026-01-05 11:35:46] INFO:     127.0.0.1:38616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:46] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-05 11:35:46] INFO:     127.0.0.1:39410 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:46] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:46] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-05 11:35:46] INFO:     127.0.0.1:38886 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:46] INFO:     127.0.0.1:39516 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 43/200 [00:07<00:13, 11.35it/s]
 22%|██▏       | 44/200 [00:07<00:12, 12.47it/s][2026-01-05 11:35:46] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:46] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-05 11:35:46] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:46] INFO:     127.0.0.1:39034 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:07<00:14, 10.46it/s][2026-01-05 11:35:46] INFO:     127.0.0.1:39384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:46] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:46] INFO:     127.0.0.1:39460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:46] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:46] INFO:     127.0.0.1:38608 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 49/200 [00:07<00:14, 10.73it/s][2026-01-05 11:35:46] INFO:     127.0.0.1:39144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:46] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:46] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:47] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:08<00:14, 10.31it/s][2026-01-05 11:35:47] INFO:     127.0.0.1:39546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:47] INFO:     127.0.0.1:39392 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-05 11:35:47] Decode batch, #running-req: 128, #token: 43392, token usage: 0.06, cpu graph: False, gen throughput (token/s): 1764.29, #queue-req: 0,
[2026-01-05 11:35:47] INFO:     127.0.0.1:38716 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:08<00:13, 10.63it/s][2026-01-05 11:35:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:47] INFO:     127.0.0.1:39098 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:47] INFO:     127.0.0.1:38828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,

 28%|██▊       | 56/200 [00:08<00:14, 10.23it/s][2026-01-05 11:35:47] INFO:     127.0.0.1:39264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-05 11:35:47] INFO:     127.0.0.1:39664 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 58/200 [00:08<00:14,  9.90it/s][2026-01-05 11:35:47] INFO:     127.0.0.1:39322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:47] INFO:     127.0.0.1:39304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:47] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-05 11:35:48] INFO:     127.0.0.1:38520 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:48] INFO:     127.0.0.1:39670 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:08<00:13, 10.34it/s]
 31%|███       | 62/200 [00:08<00:11, 11.81it/s][2026-01-05 11:35:48] INFO:     127.0.0.1:38482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:48] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-05 11:35:48] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:48] INFO:     127.0.0.1:39156 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:09<00:12, 10.49it/s][2026-01-05 11:35:48] INFO:     127.0.0.1:38478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:48] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 127, #queue-req: 0,
[2026-01-05 11:35:48] INFO:     127.0.0.1:39448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:48] INFO:     127.0.0.1:38874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:48] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:48] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 130, #queue-req: 0,
[2026-01-05 11:35:48] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:48] INFO:     127.0.0.1:39682 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:09<00:11, 11.87it/s]
 34%|███▍      | 69/200 [00:09<00:09, 13.96it/s][2026-01-05 11:35:48] INFO:     127.0.0.1:39716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:48] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 126, #queue-req: 0,
[2026-01-05 11:35:48] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:48] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.06, #running-req: 128, #queue-req: 0,
[2026-01-05 11:35:48] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:48] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.06, #running-req: 129, #queue-req: 0,
[2026-01-05 11:35:48] INFO:     127.0.0.1:39388 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:09<00:08, 14.25it/s][2026-01-05 11:35:48] INFO:     127.0.0.1:39112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:48] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:48] INFO:     127.0.0.1:39510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] INFO:     127.0.0.1:38810 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:09<00:07, 16.07it/s][2026-01-05 11:35:49] INFO:     127.0.0.1:38798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] INFO:     127.0.0.1:39446 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:10<00:08, 14.86it/s][2026-01-05 11:35:49] INFO:     127.0.0.1:39190 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] INFO:     127.0.0.1:38738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] INFO:     127.0.0.1:39426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] INFO:     127.0.0.1:39266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] INFO:     127.0.0.1:39290 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 83/200 [00:10<00:06, 17.77it/s]
 42%|████▏     | 84/200 [00:10<00:05, 21.92it/s][2026-01-05 11:35:49] INFO:     127.0.0.1:38488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] INFO:     127.0.0.1:38764 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] INFO:     127.0.0.1:39350 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:10<00:05, 19.05it/s][2026-01-05 11:35:49] INFO:     127.0.0.1:38992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] Decode batch, #running-req: 113, #token: 37504, token usage: 0.05, cpu graph: False, gen throughput (token/s): 2010.05, #queue-req: 0,
[2026-01-05 11:35:49] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] INFO:     127.0.0.1:39662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] INFO:     127.0.0.1:39960 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 90/200 [00:10<00:06, 17.18it/s]
 46%|████▌     | 91/200 [00:10<00:06, 17.41it/s][2026-01-05 11:35:49] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:49] INFO:     127.0.0.1:39352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:38898 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:11<00:06, 15.23it/s][2026-01-05 11:35:50] INFO:     127.0.0.1:38550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39248 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39702 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39732 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:11<00:05, 18.15it/s]
 50%|████▉     | 99/200 [00:11<00:04, 22.38it/s][2026-01-05 11:35:50] INFO:     127.0.0.1:38890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39040 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39820 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39886 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 103/200 [00:11<00:04, 24.10it/s]
 52%|█████▎    | 105/200 [00:11<00:02, 31.87it/s]
 52%|█████▎    | 105/200 [00:11<00:02, 31.87it/s][2026-01-05 11:35:50] INFO:     127.0.0.1:38578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:38844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39918 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:11<00:03, 23.21it/s]
 55%|█████▌    | 110/200 [00:11<00:04, 20.51it/s][2026-01-05 11:35:50] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:38626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:38956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39850 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:11<00:04, 20.87it/s]
 57%|█████▋    | 114/200 [00:11<00:03, 22.85it/s][2026-01-05 11:35:50] INFO:     127.0.0.1:39804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:50] INFO:     127.0.0.1:39758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:36166 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:11<00:04, 18.49it/s][2026-01-05 11:35:51] INFO:     127.0.0.1:38724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:38530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:38990 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:12<00:04, 18.04it/s][2026-01-05 11:35:51] INFO:     127.0.0.1:36184 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:39936 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:12<00:05, 15.22it/s][2026-01-05 11:35:51] INFO:     127.0.0.1:39690 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:36094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:39932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] Decode batch, #running-req: 75, #token: 28160, token usage: 0.04, cpu graph: False, gen throughput (token/s): 2079.30, #queue-req: 0,
[2026-01-05 11:35:51] INFO:     127.0.0.1:39412 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:12<00:04, 17.62it/s][2026-01-05 11:35:51] INFO:     127.0.0.1:38816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:39092 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:39646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:36060 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:12<00:04, 17.04it/s]
 66%|██████▌   | 131/200 [00:12<00:03, 21.23it/s]
 66%|██████▌   | 131/200 [00:12<00:03, 21.23it/s][2026-01-05 11:35:51] INFO:     127.0.0.1:38804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:36106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:51] INFO:     127.0.0.1:38504 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 135/200 [00:12<00:02, 23.55it/s][2026-01-05 11:35:51] INFO:     127.0.0.1:38914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:52] INFO:     127.0.0.1:36014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:52] INFO:     127.0.0.1:39598 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:13<00:03, 17.30it/s][2026-01-05 11:35:52] INFO:     127.0.0.1:38682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:52] INFO:     127.0.0.1:39402 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:52] INFO:     127.0.0.1:38462 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:13<00:04, 14.39it/s][2026-01-05 11:35:52] INFO:     127.0.0.1:38536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:52] INFO:     127.0.0.1:39912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:52] INFO:     127.0.0.1:38968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:52] INFO:     127.0.0.1:39060 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:13<00:03, 16.03it/s][2026-01-05 11:35:52] INFO:     127.0.0.1:39864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:53] INFO:     127.0.0.1:36134 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:13<00:04, 13.04it/s][2026-01-05 11:35:53] INFO:     127.0.0.1:39274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:53] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:14<00:03, 13.46it/s][2026-01-05 11:35:53] INFO:     127.0.0.1:39908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:53] INFO:     127.0.0.1:38926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:53] INFO:     127.0.0.1:36286 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:14<00:03, 13.82it/s]
 76%|███████▌  | 152/200 [00:14<00:03, 15.91it/s][2026-01-05 11:35:53] Decode batch, #running-req: 50, #token: 20736, token usage: 0.03, cpu graph: False, gen throughput (token/s): 1408.77, #queue-req: 0,
[2026-01-05 11:35:53] INFO:     127.0.0.1:39776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:53] INFO:     127.0.0.1:39808 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:14<00:03, 12.79it/s][2026-01-05 11:35:53] INFO:     127.0.0.1:36216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:53] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:14<00:03, 13.36it/s][2026-01-05 11:35:53] INFO:     127.0.0.1:36270 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:53] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:53] INFO:     127.0.0.1:36078 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:14<00:03, 13.82it/s]
 80%|███████▉  | 159/200 [00:14<00:02, 16.15it/s][2026-01-05 11:35:53] INFO:     127.0.0.1:38754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:53] INFO:     127.0.0.1:39334 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 161/200 [00:14<00:02, 16.05it/s][2026-01-05 11:35:54] INFO:     127.0.0.1:38566 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:54] INFO:     127.0.0.1:39950 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:15<00:03, 11.04it/s][2026-01-05 11:35:54] INFO:     127.0.0.1:39790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:54] INFO:     127.0.0.1:36032 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:15<00:03,  9.41it/s][2026-01-05 11:35:54] INFO:     127.0.0.1:39606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:54] INFO:     127.0.0.1:36188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:54] INFO:     127.0.0.1:38576 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:15<00:02, 10.71it/s][2026-01-05 11:35:55] Decode batch, #running-req: 32, #token: 14848, token usage: 0.02, cpu graph: False, gen throughput (token/s): 925.28, #queue-req: 0,
[2026-01-05 11:35:55] INFO:     127.0.0.1:39258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:55] INFO:     127.0.0.1:36038 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:15<00:03,  9.32it/s][2026-01-05 11:35:55] INFO:     127.0.0.1:39836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:55] INFO:     127.0.0.1:36142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:55] INFO:     127.0.0.1:38554 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:55] INFO:     127.0.0.1:39840 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:16<00:02, 11.84it/s][2026-01-05 11:35:55] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:55] INFO:     127.0.0.1:39858 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:16<00:02, 10.11it/s][2026-01-05 11:35:55] INFO:     127.0.0.1:36268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:55] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:16<00:02,  8.65it/s][2026-01-05 11:35:56] INFO:     127.0.0.1:36024 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:16<00:02,  8.54it/s][2026-01-05 11:35:56] INFO:     127.0.0.1:38930 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:17<00:03,  6.17it/s][2026-01-05 11:35:56] INFO:     127.0.0.1:36122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:56] INFO:     127.0.0.1:38778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:56] INFO:     127.0.0.1:39762 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:17<00:02,  7.73it/s]
 92%|█████████▏| 183/200 [00:17<00:01, 10.72it/s][2026-01-05 11:35:56] Decode batch, #running-req: 17, #token: 8448, token usage: 0.01, cpu graph: False, gen throughput (token/s): 556.46, #queue-req: 0,
[2026-01-05 11:35:56] INFO:     127.0.0.1:39684 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:56] INFO:     127.0.0.1:39902 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:17<00:01, 10.38it/s][2026-01-05 11:35:56] INFO:     127.0.0.1:39848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:56] INFO:     127.0.0.1:38594 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:17<00:01, 11.49it/s][2026-01-05 11:35:57] INFO:     127.0.0.1:36046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:57] INFO:     127.0.0.1:39788 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:18<00:01,  7.17it/s][2026-01-05 11:35:57] INFO:     127.0.0.1:36300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:57] INFO:     127.0.0.1:36196 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:18<00:01,  7.75it/s][2026-01-05 11:35:57] INFO:     127.0.0.1:39802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:35:58] INFO:     127.0.0.1:36240 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:19<00:01,  6.12it/s][2026-01-05 11:35:58] Decode batch, #running-req: 7, #token: 4480, token usage: 0.01, cpu graph: False, gen throughput (token/s): 260.44, #queue-req: 0,
[2026-01-05 11:35:58] INFO:     127.0.0.1:36068 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:19<00:01,  4.41it/s][2026-01-05 11:35:58] INFO:     127.0.0.1:36170 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:19<00:01,  4.86it/s][2026-01-05 11:35:59] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:20<00:01,  3.62it/s][2026-01-05 11:35:59] INFO:     127.0.0.1:36310 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:20<00:00,  3.37it/s][2026-01-05 11:35:59] Decode batch, #running-req: 3, #token: 2560, token usage: 0.00, cpu graph: False, gen throughput (token/s): 121.65, #queue-req: 0,
[2026-01-05 11:36:00] INFO:     127.0.0.1:36152 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:21<00:00,  2.01it/s][2026-01-05 11:36:01] Decode batch, #running-req: 2, #token: 2048, token usage: 0.00, cpu graph: False, gen throughput (token/s): 62.60, #queue-req: 0,
[2026-01-05 11:36:03] Decode batch, #running-req: 2, #token: 2176, token usage: 0.00, cpu graph: False, gen throughput (token/s): 50.22, #queue-req: 0,
[2026-01-05 11:36:04] Decode batch, #running-req: 2, #token: 0, token usage: 0.00, cpu graph: False, gen throughput (token/s): 50.12, #queue-req: 0,
[2026-01-05 11:36:04] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:36:04] INFO:     127.0.0.1:39530 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [00:25<00:01,  1.44s/it]
100%|██████████| 200/200 [00:25<00:00,  1.69s/it]
100%|██████████| 200/200 [00:25<00:00,  7.79it/s]
.
----------------------------------------------------------------------
Ran 1 test in 87.004s

OK
Accuracy: 0.465
Invalid: 0.000
Latency: 25.723 s
Output throughput: 1112.250 token/s
.
.
End (20/43):
filename='ascend/llm_models/test_ascend_granite_3_0_3b_a800m.py', elapsed=106, estimated_time=400
.
.

.
.
Begin (21/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_tp4_bf16.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:36:34] server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.7, max_running_requests=32, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=652646751, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=4, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:36:35] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:36:44 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:36:44 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:36:44 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:36:44 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 11:36:45 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:36:45 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:36:45 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:36:45 TP1] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:36:46 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:36:46 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:36:46 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:36:46 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:36:47 TP3] Load weight begin. avail mem=61.10 GB
[2026-01-05 11:36:47 TP0] Load weight begin. avail mem=60.87 GB
[2026-01-05 11:36:47 TP1] Load weight begin. avail mem=61.10 GB
[2026-01-05 11:36:47 TP2] Load weight begin. avail mem=60.88 GB

Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   6% Completed | 1/16 [00:04<01:13,  4.93s/it]

Loading safetensors checkpoint shards:  12% Completed | 2/16 [00:09<01:08,  4.89s/it]

Loading safetensors checkpoint shards:  19% Completed | 3/16 [00:14<01:04,  4.95s/it]

Loading safetensors checkpoint shards:  25% Completed | 4/16 [00:19<00:59,  4.98s/it]

Loading safetensors checkpoint shards:  31% Completed | 5/16 [00:24<00:53,  4.87s/it]

Loading safetensors checkpoint shards:  38% Completed | 6/16 [00:25<00:35,  3.57s/it]

Loading safetensors checkpoint shards:  44% Completed | 7/16 [00:30<00:36,  4.07s/it]

Loading safetensors checkpoint shards:  50% Completed | 8/16 [00:35<00:34,  4.30s/it]

Loading safetensors checkpoint shards:  56% Completed | 9/16 [00:40<00:31,  4.51s/it]

Loading safetensors checkpoint shards:  62% Completed | 10/16 [00:45<00:28,  4.69s/it]

Loading safetensors checkpoint shards:  69% Completed | 11/16 [00:50<00:23,  4.76s/it]

Loading safetensors checkpoint shards:  75% Completed | 12/16 [00:55<00:19,  4.76s/it]

Loading safetensors checkpoint shards:  81% Completed | 13/16 [01:00<00:14,  4.78s/it]

Loading safetensors checkpoint shards:  88% Completed | 14/16 [01:04<00:09,  4.78s/it]

Loading safetensors checkpoint shards:  94% Completed | 15/16 [01:09<00:04,  4.83s/it]

Loading safetensors checkpoint shards: 100% Completed | 16/16 [01:14<00:00,  4.86s/it]

Loading safetensors checkpoint shards: 100% Completed | 16/16 [01:14<00:00,  4.67s/it]

[2026-01-05 11:38:02 TP0] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=46.34 GB, mem usage=14.53 GB.
[2026-01-05 11:38:02 TP1] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=46.57 GB, mem usage=14.53 GB.
[2026-01-05 11:38:02 TP2] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=46.35 GB, mem usage=14.53 GB.
[2026-01-05 11:38:02 TP3] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=46.57 GB, mem usage=14.53 GB.
[2026-01-05 11:38:02 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:38:02 TP0] The available memory for KV cache is 28.07 GB.
[2026-01-05 11:38:02 TP3] The available memory for KV cache is 28.07 GB.
[2026-01-05 11:38:02 TP2] The available memory for KV cache is 28.07 GB.
[2026-01-05 11:38:02 TP1] The available memory for KV cache is 28.07 GB.
[2026-01-05 11:38:02 TP2] KV Cache is allocated. #tokens: 1226496, K size: 14.04 GB, V size: 14.04 GB
[2026-01-05 11:38:02 TP0] KV Cache is allocated. #tokens: 1226496, K size: 14.04 GB, V size: 14.04 GB
[2026-01-05 11:38:02 TP3] KV Cache is allocated. #tokens: 1226496, K size: 14.04 GB, V size: 14.04 GB
[2026-01-05 11:38:02 TP1] KV Cache is allocated. #tokens: 1226496, K size: 14.04 GB, V size: 14.04 GB
[2026-01-05 11:38:02 TP2] Memory pool end. avail mem=18.24 GB
[2026-01-05 11:38:02 TP0] Memory pool end. avail mem=18.24 GB
[2026-01-05 11:38:02 TP3] Memory pool end. avail mem=18.46 GB
[2026-01-05 11:38:02 TP1] Memory pool end. avail mem=18.46 GB
[2026-01-05 11:38:03 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=18.46 GB
[2026-01-05 11:38:03 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=18.24 GB
[2026-01-05 11:38:03 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=18.24 GB
[2026-01-05 11:38:03 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32]
[2026-01-05 11:38:03 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=18.46 GB

  0%|          | 0/8 [00:00<?, ?it/s]
Capturing batches (bs=32 avail_mem=18.20 GB):   0%|          | 0/8 [00:00<?, ?it/s][rank0]:[W105 11:38:07.189609505 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank2]:[W105 11:38:07.587671671 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank1]:[W105 11:38:08.720712446 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank3]:[W105 11:38:08.723572752 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank1]:[W105 11:38:08.930028736 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank3]:[W105 11:38:08.930028546 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank2]:[W105 11:38:08.930043187 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank0]:[W105 11:38:08.930044527 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())

Capturing batches (bs=32 avail_mem=18.20 GB):  12%|█▎        | 1/8 [00:05<00:41,  5.92s/it]
Capturing batches (bs=24 avail_mem=17.55 GB):  12%|█▎        | 1/8 [00:05<00:41,  5.92s/it]
Capturing batches (bs=24 avail_mem=17.55 GB):  25%|██▌       | 2/8 [00:10<00:29,  4.91s/it]
Capturing batches (bs=16 avail_mem=17.53 GB):  25%|██▌       | 2/8 [00:10<00:29,  4.91s/it]
Capturing batches (bs=16 avail_mem=17.53 GB):  38%|███▊      | 3/8 [00:12<00:18,  3.66s/it]
Capturing batches (bs=12 avail_mem=17.52 GB):  38%|███▊      | 3/8 [00:12<00:18,  3.66s/it]
Capturing batches (bs=12 avail_mem=17.52 GB):  50%|█████     | 4/8 [00:14<00:12,  3.08s/it]
Capturing batches (bs=8 avail_mem=17.51 GB):  50%|█████     | 4/8 [00:14<00:12,  3.08s/it]
Capturing batches (bs=8 avail_mem=17.51 GB):  62%|██████▎   | 5/8 [00:16<00:08,  2.76s/it]
Capturing batches (bs=4 avail_mem=17.51 GB):  62%|██████▎   | 5/8 [00:16<00:08,  2.76s/it]
Capturing batches (bs=4 avail_mem=17.51 GB):  75%|███████▌  | 6/8 [00:18<00:05,  2.56s/it]
Capturing batches (bs=2 avail_mem=17.50 GB):  75%|███████▌  | 6/8 [00:18<00:05,  2.56s/it]
Capturing batches (bs=2 avail_mem=17.50 GB):  88%|████████▊ | 7/8 [00:21<00:02,  2.45s/it]
Capturing batches (bs=1 avail_mem=17.49 GB):  88%|████████▊ | 7/8 [00:21<00:02,  2.45s/it]
Capturing batches (bs=1 avail_mem=17.49 GB): 100%|██████████| 8/8 [00:26<00:00,  3.32s/it]
Capturing batches (bs=1 avail_mem=17.49 GB): 100%|██████████| 8/8 [00:26<00:00,  3.28s/it]
[2026-01-05 11:38:30 TP3] Capture cuda graph end. Time elapsed: 27.42 s. mem usage=0.75 GB. avail mem=17.71 GB.
[2026-01-05 11:38:30 TP0] Capture cuda graph end. Time elapsed: 27.52 s. mem usage=0.75 GB. avail mem=17.49 GB.
[2026-01-05 11:38:30 TP1] Capture cuda graph end. Time elapsed: 27.57 s. mem usage=0.75 GB. avail mem=17.71 GB.
[2026-01-05 11:38:30 TP2] Capture cuda graph end. Time elapsed: 27.58 s. mem usage=0.75 GB. avail mem=17.49 GB.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:38:30 TP0] max_total_num_tokens=1226496, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=32, context_len=262144, available_gpu_mem=17.49 GB
[2026-01-05 11:38:31] INFO:     Started server process [178612]
[2026-01-05 11:38:31] INFO:     Waiting for application startup.
[2026-01-05 11:38:31] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-05 11:38:31] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-05 11:38:31] INFO:     Application startup complete.
[2026-01-05 11:38:31] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:38:32] INFO:     127.0.0.1:51542 - "GET /model_info HTTP/1.1" 200 OK
[rank3]:[W105 11:38:32.424106176 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank1]:[W105 11:38:32.424257502 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 11:38:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:38:32.424900776 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank2]:[W105 11:38:32.424996559 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 11:38:33] INFO:     127.0.0.1:51568 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:38:43] INFO:     127.0.0.1:41192 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[rank2]:[W105 11:38:45.973859875 compiler_depend.ts:3136] Warning: The indexFromRank 2is not equal indexFromCurDevice 6 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[rank1]:[W105 11:38:45.063579567 compiler_depend.ts:3136] Warning: The indexFromRank 1is not equal indexFromCurDevice 5 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[rank0]:[W105 11:38:45.227241475 compiler_depend.ts:3136] Warning: The indexFromRank 0is not equal indexFromCurDevice 4 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[rank3]:[W105 11:38:45.303767746 compiler_depend.ts:3136] Warning: The indexFromRank 3is not equal indexFromCurDevice 7 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[2026-01-05 11:38:47] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:38:47] The server is fired up and ready to roll!
[2026-01-05 11:38:53 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:38:54] INFO:     127.0.0.1:39980 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:38:54] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:38:54] INFO:     127.0.0.1:39984 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:38:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:38:54] INFO:     127.0.0.1:39988 - "POST /generate HTTP/1.1" 200 OK
[CI Test Method] TestAscendTp4Bf16.test_a_gsm8k
##=== Testing accuracy: /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507 ===##
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507 --trust-remote-code --mem-fraction-static 0.7 --max-running-requests 32 --attention-backend ascend --disable-radix-cache --cuda-graph-max-bs 32 --tp-size 4 --base-gpu-id 4 --device npu --host 127.0.0.1 --port 21000

  0%|          | 0/1319 [00:00<?, ?it/s][2026-01-05 11:38:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:38:54 TP0] Prefill batch, #new-seq: 9, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 22,
[2026-01-05 11:38:55 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 9, #queue-req: 109,
[2026-01-05 11:38:55 TP0] Prefill batch, #new-seq: 10, #new-token: 8192, #cached-token: 0, token usage: 0.01, #running-req: 19, #queue-req: 99,
[2026-01-05 11:38:56 TP0] Prefill batch, #new-seq: 4, #new-token: 3456, #cached-token: 0, token usage: 0.02, #running-req: 28, #queue-req: 96,
[2026-01-05 11:38:58 TP0] Decode batch, #running-req: 32, #token: 29056, token usage: 0.02, cpu graph: True, gen throughput (token/s): 7.44, #queue-req: 96,
[2026-01-05 11:38:58] INFO:     127.0.0.1:40020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:38:58] INFO:     127.0.0.1:40046 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/1319 [00:04<1:38:10,  4.47s/it]
  0%|          | 2/1319 [00:04<1:01:46,  2.81s/it][2026-01-05 11:38:58 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:00] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 3/1319 [00:06<52:43,  2.40s/it]  [2026-01-05 11:39:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:01] INFO:     127.0.0.1:40188 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 4/1319 [00:07<45:15,  2.06s/it][2026-01-05 11:39:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:01 TP0] Decode batch, #running-req: 32, #token: 32128, token usage: 0.03, cpu graph: True, gen throughput (token/s): 353.17, #queue-req: 96,
[2026-01-05 11:39:02] INFO:     127.0.0.1:40242 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 5/1319 [00:07<33:07,  1.51s/it][2026-01-05 11:39:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:02] INFO:     127.0.0.1:40262 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 6/1319 [00:07<23:31,  1.08s/it][2026-01-05 11:39:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:02] INFO:     127.0.0.1:40332 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 7/1319 [00:08<17:26,  1.25it/s][2026-01-05 11:39:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:02] INFO:     127.0.0.1:40302 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 8/1319 [00:08<15:56,  1.37it/s][2026-01-05 11:39:02] INFO:     127.0.0.1:40294 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:03] INFO:     127.0.0.1:40074 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 10/1319 [00:08<09:51,  2.21it/s][2026-01-05 11:39:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:03] INFO:     127.0.0.1:40200 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 11/1319 [00:09<08:02,  2.71it/s][2026-01-05 11:39:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:03] INFO:     127.0.0.1:40060 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 12/1319 [00:09<06:33,  3.32it/s][2026-01-05 11:39:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:03 TP0] Decode batch, #running-req: 32, #token: 31360, token usage: 0.03, cpu graph: True, gen throughput (token/s): 743.31, #queue-req: 96,
[2026-01-05 11:39:03] INFO:     127.0.0.1:40062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:03] INFO:     127.0.0.1:40250 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 13/1319 [00:09<06:36,  3.29it/s]
  1%|          | 14/1319 [00:09<05:11,  4.18it/s][2026-01-05 11:39:03 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:04] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 15/1319 [00:09<05:53,  3.68it/s][2026-01-05 11:39:04] INFO:     127.0.0.1:40126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:04] INFO:     127.0.0.1:40276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:04] INFO:     127.0.0.1:40318 - "POST /generate HTTP/1.1" 200 OK

  1%|▏         | 17/1319 [00:10<05:16,  4.11it/s]
  1%|▏         | 18/1319 [00:10<04:11,  5.18it/s][2026-01-05 11:39:04 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:04] INFO:     127.0.0.1:40148 - "POST /generate HTTP/1.1" 200 OK

  1%|▏         | 19/1319 [00:10<04:53,  4.44it/s][2026-01-05 11:39:04] INFO:     127.0.0.1:40110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:05 TP0] Decode batch, #running-req: 31, #token: 32128, token usage: 0.03, cpu graph: True, gen throughput (token/s): 824.91, #queue-req: 96,
[2026-01-05 11:39:05] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 21/1319 [00:10<04:24,  4.91it/s][2026-01-05 11:39:05] INFO:     127.0.0.1:40172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:05] INFO:     127.0.0.1:40334 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 23/1319 [00:11<04:39,  4.64it/s][2026-01-05 11:39:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:05] INFO:     127.0.0.1:40296 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 24/1319 [00:11<04:26,  4.86it/s][2026-01-05 11:39:05] INFO:     127.0.0.1:40198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:06] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 26/1319 [00:11<03:59,  5.41it/s][2026-01-05 11:39:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:06] INFO:     127.0.0.1:40430 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 27/1319 [00:12<03:58,  5.42it/s][2026-01-05 11:39:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:06] INFO:     127.0.0.1:40380 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 28/1319 [00:12<04:06,  5.24it/s][2026-01-05 11:39:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:06 TP0] Decode batch, #running-req: 32, #token: 32640, token usage: 0.03, cpu graph: True, gen throughput (token/s): 740.67, #queue-req: 96,
[2026-01-05 11:39:07] INFO:     127.0.0.1:40286 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 29/1319 [00:12<05:26,  3.95it/s][2026-01-05 11:39:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:07] INFO:     127.0.0.1:40454 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 30/1319 [00:13<05:38,  3.81it/s][2026-01-05 11:39:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:07] INFO:     127.0.0.1:40350 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:07] INFO:     127.0.0.1:40384 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 31/1319 [00:13<04:49,  4.44it/s]
  2%|▏         | 32/1319 [00:13<03:18,  6.50it/s][2026-01-05 11:39:07 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:07] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 33/1319 [00:13<03:15,  6.58it/s][2026-01-05 11:39:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:07] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 34/1319 [00:13<03:05,  6.94it/s][2026-01-05 11:39:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:07] INFO:     127.0.0.1:40502 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 35/1319 [00:13<03:22,  6.33it/s][2026-01-05 11:39:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:08] INFO:     127.0.0.1:40190 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 36/1319 [00:13<03:59,  5.36it/s][2026-01-05 11:39:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:08] INFO:     127.0.0.1:40100 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 37/1319 [00:14<04:08,  5.16it/s][2026-01-05 11:39:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:08 TP0] Decode batch, #running-req: 32, #token: 32256, token usage: 0.03, cpu graph: True, gen throughput (token/s): 751.98, #queue-req: 96,
[2026-01-05 11:39:09] INFO:     127.0.0.1:40088 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 38/1319 [00:14<06:57,  3.07it/s][2026-01-05 11:39:09] INFO:     127.0.0.1:40226 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:09] INFO:     127.0.0.1:40510 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:09] INFO:     127.0.0.1:40598 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 40/1319 [00:15<05:35,  3.81it/s]
  3%|▎         | 41/1319 [00:15<04:08,  5.15it/s][2026-01-05 11:39:09 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:09] INFO:     127.0.0.1:40480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:09] INFO:     127.0.0.1:40548 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 42/1319 [00:15<04:06,  5.19it/s]
  3%|▎         | 43/1319 [00:15<03:21,  6.34it/s][2026-01-05 11:39:09 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:10] INFO:     127.0.0.1:40588 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 44/1319 [00:16<08:29,  2.50it/s][2026-01-05 11:39:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:11] INFO:     127.0.0.1:40528 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 45/1319 [00:16<07:40,  2.77it/s][2026-01-05 11:39:11 TP0] Decode batch, #running-req: 32, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 481.15, #queue-req: 96,
[2026-01-05 11:39:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:11] INFO:     127.0.0.1:40158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:11] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 46/1319 [00:17<06:27,  3.29it/s]
  4%|▎         | 47/1319 [00:17<04:21,  4.86it/s][2026-01-05 11:39:11 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:11] INFO:     127.0.0.1:40412 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 48/1319 [00:17<05:12,  4.07it/s][2026-01-05 11:39:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:11] INFO:     127.0.0.1:40212 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 49/1319 [00:17<04:46,  4.43it/s][2026-01-05 11:39:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:12] INFO:     127.0.0.1:40552 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 50/1319 [00:17<05:26,  3.89it/s][2026-01-05 11:39:12] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:12] INFO:     127.0.0.1:40472 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 52/1319 [00:18<05:04,  4.17it/s][2026-01-05 11:39:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:12 TP0] Decode batch, #running-req: 32, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 773.72, #queue-req: 96,
[2026-01-05 11:39:12] INFO:     127.0.0.1:40486 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 53/1319 [00:18<05:05,  4.15it/s][2026-01-05 11:39:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:13] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 54/1319 [00:18<04:49,  4.37it/s][2026-01-05 11:39:13] INFO:     127.0.0.1:40420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:13] INFO:     127.0.0.1:40526 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 56/1319 [00:19<04:21,  4.83it/s][2026-01-05 11:39:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:13] INFO:     127.0.0.1:40786 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 57/1319 [00:19<04:54,  4.28it/s][2026-01-05 11:39:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:13] INFO:     127.0.0.1:40442 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 58/1319 [00:19<05:06,  4.11it/s][2026-01-05 11:39:14] INFO:     127.0.0.1:40646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:14] INFO:     127.0.0.1:40614 - "POST /generate HTTP/1.1" 200 OK

  5%|▍         | 60/1319 [00:19<03:30,  5.99it/s][2026-01-05 11:39:14 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:14] INFO:     127.0.0.1:40716 - "POST /generate HTTP/1.1" 200 OK

  5%|▍         | 61/1319 [00:19<03:14,  6.46it/s][2026-01-05 11:39:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:14 TP0] Decode batch, #running-req: 32, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 731.61, #queue-req: 96,
[2026-01-05 11:39:14] INFO:     127.0.0.1:40636 - "POST /generate HTTP/1.1" 200 OK

  5%|▍         | 62/1319 [00:20<05:15,  3.98it/s][2026-01-05 11:39:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:15] INFO:     127.0.0.1:40660 - "POST /generate HTTP/1.1" 200 OK

  5%|▍         | 63/1319 [00:20<05:22,  3.89it/s][2026-01-05 11:39:15] INFO:     127.0.0.1:40672 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:15] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK

  5%|▍         | 65/1319 [00:21<04:27,  4.70it/s][2026-01-05 11:39:15] INFO:     127.0.0.1:40692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:15] INFO:     127.0.0.1:40710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:15] INFO:     127.0.0.1:40728 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 67/1319 [00:21<03:40,  5.68it/s]
  5%|▌         | 68/1319 [00:21<02:44,  7.60it/s][2026-01-05 11:39:15 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:15] INFO:     127.0.0.1:40482 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 69/1319 [00:21<03:13,  6.45it/s][2026-01-05 11:39:15] INFO:     127.0.0.1:40680 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:16] INFO:     127.0.0.1:40544 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 71/1319 [00:21<03:23,  6.12it/s][2026-01-05 11:39:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:16 TP0] Decode batch, #running-req: 32, #token: 31104, token usage: 0.03, cpu graph: True, gen throughput (token/s): 691.62, #queue-req: 96,
[2026-01-05 11:39:16] INFO:     127.0.0.1:40800 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 72/1319 [00:22<03:30,  5.93it/s][2026-01-05 11:39:16] INFO:     127.0.0.1:40640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:17] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 74/1319 [00:22<04:49,  4.30it/s][2026-01-05 11:39:17] INFO:     127.0.0.1:40772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:17] INFO:     127.0.0.1:40756 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 76/1319 [00:23<04:10,  4.97it/s][2026-01-05 11:39:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:17] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 77/1319 [00:23<04:08,  5.01it/s][2026-01-05 11:39:17] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:17] INFO:     127.0.0.1:40758 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 79/1319 [00:23<03:06,  6.65it/s][2026-01-05 11:39:17 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:17] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 80/1319 [00:23<03:08,  6.58it/s][2026-01-05 11:39:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:18] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 81/1319 [00:23<03:10,  6.52it/s][2026-01-05 11:39:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:18 TP0] Decode batch, #running-req: 32, #token: 31872, token usage: 0.03, cpu graph: True, gen throughput (token/s): 696.11, #queue-req: 96,
[2026-01-05 11:39:18] INFO:     127.0.0.1:40706 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 82/1319 [00:24<03:36,  5.71it/s][2026-01-05 11:39:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:18] INFO:     127.0.0.1:40762 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 83/1319 [00:24<04:04,  5.05it/s][2026-01-05 11:39:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:18] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 84/1319 [00:24<03:38,  5.64it/s][2026-01-05 11:39:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:18] INFO:     127.0.0.1:40624 - "POST /generate HTTP/1.1" 200 OK

  6%|▋         | 85/1319 [00:24<03:45,  5.47it/s][2026-01-05 11:39:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:18] INFO:     127.0.0.1:40826 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 86/1319 [00:24<03:36,  5.68it/s][2026-01-05 11:39:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:19] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 87/1319 [00:25<05:29,  3.74it/s][2026-01-05 11:39:19] INFO:     127.0.0.1:40836 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:19] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 89/1319 [00:25<03:33,  5.75it/s][2026-01-05 11:39:19 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:19] INFO:     127.0.0.1:40558 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 90/1319 [00:25<03:13,  6.34it/s][2026-01-05 11:39:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:19 TP0] Decode batch, #running-req: 32, #token: 31360, token usage: 0.03, cpu graph: True, gen throughput (token/s): 754.11, #queue-req: 96,
[2026-01-05 11:39:20] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 91/1319 [00:25<03:59,  5.14it/s][2026-01-05 11:39:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:20] INFO:     127.0.0.1:40910 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 92/1319 [00:25<03:54,  5.24it/s][2026-01-05 11:39:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:20] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 93/1319 [00:26<03:50,  5.31it/s][2026-01-05 11:39:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:20] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 94/1319 [00:26<03:26,  5.93it/s][2026-01-05 11:39:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:20] INFO:     127.0.0.1:40880 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 95/1319 [00:26<03:14,  6.28it/s][2026-01-05 11:39:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:20] INFO:     127.0.0.1:41042 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 96/1319 [00:26<04:00,  5.09it/s][2026-01-05 11:39:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:21] INFO:     127.0.0.1:40940 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 97/1319 [00:26<04:03,  5.02it/s][2026-01-05 11:39:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 95,
[2026-01-05 11:39:21] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:21] INFO:     127.0.0.1:41002 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 98/1319 [00:27<04:44,  4.29it/s]
  8%|▊         | 99/1319 [00:27<04:01,  5.04it/s][2026-01-05 11:39:21] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:21] INFO:     127.0.0.1:41060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:21 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:21 TP0] Decode batch, #running-req: 30, #token: 29824, token usage: 0.02, cpu graph: True, gen throughput (token/s): 770.61, #queue-req: 96,
[2026-01-05 11:39:21 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:21] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 102/1319 [00:27<02:47,  7.27it/s][2026-01-05 11:39:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:21] INFO:     127.0.0.1:41068 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 103/1319 [00:27<03:24,  5.94it/s][2026-01-05 11:39:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:22] INFO:     127.0.0.1:41104 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 104/1319 [00:27<03:42,  5.46it/s][2026-01-05 11:39:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:22] INFO:     127.0.0.1:41092 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 105/1319 [00:28<03:35,  5.64it/s][2026-01-05 11:39:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:22] INFO:     127.0.0.1:40818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:22] INFO:     127.0.0.1:40960 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:22] INFO:     127.0.0.1:41114 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 106/1319 [00:28<03:39,  5.51it/s]
  8%|▊         | 108/1319 [00:28<02:10,  9.30it/s]
  8%|▊         | 108/1319 [00:28<02:10,  9.30it/s][2026-01-05 11:39:22 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, token usage: 0.02, #running-req: 29, #queue-req: 96,
[2026-01-05 11:39:22] INFO:     127.0.0.1:41116 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 109/1319 [00:28<02:12,  9.12it/s][2026-01-05 11:39:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:23] INFO:     127.0.0.1:40928 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 110/1319 [00:28<03:22,  5.96it/s][2026-01-05 11:39:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:23] INFO:     127.0.0.1:41168 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 111/1319 [00:29<03:26,  5.85it/s][2026-01-05 11:39:23 TP0] Decode batch, #running-req: 32, #token: 29312, token usage: 0.02, cpu graph: True, gen throughput (token/s): 742.02, #queue-req: 96,
[2026-01-05 11:39:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:23] INFO:     127.0.0.1:40848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:23] INFO:     127.0.0.1:41180 - "POST /generate HTTP/1.1" 200 OK

  9%|▊         | 113/1319 [00:29<04:07,  4.88it/s][2026-01-05 11:39:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:24] INFO:     127.0.0.1:41020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:24] INFO:     127.0.0.1:41162 - "POST /generate HTTP/1.1" 200 OK

  9%|▊         | 114/1319 [00:29<05:00,  4.01it/s]
  9%|▊         | 115/1319 [00:29<04:38,  4.32it/s][2026-01-05 11:39:24] INFO:     127.0.0.1:41010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:24 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:24] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 117/1319 [00:30<04:09,  4.83it/s][2026-01-05 11:39:24] INFO:     127.0.0.1:41076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:24] INFO:     127.0.0.1:41196 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 119/1319 [00:30<03:31,  5.68it/s][2026-01-05 11:39:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:24] INFO:     127.0.0.1:42986 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 120/1319 [00:30<03:37,  5.51it/s][2026-01-05 11:39:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:25 TP0] Decode batch, #running-req: 31, #token: 31872, token usage: 0.03, cpu graph: True, gen throughput (token/s): 715.78, #queue-req: 96,
[2026-01-05 11:39:25] INFO:     127.0.0.1:40978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:25] INFO:     127.0.0.1:41144 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 121/1319 [00:31<04:40,  4.27it/s]
  9%|▉         | 122/1319 [00:31<04:29,  4.43it/s][2026-01-05 11:39:25] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:25 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:25] INFO:     127.0.0.1:40618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:25] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 124/1319 [00:31<03:57,  5.04it/s]
  9%|▉         | 125/1319 [00:31<03:05,  6.43it/s][2026-01-05 11:39:25] INFO:     127.0.0.1:43002 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:25 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:25] INFO:     127.0.0.1:40992 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 127/1319 [00:31<02:51,  6.94it/s][2026-01-05 11:39:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:25] INFO:     127.0.0.1:43036 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 128/1319 [00:31<02:44,  7.23it/s][2026-01-05 11:39:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:26] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 129/1319 [00:31<02:37,  7.55it/s][2026-01-05 11:39:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:26] INFO:     127.0.0.1:43052 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 130/1319 [00:32<02:42,  7.31it/s][2026-01-05 11:39:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:26] INFO:     127.0.0.1:41140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:26] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 131/1319 [00:32<03:48,  5.20it/s]
 10%|█         | 132/1319 [00:32<03:44,  5.29it/s][2026-01-05 11:39:26 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:26 TP0] Decode batch, #running-req: 32, #token: 31488, token usage: 0.03, cpu graph: True, gen throughput (token/s): 699.20, #queue-req: 96,
[2026-01-05 11:39:26] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:26] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 133/1319 [00:32<04:26,  4.46it/s]
 10%|█         | 134/1319 [00:32<04:02,  4.89it/s][2026-01-05 11:39:26 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:27] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 135/1319 [00:33<04:39,  4.24it/s][2026-01-05 11:39:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:27] INFO:     127.0.0.1:43130 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 136/1319 [00:33<04:56,  3.99it/s][2026-01-05 11:39:27] INFO:     127.0.0.1:41130 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:28 TP0] Decode batch, #running-req: 32, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 918.98, #queue-req: 96,
[2026-01-05 11:39:28] INFO:     127.0.0.1:43118 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 138/1319 [00:34<05:40,  3.47it/s][2026-01-05 11:39:28] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:28] INFO:     127.0.0.1:41058 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 140/1319 [00:34<05:19,  3.69it/s][2026-01-05 11:39:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:28] INFO:     127.0.0.1:43050 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 141/1319 [00:34<04:58,  3.94it/s][2026-01-05 11:39:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:29] INFO:     127.0.0.1:40754 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 142/1319 [00:34<04:39,  4.22it/s][2026-01-05 11:39:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:29] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 143/1319 [00:35<04:31,  4.33it/s][2026-01-05 11:39:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:29 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 828.42, #queue-req: 96,
[2026-01-05 11:39:29] INFO:     127.0.0.1:43162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:29] INFO:     127.0.0.1:40750 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 144/1319 [00:35<05:54,  3.32it/s]
 11%|█         | 145/1319 [00:35<05:28,  3.57it/s][2026-01-05 11:39:29] INFO:     127.0.0.1:40834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:29 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:30] INFO:     127.0.0.1:40726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:30] INFO:     127.0.0.1:40822 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 147/1319 [00:35<04:13,  4.62it/s]
 11%|█         | 148/1319 [00:35<02:59,  6.53it/s][2026-01-05 11:39:30 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:30] INFO:     127.0.0.1:43098 - "POST /generate HTTP/1.1" 200 OK

 11%|█▏        | 149/1319 [00:36<03:24,  5.72it/s][2026-01-05 11:39:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:30] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK

 11%|█▏        | 150/1319 [00:36<03:12,  6.07it/s][2026-01-05 11:39:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:31] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK

 11%|█▏        | 151/1319 [00:36<05:03,  3.85it/s][2026-01-05 11:39:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:31 TP0] Decode batch, #running-req: 32, #token: 33408, token usage: 0.03, cpu graph: True, gen throughput (token/s): 822.65, #queue-req: 96,
[2026-01-05 11:39:31] INFO:     127.0.0.1:40804 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 152/1319 [00:37<05:22,  3.61it/s][2026-01-05 11:39:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:31] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 153/1319 [00:37<05:37,  3.45it/s][2026-01-05 11:39:31] INFO:     127.0.0.1:42956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:31] INFO:     127.0.0.1:40760 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 155/1319 [00:37<03:45,  5.16it/s][2026-01-05 11:39:31 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:32] INFO:     127.0.0.1:40770 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 156/1319 [00:37<03:50,  5.04it/s][2026-01-05 11:39:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:32] INFO:     127.0.0.1:43148 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 157/1319 [00:38<03:40,  5.26it/s][2026-01-05 11:39:32] INFO:     127.0.0.1:40784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:32] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 159/1319 [00:38<02:38,  7.34it/s][2026-01-05 11:39:32 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:32] INFO:     127.0.0.1:43088 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 160/1319 [00:38<03:04,  6.29it/s][2026-01-05 11:39:32] INFO:     127.0.0.1:43140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:33] INFO:     127.0.0.1:42958 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 162/1319 [00:38<03:29,  5.53it/s][2026-01-05 11:39:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:33 TP0] Decode batch, #running-req: 31, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 697.36, #queue-req: 96,
[2026-01-05 11:39:33] INFO:     127.0.0.1:40852 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 163/1319 [00:39<03:58,  4.85it/s][2026-01-05 11:39:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:33] INFO:     127.0.0.1:43048 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 164/1319 [00:39<04:15,  4.52it/s][2026-01-05 11:39:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:33] INFO:     127.0.0.1:40854 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 165/1319 [00:39<03:44,  5.14it/s][2026-01-05 11:39:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:33] INFO:     127.0.0.1:40918 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 166/1319 [00:39<03:18,  5.82it/s][2026-01-05 11:39:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:34] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 167/1319 [00:39<03:29,  5.50it/s][2026-01-05 11:39:34] INFO:     127.0.0.1:40948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:34] INFO:     127.0.0.1:40708 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 169/1319 [00:40<03:52,  4.94it/s][2026-01-05 11:39:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:34] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 170/1319 [00:40<04:10,  4.59it/s][2026-01-05 11:39:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:34 TP0] Decode batch, #running-req: 31, #token: 32000, token usage: 0.03, cpu graph: True, gen throughput (token/s): 739.77, #queue-req: 96,
[2026-01-05 11:39:34] INFO:     127.0.0.1:40736 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 171/1319 [00:40<04:00,  4.77it/s][2026-01-05 11:39:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:35] INFO:     127.0.0.1:40828 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 172/1319 [00:40<04:17,  4.45it/s][2026-01-05 11:39:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:35] INFO:     127.0.0.1:40882 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 173/1319 [00:41<04:15,  4.49it/s][2026-01-05 11:39:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:35] INFO:     127.0.0.1:40934 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 174/1319 [00:41<03:41,  5.17it/s][2026-01-05 11:39:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:35] INFO:     127.0.0.1:40864 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 175/1319 [00:41<03:36,  5.28it/s][2026-01-05 11:39:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:35] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 176/1319 [00:41<04:01,  4.73it/s][2026-01-05 11:39:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:36] INFO:     127.0.0.1:40888 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 177/1319 [00:41<04:03,  4.70it/s][2026-01-05 11:39:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:36] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 178/1319 [00:42<04:03,  4.69it/s][2026-01-05 11:39:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:36 TP0] Decode batch, #running-req: 32, #token: 33024, token usage: 0.03, cpu graph: True, gen throughput (token/s): 734.01, #queue-req: 96,
[2026-01-05 11:39:36] INFO:     127.0.0.1:40712 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 179/1319 [00:42<04:12,  4.51it/s][2026-01-05 11:39:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:36] INFO:     127.0.0.1:41084 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 180/1319 [00:42<04:00,  4.74it/s][2026-01-05 11:39:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:37] INFO:     127.0.0.1:43112 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 181/1319 [00:42<04:38,  4.09it/s][2026-01-05 11:39:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:37] INFO:     127.0.0.1:40896 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 182/1319 [00:43<04:27,  4.25it/s][2026-01-05 11:39:37] INFO:     127.0.0.1:40962 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:37] INFO:     127.0.0.1:41038 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 184/1319 [00:43<03:37,  5.23it/s][2026-01-05 11:39:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:37] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 185/1319 [00:43<03:16,  5.77it/s][2026-01-05 11:39:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:38] INFO:     127.0.0.1:48498 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 186/1319 [00:43<04:31,  4.17it/s][2026-01-05 11:39:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:38 TP0] Decode batch, #running-req: 31, #token: 31360, token usage: 0.03, cpu graph: True, gen throughput (token/s): 736.62, #queue-req: 96,
[2026-01-05 11:39:38] INFO:     127.0.0.1:40704 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 187/1319 [00:44<03:55,  4.80it/s][2026-01-05 11:39:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:38] INFO:     127.0.0.1:41098 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 188/1319 [00:44<03:25,  5.52it/s][2026-01-05 11:39:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:38] INFO:     127.0.0.1:41118 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 189/1319 [00:44<03:43,  5.07it/s][2026-01-05 11:39:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:38] INFO:     127.0.0.1:48522 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 190/1319 [00:44<03:58,  4.72it/s][2026-01-05 11:39:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:39] INFO:     127.0.0.1:41044 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 191/1319 [00:45<06:10,  3.05it/s][2026-01-05 11:39:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:39 TP0] Decode batch, #running-req: 32, #token: 32768, token usage: 0.03, cpu graph: True, gen throughput (token/s): 847.28, #queue-req: 96,
[2026-01-05 11:39:39] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:39] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK

 15%|█▍        | 192/1319 [00:45<06:26,  2.92it/s]
 15%|█▍        | 193/1319 [00:45<05:06,  3.67it/s][2026-01-05 11:39:39] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:39 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:40] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK

 15%|█▍        | 195/1319 [00:46<04:24,  4.26it/s][2026-01-05 11:39:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:40] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK

 15%|█▍        | 196/1319 [00:46<05:11,  3.60it/s][2026-01-05 11:39:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:41] INFO:     127.0.0.1:41078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:41] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK

 15%|█▍        | 197/1319 [00:46<05:24,  3.46it/s]
 15%|█▌        | 198/1319 [00:46<04:26,  4.21it/s][2026-01-05 11:39:41 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:41] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 199/1319 [00:47<04:21,  4.28it/s][2026-01-05 11:39:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:41 TP0] Decode batch, #running-req: 32, #token: 32896, token usage: 0.03, cpu graph: True, gen throughput (token/s): 809.19, #queue-req: 96,
[2026-01-05 11:39:41] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 200/1319 [00:47<04:09,  4.48it/s][2026-01-05 11:39:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:41] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 201/1319 [00:47<04:07,  4.51it/s][2026-01-05 11:39:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:41] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 202/1319 [00:47<04:21,  4.27it/s][2026-01-05 11:39:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:42] INFO:     127.0.0.1:48514 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 203/1319 [00:47<04:16,  4.35it/s][2026-01-05 11:39:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:42] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 204/1319 [00:48<04:04,  4.57it/s][2026-01-05 11:39:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:42] INFO:     127.0.0.1:40912 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 205/1319 [00:48<04:11,  4.43it/s][2026-01-05 11:39:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:42] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 206/1319 [00:48<04:08,  4.48it/s][2026-01-05 11:39:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:43] INFO:     127.0.0.1:40922 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 207/1319 [00:48<04:14,  4.37it/s][2026-01-05 11:39:43 TP0] Decode batch, #running-req: 32, #token: 31616, token usage: 0.03, cpu graph: True, gen throughput (token/s): 771.90, #queue-req: 96,
[2026-01-05 11:39:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:43] INFO:     127.0.0.1:48688 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 208/1319 [00:48<03:38,  5.09it/s][2026-01-05 11:39:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:43] INFO:     127.0.0.1:48592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:43] INFO:     127.0.0.1:48610 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 209/1319 [00:49<03:32,  5.21it/s]
 16%|█▌        | 210/1319 [00:49<02:41,  6.88it/s][2026-01-05 11:39:43 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:43] INFO:     127.0.0.1:48682 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 211/1319 [00:49<03:16,  5.65it/s][2026-01-05 11:39:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:43] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 212/1319 [00:49<02:59,  6.16it/s][2026-01-05 11:39:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:43] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 213/1319 [00:49<02:44,  6.74it/s][2026-01-05 11:39:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:44] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:44] INFO:     127.0.0.1:48724 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 214/1319 [00:49<03:12,  5.75it/s]
 16%|█▋        | 215/1319 [00:49<02:45,  6.67it/s][2026-01-05 11:39:44 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:44] INFO:     127.0.0.1:48636 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 216/1319 [00:50<02:56,  6.23it/s][2026-01-05 11:39:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:44] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 217/1319 [00:50<02:45,  6.66it/s][2026-01-05 11:39:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:44] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 218/1319 [00:50<03:48,  4.82it/s][2026-01-05 11:39:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:44 TP0] Decode batch, #running-req: 32, #token: 31104, token usage: 0.03, cpu graph: True, gen throughput (token/s): 662.34, #queue-req: 96,
[2026-01-05 11:39:44] INFO:     127.0.0.1:48766 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 219/1319 [00:50<03:50,  4.77it/s][2026-01-05 11:39:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:45] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 220/1319 [00:50<04:01,  4.55it/s][2026-01-05 11:39:45 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:45] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 221/1319 [00:51<03:42,  4.93it/s][2026-01-05 11:39:45 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:45] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 222/1319 [00:51<03:46,  4.84it/s][2026-01-05 11:39:45 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:45] INFO:     127.0.0.1:48578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:45] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 223/1319 [00:51<03:18,  5.52it/s]
 17%|█▋        | 224/1319 [00:51<02:17,  7.94it/s][2026-01-05 11:39:45 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:46] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 225/1319 [00:51<03:36,  5.05it/s][2026-01-05 11:39:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:46] INFO:     127.0.0.1:48846 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 226/1319 [00:52<04:20,  4.20it/s][2026-01-05 11:39:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:46 TP0] Decode batch, #running-req: 31, #token: 31616, token usage: 0.03, cpu graph: True, gen throughput (token/s): 778.76, #queue-req: 96,
[2026-01-05 11:39:46] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 227/1319 [00:52<03:45,  4.85it/s][2026-01-05 11:39:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:47] INFO:     127.0.0.1:40994 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 228/1319 [00:52<04:56,  3.68it/s][2026-01-05 11:39:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:47] INFO:     127.0.0.1:48722 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 229/1319 [00:53<04:39,  3.91it/s][2026-01-05 11:39:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:47] INFO:     127.0.0.1:48782 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 230/1319 [00:53<04:19,  4.20it/s][2026-01-05 11:39:47] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:47] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 232/1319 [00:53<03:28,  5.22it/s][2026-01-05 11:39:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:47] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 233/1319 [00:53<03:09,  5.74it/s][2026-01-05 11:39:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:48] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:48] INFO:     127.0.0.1:48786 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 234/1319 [00:53<03:55,  4.61it/s]
 18%|█▊        | 235/1319 [00:53<03:33,  5.08it/s][2026-01-05 11:39:48 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:48] INFO:     127.0.0.1:48954 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 236/1319 [00:54<03:13,  5.59it/s][2026-01-05 11:39:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:48 TP0] Decode batch, #running-req: 31, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 701.51, #queue-req: 96,
[2026-01-05 11:39:48] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 237/1319 [00:54<03:19,  5.42it/s][2026-01-05 11:39:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:48] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 238/1319 [00:54<03:42,  4.86it/s][2026-01-05 11:39:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:48] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 239/1319 [00:54<03:44,  4.81it/s][2026-01-05 11:39:49] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:49] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 241/1319 [00:55<03:53,  4.62it/s][2026-01-05 11:39:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:49] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 242/1319 [00:55<03:38,  4.93it/s][2026-01-05 11:39:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:49 TP0] Decode batch, #running-req: 32, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 818.10, #queue-req: 96,
[2026-01-05 11:39:49] INFO:     127.0.0.1:48868 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 243/1319 [00:55<04:23,  4.08it/s][2026-01-05 11:39:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:50] INFO:     127.0.0.1:48958 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 244/1319 [00:56<04:44,  3.78it/s][2026-01-05 11:39:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:50] INFO:     127.0.0.1:41418 - "POST /generate HTTP/1.1" 200 OK

 19%|█▊        | 245/1319 [00:56<06:25,  2.79it/s][2026-01-05 11:39:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:51] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK

 19%|█▊        | 246/1319 [00:56<05:33,  3.22it/s][2026-01-05 11:39:51] INFO:     127.0.0.1:48858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:51] INFO:     127.0.0.1:41472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:51 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:51] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 249/1319 [00:57<03:15,  5.48it/s][2026-01-05 11:39:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:51 TP0] Decode batch, #running-req: 32, #token: 32256, token usage: 0.03, cpu graph: True, gen throughput (token/s): 824.91, #queue-req: 96,
[2026-01-05 11:39:51] INFO:     127.0.0.1:41458 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 250/1319 [00:57<03:19,  5.37it/s][2026-01-05 11:39:51] INFO:     127.0.0.1:48974 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:51] INFO:     127.0.0.1:48762 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 252/1319 [00:57<02:58,  5.98it/s][2026-01-05 11:39:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:52] INFO:     127.0.0.1:41494 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 253/1319 [00:57<03:32,  5.01it/s][2026-01-05 11:39:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:52] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 254/1319 [00:57<03:14,  5.49it/s][2026-01-05 11:39:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:52] INFO:     127.0.0.1:41518 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 255/1319 [00:58<04:15,  4.17it/s][2026-01-05 11:39:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:52] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 256/1319 [00:58<03:52,  4.58it/s][2026-01-05 11:39:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:52] INFO:     127.0.0.1:41442 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:52] INFO:     127.0.0.1:41478 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 257/1319 [00:58<03:42,  4.78it/s]
 20%|█▉        | 258/1319 [00:58<02:47,  6.33it/s][2026-01-05 11:39:53] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:53 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:53] INFO:     127.0.0.1:48874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:53] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 260/1319 [00:59<02:39,  6.65it/s]
 20%|█▉        | 261/1319 [00:59<02:11,  8.05it/s][2026-01-05 11:39:53 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:53 TP0] Decode batch, #running-req: 30, #token: 31232, token usage: 0.03, cpu graph: True, gen throughput (token/s): 677.68, #queue-req: 96,
[2026-01-05 11:39:53] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 262/1319 [00:59<02:10,  8.08it/s][2026-01-05 11:39:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:53] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:53] INFO:     127.0.0.1:41546 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 263/1319 [00:59<02:41,  6.55it/s]
 20%|██        | 264/1319 [00:59<02:33,  6.86it/s][2026-01-05 11:39:53 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:53] INFO:     127.0.0.1:41552 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 265/1319 [00:59<02:28,  7.08it/s][2026-01-05 11:39:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:54] INFO:     127.0.0.1:41584 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 266/1319 [01:00<05:09,  3.41it/s][2026-01-05 11:39:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:54 TP0] Decode batch, #running-req: 32, #token: 32000, token usage: 0.03, cpu graph: True, gen throughput (token/s): 902.54, #queue-req: 96,
[2026-01-05 11:39:54] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 267/1319 [01:00<04:47,  3.65it/s][2026-01-05 11:39:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:54] INFO:     127.0.0.1:41656 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 268/1319 [01:00<04:23,  3.99it/s][2026-01-05 11:39:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:55] INFO:     127.0.0.1:48944 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 269/1319 [01:00<04:20,  4.04it/s][2026-01-05 11:39:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:55] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 270/1319 [01:01<04:02,  4.33it/s][2026-01-05 11:39:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:55] INFO:     127.0.0.1:41508 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 271/1319 [01:01<03:29,  5.01it/s][2026-01-05 11:39:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:55] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 272/1319 [01:01<03:13,  5.42it/s][2026-01-05 11:39:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:55] INFO:     127.0.0.1:41516 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 273/1319 [01:01<03:30,  4.97it/s][2026-01-05 11:39:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:56 TP0] Decode batch, #running-req: 32, #token: 33536, token usage: 0.03, cpu graph: True, gen throughput (token/s): 780.40, #queue-req: 96,
[2026-01-05 11:39:56] INFO:     127.0.0.1:48544 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 274/1319 [01:02<06:11,  2.81it/s][2026-01-05 11:39:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:56] INFO:     127.0.0.1:41708 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 275/1319 [01:02<05:28,  3.18it/s][2026-01-05 11:39:56] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:57] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 277/1319 [01:02<04:09,  4.17it/s][2026-01-05 11:39:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:57] INFO:     127.0.0.1:41570 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 278/1319 [01:03<03:56,  4.39it/s][2026-01-05 11:39:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:57] INFO:     127.0.0.1:48626 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:57] INFO:     127.0.0.1:41724 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 279/1319 [01:03<03:28,  4.99it/s]
 21%|██        | 280/1319 [01:03<02:26,  7.09it/s][2026-01-05 11:39:57 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:39:57] INFO:     127.0.0.1:41776 - "POST /generate HTTP/1.1" 200 OK

 21%|██▏       | 281/1319 [01:03<02:27,  7.05it/s][2026-01-05 11:39:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:58 TP0] Decode batch, #running-req: 32, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 776.08, #queue-req: 96,
[2026-01-05 11:39:58] INFO:     127.0.0.1:41722 - "POST /generate HTTP/1.1" 200 OK

 21%|██▏       | 282/1319 [01:03<04:00,  4.31it/s][2026-01-05 11:39:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:58] INFO:     127.0.0.1:41842 - "POST /generate HTTP/1.1" 200 OK

 21%|██▏       | 283/1319 [01:04<03:30,  4.92it/s][2026-01-05 11:39:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:58] INFO:     127.0.0.1:41684 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 284/1319 [01:04<03:21,  5.13it/s][2026-01-05 11:39:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:58] INFO:     127.0.0.1:41734 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 285/1319 [01:04<03:57,  4.35it/s][2026-01-05 11:39:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:58] INFO:     127.0.0.1:41666 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 286/1319 [01:04<03:26,  5.01it/s][2026-01-05 11:39:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:59] INFO:     127.0.0.1:41506 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 287/1319 [01:04<03:11,  5.39it/s][2026-01-05 11:39:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:59] INFO:     127.0.0.1:41852 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 288/1319 [01:05<04:23,  3.91it/s][2026-01-05 11:39:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:59 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 778.43, #queue-req: 96,
[2026-01-05 11:39:59] INFO:     127.0.0.1:41862 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 289/1319 [01:05<04:42,  3.65it/s][2026-01-05 11:39:59] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:39:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:39:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:00] INFO:     127.0.0.1:41600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:00] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 291/1319 [01:05<03:58,  4.30it/s]
 22%|██▏       | 292/1319 [01:05<03:02,  5.63it/s][2026-01-05 11:40:00 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:00] INFO:     127.0.0.1:41608 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 293/1319 [01:06<02:52,  5.94it/s][2026-01-05 11:40:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:00] INFO:     127.0.0.1:41794 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 294/1319 [01:06<02:48,  6.08it/s][2026-01-05 11:40:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:00] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 295/1319 [01:06<02:54,  5.86it/s][2026-01-05 11:40:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:00] INFO:     127.0.0.1:41698 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 296/1319 [01:06<02:52,  5.92it/s][2026-01-05 11:40:00] INFO:     127.0.0.1:41824 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:01] INFO:     127.0.0.1:60832 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 298/1319 [01:06<02:55,  5.80it/s][2026-01-05 11:40:01] INFO:     127.0.0.1:41808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:01 TP0] Decode batch, #running-req: 32, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 663.27, #queue-req: 96,
[2026-01-05 11:40:01] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 300/1319 [01:07<03:36,  4.71it/s][2026-01-05 11:40:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:01] INFO:     127.0.0.1:41664 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 301/1319 [01:07<03:42,  4.58it/s][2026-01-05 11:40:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:02] INFO:     127.0.0.1:60830 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 302/1319 [01:07<03:21,  5.04it/s][2026-01-05 11:40:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:02] INFO:     127.0.0.1:60852 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 303/1319 [01:07<02:59,  5.65it/s][2026-01-05 11:40:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:02] INFO:     127.0.0.1:60848 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 304/1319 [01:08<03:20,  5.07it/s][2026-01-05 11:40:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:02] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 305/1319 [01:08<03:24,  4.96it/s][2026-01-05 11:40:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:02] INFO:     127.0.0.1:60982 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 306/1319 [01:08<03:20,  5.06it/s][2026-01-05 11:40:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:02] INFO:     127.0.0.1:41676 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 307/1319 [01:08<03:10,  5.32it/s][2026-01-05 11:40:02] INFO:     127.0.0.1:41788 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:03] INFO:     127.0.0.1:60962 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 309/1319 [01:08<02:11,  7.68it/s][2026-01-05 11:40:03 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:03] INFO:     127.0.0.1:60874 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 310/1319 [01:08<02:07,  7.94it/s][2026-01-05 11:40:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:03] INFO:     127.0.0.1:60810 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 311/1319 [01:09<02:46,  6.06it/s][2026-01-05 11:40:03 TP0] Decode batch, #running-req: 32, #token: 31360, token usage: 0.03, cpu graph: True, gen throughput (token/s): 670.25, #queue-req: 96,
[2026-01-05 11:40:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:03] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 312/1319 [01:09<03:13,  5.21it/s][2026-01-05 11:40:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:04] INFO:     127.0.0.1:32824 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 313/1319 [01:10<04:40,  3.59it/s][2026-01-05 11:40:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:04] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 314/1319 [01:10<04:35,  3.64it/s][2026-01-05 11:40:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:04] INFO:     127.0.0.1:41642 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 315/1319 [01:10<04:09,  4.02it/s][2026-01-05 11:40:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:04 TP0] Decode batch, #running-req: 32, #token: 32896, token usage: 0.03, cpu graph: True, gen throughput (token/s): 874.01, #queue-req: 96,
[2026-01-05 11:40:05] INFO:     127.0.0.1:60924 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 316/1319 [01:10<05:09,  3.24it/s][2026-01-05 11:40:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:05] INFO:     127.0.0.1:32856 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 317/1319 [01:11<04:14,  3.94it/s][2026-01-05 11:40:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:05] INFO:     127.0.0.1:60968 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 318/1319 [01:11<04:44,  3.52it/s][2026-01-05 11:40:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:05] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 319/1319 [01:11<05:01,  3.31it/s][2026-01-05 11:40:05] INFO:     127.0.0.1:32906 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:06] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 321/1319 [01:11<03:35,  4.64it/s][2026-01-05 11:40:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:06] INFO:     127.0.0.1:41760 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 322/1319 [01:12<03:07,  5.31it/s][2026-01-05 11:40:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:06] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 323/1319 [01:12<02:46,  6.00it/s][2026-01-05 11:40:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:06 TP0] Decode batch, #running-req: 32, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 750.38, #queue-req: 96,
[2026-01-05 11:40:06] INFO:     127.0.0.1:41864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:06] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK

 25%|██▍       | 324/1319 [01:12<03:43,  4.46it/s]
 25%|██▍       | 325/1319 [01:12<03:27,  4.79it/s][2026-01-05 11:40:06] INFO:     127.0.0.1:32896 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:06 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:06] INFO:     127.0.0.1:41830 - "POST /generate HTTP/1.1" 200 OK

 25%|██▍       | 327/1319 [01:12<02:31,  6.57it/s][2026-01-05 11:40:06 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:07] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK

 25%|██▍       | 328/1319 [01:12<02:30,  6.60it/s][2026-01-05 11:40:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:07] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK

 25%|██▍       | 329/1319 [01:13<02:37,  6.29it/s][2026-01-05 11:40:07] INFO:     127.0.0.1:32826 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:07] INFO:     127.0.0.1:32780 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 331/1319 [01:13<02:25,  6.77it/s][2026-01-05 11:40:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:07] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 332/1319 [01:13<02:27,  6.70it/s][2026-01-05 11:40:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:08] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 333/1319 [01:13<03:43,  4.40it/s][2026-01-05 11:40:08] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:08 TP0] Decode batch, #running-req: 32, #token: 32256, token usage: 0.03, cpu graph: True, gen throughput (token/s): 719.54, #queue-req: 96,
[2026-01-05 11:40:08] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 335/1319 [01:14<03:36,  4.54it/s][2026-01-05 11:40:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:08] INFO:     127.0.0.1:60996 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 336/1319 [01:14<03:51,  4.24it/s][2026-01-05 11:40:08] INFO:     127.0.0.1:32904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:08] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:08 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:09] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 339/1319 [01:15<03:01,  5.40it/s][2026-01-05 11:40:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:09] INFO:     127.0.0.1:33004 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 340/1319 [01:15<03:30,  4.65it/s][2026-01-05 11:40:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:09] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 341/1319 [01:15<03:41,  4.43it/s][2026-01-05 11:40:09 TP0] Decode batch, #running-req: 32, #token: 31616, token usage: 0.03, cpu graph: True, gen throughput (token/s): 875.63, #queue-req: 96,
[2026-01-05 11:40:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:10] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 342/1319 [01:15<03:25,  4.75it/s][2026-01-05 11:40:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:10] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 343/1319 [01:16<03:32,  4.59it/s][2026-01-05 11:40:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:10] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 344/1319 [01:16<03:08,  5.18it/s][2026-01-05 11:40:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:10] INFO:     127.0.0.1:32956 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 345/1319 [01:16<03:30,  4.62it/s][2026-01-05 11:40:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:10] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 346/1319 [01:16<03:30,  4.62it/s][2026-01-05 11:40:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:11] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 347/1319 [01:16<03:46,  4.29it/s][2026-01-05 11:40:11] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:11 TP0] Decode batch, #running-req: 32, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 740.70, #queue-req: 96,
[2026-01-05 11:40:11] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:11] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 349/1319 [01:17<04:05,  3.95it/s]
 27%|██▋       | 350/1319 [01:17<03:33,  4.54it/s][2026-01-05 11:40:11 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:12] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 351/1319 [01:17<03:55,  4.11it/s][2026-01-05 11:40:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:12] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 352/1319 [01:17<03:38,  4.43it/s][2026-01-05 11:40:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:12] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 353/1319 [01:18<03:22,  4.76it/s][2026-01-05 11:40:12] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:12] INFO:     127.0.0.1:52940 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 355/1319 [01:18<02:45,  5.82it/s][2026-01-05 11:40:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:12] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 356/1319 [01:18<02:50,  5.63it/s][2026-01-05 11:40:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:12] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 357/1319 [01:18<02:37,  6.09it/s][2026-01-05 11:40:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:13] INFO:     127.0.0.1:32992 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 358/1319 [01:18<02:25,  6.61it/s][2026-01-05 11:40:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:13] INFO:     127.0.0.1:60866 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 359/1319 [01:18<02:23,  6.67it/s][2026-01-05 11:40:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:13 TP0] Decode batch, #running-req: 32, #token: 31104, token usage: 0.03, cpu graph: True, gen throughput (token/s): 677.24, #queue-req: 96,
[2026-01-05 11:40:13] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 360/1319 [01:19<03:02,  5.24it/s][2026-01-05 11:40:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:13] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 361/1319 [01:19<02:43,  5.85it/s][2026-01-05 11:40:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:13] INFO:     127.0.0.1:53060 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 362/1319 [01:19<02:43,  5.87it/s][2026-01-05 11:40:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:14] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 363/1319 [01:19<03:16,  4.87it/s][2026-01-05 11:40:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:14] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 364/1319 [01:20<03:47,  4.20it/s][2026-01-05 11:40:14] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:14] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 366/1319 [01:20<03:55,  4.05it/s][2026-01-05 11:40:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:15 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 767.17, #queue-req: 96,
[2026-01-05 11:40:15] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 367/1319 [01:20<04:07,  3.84it/s][2026-01-05 11:40:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:15] INFO:     127.0.0.1:53072 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 368/1319 [01:21<04:03,  3.91it/s][2026-01-05 11:40:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:15] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:15] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 369/1319 [01:21<04:20,  3.65it/s]
 28%|██▊       | 370/1319 [01:21<03:33,  4.44it/s][2026-01-05 11:40:15 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:15] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 371/1319 [01:21<03:20,  4.74it/s][2026-01-05 11:40:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:16] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 372/1319 [01:21<03:15,  4.84it/s][2026-01-05 11:40:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:16] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 373/1319 [01:22<03:11,  4.94it/s][2026-01-05 11:40:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:16] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 374/1319 [01:22<02:50,  5.55it/s][2026-01-05 11:40:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:16] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 375/1319 [01:22<03:04,  5.11it/s][2026-01-05 11:40:16] INFO:     127.0.0.1:53094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:16 TP0] Decode batch, #running-req: 32, #token: 32000, token usage: 0.03, cpu graph: True, gen throughput (token/s): 694.03, #queue-req: 96,
[2026-01-05 11:40:16] INFO:     127.0.0.1:53048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:16] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK

 29%|██▊       | 377/1319 [01:22<02:51,  5.50it/s][2026-01-05 11:40:17] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:17 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:17] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 380/1319 [01:22<01:46,  8.86it/s][2026-01-05 11:40:17 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:17] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:17] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 382/1319 [01:23<03:02,  5.14it/s][2026-01-05 11:40:17 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:17] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 383/1319 [01:23<02:50,  5.50it/s][2026-01-05 11:40:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:18] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 384/1319 [01:23<03:01,  5.16it/s][2026-01-05 11:40:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:18] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 385/1319 [01:24<02:45,  5.63it/s][2026-01-05 11:40:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:18 TP0] Decode batch, #running-req: 32, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 801.33, #queue-req: 96,
[2026-01-05 11:40:18] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 386/1319 [01:24<02:52,  5.41it/s][2026-01-05 11:40:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:18] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 387/1319 [01:24<03:20,  4.66it/s][2026-01-05 11:40:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:19] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 388/1319 [01:24<03:15,  4.77it/s][2026-01-05 11:40:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:19] INFO:     127.0.0.1:53154 - "POST /generate HTTP/1.1" 200 OK

 29%|██▉       | 389/1319 [01:25<03:38,  4.27it/s][2026-01-05 11:40:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:19] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:19] INFO:     127.0.0.1:53224 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 390/1319 [01:25<03:46,  4.10it/s]
 30%|██▉       | 391/1319 [01:25<03:00,  5.14it/s][2026-01-05 11:40:19 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:19] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 392/1319 [01:25<02:58,  5.18it/s][2026-01-05 11:40:19] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:20] INFO:     127.0.0.1:53230 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 394/1319 [01:25<02:58,  5.18it/s][2026-01-05 11:40:20 TP0] Decode batch, #running-req: 32, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 765.98, #queue-req: 96,
[2026-01-05 11:40:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:20] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 395/1319 [01:26<04:25,  3.49it/s][2026-01-05 11:40:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:20] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 396/1319 [01:26<04:01,  3.82it/s][2026-01-05 11:40:20] INFO:     127.0.0.1:53266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:21] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 398/1319 [01:26<03:04,  4.99it/s][2026-01-05 11:40:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:21] INFO:     127.0.0.1:49604 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 399/1319 [01:27<02:44,  5.59it/s][2026-01-05 11:40:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:21] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 400/1319 [01:27<03:35,  4.26it/s][2026-01-05 11:40:21 TP0] Decode batch, #running-req: 32, #token: 30080, token usage: 0.02, cpu graph: True, gen throughput (token/s): 832.20, #queue-req: 96,
[2026-01-05 11:40:21] INFO:     127.0.0.1:32842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:22] INFO:     127.0.0.1:49616 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 402/1319 [01:28<04:11,  3.64it/s][2026-01-05 11:40:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:22] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 403/1319 [01:28<03:39,  4.17it/s][2026-01-05 11:40:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:22] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 404/1319 [01:28<03:23,  4.50it/s][2026-01-05 11:40:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:22] INFO:     127.0.0.1:53282 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 405/1319 [01:28<02:59,  5.10it/s][2026-01-05 11:40:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:22] INFO:     127.0.0.1:49534 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 406/1319 [01:28<02:58,  5.10it/s][2026-01-05 11:40:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:23 TP0] Decode batch, #running-req: 32, #token: 32896, token usage: 0.03, cpu graph: True, gen throughput (token/s): 790.12, #queue-req: 96,
[2026-01-05 11:40:23] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 407/1319 [01:29<04:02,  3.76it/s][2026-01-05 11:40:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:23] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 408/1319 [01:29<04:56,  3.08it/s][2026-01-05 11:40:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:24] INFO:     127.0.0.1:49746 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 409/1319 [01:29<04:41,  3.23it/s][2026-01-05 11:40:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:24] INFO:     127.0.0.1:49566 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 410/1319 [01:30<04:14,  3.57it/s][2026-01-05 11:40:24] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:24] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 412/1319 [01:30<03:04,  4.90it/s][2026-01-05 11:40:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:24] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK

 31%|███▏      | 413/1319 [01:30<03:02,  4.97it/s][2026-01-05 11:40:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:24] INFO:     127.0.0.1:49766 - "POST /generate HTTP/1.1" 200 OK

 31%|███▏      | 414/1319 [01:30<02:44,  5.51it/s][2026-01-05 11:40:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:25 TP0] Decode batch, #running-req: 31, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 754.46, #queue-req: 96,
[2026-01-05 11:40:25] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK

 31%|███▏      | 415/1319 [01:30<02:42,  5.56it/s][2026-01-05 11:40:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:25] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 416/1319 [01:31<03:10,  4.73it/s][2026-01-05 11:40:25] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:25] INFO:     127.0.0.1:49596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:25] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 418/1319 [01:31<02:44,  5.48it/s]
 32%|███▏      | 419/1319 [01:31<02:06,  7.09it/s][2026-01-05 11:40:25] INFO:     127.0.0.1:49688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:25 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:26] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 421/1319 [01:31<02:13,  6.72it/s][2026-01-05 11:40:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:26] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 422/1319 [01:32<02:46,  5.38it/s][2026-01-05 11:40:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:26] INFO:     127.0.0.1:49628 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 423/1319 [01:32<02:42,  5.51it/s][2026-01-05 11:40:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:26] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 424/1319 [01:32<02:29,  5.97it/s][2026-01-05 11:40:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:26] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 425/1319 [01:32<02:33,  5.84it/s][2026-01-05 11:40:26 TP0] Decode batch, #running-req: 32, #token: 31104, token usage: 0.03, cpu graph: True, gen throughput (token/s): 695.81, #queue-req: 96,
[2026-01-05 11:40:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:26] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 426/1319 [01:32<02:20,  6.34it/s][2026-01-05 11:40:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:27] INFO:     127.0.0.1:49672 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 427/1319 [01:32<02:08,  6.92it/s][2026-01-05 11:40:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:27] INFO:     127.0.0.1:49522 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 428/1319 [01:33<02:16,  6.54it/s][2026-01-05 11:40:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:27] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 429/1319 [01:33<02:19,  6.37it/s][2026-01-05 11:40:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:27] INFO:     127.0.0.1:49826 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 430/1319 [01:33<03:56,  3.77it/s][2026-01-05 11:40:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:28 TP0] Decode batch, #running-req: 32, #token: 32640, token usage: 0.03, cpu graph: True, gen throughput (token/s): 818.10, #queue-req: 96,
[2026-01-05 11:40:28] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 431/1319 [01:34<06:00,  2.46it/s][2026-01-05 11:40:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:28] INFO:     127.0.0.1:49790 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 432/1319 [01:34<05:02,  2.93it/s][2026-01-05 11:40:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:29] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 433/1319 [01:34<04:41,  3.15it/s][2026-01-05 11:40:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:29] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 434/1319 [01:35<04:08,  3.56it/s][2026-01-05 11:40:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:29] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 435/1319 [01:35<03:51,  3.81it/s][2026-01-05 11:40:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:29] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 436/1319 [01:35<03:25,  4.30it/s][2026-01-05 11:40:29] INFO:     127.0.0.1:46220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:29] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 438/1319 [01:35<02:37,  5.58it/s][2026-01-05 11:40:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:30 TP0] Decode batch, #running-req: 32, #token: 32768, token usage: 0.03, cpu graph: True, gen throughput (token/s): 735.00, #queue-req: 96,
[2026-01-05 11:40:30] INFO:     127.0.0.1:49840 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 439/1319 [01:36<04:10,  3.51it/s][2026-01-05 11:40:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:30] INFO:     127.0.0.1:49864 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 440/1319 [01:36<03:32,  4.13it/s][2026-01-05 11:40:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:30] INFO:     127.0.0.1:49582 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 441/1319 [01:36<03:00,  4.87it/s][2026-01-05 11:40:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:31] INFO:     127.0.0.1:46248 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 442/1319 [01:36<03:09,  4.62it/s][2026-01-05 11:40:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:31] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 443/1319 [01:37<03:47,  3.86it/s][2026-01-05 11:40:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:31] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 444/1319 [01:37<03:21,  4.35it/s][2026-01-05 11:40:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:31 TP0] Decode batch, #running-req: 31, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 841.99, #queue-req: 96,
[2026-01-05 11:40:31] INFO:     127.0.0.1:46346 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 445/1319 [01:37<03:30,  4.15it/s][2026-01-05 11:40:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:32] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 446/1319 [01:37<03:48,  3.82it/s][2026-01-05 11:40:32] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:32] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:32 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:32] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 449/1319 [01:38<02:24,  6.04it/s][2026-01-05 11:40:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:32] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 450/1319 [01:38<02:27,  5.89it/s][2026-01-05 11:40:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:32] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 451/1319 [01:38<02:41,  5.39it/s][2026-01-05 11:40:32] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:33] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 453/1319 [01:39<02:51,  5.05it/s][2026-01-05 11:40:33] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:33 TP0] Decode batch, #running-req: 31, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 743.77, #queue-req: 96,
[2026-01-05 11:40:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:33] INFO:     127.0.0.1:49862 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 455/1319 [01:39<02:39,  5.41it/s][2026-01-05 11:40:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:33] INFO:     127.0.0.1:49624 - "POST /generate HTTP/1.1" 200 OK

 35%|███▍      | 456/1319 [01:39<02:49,  5.10it/s][2026-01-05 11:40:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:34] INFO:     127.0.0.1:46234 - "POST /generate HTTP/1.1" 200 OK

 35%|███▍      | 457/1319 [01:39<02:57,  4.85it/s][2026-01-05 11:40:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:34] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK

 35%|███▍      | 458/1319 [01:40<02:58,  4.81it/s][2026-01-05 11:40:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:34] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:34] INFO:     127.0.0.1:46294 - "POST /generate HTTP/1.1" 200 OK

 35%|███▍      | 459/1319 [01:40<02:48,  5.10it/s]
 35%|███▍      | 460/1319 [01:40<02:06,  6.79it/s][2026-01-05 11:40:34 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:34] INFO:     127.0.0.1:46306 - "POST /generate HTTP/1.1" 200 OK

 35%|███▍      | 461/1319 [01:40<02:01,  7.07it/s][2026-01-05 11:40:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:34] INFO:     127.0.0.1:49880 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 462/1319 [01:40<02:08,  6.66it/s][2026-01-05 11:40:34] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:34] INFO:     127.0.0.1:46438 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 464/1319 [01:40<01:36,  8.87it/s][2026-01-05 11:40:34 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:35] INFO:     127.0.0.1:46334 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 465/1319 [01:40<01:48,  7.88it/s][2026-01-05 11:40:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:35 TP0] Decode batch, #running-req: 32, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 670.88, #queue-req: 96,
[2026-01-05 11:40:35] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 466/1319 [01:41<03:18,  4.30it/s][2026-01-05 11:40:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:35] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 467/1319 [01:41<02:53,  4.92it/s][2026-01-05 11:40:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:36] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 468/1319 [01:41<03:40,  3.86it/s][2026-01-05 11:40:36] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:36] INFO:     127.0.0.1:46506 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 470/1319 [01:42<02:45,  5.12it/s][2026-01-05 11:40:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:36] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 471/1319 [01:42<02:41,  5.24it/s][2026-01-05 11:40:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:36] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 472/1319 [01:42<02:51,  4.93it/s][2026-01-05 11:40:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:36 TP0] Decode batch, #running-req: 31, #token: 32256, token usage: 0.03, cpu graph: True, gen throughput (token/s): 792.99, #queue-req: 96,
[2026-01-05 11:40:36] INFO:     127.0.0.1:46548 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:36] INFO:     127.0.0.1:46594 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 473/1319 [01:42<02:42,  5.21it/s]
 36%|███▌      | 474/1319 [01:42<02:02,  6.93it/s][2026-01-05 11:40:36 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:37] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 475/1319 [01:42<02:05,  6.74it/s][2026-01-05 11:40:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:37] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 476/1319 [01:43<02:12,  6.35it/s][2026-01-05 11:40:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:37] INFO:     127.0.0.1:46574 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 477/1319 [01:43<02:13,  6.31it/s][2026-01-05 11:40:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:37] INFO:     127.0.0.1:46518 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 478/1319 [01:43<02:19,  6.01it/s][2026-01-05 11:40:37] INFO:     127.0.0.1:46466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:37] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 480/1319 [01:43<02:00,  6.99it/s][2026-01-05 11:40:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:38] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 481/1319 [01:43<02:17,  6.11it/s][2026-01-05 11:40:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:38] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 482/1319 [01:44<02:21,  5.90it/s][2026-01-05 11:40:38] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:38] INFO:     127.0.0.1:46572 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 484/1319 [01:44<01:42,  8.11it/s][2026-01-05 11:40:38 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:38] INFO:     127.0.0.1:46462 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 485/1319 [01:44<02:01,  6.87it/s][2026-01-05 11:40:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:38 TP0] Decode batch, #running-req: 32, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 648.88, #queue-req: 96,
[2026-01-05 11:40:38] INFO:     127.0.0.1:46368 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 486/1319 [01:44<02:25,  5.74it/s][2026-01-05 11:40:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:38] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 487/1319 [01:44<02:20,  5.91it/s][2026-01-05 11:40:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:39] INFO:     127.0.0.1:46510 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 488/1319 [01:45<03:07,  4.44it/s][2026-01-05 11:40:39] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:40] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 490/1319 [01:45<04:11,  3.30it/s][2026-01-05 11:40:40 TP0] Decode batch, #running-req: 32, #token: 31872, token usage: 0.03, cpu graph: True, gen throughput (token/s): 918.57, #queue-req: 96,
[2026-01-05 11:40:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:40] INFO:     127.0.0.1:46392 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 491/1319 [01:46<04:25,  3.12it/s][2026-01-05 11:40:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:40] INFO:     127.0.0.1:46650 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 492/1319 [01:46<04:03,  3.40it/s][2026-01-05 11:40:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:40] INFO:     127.0.0.1:46384 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 493/1319 [01:46<03:40,  3.75it/s][2026-01-05 11:40:40] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:41] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 495/1319 [01:46<02:26,  5.63it/s][2026-01-05 11:40:41 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:41] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 496/1319 [01:47<02:27,  5.57it/s][2026-01-05 11:40:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:41] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 497/1319 [01:47<02:24,  5.68it/s][2026-01-05 11:40:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:41] INFO:     127.0.0.1:48730 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 498/1319 [01:47<02:12,  6.20it/s][2026-01-05 11:40:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:41] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 499/1319 [01:47<02:01,  6.77it/s][2026-01-05 11:40:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:41] INFO:     127.0.0.1:46648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:41] INFO:     127.0.0.1:48842 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 500/1319 [01:47<02:09,  6.31it/s]
 38%|███▊      | 501/1319 [01:47<01:45,  7.74it/s][2026-01-05 11:40:41 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:42] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 502/1319 [01:47<02:08,  6.37it/s][2026-01-05 11:40:42 TP0] Decode batch, #running-req: 32, #token: 31232, token usage: 0.03, cpu graph: True, gen throughput (token/s): 660.21, #queue-req: 96,
[2026-01-05 11:40:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:42] INFO:     127.0.0.1:48742 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 503/1319 [01:48<02:09,  6.29it/s][2026-01-05 11:40:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:42] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 504/1319 [01:48<02:44,  4.96it/s][2026-01-05 11:40:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:42] INFO:     127.0.0.1:48794 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 505/1319 [01:48<02:34,  5.27it/s][2026-01-05 11:40:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:42] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 506/1319 [01:48<02:17,  5.91it/s][2026-01-05 11:40:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:42] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 507/1319 [01:48<02:02,  6.63it/s][2026-01-05 11:40:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:43] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK

 39%|███▊      | 508/1319 [01:48<02:19,  5.82it/s][2026-01-05 11:40:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:43] INFO:     127.0.0.1:46652 - "POST /generate HTTP/1.1" 200 OK

 39%|███▊      | 509/1319 [01:49<02:22,  5.70it/s][2026-01-05 11:40:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:43 TP0] Decode batch, #running-req: 32, #token: 31872, token usage: 0.03, cpu graph: True, gen throughput (token/s): 753.52, #queue-req: 96,
[2026-01-05 11:40:43] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK

 39%|███▊      | 510/1319 [01:49<03:50,  3.50it/s][2026-01-05 11:40:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:44] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK

 39%|███▊      | 511/1319 [01:49<03:57,  3.40it/s][2026-01-05 11:40:44] INFO:     127.0.0.1:48830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:44] INFO:     127.0.0.1:49022 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 513/1319 [01:50<03:18,  4.06it/s][2026-01-05 11:40:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:45] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 514/1319 [01:50<03:48,  3.53it/s][2026-01-05 11:40:45 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:45] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:45] INFO:     127.0.0.1:48876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:45] INFO:     127.0.0.1:49016 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 515/1319 [01:50<03:34,  3.76it/s]
 39%|███▉      | 517/1319 [01:50<01:51,  7.22it/s]
 39%|███▉      | 517/1319 [01:50<01:51,  7.22it/s][2026-01-05 11:40:45 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, token usage: 0.02, #running-req: 29, #queue-req: 96,
[2026-01-05 11:40:47 TP0] Decode batch, #running-req: 29, #token: 32128, token usage: 0.03, cpu graph: True, gen throughput (token/s): 316.20, #queue-req: 96,
[2026-01-05 11:40:47] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 518/1319 [01:53<08:07,  1.64it/s][2026-01-05 11:40:47] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:47] INFO:     127.0.0.1:48898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:47] INFO:     127.0.0.1:48958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:48 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, token usage: 0.02, #running-req: 29, #queue-req: 96,
[2026-01-05 11:40:48] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 522/1319 [01:53<04:29,  2.96it/s][2026-01-05 11:40:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:48] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 523/1319 [01:54<04:07,  3.22it/s][2026-01-05 11:40:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:48] INFO:     127.0.0.1:48920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:48] INFO:     127.0.0.1:49002 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 524/1319 [01:54<04:00,  3.31it/s][2026-01-05 11:40:48 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:48] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 526/1319 [01:54<02:59,  4.41it/s][2026-01-05 11:40:48] INFO:     127.0.0.1:49050 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:49] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 528/1319 [01:54<02:42,  4.87it/s][2026-01-05 11:40:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:49] INFO:     127.0.0.1:48942 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 529/1319 [01:55<02:39,  4.96it/s][2026-01-05 11:40:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:49] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 530/1319 [01:55<02:36,  5.04it/s][2026-01-05 11:40:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:49 TP0] Decode batch, #running-req: 32, #token: 31360, token usage: 0.03, cpu graph: True, gen throughput (token/s): 670.50, #queue-req: 96,
[2026-01-05 11:40:49] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 531/1319 [01:55<02:53,  4.55it/s][2026-01-05 11:40:49] INFO:     127.0.0.1:46614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:50] INFO:     127.0.0.1:48886 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 533/1319 [01:55<02:46,  4.71it/s][2026-01-05 11:40:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:50] INFO:     127.0.0.1:49064 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 534/1319 [01:56<02:56,  4.45it/s][2026-01-05 11:40:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:50] INFO:     127.0.0.1:49126 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 535/1319 [01:56<02:53,  4.51it/s][2026-01-05 11:40:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:50] INFO:     127.0.0.1:49112 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 536/1319 [01:56<03:02,  4.29it/s][2026-01-05 11:40:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:51 TP0] Decode batch, #running-req: 32, #token: 32000, token usage: 0.03, cpu graph: True, gen throughput (token/s): 825.60, #queue-req: 96,
[2026-01-05 11:40:51] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 537/1319 [01:57<03:30,  3.71it/s][2026-01-05 11:40:51] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:51] INFO:     127.0.0.1:48978 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 539/1319 [01:57<02:48,  4.62it/s][2026-01-05 11:40:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:51] INFO:     127.0.0.1:49108 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 540/1319 [01:57<02:42,  4.79it/s][2026-01-05 11:40:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:52] INFO:     127.0.0.1:53890 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 541/1319 [01:57<02:48,  4.61it/s][2026-01-05 11:40:52] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:52] INFO:     127.0.0.1:48954 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 543/1319 [01:58<02:15,  5.72it/s][2026-01-05 11:40:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:52] INFO:     127.0.0.1:53860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:52] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 544/1319 [01:58<02:30,  5.15it/s]
 41%|████▏     | 545/1319 [01:58<02:10,  5.93it/s][2026-01-05 11:40:52 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:52] INFO:     127.0.0.1:48986 - "POST /generate HTTP/1.1" 200 OK

 41%|████▏     | 546/1319 [01:58<02:22,  5.41it/s][2026-01-05 11:40:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:53 TP0] Decode batch, #running-req: 32, #token: 30464, token usage: 0.02, cpu graph: True, gen throughput (token/s): 706.49, #queue-req: 96,
[2026-01-05 11:40:53] INFO:     127.0.0.1:49080 - "POST /generate HTTP/1.1" 200 OK

 41%|████▏     | 547/1319 [01:58<02:42,  4.75it/s][2026-01-05 11:40:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:53] INFO:     127.0.0.1:53912 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 548/1319 [01:58<02:24,  5.32it/s][2026-01-05 11:40:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:53] INFO:     127.0.0.1:49138 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 549/1319 [01:59<02:38,  4.85it/s][2026-01-05 11:40:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:53] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:53] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 550/1319 [01:59<02:45,  4.64it/s]
 42%|████▏     | 551/1319 [01:59<02:13,  5.77it/s][2026-01-05 11:40:53 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:54] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 552/1319 [01:59<02:39,  4.81it/s][2026-01-05 11:40:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:54] INFO:     127.0.0.1:53978 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 553/1319 [01:59<02:30,  5.08it/s][2026-01-05 11:40:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:54] INFO:     127.0.0.1:49128 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:54] INFO:     127.0.0.1:54016 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 554/1319 [02:00<02:29,  5.13it/s]
 42%|████▏     | 555/1319 [02:00<01:55,  6.60it/s][2026-01-05 11:40:54 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:54] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 556/1319 [02:00<01:51,  6.87it/s][2026-01-05 11:40:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:54] INFO:     127.0.0.1:53820 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 557/1319 [02:00<02:01,  6.28it/s][2026-01-05 11:40:54] INFO:     127.0.0.1:49094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:54] INFO:     127.0.0.1:53948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:54 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:40:54 TP0] Decode batch, #running-req: 32, #token: 31360, token usage: 0.03, cpu graph: True, gen throughput (token/s): 670.98, #queue-req: 96,
[2026-01-05 11:40:55] INFO:     127.0.0.1:53940 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 560/1319 [02:00<01:55,  6.59it/s][2026-01-05 11:40:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:55] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 561/1319 [02:01<01:50,  6.85it/s][2026-01-05 11:40:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:55] INFO:     127.0.0.1:53928 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 562/1319 [02:01<01:50,  6.82it/s][2026-01-05 11:40:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:55] INFO:     127.0.0.1:53992 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 563/1319 [02:01<01:58,  6.37it/s][2026-01-05 11:40:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:56] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 564/1319 [02:01<02:49,  4.46it/s][2026-01-05 11:40:56] INFO:     127.0.0.1:48938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:56 TP0] Decode batch, #running-req: 32, #token: 32640, token usage: 0.03, cpu graph: True, gen throughput (token/s): 812.89, #queue-req: 96,
[2026-01-05 11:40:56] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 566/1319 [02:02<03:30,  3.58it/s][2026-01-05 11:40:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:56] INFO:     127.0.0.1:53846 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 567/1319 [02:02<03:03,  4.11it/s][2026-01-05 11:40:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:56] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 568/1319 [02:02<02:45,  4.55it/s][2026-01-05 11:40:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:57] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 569/1319 [02:03<03:26,  3.64it/s][2026-01-05 11:40:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:57] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 570/1319 [02:03<03:50,  3.25it/s][2026-01-05 11:40:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:57 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 868.88, #queue-req: 96,
[2026-01-05 11:40:58] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 571/1319 [02:03<03:29,  3.57it/s][2026-01-05 11:40:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:58] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 572/1319 [02:03<03:03,  4.07it/s][2026-01-05 11:40:58] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:58] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 574/1319 [02:04<03:05,  4.01it/s][2026-01-05 11:40:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:58] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 575/1319 [02:04<02:53,  4.28it/s][2026-01-05 11:40:58] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:59] INFO:     127.0.0.1:54184 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 577/1319 [02:04<02:30,  4.93it/s][2026-01-05 11:40:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:59] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 578/1319 [02:05<02:35,  4.76it/s][2026-01-05 11:40:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:59] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 579/1319 [02:05<02:26,  5.06it/s][2026-01-05 11:40:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:40:59 TP0] Decode batch, #running-req: 32, #token: 32256, token usage: 0.03, cpu graph: True, gen throughput (token/s): 716.44, #queue-req: 96,
[2026-01-05 11:40:59] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 580/1319 [02:05<02:43,  4.52it/s][2026-01-05 11:40:59] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:40:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:00] INFO:     127.0.0.1:54138 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 582/1319 [02:05<02:09,  5.69it/s][2026-01-05 11:41:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:00] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 583/1319 [02:06<02:08,  5.73it/s][2026-01-05 11:41:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:00] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 584/1319 [02:06<02:14,  5.46it/s][2026-01-05 11:41:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:00] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 585/1319 [02:06<03:15,  3.75it/s][2026-01-05 11:41:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:01] INFO:     127.0.0.1:45058 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 586/1319 [02:06<03:09,  3.88it/s][2026-01-05 11:41:01] INFO:     127.0.0.1:54108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:01] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:01] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:01 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, token usage: 0.02, #running-req: 29, #queue-req: 96,
[2026-01-05 11:41:01 TP0] Decode batch, #running-req: 29, #token: 30464, token usage: 0.02, cpu graph: True, gen throughput (token/s): 755.95, #queue-req: 96,
[2026-01-05 11:41:01] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:01] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK

 45%|████▍     | 590/1319 [02:07<01:39,  7.34it/s]
 45%|████▍     | 591/1319 [02:07<01:05, 11.17it/s][2026-01-05 11:41:01 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:01] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:02] INFO:     127.0.0.1:45070 - "POST /generate HTTP/1.1" 200 OK

 45%|████▍     | 593/1319 [02:07<01:42,  7.12it/s][2026-01-05 11:41:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:02] INFO:     127.0.0.1:45056 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 594/1319 [02:07<01:39,  7.25it/s][2026-01-05 11:41:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:02] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 595/1319 [02:08<02:03,  5.88it/s][2026-01-05 11:41:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:02] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 596/1319 [02:08<02:16,  5.31it/s][2026-01-05 11:41:02] INFO:     127.0.0.1:45142 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:03 TP0] Decode batch, #running-req: 32, #token: 32000, token usage: 0.03, cpu graph: True, gen throughput (token/s): 777.34, #queue-req: 96,
[2026-01-05 11:41:03] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 598/1319 [02:09<02:25,  4.95it/s][2026-01-05 11:41:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:03] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK

 45%|████▌     | 599/1319 [02:09<02:19,  5.17it/s][2026-01-05 11:41:03] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:03] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 601/1319 [02:09<02:06,  5.69it/s][2026-01-05 11:41:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:03] INFO:     127.0.0.1:45040 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 602/1319 [02:09<02:07,  5.64it/s][2026-01-05 11:41:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:04] INFO:     127.0.0.1:45194 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 603/1319 [02:09<02:08,  5.58it/s][2026-01-05 11:41:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:04] INFO:     127.0.0.1:45172 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 604/1319 [02:09<02:04,  5.73it/s][2026-01-05 11:41:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:04] INFO:     127.0.0.1:54084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:04] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 605/1319 [02:10<02:50,  4.20it/s]
 46%|████▌     | 606/1319 [02:10<02:41,  4.42it/s][2026-01-05 11:41:04 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:04 TP0] Decode batch, #running-req: 30, #token: 31232, token usage: 0.03, cpu graph: True, gen throughput (token/s): 758.64, #queue-req: 96,
[2026-01-05 11:41:04] INFO:     127.0.0.1:45262 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 607/1319 [02:10<02:46,  4.27it/s][2026-01-05 11:41:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:05] INFO:     127.0.0.1:45156 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 608/1319 [02:10<02:26,  4.86it/s][2026-01-05 11:41:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:05] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 609/1319 [02:10<02:07,  5.56it/s][2026-01-05 11:41:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:05] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 610/1319 [02:11<02:46,  4.25it/s][2026-01-05 11:41:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:05] INFO:     127.0.0.1:45264 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 611/1319 [02:11<03:02,  3.89it/s][2026-01-05 11:41:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:06] INFO:     127.0.0.1:45174 - "POST /generate HTTP/1.1" 200 OK

 46%|████▋     | 612/1319 [02:11<02:51,  4.12it/s][2026-01-05 11:41:06] INFO:     127.0.0.1:45144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:06] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:06] INFO:     127.0.0.1:45238 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 615/1319 [02:11<01:32,  7.60it/s][2026-01-05 11:41:06 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, token usage: 0.02, #running-req: 29, #queue-req: 96,
[2026-01-05 11:41:06 TP0] Decode batch, #running-req: 32, #token: 30976, token usage: 0.03, cpu graph: True, gen throughput (token/s): 782.51, #queue-req: 96,
[2026-01-05 11:41:06] INFO:     127.0.0.1:45246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:06] INFO:     127.0.0.1:45140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:06] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 617/1319 [02:12<01:42,  6.83it/s][2026-01-05 11:41:06 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:06] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 619/1319 [02:12<01:43,  6.74it/s][2026-01-05 11:41:06] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:07] INFO:     127.0.0.1:45104 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 621/1319 [02:12<01:40,  6.97it/s][2026-01-05 11:41:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:07] INFO:     127.0.0.1:45186 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 622/1319 [02:13<01:55,  6.01it/s][2026-01-05 11:41:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:07] INFO:     127.0.0.1:45298 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 623/1319 [02:13<02:02,  5.69it/s][2026-01-05 11:41:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:07] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 624/1319 [02:13<01:59,  5.84it/s][2026-01-05 11:41:07] INFO:     127.0.0.1:45336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:08 TP0] Decode batch, #running-req: 32, #token: 31872, token usage: 0.03, cpu graph: True, gen throughput (token/s): 717.89, #queue-req: 96,
[2026-01-05 11:41:08] INFO:     127.0.0.1:45284 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 626/1319 [02:13<02:16,  5.08it/s][2026-01-05 11:41:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:08] INFO:     127.0.0.1:45390 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 627/1319 [02:14<02:10,  5.32it/s][2026-01-05 11:41:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:08] INFO:     127.0.0.1:45290 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 628/1319 [02:14<02:09,  5.35it/s][2026-01-05 11:41:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:08] INFO:     127.0.0.1:45332 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 629/1319 [02:14<02:03,  5.57it/s][2026-01-05 11:41:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:08] INFO:     127.0.0.1:45120 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 630/1319 [02:14<01:52,  6.12it/s][2026-01-05 11:41:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:09] INFO:     127.0.0.1:45222 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 631/1319 [02:14<02:13,  5.16it/s][2026-01-05 11:41:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:09] INFO:     127.0.0.1:37894 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 632/1319 [02:14<02:06,  5.44it/s][2026-01-05 11:41:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:09] INFO:     127.0.0.1:45084 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 633/1319 [02:15<02:18,  4.94it/s][2026-01-05 11:41:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:09] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 634/1319 [02:15<02:02,  5.60it/s][2026-01-05 11:41:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:09] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 635/1319 [02:15<01:48,  6.29it/s][2026-01-05 11:41:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:09] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 636/1319 [02:15<01:38,  6.90it/s][2026-01-05 11:41:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:10] INFO:     127.0.0.1:45314 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 637/1319 [02:15<01:56,  5.87it/s][2026-01-05 11:41:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:10 TP0] Decode batch, #running-req: 31, #token: 31616, token usage: 0.03, cpu graph: True, gen throughput (token/s): 626.87, #queue-req: 96,
[2026-01-05 11:41:10] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 638/1319 [02:16<02:48,  4.04it/s][2026-01-05 11:41:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:10] INFO:     127.0.0.1:45426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:10] INFO:     127.0.0.1:37906 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 639/1319 [02:16<02:53,  3.93it/s]
 49%|████▊     | 640/1319 [02:16<02:15,  5.00it/s][2026-01-05 11:41:10] INFO:     127.0.0.1:45428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:10 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:11] INFO:     127.0.0.1:37994 - "POST /generate HTTP/1.1" 200 OK

 49%|████▊     | 642/1319 [02:16<02:05,  5.39it/s][2026-01-05 11:41:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:11] INFO:     127.0.0.1:37922 - "POST /generate HTTP/1.1" 200 OK

 49%|████▊     | 643/1319 [02:17<02:10,  5.17it/s][2026-01-05 11:41:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:11] INFO:     127.0.0.1:37976 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 644/1319 [02:17<02:22,  4.72it/s][2026-01-05 11:41:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:11 TP0] Decode batch, #running-req: 32, #token: 32000, token usage: 0.03, cpu graph: True, gen throughput (token/s): 802.61, #queue-req: 96,
[2026-01-05 11:41:11] INFO:     127.0.0.1:38000 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 645/1319 [02:17<02:55,  3.84it/s][2026-01-05 11:41:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:12] INFO:     127.0.0.1:37988 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 646/1319 [02:18<03:30,  3.20it/s][2026-01-05 11:41:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:12] INFO:     127.0.0.1:37946 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 647/1319 [02:18<03:26,  3.25it/s][2026-01-05 11:41:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:12] INFO:     127.0.0.1:45308 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 648/1319 [02:18<02:59,  3.74it/s][2026-01-05 11:41:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:13] INFO:     127.0.0.1:38114 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 649/1319 [02:18<02:38,  4.22it/s][2026-01-05 11:41:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:13 TP0] Decode batch, #running-req: 32, #token: 32896, token usage: 0.03, cpu graph: True, gen throughput (token/s): 859.58, #queue-req: 96,
[2026-01-05 11:41:13] INFO:     127.0.0.1:45148 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 650/1319 [02:19<03:06,  3.60it/s][2026-01-05 11:41:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:13] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 651/1319 [02:19<02:42,  4.10it/s][2026-01-05 11:41:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:13] INFO:     127.0.0.1:38062 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 652/1319 [02:19<02:42,  4.10it/s][2026-01-05 11:41:13] INFO:     127.0.0.1:45376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:13] INFO:     127.0.0.1:38116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:13] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 655/1319 [02:19<01:26,  7.66it/s][2026-01-05 11:41:13 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, token usage: 0.02, #running-req: 29, #queue-req: 96,
[2026-01-05 11:41:14] INFO:     127.0.0.1:38066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:14] INFO:     127.0.0.1:38134 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 657/1319 [02:19<01:17,  8.55it/s][2026-01-05 11:41:14 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:14] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:14] INFO:     127.0.0.1:38130 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 659/1319 [02:20<01:27,  7.52it/s][2026-01-05 11:41:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:14] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 660/1319 [02:20<01:51,  5.90it/s][2026-01-05 11:41:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:14 TP0] Decode batch, #running-req: 32, #token: 30848, token usage: 0.03, cpu graph: True, gen throughput (token/s): 728.95, #queue-req: 96,
[2026-01-05 11:41:14] INFO:     127.0.0.1:38058 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 661/1319 [02:20<01:54,  5.74it/s][2026-01-05 11:41:15] INFO:     127.0.0.1:45398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:15] INFO:     127.0.0.1:38082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:15 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:15] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:15] INFO:     127.0.0.1:38026 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 664/1319 [02:21<01:28,  7.38it/s]
 50%|█████     | 665/1319 [02:21<01:08,  9.58it/s][2026-01-05 11:41:15 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:15] INFO:     127.0.0.1:38166 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 666/1319 [02:21<01:16,  8.57it/s][2026-01-05 11:41:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:15] INFO:     127.0.0.1:38212 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 667/1319 [02:21<01:32,  7.02it/s][2026-01-05 11:41:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:16] INFO:     127.0.0.1:38230 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 668/1319 [02:22<02:22,  4.57it/s][2026-01-05 11:41:16] INFO:     127.0.0.1:38044 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:16] INFO:     127.0.0.1:38032 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 670/1319 [02:22<01:44,  6.20it/s][2026-01-05 11:41:16 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:16] INFO:     127.0.0.1:37966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:16] INFO:     127.0.0.1:38282 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 671/1319 [02:22<01:50,  5.86it/s]
 51%|█████     | 672/1319 [02:22<01:34,  6.86it/s][2026-01-05 11:41:16 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:16 TP0] Decode batch, #running-req: 30, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 741.85, #queue-req: 96,
[2026-01-05 11:41:16] INFO:     127.0.0.1:38152 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 673/1319 [02:22<01:36,  6.71it/s][2026-01-05 11:41:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:17] INFO:     127.0.0.1:38176 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 674/1319 [02:22<02:05,  5.13it/s][2026-01-05 11:41:17] INFO:     127.0.0.1:38222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:17] INFO:     127.0.0.1:37960 - "POST /generate HTTP/1.1" 200 OK

 51%|█████▏    | 676/1319 [02:23<02:00,  5.32it/s][2026-01-05 11:41:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:17] INFO:     127.0.0.1:38186 - "POST /generate HTTP/1.1" 200 OK

 51%|█████▏    | 677/1319 [02:23<02:12,  4.86it/s][2026-01-05 11:41:17] INFO:     127.0.0.1:38290 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:18] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK

 51%|█████▏    | 679/1319 [02:23<02:09,  4.93it/s][2026-01-05 11:41:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:18 TP0] Decode batch, #running-req: 32, #token: 32128, token usage: 0.03, cpu graph: True, gen throughput (token/s): 770.22, #queue-req: 96,
[2026-01-05 11:41:18] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 680/1319 [02:24<02:25,  4.38it/s][2026-01-05 11:41:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:18] INFO:     127.0.0.1:45354 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 681/1319 [02:24<02:58,  3.58it/s][2026-01-05 11:41:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:19] INFO:     127.0.0.1:38246 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 682/1319 [02:24<02:38,  4.03it/s][2026-01-05 11:41:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:19] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:19] INFO:     127.0.0.1:38198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:19] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 683/1319 [02:25<02:40,  3.95it/s]
 52%|█████▏    | 685/1319 [02:25<01:33,  6.80it/s]
 52%|█████▏    | 685/1319 [02:25<01:33,  6.80it/s][2026-01-05 11:41:19 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, token usage: 0.02, #running-req: 29, #queue-req: 96,
[2026-01-05 11:41:19] INFO:     127.0.0.1:38278 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 686/1319 [02:25<01:30,  6.98it/s][2026-01-05 11:41:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:19] INFO:     127.0.0.1:51218 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 687/1319 [02:25<01:47,  5.87it/s][2026-01-05 11:41:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:19 TP0] Decode batch, #running-req: 32, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 832.59, #queue-req: 96,
[2026-01-05 11:41:20] INFO:     127.0.0.1:38324 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 688/1319 [02:25<02:08,  4.91it/s][2026-01-05 11:41:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:20] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 689/1319 [02:25<01:55,  5.45it/s][2026-01-05 11:41:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:20] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 690/1319 [02:26<02:35,  4.05it/s][2026-01-05 11:41:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:20] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 691/1319 [02:26<02:33,  4.10it/s][2026-01-05 11:41:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:21] INFO:     127.0.0.1:38262 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 692/1319 [02:26<02:45,  3.79it/s][2026-01-05 11:41:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:21] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 693/1319 [02:26<02:19,  4.47it/s][2026-01-05 11:41:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:21 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 835.52, #queue-req: 96,
[2026-01-05 11:41:21] INFO:     127.0.0.1:38354 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 694/1319 [02:27<02:34,  4.05it/s][2026-01-05 11:41:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:21] INFO:     127.0.0.1:38364 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 695/1319 [02:27<02:11,  4.76it/s][2026-01-05 11:41:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:21] INFO:     127.0.0.1:38310 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 696/1319 [02:27<02:08,  4.86it/s][2026-01-05 11:41:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:22] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 697/1319 [02:27<02:13,  4.66it/s][2026-01-05 11:41:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:22] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 698/1319 [02:28<02:19,  4.45it/s][2026-01-05 11:41:22] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:22] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 700/1319 [02:28<01:56,  5.32it/s][2026-01-05 11:41:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:22] INFO:     127.0.0.1:38374 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 701/1319 [02:28<01:51,  5.53it/s][2026-01-05 11:41:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:22] INFO:     127.0.0.1:51430 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 702/1319 [02:28<01:47,  5.72it/s][2026-01-05 11:41:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:23 TP0] Decode batch, #running-req: 32, #token: 32640, token usage: 0.03, cpu graph: True, gen throughput (token/s): 715.82, #queue-req: 96,
[2026-01-05 11:41:23] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 703/1319 [02:28<02:06,  4.86it/s][2026-01-05 11:41:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:23] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 704/1319 [02:29<01:51,  5.49it/s][2026-01-05 11:41:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:23] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 705/1319 [02:29<01:58,  5.17it/s][2026-01-05 11:41:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:23] INFO:     127.0.0.1:38136 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 706/1319 [02:29<02:10,  4.69it/s][2026-01-05 11:41:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:24] INFO:     127.0.0.1:51358 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 707/1319 [02:29<02:05,  4.89it/s][2026-01-05 11:41:24] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:24] INFO:     127.0.0.1:51340 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 709/1319 [02:29<01:24,  7.19it/s][2026-01-05 11:41:24 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:24] INFO:     127.0.0.1:51288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:24] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 710/1319 [02:30<01:42,  5.95it/s]
 54%|█████▍    | 711/1319 [02:30<01:32,  6.59it/s][2026-01-05 11:41:24] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:24 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:24] INFO:     127.0.0.1:51402 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 713/1319 [02:30<01:48,  5.58it/s][2026-01-05 11:41:24 TP0] Decode batch, #running-req: 32, #token: 30848, token usage: 0.03, cpu graph: True, gen throughput (token/s): 748.22, #queue-req: 96,
[2026-01-05 11:41:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:25] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 714/1319 [02:30<02:02,  4.95it/s][2026-01-05 11:41:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:25] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 715/1319 [02:31<01:51,  5.41it/s][2026-01-05 11:41:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:25] INFO:     127.0.0.1:51394 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 716/1319 [02:31<02:05,  4.79it/s][2026-01-05 11:41:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:25] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 717/1319 [02:31<01:51,  5.38it/s][2026-01-05 11:41:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:25] INFO:     127.0.0.1:38258 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 718/1319 [02:31<01:44,  5.75it/s][2026-01-05 11:41:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:25] INFO:     127.0.0.1:51548 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 719/1319 [02:31<01:45,  5.69it/s][2026-01-05 11:41:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:26] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 720/1319 [02:31<01:46,  5.64it/s][2026-01-05 11:41:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:26] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 721/1319 [02:32<02:00,  4.95it/s][2026-01-05 11:41:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:26 TP0] Decode batch, #running-req: 32, #token: 32256, token usage: 0.03, cpu graph: True, gen throughput (token/s): 718.68, #queue-req: 96,
[2026-01-05 11:41:26] INFO:     127.0.0.1:38340 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 722/1319 [02:32<02:11,  4.55it/s][2026-01-05 11:41:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:26] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 723/1319 [02:32<02:18,  4.31it/s][2026-01-05 11:41:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:27] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 724/1319 [02:32<02:19,  4.27it/s][2026-01-05 11:41:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:27] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 725/1319 [02:33<02:51,  3.46it/s][2026-01-05 11:41:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:27] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 726/1319 [02:33<02:55,  3.37it/s][2026-01-05 11:41:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:28] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 727/1319 [02:33<02:24,  4.10it/s][2026-01-05 11:41:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:28 TP0] Decode batch, #running-req: 31, #token: 32128, token usage: 0.03, cpu graph: True, gen throughput (token/s): 842.30, #queue-req: 96,
[2026-01-05 11:41:28] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:28] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 728/1319 [02:34<02:50,  3.47it/s]
 55%|█████▌    | 729/1319 [02:34<02:24,  4.08it/s][2026-01-05 11:41:28] INFO:     127.0.0.1:48830 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:28 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:28] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 731/1319 [02:34<02:06,  4.65it/s][2026-01-05 11:41:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:28] INFO:     127.0.0.1:51474 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▌    | 732/1319 [02:34<02:02,  4.81it/s][2026-01-05 11:41:28] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:29] INFO:     127.0.0.1:51492 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 734/1319 [02:34<01:28,  6.64it/s][2026-01-05 11:41:29 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:29] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 735/1319 [02:35<01:33,  6.22it/s][2026-01-05 11:41:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:29] INFO:     127.0.0.1:48814 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 736/1319 [02:35<01:36,  6.04it/s][2026-01-05 11:41:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:29] INFO:     127.0.0.1:51462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:29] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 737/1319 [02:35<01:29,  6.47it/s]
 56%|█████▌    | 738/1319 [02:35<01:06,  8.68it/s][2026-01-05 11:41:29 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 95,
[2026-01-05 11:41:29] INFO:     127.0.0.1:51488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:29] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 740/1319 [02:35<01:04,  8.99it/s][2026-01-05 11:41:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:29] INFO:     127.0.0.1:51618 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 741/1319 [02:35<01:03,  9.10it/s][2026-01-05 11:41:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:30 TP0] Decode batch, #running-req: 32, #token: 31360, token usage: 0.03, cpu graph: True, gen throughput (token/s): 654.86, #queue-req: 96,
[2026-01-05 11:41:30] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 742/1319 [02:35<01:32,  6.27it/s][2026-01-05 11:41:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:30] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 743/1319 [02:36<01:31,  6.29it/s][2026-01-05 11:41:30] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:30] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 745/1319 [02:36<01:39,  5.78it/s][2026-01-05 11:41:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:30] INFO:     127.0.0.1:48934 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 746/1319 [02:36<01:41,  5.62it/s][2026-01-05 11:41:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:31] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 747/1319 [02:37<02:05,  4.54it/s][2026-01-05 11:41:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:31] INFO:     127.0.0.1:48966 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 748/1319 [02:37<02:20,  4.06it/s][2026-01-05 11:41:31 TP0] Decode batch, #running-req: 32, #token: 31104, token usage: 0.03, cpu graph: True, gen throughput (token/s): 814.05, #queue-req: 96,
[2026-01-05 11:41:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:31] INFO:     127.0.0.1:51506 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 749/1319 [02:37<02:15,  4.19it/s][2026-01-05 11:41:31] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:32] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 751/1319 [02:38<02:17,  4.13it/s][2026-01-05 11:41:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:32] INFO:     127.0.0.1:49046 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 752/1319 [02:38<02:45,  3.42it/s][2026-01-05 11:41:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:32] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 753/1319 [02:38<02:29,  3.77it/s][2026-01-05 11:41:32] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:33] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 755/1319 [02:38<01:40,  5.59it/s][2026-01-05 11:41:33 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:33 TP0] Decode batch, #running-req: 32, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 775.03, #queue-req: 96,
[2026-01-05 11:41:33] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 756/1319 [02:39<01:43,  5.46it/s][2026-01-05 11:41:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:33] INFO:     127.0.0.1:48834 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 757/1319 [02:39<01:34,  5.96it/s][2026-01-05 11:41:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:33] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 758/1319 [02:39<01:34,  5.94it/s][2026-01-05 11:41:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:33] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 759/1319 [02:39<01:35,  5.89it/s][2026-01-05 11:41:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:33] INFO:     127.0.0.1:49000 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 760/1319 [02:39<01:37,  5.73it/s][2026-01-05 11:41:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:34] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 761/1319 [02:39<01:39,  5.61it/s][2026-01-05 11:41:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:34] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 762/1319 [02:40<01:57,  4.72it/s][2026-01-05 11:41:34] INFO:     127.0.0.1:49026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:34] INFO:     127.0.0.1:48846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:34] INFO:     127.0.0.1:48868 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 764/1319 [02:40<01:19,  6.99it/s]
 58%|█████▊    | 765/1319 [02:40<00:51, 10.82it/s][2026-01-05 11:41:34 TP0] Prefill batch, #new-seq: 3, #new-token: 2688, #cached-token: 0, token usage: 0.02, #running-req: 29, #queue-req: 96,
[2026-01-05 11:41:34] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:34] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 767/1319 [02:40<00:44, 12.50it/s][2026-01-05 11:41:34 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:35 TP0] Decode batch, #running-req: 32, #token: 31616, token usage: 0.03, cpu graph: True, gen throughput (token/s): 704.89, #queue-req: 96,
[2026-01-05 11:41:35] INFO:     127.0.0.1:49062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:35] INFO:     127.0.0.1:49104 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 769/1319 [02:41<01:38,  5.60it/s][2026-01-05 11:41:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:35] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:35] INFO:     127.0.0.1:48980 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 771/1319 [02:41<01:51,  4.93it/s][2026-01-05 11:41:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:36] INFO:     127.0.0.1:49130 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▊    | 772/1319 [02:41<01:49,  5.00it/s][2026-01-05 11:41:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:36] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▊    | 773/1319 [02:42<01:45,  5.20it/s][2026-01-05 11:41:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:36] INFO:     127.0.0.1:48964 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▊    | 774/1319 [02:42<01:35,  5.70it/s][2026-01-05 11:41:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:36] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 775/1319 [02:42<01:33,  5.79it/s][2026-01-05 11:41:36] INFO:     127.0.0.1:49012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:36 TP0] Decode batch, #running-req: 32, #token: 31872, token usage: 0.03, cpu graph: True, gen throughput (token/s): 707.53, #queue-req: 96,
[2026-01-05 11:41:36] INFO:     127.0.0.1:49204 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 777/1319 [02:42<01:37,  5.57it/s][2026-01-05 11:41:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:37] INFO:     127.0.0.1:49088 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 778/1319 [02:43<02:13,  4.07it/s][2026-01-05 11:41:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:37] INFO:     127.0.0.1:49198 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 779/1319 [02:43<02:08,  4.20it/s][2026-01-05 11:41:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:37] INFO:     127.0.0.1:49162 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:37] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 780/1319 [02:43<01:52,  4.79it/s]
 59%|█████▉    | 781/1319 [02:43<01:18,  6.83it/s][2026-01-05 11:41:37 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 95,
[2026-01-05 11:41:37] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 782/1319 [02:43<01:18,  6.80it/s][2026-01-05 11:41:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:38] INFO:     127.0.0.1:49070 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 783/1319 [02:43<01:24,  6.35it/s][2026-01-05 11:41:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:38] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 784/1319 [02:44<01:36,  5.54it/s][2026-01-05 11:41:38] INFO:     127.0.0.1:49202 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:38 TP0] Decode batch, #running-req: 31, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 744.75, #queue-req: 96,
[2026-01-05 11:41:38] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 786/1319 [02:44<01:35,  5.61it/s][2026-01-05 11:41:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:38] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 787/1319 [02:44<01:27,  6.05it/s][2026-01-05 11:41:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:39] INFO:     127.0.0.1:49138 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 788/1319 [02:45<02:08,  4.12it/s][2026-01-05 11:41:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:39] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 789/1319 [02:45<01:52,  4.71it/s][2026-01-05 11:41:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:39] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 790/1319 [02:45<01:37,  5.41it/s][2026-01-05 11:41:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:39] INFO:     127.0.0.1:48920 - "POST /generate HTTP/1.1" 200 OK

 60%|█████▉    | 791/1319 [02:45<02:08,  4.12it/s][2026-01-05 11:41:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:40] INFO:     127.0.0.1:49230 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 792/1319 [02:45<02:03,  4.26it/s][2026-01-05 11:41:40 TP0] Decode batch, #running-req: 32, #token: 31360, token usage: 0.03, cpu graph: True, gen throughput (token/s): 804.45, #queue-req: 96,
[2026-01-05 11:41:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:40] INFO:     127.0.0.1:36558 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 793/1319 [02:46<02:21,  3.73it/s][2026-01-05 11:41:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:40] INFO:     127.0.0.1:36600 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 794/1319 [02:46<02:20,  3.73it/s][2026-01-05 11:41:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:40] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 795/1319 [02:46<01:57,  4.45it/s][2026-01-05 11:41:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:41] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 796/1319 [02:46<01:39,  5.24it/s][2026-01-05 11:41:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:41] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 797/1319 [02:47<01:54,  4.57it/s][2026-01-05 11:41:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:41] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 798/1319 [02:47<01:46,  4.91it/s][2026-01-05 11:41:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:41] INFO:     127.0.0.1:36594 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 799/1319 [02:47<01:33,  5.57it/s][2026-01-05 11:41:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:41] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 800/1319 [02:47<01:54,  4.53it/s][2026-01-05 11:41:41 TP0] Decode batch, #running-req: 32, #token: 31488, token usage: 0.03, cpu graph: True, gen throughput (token/s): 729.21, #queue-req: 96,
[2026-01-05 11:41:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:42] INFO:     127.0.0.1:36658 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 801/1319 [02:48<02:43,  3.16it/s][2026-01-05 11:41:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:42] INFO:     127.0.0.1:36682 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 802/1319 [02:48<02:13,  3.88it/s][2026-01-05 11:41:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:43] INFO:     127.0.0.1:36642 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 803/1319 [02:48<02:58,  2.89it/s][2026-01-05 11:41:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:43] INFO:     127.0.0.1:36544 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 804/1319 [02:49<02:33,  3.35it/s][2026-01-05 11:41:43 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 912.27, #queue-req: 96,
[2026-01-05 11:41:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:43] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 805/1319 [02:49<02:12,  3.88it/s][2026-01-05 11:41:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:43] INFO:     127.0.0.1:36762 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 806/1319 [02:49<02:09,  3.95it/s][2026-01-05 11:41:43] INFO:     127.0.0.1:36748 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:43] INFO:     127.0.0.1:36570 - "POST /generate HTTP/1.1" 200 OK

 61%|██████▏   | 808/1319 [02:49<01:37,  5.26it/s][2026-01-05 11:41:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:44] INFO:     127.0.0.1:36720 - "POST /generate HTTP/1.1" 200 OK

 61%|██████▏   | 809/1319 [02:49<01:38,  5.15it/s][2026-01-05 11:41:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:44] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK

 61%|██████▏   | 810/1319 [02:50<01:28,  5.73it/s][2026-01-05 11:41:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:44] INFO:     127.0.0.1:36750 - "POST /generate HTTP/1.1" 200 OK

 61%|██████▏   | 811/1319 [02:50<01:45,  4.80it/s][2026-01-05 11:41:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:44] INFO:     127.0.0.1:36632 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 812/1319 [02:50<01:33,  5.44it/s][2026-01-05 11:41:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:45] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:45] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 813/1319 [02:50<01:55,  4.39it/s]
 62%|██████▏   | 814/1319 [02:50<01:41,  4.96it/s][2026-01-05 11:41:45 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:45 TP0] Decode batch, #running-req: 32, #token: 32768, token usage: 0.03, cpu graph: True, gen throughput (token/s): 663.59, #queue-req: 96,
[2026-01-05 11:41:45] INFO:     127.0.0.1:36686 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 815/1319 [02:51<01:49,  4.59it/s][2026-01-05 11:41:45 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:45] INFO:     127.0.0.1:36778 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 816/1319 [02:51<01:56,  4.33it/s][2026-01-05 11:41:45 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:45] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 817/1319 [02:51<02:11,  3.82it/s][2026-01-05 11:41:45 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:46] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 818/1319 [02:52<02:30,  3.32it/s][2026-01-05 11:41:46] INFO:     127.0.0.1:36804 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:46] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 820/1319 [02:52<02:02,  4.07it/s][2026-01-05 11:41:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:46 TP0] Decode batch, #running-req: 31, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 826.27, #queue-req: 96,
[2026-01-05 11:41:46] INFO:     127.0.0.1:36838 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 821/1319 [02:52<02:01,  4.09it/s][2026-01-05 11:41:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:47] INFO:     127.0.0.1:36944 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 822/1319 [02:52<02:04,  3.99it/s][2026-01-05 11:41:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:47] INFO:     127.0.0.1:36888 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 823/1319 [02:53<02:00,  4.12it/s][2026-01-05 11:41:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:47] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 824/1319 [02:53<02:14,  3.69it/s][2026-01-05 11:41:47] INFO:     127.0.0.1:36898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:48 TP0] Decode batch, #running-req: 32, #token: 33664, token usage: 0.03, cpu graph: True, gen throughput (token/s): 855.85, #queue-req: 96,
[2026-01-05 11:41:48] INFO:     127.0.0.1:36586 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 826/1319 [02:54<02:21,  3.49it/s][2026-01-05 11:41:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:48] INFO:     127.0.0.1:36840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:48] INFO:     127.0.0.1:36914 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 827/1319 [02:54<02:01,  4.04it/s][2026-01-05 11:41:48 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:48] INFO:     127.0.0.1:36672 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 829/1319 [02:54<01:32,  5.31it/s][2026-01-05 11:41:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:49] INFO:     127.0.0.1:36866 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 830/1319 [02:54<01:55,  4.24it/s][2026-01-05 11:41:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:49] INFO:     127.0.0.1:36930 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 831/1319 [02:54<01:46,  4.58it/s][2026-01-05 11:41:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:49] INFO:     127.0.0.1:36832 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 832/1319 [02:55<01:52,  4.32it/s][2026-01-05 11:41:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:49] INFO:     127.0.0.1:36880 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 833/1319 [02:55<01:53,  4.27it/s][2026-01-05 11:41:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:49] INFO:     127.0.0.1:36912 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 834/1319 [02:55<01:38,  4.93it/s][2026-01-05 11:41:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:49 TP0] Decode batch, #running-req: 31, #token: 33024, token usage: 0.03, cpu graph: True, gen throughput (token/s): 742.87, #queue-req: 96,
[2026-01-05 11:41:50] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 835/1319 [02:55<01:31,  5.30it/s][2026-01-05 11:41:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:50] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 836/1319 [02:56<01:42,  4.71it/s][2026-01-05 11:41:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:50] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK

 63%|██████▎   | 837/1319 [02:56<01:57,  4.09it/s][2026-01-05 11:41:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:50] INFO:     127.0.0.1:36946 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 838/1319 [02:56<01:49,  4.38it/s][2026-01-05 11:41:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:51] INFO:     127.0.0.1:36970 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 839/1319 [02:56<01:51,  4.29it/s][2026-01-05 11:41:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:51 TP0] Decode batch, #running-req: 32, #token: 33664, token usage: 0.03, cpu graph: True, gen throughput (token/s): 852.63, #queue-req: 96,
[2026-01-05 11:41:51] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 840/1319 [02:57<02:22,  3.36it/s][2026-01-05 11:41:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:51] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 841/1319 [02:57<02:18,  3.45it/s][2026-01-05 11:41:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:51] INFO:     127.0.0.1:36854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:51] INFO:     127.0.0.1:36890 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 842/1319 [02:57<01:54,  4.17it/s]
 64%|██████▍   | 843/1319 [02:57<01:14,  6.36it/s][2026-01-05 11:41:51 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:52] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 844/1319 [02:57<01:23,  5.72it/s][2026-01-05 11:41:52] INFO:     127.0.0.1:36978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:52] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 846/1319 [02:58<01:00,  7.84it/s][2026-01-05 11:41:52 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:52] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:52] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 848/1319 [02:58<01:38,  4.80it/s][2026-01-05 11:41:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:53] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 849/1319 [02:58<01:33,  5.03it/s][2026-01-05 11:41:53 TP0] Decode batch, #running-req: 32, #token: 30720, token usage: 0.03, cpu graph: True, gen throughput (token/s): 771.56, #queue-req: 96,
[2026-01-05 11:41:53] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:53] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▍   | 851/1319 [02:59<01:18,  5.95it/s][2026-01-05 11:41:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:53] INFO:     127.0.0.1:36808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:53] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▍   | 852/1319 [02:59<01:13,  6.38it/s]
 65%|██████▍   | 853/1319 [02:59<00:55,  8.38it/s][2026-01-05 11:41:53 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:53] INFO:     127.0.0.1:36954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:53] INFO:     127.0.0.1:55224 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▍   | 855/1319 [02:59<01:12,  6.37it/s][2026-01-05 11:41:53] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:54] INFO:     127.0.0.1:55166 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▍   | 857/1319 [02:59<01:13,  6.27it/s][2026-01-05 11:41:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:54] INFO:     127.0.0.1:55242 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 858/1319 [03:00<01:16,  6.04it/s][2026-01-05 11:41:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:54] INFO:     127.0.0.1:55240 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 859/1319 [03:00<01:18,  5.86it/s][2026-01-05 11:41:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:54] INFO:     127.0.0.1:55360 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 860/1319 [03:00<01:12,  6.29it/s][2026-01-05 11:41:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:55] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 861/1319 [03:00<01:27,  5.24it/s][2026-01-05 11:41:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:55 TP0] Decode batch, #running-req: 32, #token: 31872, token usage: 0.03, cpu graph: True, gen throughput (token/s): 608.99, #queue-req: 96,
[2026-01-05 11:41:55] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 862/1319 [03:01<01:33,  4.90it/s][2026-01-05 11:41:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:55] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 863/1319 [03:01<01:31,  5.00it/s][2026-01-05 11:41:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:55] INFO:     127.0.0.1:36712 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 864/1319 [03:01<01:36,  4.73it/s][2026-01-05 11:41:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:55] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 865/1319 [03:01<01:36,  4.71it/s][2026-01-05 11:41:55] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:56] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 867/1319 [03:02<01:28,  5.13it/s][2026-01-05 11:41:56] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:56] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 869/1319 [03:02<01:30,  4.97it/s][2026-01-05 11:41:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:56 TP0] Decode batch, #running-req: 32, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 748.17, #queue-req: 96,
[2026-01-05 11:41:57] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 870/1319 [03:02<01:46,  4.22it/s][2026-01-05 11:41:57] INFO:     127.0.0.1:36846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:57] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 872/1319 [03:02<01:15,  5.91it/s][2026-01-05 11:41:57 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:57] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 873/1319 [03:03<01:09,  6.42it/s][2026-01-05 11:41:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:57] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 874/1319 [03:03<01:19,  5.59it/s][2026-01-05 11:41:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:57] INFO:     127.0.0.1:55540 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 875/1319 [03:03<01:26,  5.15it/s][2026-01-05 11:41:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:57] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:57] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 876/1319 [03:03<01:31,  4.87it/s]
 66%|██████▋   | 877/1319 [03:03<01:14,  5.95it/s][2026-01-05 11:41:58 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:58] INFO:     127.0.0.1:55504 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 878/1319 [03:03<01:09,  6.37it/s][2026-01-05 11:41:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:58] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 879/1319 [03:04<01:24,  5.18it/s][2026-01-05 11:41:58] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:58] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:58 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:58 TP0] Decode batch, #running-req: 32, #token: 31616, token usage: 0.03, cpu graph: True, gen throughput (token/s): 710.26, #queue-req: 96,
[2026-01-05 11:41:58] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 882/1319 [03:04<01:13,  5.98it/s][2026-01-05 11:41:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:58] INFO:     127.0.0.1:55562 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 883/1319 [03:04<01:12,  6.04it/s][2026-01-05 11:41:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:59] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 884/1319 [03:04<01:07,  6.44it/s][2026-01-05 11:41:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:59] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:41:59] INFO:     127.0.0.1:55594 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 885/1319 [03:05<01:06,  6.56it/s]
 67%|██████▋   | 886/1319 [03:05<00:51,  8.35it/s][2026-01-05 11:41:59 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:41:59] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 887/1319 [03:05<00:51,  8.38it/s][2026-01-05 11:41:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:59] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 888/1319 [03:05<00:50,  8.58it/s][2026-01-05 11:41:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:59] INFO:     127.0.0.1:55572 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 889/1319 [03:05<00:58,  7.32it/s][2026-01-05 11:41:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:41:59] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 890/1319 [03:05<01:09,  6.13it/s][2026-01-05 11:41:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:00] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 891/1319 [03:05<01:18,  5.45it/s][2026-01-05 11:42:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:00] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 892/1319 [03:06<01:21,  5.25it/s][2026-01-05 11:42:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:00 TP0] Decode batch, #running-req: 32, #token: 31488, token usage: 0.03, cpu graph: True, gen throughput (token/s): 692.64, #queue-req: 96,
[2026-01-05 11:42:00] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 893/1319 [03:06<01:23,  5.12it/s][2026-01-05 11:42:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:00] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:00] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:00] INFO:     127.0.0.1:52676 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 894/1319 [03:06<01:34,  4.50it/s]
 68%|██████▊   | 896/1319 [03:06<01:00,  7.05it/s]
 68%|██████▊   | 896/1319 [03:06<01:00,  7.05it/s][2026-01-05 11:42:00 TP0] Prefill batch, #new-seq: 3, #new-token: 2816, #cached-token: 0, token usage: 0.02, #running-req: 29, #queue-req: 96,
[2026-01-05 11:42:01] INFO:     127.0.0.1:52652 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 897/1319 [03:07<01:20,  5.25it/s][2026-01-05 11:42:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:01] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 898/1319 [03:07<01:13,  5.71it/s][2026-01-05 11:42:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:01] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 899/1319 [03:07<01:21,  5.18it/s][2026-01-05 11:42:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:01] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 900/1319 [03:07<01:30,  4.61it/s][2026-01-05 11:42:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:02 TP0] Decode batch, #running-req: 31, #token: 32128, token usage: 0.03, cpu graph: True, gen throughput (token/s): 840.23, #queue-req: 96,
[2026-01-05 11:42:02] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 901/1319 [03:07<01:26,  4.81it/s][2026-01-05 11:42:02] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:02] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 903/1319 [03:08<01:33,  4.43it/s][2026-01-05 11:42:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:02] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▊   | 904/1319 [03:08<01:29,  4.62it/s][2026-01-05 11:42:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:02] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▊   | 905/1319 [03:08<01:26,  4.80it/s][2026-01-05 11:42:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:03] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▊   | 906/1319 [03:08<01:23,  4.94it/s][2026-01-05 11:42:03] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:03] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 908/1319 [03:09<01:15,  5.45it/s][2026-01-05 11:42:03] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:03] INFO:     127.0.0.1:52644 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 910/1319 [03:09<01:04,  6.34it/s][2026-01-05 11:42:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:03] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 911/1319 [03:09<01:05,  6.22it/s][2026-01-05 11:42:03 TP0] Decode batch, #running-req: 32, #token: 30848, token usage: 0.03, cpu graph: True, gen throughput (token/s): 680.75, #queue-req: 96,
[2026-01-05 11:42:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:03] INFO:     127.0.0.1:52828 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 912/1319 [03:09<01:01,  6.60it/s][2026-01-05 11:42:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:04] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 913/1319 [03:09<01:03,  6.38it/s][2026-01-05 11:42:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:04] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 914/1319 [03:10<01:06,  6.11it/s][2026-01-05 11:42:04] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:04] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 916/1319 [03:10<01:07,  6.01it/s][2026-01-05 11:42:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:04] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 917/1319 [03:10<01:08,  5.87it/s][2026-01-05 11:42:04] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:04] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 919/1319 [03:10<00:50,  7.96it/s][2026-01-05 11:42:04 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:05] INFO:     127.0.0.1:52944 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 920/1319 [03:11<01:17,  5.13it/s][2026-01-05 11:42:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:05 TP0] Decode batch, #running-req: 32, #token: 32128, token usage: 0.03, cpu graph: True, gen throughput (token/s): 720.76, #queue-req: 96,
[2026-01-05 11:42:05] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 921/1319 [03:11<01:23,  4.74it/s][2026-01-05 11:42:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:06] INFO:     127.0.0.1:52840 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 922/1319 [03:12<02:14,  2.95it/s][2026-01-05 11:42:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:06] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 923/1319 [03:12<02:25,  2.71it/s][2026-01-05 11:42:06 TP0] Decode batch, #running-req: 32, #token: 32256, token usage: 0.03, cpu graph: True, gen throughput (token/s): 1053.45, #queue-req: 96,
[2026-01-05 11:42:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:07] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 924/1319 [03:12<02:11,  3.01it/s][2026-01-05 11:42:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:07] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 925/1319 [03:12<01:47,  3.67it/s][2026-01-05 11:42:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:07] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:07] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 926/1319 [03:13<01:41,  3.88it/s]
 70%|███████   | 927/1319 [03:13<01:15,  5.22it/s][2026-01-05 11:42:07] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:07 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:07] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 929/1319 [03:13<01:07,  5.76it/s][2026-01-05 11:42:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:07] INFO:     127.0.0.1:47094 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 930/1319 [03:13<01:03,  6.16it/s][2026-01-05 11:42:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:07] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:07] INFO:     127.0.0.1:47076 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 931/1319 [03:13<00:57,  6.71it/s]
 71%|███████   | 932/1319 [03:13<00:42,  9.07it/s][2026-01-05 11:42:07 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:08] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:08] INFO:     127.0.0.1:55588 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 934/1319 [03:14<01:08,  5.59it/s][2026-01-05 11:42:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:08] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 935/1319 [03:14<01:07,  5.72it/s][2026-01-05 11:42:08 TP0] Decode batch, #running-req: 32, #token: 31488, token usage: 0.03, cpu graph: True, gen throughput (token/s): 683.21, #queue-req: 96,
[2026-01-05 11:42:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:08] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:08] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 936/1319 [03:14<01:05,  5.85it/s]
 71%|███████   | 937/1319 [03:14<00:51,  7.42it/s][2026-01-05 11:42:08 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:09] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 938/1319 [03:14<01:11,  5.35it/s][2026-01-05 11:42:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:09] INFO:     127.0.0.1:53042 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 939/1319 [03:15<01:08,  5.55it/s][2026-01-05 11:42:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:09] INFO:     127.0.0.1:53072 - "POST /generate HTTP/1.1" 200 OK

 71%|███████▏  | 940/1319 [03:15<01:16,  4.98it/s][2026-01-05 11:42:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:09] INFO:     127.0.0.1:53022 - "POST /generate HTTP/1.1" 200 OK

 71%|███████▏  | 941/1319 [03:15<01:12,  5.19it/s][2026-01-05 11:42:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:09] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK

 71%|███████▏  | 942/1319 [03:15<01:05,  5.80it/s][2026-01-05 11:42:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:10] INFO:     127.0.0.1:52802 - "POST /generate HTTP/1.1" 200 OK

 71%|███████▏  | 943/1319 [03:15<01:10,  5.37it/s][2026-01-05 11:42:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:10] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 944/1319 [03:16<01:11,  5.28it/s][2026-01-05 11:42:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:10 TP0] Decode batch, #running-req: 32, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 694.36, #queue-req: 96,
[2026-01-05 11:42:10] INFO:     127.0.0.1:47164 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 945/1319 [03:16<01:17,  4.83it/s][2026-01-05 11:42:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:10] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 946/1319 [03:16<01:30,  4.12it/s][2026-01-05 11:42:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:11] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 947/1319 [03:17<01:56,  3.18it/s][2026-01-05 11:42:11] INFO:     127.0.0.1:47172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:11] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 949/1319 [03:17<01:13,  5.06it/s][2026-01-05 11:42:11 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:11] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 950/1319 [03:17<01:15,  4.89it/s][2026-01-05 11:42:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:12] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 951/1319 [03:17<01:19,  4.62it/s][2026-01-05 11:42:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:12 TP0] Decode batch, #running-req: 31, #token: 31616, token usage: 0.03, cpu graph: True, gen throughput (token/s): 805.26, #queue-req: 96,
[2026-01-05 11:42:12] INFO:     127.0.0.1:52922 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 952/1319 [03:17<01:09,  5.26it/s][2026-01-05 11:42:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:12] INFO:     127.0.0.1:47184 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 953/1319 [03:18<01:18,  4.69it/s][2026-01-05 11:42:12] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:12] INFO:     127.0.0.1:47204 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 955/1319 [03:18<01:29,  4.06it/s][2026-01-05 11:42:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:13] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 956/1319 [03:18<01:24,  4.28it/s][2026-01-05 11:42:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:13] INFO:     127.0.0.1:47070 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 957/1319 [03:19<01:18,  4.60it/s][2026-01-05 11:42:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:13] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 958/1319 [03:19<01:19,  4.57it/s][2026-01-05 11:42:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:13] INFO:     127.0.0.1:47126 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 959/1319 [03:19<01:16,  4.70it/s][2026-01-05 11:42:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:13 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 706.28, #queue-req: 96,
[2026-01-05 11:42:14] INFO:     127.0.0.1:47300 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 960/1319 [03:20<01:44,  3.44it/s][2026-01-05 11:42:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:14] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 961/1319 [03:20<01:37,  3.68it/s][2026-01-05 11:42:14] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:14] INFO:     127.0.0.1:47254 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 963/1319 [03:20<01:20,  4.42it/s][2026-01-05 11:42:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:15] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 964/1319 [03:20<01:31,  3.88it/s][2026-01-05 11:42:15] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:15] INFO:     127.0.0.1:47358 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 966/1319 [03:21<01:02,  5.67it/s][2026-01-05 11:42:15 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:15] INFO:     127.0.0.1:47266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:15] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 967/1319 [03:21<01:03,  5.51it/s]
 73%|███████▎  | 968/1319 [03:21<00:52,  6.70it/s][2026-01-05 11:42:15] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:15] INFO:     127.0.0.1:47386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:15 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:15 TP0] Decode batch, #running-req: 30, #token: 30976, token usage: 0.03, cpu graph: True, gen throughput (token/s): 756.78, #queue-req: 96,
[2026-01-05 11:42:15 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:15] INFO:     127.0.0.1:47188 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 971/1319 [03:21<00:47,  7.27it/s][2026-01-05 11:42:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:16] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 972/1319 [03:21<00:59,  5.82it/s][2026-01-05 11:42:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:16] INFO:     127.0.0.1:47030 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 973/1319 [03:22<00:55,  6.21it/s][2026-01-05 11:42:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:16] INFO:     127.0.0.1:47306 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 974/1319 [03:22<00:56,  6.15it/s][2026-01-05 11:42:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:16] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 975/1319 [03:22<01:03,  5.45it/s][2026-01-05 11:42:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:16] INFO:     127.0.0.1:47362 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 976/1319 [03:22<01:01,  5.55it/s][2026-01-05 11:42:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:17] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:17] INFO:     127.0.0.1:47238 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 977/1319 [03:22<01:00,  5.63it/s][2026-01-05 11:42:17 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:17] INFO:     127.0.0.1:47280 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 979/1319 [03:22<00:42,  7.93it/s][2026-01-05 11:42:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:17 TP0] Decode batch, #running-req: 32, #token: 32128, token usage: 0.03, cpu graph: True, gen throughput (token/s): 667.57, #queue-req: 96,
[2026-01-05 11:42:18] INFO:     127.0.0.1:52954 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 980/1319 [03:23<01:47,  3.15it/s][2026-01-05 11:42:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:18] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 981/1319 [03:24<02:03,  2.73it/s][2026-01-05 11:42:18 TP0] Decode batch, #running-req: 32, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 1118.21, #queue-req: 96,
[2026-01-05 11:42:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:18] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 982/1319 [03:24<02:02,  2.75it/s][2026-01-05 11:42:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:19] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▍  | 983/1319 [03:24<01:44,  3.23it/s][2026-01-05 11:42:19] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:19] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▍  | 985/1319 [03:25<01:19,  4.22it/s][2026-01-05 11:42:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:19] INFO:     127.0.0.1:47346 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▍  | 986/1319 [03:25<01:21,  4.06it/s][2026-01-05 11:42:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:19] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▍  | 987/1319 [03:25<01:16,  4.35it/s][2026-01-05 11:42:19] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:19] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:20 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:20] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 990/1319 [03:25<00:51,  6.40it/s][2026-01-05 11:42:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:20] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 991/1319 [03:26<00:48,  6.70it/s][2026-01-05 11:42:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:20] INFO:     127.0.0.1:47312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:20] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 992/1319 [03:26<00:48,  6.70it/s]
 75%|███████▌  | 993/1319 [03:26<00:39,  8.30it/s][2026-01-05 11:42:20 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:20 TP0] Decode batch, #running-req: 32, #token: 31872, token usage: 0.03, cpu graph: True, gen throughput (token/s): 636.97, #queue-req: 96,
[2026-01-05 11:42:20] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 994/1319 [03:26<00:58,  5.57it/s][2026-01-05 11:42:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:21] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 995/1319 [03:26<00:58,  5.53it/s][2026-01-05 11:42:21] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:21] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 997/1319 [03:26<00:42,  7.59it/s][2026-01-05 11:42:21 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:21] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 998/1319 [03:27<00:50,  6.33it/s][2026-01-05 11:42:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:21] INFO:     127.0.0.1:47210 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 999/1319 [03:27<01:11,  4.51it/s][2026-01-05 11:42:21] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:22] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 1001/1319 [03:27<00:56,  5.64it/s][2026-01-05 11:42:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:22 TP0] Decode batch, #running-req: 32, #token: 32256, token usage: 0.03, cpu graph: True, gen throughput (token/s): 792.48, #queue-req: 96,
[2026-01-05 11:42:22] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 1002/1319 [03:28<01:16,  4.15it/s][2026-01-05 11:42:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:22] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 1003/1319 [03:28<01:22,  3.85it/s][2026-01-05 11:42:22] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:23] INFO:     127.0.0.1:55916 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 1005/1319 [03:28<01:02,  5.02it/s][2026-01-05 11:42:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:23] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:23] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 1006/1319 [03:29<01:15,  4.15it/s]
 76%|███████▋  | 1007/1319 [03:29<01:09,  4.52it/s][2026-01-05 11:42:23 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:23] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 1008/1319 [03:29<01:10,  4.42it/s][2026-01-05 11:42:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:23 TP0] Decode batch, #running-req: 31, #token: 32896, token usage: 0.03, cpu graph: True, gen throughput (token/s): 831.44, #queue-req: 96,
[2026-01-05 11:42:24] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 1009/1319 [03:29<01:24,  3.66it/s][2026-01-05 11:42:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:24] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 1010/1319 [03:30<01:40,  3.07it/s][2026-01-05 11:42:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:24] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 1011/1319 [03:30<01:35,  3.24it/s][2026-01-05 11:42:24] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:25 TP0] Decode batch, #running-req: 32, #token: 32640, token usage: 0.03, cpu graph: True, gen throughput (token/s): 920.63, #queue-req: 96,
[2026-01-05 11:42:25] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 1013/1319 [03:30<01:19,  3.83it/s][2026-01-05 11:42:25] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:25] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 1015/1319 [03:31<01:04,  4.71it/s][2026-01-05 11:42:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:25] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 1016/1319 [03:31<01:04,  4.71it/s][2026-01-05 11:42:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:25] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 1017/1319 [03:31<01:00,  5.00it/s][2026-01-05 11:42:25] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:25 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:26] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 1019/1319 [03:31<00:55,  5.43it/s][2026-01-05 11:42:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:26] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 1020/1319 [03:32<00:53,  5.59it/s][2026-01-05 11:42:26] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:26] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 1022/1319 [03:32<00:51,  5.77it/s][2026-01-05 11:42:26 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:27 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 677.14, #queue-req: 96,
[2026-01-05 11:42:27] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 1023/1319 [03:32<01:10,  4.18it/s][2026-01-05 11:42:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:27] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 1024/1319 [03:33<01:04,  4.54it/s][2026-01-05 11:42:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:27] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 1025/1319 [03:33<01:33,  3.14it/s][2026-01-05 11:42:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:28] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 1026/1319 [03:34<01:35,  3.08it/s][2026-01-05 11:42:28] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:28 TP0] Decode batch, #running-req: 31, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 883.20, #queue-req: 96,
[2026-01-05 11:42:28] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 1028/1319 [03:34<01:08,  4.27it/s][2026-01-05 11:42:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:28] INFO:     127.0.0.1:56228 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 1029/1319 [03:34<01:07,  4.32it/s][2026-01-05 11:42:28 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:28] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:28] INFO:     127.0.0.1:56266 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 1030/1319 [03:34<00:58,  4.91it/s]
 78%|███████▊  | 1031/1319 [03:34<00:41,  6.94it/s][2026-01-05 11:42:28 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:29] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 1032/1319 [03:34<00:55,  5.20it/s][2026-01-05 11:42:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:29] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:29] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 1033/1319 [03:35<00:56,  5.07it/s][2026-01-05 11:42:29 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:29] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 1035/1319 [03:35<00:39,  7.12it/s][2026-01-05 11:42:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:29] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▊  | 1036/1319 [03:35<00:37,  7.57it/s][2026-01-05 11:42:29 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:29] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:29] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▊  | 1037/1319 [03:35<00:52,  5.35it/s]
 79%|███████▊  | 1038/1319 [03:35<00:51,  5.46it/s][2026-01-05 11:42:30 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:30 TP0] Decode batch, #running-req: 32, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 747.15, #queue-req: 96,
[2026-01-05 11:42:30] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 1039/1319 [03:35<00:54,  5.13it/s][2026-01-05 11:42:30] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:30] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 1041/1319 [03:36<00:54,  5.07it/s][2026-01-05 11:42:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:30] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 1042/1319 [03:36<00:55,  5.00it/s][2026-01-05 11:42:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:31] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 1043/1319 [03:36<01:00,  4.58it/s][2026-01-05 11:42:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:31] INFO:     127.0.0.1:56150 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 1044/1319 [03:37<01:11,  3.87it/s][2026-01-05 11:42:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:31] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 1045/1319 [03:37<01:09,  3.93it/s][2026-01-05 11:42:31 TP0] Decode batch, #running-req: 32, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 817.16, #queue-req: 96,
[2026-01-05 11:42:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:32] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 1046/1319 [03:37<01:12,  3.77it/s][2026-01-05 11:42:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:32] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 1047/1319 [03:37<01:01,  4.43it/s][2026-01-05 11:42:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:32] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 1048/1319 [03:38<00:52,  5.19it/s][2026-01-05 11:42:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:32] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 1049/1319 [03:38<00:52,  5.10it/s][2026-01-05 11:42:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:32] INFO:     127.0.0.1:55166 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 1050/1319 [03:38<01:00,  4.43it/s][2026-01-05 11:42:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:33] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 1051/1319 [03:38<01:18,  3.42it/s][2026-01-05 11:42:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:33] INFO:     127.0.0.1:55266 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 1052/1319 [03:39<01:07,  3.94it/s][2026-01-05 11:42:33 TP0] Decode batch, #running-req: 32, #token: 31104, token usage: 0.03, cpu graph: True, gen throughput (token/s): 773.66, #queue-req: 96,
[2026-01-05 11:42:33] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:33] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK

 80%|███████▉  | 1054/1319 [03:39<00:55,  4.78it/s][2026-01-05 11:42:33] INFO:     127.0.0.1:56308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:33] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:33 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:33] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 1057/1319 [03:39<00:37,  6.99it/s][2026-01-05 11:42:33 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:34] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 1058/1319 [03:40<00:56,  4.58it/s][2026-01-05 11:42:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:34] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 1059/1319 [03:40<00:57,  4.49it/s][2026-01-05 11:42:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:34] INFO:     127.0.0.1:56000 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 1060/1319 [03:40<00:51,  5.03it/s][2026-01-05 11:42:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:35 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 667.92, #queue-req: 96,
[2026-01-05 11:42:35] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 1061/1319 [03:41<01:13,  3.50it/s][2026-01-05 11:42:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:35] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 1062/1319 [03:41<01:09,  3.72it/s][2026-01-05 11:42:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:35] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 1063/1319 [03:41<00:58,  4.39it/s][2026-01-05 11:42:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:35] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 1064/1319 [03:41<00:49,  5.14it/s][2026-01-05 11:42:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:36] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 1065/1319 [03:41<00:57,  4.39it/s][2026-01-05 11:42:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:36] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 1066/1319 [03:42<00:52,  4.79it/s][2026-01-05 11:42:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:36] INFO:     127.0.0.1:55450 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 1067/1319 [03:42<01:06,  3.78it/s][2026-01-05 11:42:36 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:36 TP0] Decode batch, #running-req: 32, #token: 33152, token usage: 0.03, cpu graph: True, gen throughput (token/s): 774.35, #queue-req: 96,
[2026-01-05 11:42:37] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 1068/1319 [03:42<01:16,  3.28it/s][2026-01-05 11:42:37] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:37] INFO:     127.0.0.1:55276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:37] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 1070/1319 [03:43<01:04,  3.88it/s]
 81%|████████  | 1071/1319 [03:43<00:48,  5.08it/s][2026-01-05 11:42:37 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:37] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK

 81%|████████▏ | 1072/1319 [03:43<00:55,  4.45it/s][2026-01-05 11:42:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:37] INFO:     127.0.0.1:55280 - "POST /generate HTTP/1.1" 200 OK

 81%|████████▏ | 1073/1319 [03:43<00:51,  4.73it/s][2026-01-05 11:42:38] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:38] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 1075/1319 [03:43<00:37,  6.49it/s][2026-01-05 11:42:38 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:38] INFO:     127.0.0.1:55290 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 1076/1319 [03:44<00:37,  6.51it/s][2026-01-05 11:42:38] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:38] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 1078/1319 [03:44<00:38,  6.33it/s][2026-01-05 11:42:38] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:38 TP0] Decode batch, #running-req: 31, #token: 32128, token usage: 0.03, cpu graph: True, gen throughput (token/s): 676.41, #queue-req: 96,
[2026-01-05 11:42:38] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 1080/1319 [03:44<00:35,  6.65it/s][2026-01-05 11:42:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:39] INFO:     127.0.0.1:55310 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 1081/1319 [03:44<00:38,  6.12it/s][2026-01-05 11:42:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:39] INFO:     127.0.0.1:55490 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 1082/1319 [03:45<00:51,  4.64it/s][2026-01-05 11:42:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:39] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 1083/1319 [03:45<01:04,  3.66it/s][2026-01-05 11:42:39] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:39 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:40] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 1085/1319 [03:45<00:51,  4.52it/s][2026-01-05 11:42:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:40 TP0] Decode batch, #running-req: 32, #token: 32384, token usage: 0.03, cpu graph: True, gen throughput (token/s): 818.05, #queue-req: 96,
[2026-01-05 11:42:40] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 1086/1319 [03:46<01:00,  3.84it/s][2026-01-05 11:42:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:40] INFO:     127.0.0.1:50468 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 1087/1319 [03:46<00:52,  4.42it/s][2026-01-05 11:42:40 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:40] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 1088/1319 [03:46<00:50,  4.60it/s][2026-01-05 11:42:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:41] INFO:     127.0.0.1:50448 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 1089/1319 [03:46<00:47,  4.81it/s][2026-01-05 11:42:41] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:41] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 1091/1319 [03:47<00:56,  4.03it/s][2026-01-05 11:42:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:41] INFO:     127.0.0.1:50444 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 1092/1319 [03:47<00:49,  4.59it/s][2026-01-05 11:42:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:41] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 1093/1319 [03:47<00:42,  5.29it/s][2026-01-05 11:42:41 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:42 TP0] Decode batch, #running-req: 31, #token: 33024, token usage: 0.03, cpu graph: True, gen throughput (token/s): 765.21, #queue-req: 96,
[2026-01-05 11:42:42] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 1094/1319 [03:47<00:39,  5.68it/s][2026-01-05 11:42:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:42] INFO:     127.0.0.1:50490 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 1095/1319 [03:47<00:35,  6.23it/s][2026-01-05 11:42:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:42] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 1096/1319 [03:48<00:39,  5.65it/s][2026-01-05 11:42:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:42] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 1097/1319 [03:48<00:39,  5.58it/s][2026-01-05 11:42:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:42] INFO:     127.0.0.1:50446 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 1098/1319 [03:48<00:37,  5.82it/s][2026-01-05 11:42:42] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:43] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 1100/1319 [03:48<00:34,  6.30it/s][2026-01-05 11:42:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:43] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:43] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 1101/1319 [03:49<00:41,  5.25it/s]
 84%|████████▎ | 1102/1319 [03:49<00:37,  5.83it/s][2026-01-05 11:42:43 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:43] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 1103/1319 [03:49<00:45,  4.77it/s][2026-01-05 11:42:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:43 TP0] Decode batch, #running-req: 31, #token: 32000, token usage: 0.03, cpu graph: True, gen throughput (token/s): 726.42, #queue-req: 96,
[2026-01-05 11:42:43] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 1104/1319 [03:49<00:43,  4.92it/s][2026-01-05 11:42:43 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:43] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 1105/1319 [03:49<00:40,  5.25it/s][2026-01-05 11:42:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:44] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 1106/1319 [03:49<00:36,  5.83it/s][2026-01-05 11:42:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:44] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 1107/1319 [03:50<00:53,  4.00it/s][2026-01-05 11:42:44] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:44] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:44 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:44] INFO:     127.0.0.1:50464 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 1110/1319 [03:50<00:33,  6.20it/s][2026-01-05 11:42:44 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:45] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 1111/1319 [03:50<00:34,  6.05it/s][2026-01-05 11:42:45 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:45] INFO:     127.0.0.1:55356 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 1112/1319 [03:51<00:38,  5.36it/s][2026-01-05 11:42:45 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:45 TP0] Decode batch, #running-req: 32, #token: 31232, token usage: 0.03, cpu graph: True, gen throughput (token/s): 759.91, #queue-req: 96,
[2026-01-05 11:42:45] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 1113/1319 [03:51<00:38,  5.39it/s][2026-01-05 11:42:45 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:45] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 1114/1319 [03:51<00:44,  4.58it/s][2026-01-05 11:42:45 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:46] INFO:     127.0.0.1:50622 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▍ | 1115/1319 [03:51<00:45,  4.50it/s][2026-01-05 11:42:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:46] INFO:     127.0.0.1:55506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:46] INFO:     127.0.0.1:50704 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▍ | 1116/1319 [03:52<00:47,  4.29it/s]
 85%|████████▍ | 1117/1319 [03:52<00:37,  5.32it/s][2026-01-05 11:42:46 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:46] INFO:     127.0.0.1:50676 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▍ | 1118/1319 [03:52<00:34,  5.84it/s][2026-01-05 11:42:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:46] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▍ | 1119/1319 [03:52<00:34,  5.77it/s][2026-01-05 11:42:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:46] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▍ | 1120/1319 [03:52<00:39,  5.06it/s][2026-01-05 11:42:46] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:46 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:47 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 752.52, #queue-req: 96,
[2026-01-05 11:42:47] INFO:     127.0.0.1:50658 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 1122/1319 [03:53<00:40,  4.89it/s][2026-01-05 11:42:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:47] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 1123/1319 [03:53<00:39,  5.02it/s][2026-01-05 11:42:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:47] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 1124/1319 [03:53<00:34,  5.58it/s][2026-01-05 11:42:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:47] INFO:     127.0.0.1:50778 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 1125/1319 [03:53<00:39,  4.92it/s][2026-01-05 11:42:47 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:47] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 1126/1319 [03:53<00:36,  5.26it/s][2026-01-05 11:42:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:48] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 1127/1319 [03:54<00:57,  3.36it/s][2026-01-05 11:42:48 TP0] Decode batch, #running-req: 32, #token: 31488, token usage: 0.03, cpu graph: True, gen throughput (token/s): 890.65, #queue-req: 96,
[2026-01-05 11:42:48 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:48] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:48] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 1128/1319 [03:54<00:57,  3.35it/s]
 86%|████████▌ | 1129/1319 [03:54<00:44,  4.31it/s][2026-01-05 11:42:48 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:49] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 1130/1319 [03:54<00:41,  4.50it/s][2026-01-05 11:42:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:49] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 1131/1319 [03:54<00:40,  4.69it/s][2026-01-05 11:42:49] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:49] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 1133/1319 [03:55<00:39,  4.75it/s][2026-01-05 11:42:49] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:49 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:49] INFO:     127.0.0.1:50842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:49] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 1135/1319 [03:55<00:32,  5.71it/s]
 86%|████████▌ | 1136/1319 [03:55<00:24,  7.59it/s][2026-01-05 11:42:49 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:49] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 1137/1319 [03:55<00:23,  7.79it/s][2026-01-05 11:42:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:50 TP0] Decode batch, #running-req: 32, #token: 32256, token usage: 0.03, cpu graph: True, gen throughput (token/s): 690.77, #queue-req: 96,
[2026-01-05 11:42:50] INFO:     127.0.0.1:50818 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 1138/1319 [03:56<00:48,  3.70it/s][2026-01-05 11:42:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:50] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 1139/1319 [03:56<00:42,  4.23it/s][2026-01-05 11:42:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:51] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▋ | 1140/1319 [03:56<00:40,  4.47it/s][2026-01-05 11:42:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:51] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 1141/1319 [03:57<00:35,  5.08it/s][2026-01-05 11:42:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:51] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 1142/1319 [03:57<00:39,  4.52it/s][2026-01-05 11:42:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:51] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:51] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 1143/1319 [03:57<00:34,  5.16it/s]
 87%|████████▋ | 1144/1319 [03:57<00:23,  7.40it/s][2026-01-05 11:42:51 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:51] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 1145/1319 [03:57<00:22,  7.64it/s][2026-01-05 11:42:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:51] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 1146/1319 [03:57<00:22,  7.85it/s][2026-01-05 11:42:51 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:52 TP0] Decode batch, #running-req: 32, #token: 30336, token usage: 0.02, cpu graph: True, gen throughput (token/s): 728.12, #queue-req: 96,
[2026-01-05 11:42:52] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:52] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 1147/1319 [03:57<00:28,  6.08it/s]
 87%|████████▋ | 1148/1319 [03:57<00:25,  6.62it/s][2026-01-05 11:42:52 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:52] INFO:     127.0.0.1:50740 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 1149/1319 [03:58<00:28,  6.03it/s][2026-01-05 11:42:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:52] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 1150/1319 [03:58<00:36,  4.61it/s][2026-01-05 11:42:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:53] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:53] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 1151/1319 [03:58<00:45,  3.70it/s]
 87%|████████▋ | 1152/1319 [03:58<00:40,  4.09it/s][2026-01-05 11:42:53] INFO:     127.0.0.1:50642 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:53 TP0] Prefill batch, #new-seq: 2, #new-token: 1920, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:53] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 1154/1319 [03:59<00:35,  4.71it/s][2026-01-05 11:42:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:53 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 829.03, #queue-req: 96,
[2026-01-05 11:42:53] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1155/1319 [03:59<00:39,  4.17it/s][2026-01-05 11:42:53 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:54] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1156/1319 [03:59<00:36,  4.41it/s][2026-01-05 11:42:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:54] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1157/1319 [03:59<00:35,  4.63it/s][2026-01-05 11:42:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:54] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1158/1319 [04:00<00:33,  4.81it/s][2026-01-05 11:42:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:54] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1159/1319 [04:00<00:39,  4.07it/s][2026-01-05 11:42:54 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:54] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:54] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1160/1319 [04:00<00:33,  4.75it/s]
 88%|████████▊ | 1161/1319 [04:00<00:22,  6.96it/s][2026-01-05 11:42:54 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 95,
[2026-01-05 11:42:54] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1162/1319 [04:00<00:22,  6.97it/s][2026-01-05 11:42:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:55] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1163/1319 [04:00<00:24,  6.49it/s][2026-01-05 11:42:55 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:55 TP0] Decode batch, #running-req: 32, #token: 31616, token usage: 0.03, cpu graph: True, gen throughput (token/s): 753.50, #queue-req: 96,
[2026-01-05 11:42:55] INFO:     127.0.0.1:59654 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1164/1319 [04:01<00:48,  3.22it/s][2026-01-05 11:42:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:56] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1165/1319 [04:02<00:53,  2.86it/s][2026-01-05 11:42:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:56 TP0] Decode batch, #running-req: 32, #token: 33152, token usage: 0.03, cpu graph: True, gen throughput (token/s): 1049.46, #queue-req: 96,
[2026-01-05 11:42:56] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1166/1319 [04:02<00:49,  3.07it/s][2026-01-05 11:42:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:56] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 1167/1319 [04:02<00:44,  3.42it/s][2026-01-05 11:42:56 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:57] INFO:     127.0.0.1:50862 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▊ | 1168/1319 [04:02<00:46,  3.26it/s][2026-01-05 11:42:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:57] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:57] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▊ | 1169/1319 [04:03<00:39,  3.79it/s]
 89%|████████▊ | 1170/1319 [04:03<00:26,  5.55it/s][2026-01-05 11:42:57 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:57] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 1171/1319 [04:03<00:30,  4.81it/s][2026-01-05 11:42:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:57] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 1172/1319 [04:03<00:27,  5.40it/s][2026-01-05 11:42:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:57] INFO:     127.0.0.1:59840 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 1173/1319 [04:03<00:27,  5.33it/s][2026-01-05 11:42:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:58] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:58] INFO:     127.0.0.1:36296 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 1174/1319 [04:03<00:28,  5.16it/s]
 89%|████████▉ | 1175/1319 [04:03<00:22,  6.48it/s][2026-01-05 11:42:58 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:58 TP0] Decode batch, #running-req: 30, #token: 32768, token usage: 0.03, cpu graph: True, gen throughput (token/s): 757.29, #queue-req: 96,
[2026-01-05 11:42:58] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 1176/1319 [04:04<00:23,  5.97it/s][2026-01-05 11:42:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:58] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 1177/1319 [04:04<00:22,  6.41it/s][2026-01-05 11:42:58 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:58] INFO:     127.0.0.1:59714 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 1178/1319 [04:04<00:25,  5.54it/s][2026-01-05 11:42:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:59] INFO:     127.0.0.1:59598 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 1179/1319 [04:04<00:31,  4.44it/s][2026-01-05 11:42:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:59] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 1180/1319 [04:04<00:28,  4.86it/s][2026-01-05 11:42:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:59] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:42:59] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 1181/1319 [04:05<00:28,  4.83it/s]
 90%|████████▉ | 1182/1319 [04:05<00:22,  6.21it/s][2026-01-05 11:42:59 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 96,
[2026-01-05 11:42:59] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 1183/1319 [04:05<00:23,  5.86it/s][2026-01-05 11:42:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:42:59] INFO:     127.0.0.1:36352 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 1184/1319 [04:05<00:23,  5.67it/s][2026-01-05 11:42:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:43:00 TP0] Decode batch, #running-req: 32, #token: 32896, token usage: 0.03, cpu graph: True, gen throughput (token/s): 735.17, #queue-req: 96,
[2026-01-05 11:43:00] INFO:     127.0.0.1:59754 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 1185/1319 [04:05<00:25,  5.32it/s][2026-01-05 11:43:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:43:00] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:00] INFO:     127.0.0.1:36284 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 1186/1319 [04:06<00:25,  5.27it/s]
 90%|████████▉ | 1187/1319 [04:06<00:19,  6.72it/s][2026-01-05 11:43:00 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 96,
[2026-01-05 11:43:00] INFO:     127.0.0.1:36316 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 1188/1319 [04:06<00:25,  5.05it/s][2026-01-05 11:43:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:43:00] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 1189/1319 [04:06<00:28,  4.60it/s][2026-01-05 11:43:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:43:01] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 1190/1319 [04:07<00:33,  3.83it/s][2026-01-05 11:43:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:43:01 TP0] Decode batch, #running-req: 32, #token: 33536, token usage: 0.03, cpu graph: True, gen throughput (token/s): 847.74, #queue-req: 96,
[2026-01-05 11:43:01] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 1191/1319 [04:07<00:43,  2.95it/s][2026-01-05 11:43:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 96,
[2026-01-05 11:43:01] INFO:     127.0.0.1:36396 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 1192/1319 [04:07<00:36,  3.45it/s][2026-01-05 11:43:01] INFO:     127.0.0.1:36422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:01 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 95,
[2026-01-05 11:43:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 94,
[2026-01-05 11:43:02] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 1194/1319 [04:07<00:26,  4.71it/s][2026-01-05 11:43:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 93,
[2026-01-05 11:43:02] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 1195/1319 [04:08<00:25,  4.92it/s][2026-01-05 11:43:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 92,
[2026-01-05 11:43:02] INFO:     127.0.0.1:36430 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 1196/1319 [04:08<00:30,  4.01it/s][2026-01-05 11:43:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 91,
[2026-01-05 11:43:02] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 1197/1319 [04:08<00:28,  4.29it/s][2026-01-05 11:43:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 90,
[2026-01-05 11:43:03 TP0] Decode batch, #running-req: 32, #token: 33280, token usage: 0.03, cpu graph: True, gen throughput (token/s): 770.39, #queue-req: 90,
[2026-01-05 11:43:03] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 1198/1319 [04:08<00:29,  4.13it/s][2026-01-05 11:43:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 89,
[2026-01-05 11:43:03] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 1199/1319 [04:09<00:31,  3.79it/s][2026-01-05 11:43:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 88,
[2026-01-05 11:43:03] INFO:     127.0.0.1:36452 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 1200/1319 [04:09<00:35,  3.40it/s][2026-01-05 11:43:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 87,
[2026-01-05 11:43:04] INFO:     127.0.0.1:36374 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 1201/1319 [04:09<00:32,  3.61it/s][2026-01-05 11:43:04] INFO:     127.0.0.1:59770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:04] INFO:     127.0.0.1:36544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 86,
[2026-01-05 11:43:04 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 84,
[2026-01-05 11:43:04] INFO:     127.0.0.1:59610 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:04] INFO:     127.0.0.1:36438 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████▏| 1204/1319 [04:10<00:19,  6.01it/s]
 91%|█████████▏| 1205/1319 [04:10<00:12,  9.02it/s][2026-01-05 11:43:04 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.03, #running-req: 30, #queue-req: 82,
[2026-01-05 11:43:04] INFO:     127.0.0.1:36328 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████▏| 1206/1319 [04:10<00:14,  8.01it/s][2026-01-05 11:43:04] INFO:     127.0.0.1:36492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 81,
[2026-01-05 11:43:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 80,
[2026-01-05 11:43:04 TP0] Decode batch, #running-req: 32, #token: 32512, token usage: 0.03, cpu graph: True, gen throughput (token/s): 744.53, #queue-req: 80,
[2026-01-05 11:43:04] INFO:     127.0.0.1:36300 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 1208/1319 [04:10<00:15,  7.36it/s][2026-01-05 11:43:04] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 79,
[2026-01-05 11:43:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 78,
[2026-01-05 11:43:05] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 1210/1319 [04:11<00:17,  6.29it/s][2026-01-05 11:43:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 77,
[2026-01-05 11:43:05] INFO:     127.0.0.1:36440 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 1211/1319 [04:11<00:17,  6.27it/s][2026-01-05 11:43:05] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 76,
[2026-01-05 11:43:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 75,
[2026-01-05 11:43:05] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 1213/1319 [04:11<00:15,  6.88it/s][2026-01-05 11:43:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 74,
[2026-01-05 11:43:05] INFO:     127.0.0.1:36326 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 1214/1319 [04:11<00:16,  6.41it/s][2026-01-05 11:43:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 73,
[2026-01-05 11:43:06] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 1215/1319 [04:11<00:15,  6.75it/s][2026-01-05 11:43:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 72,
[2026-01-05 11:43:06] INFO:     127.0.0.1:36570 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 1216/1319 [04:12<00:17,  5.76it/s][2026-01-05 11:43:06] INFO:     127.0.0.1:36390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:06] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 71,
[2026-01-05 11:43:06 TP0] Prefill batch, #new-seq: 2, #new-token: 2048, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 69,
[2026-01-05 11:43:06] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 1219/1319 [04:12<00:14,  6.88it/s][2026-01-05 11:43:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 68,
[2026-01-05 11:43:06] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 1220/1319 [04:12<00:13,  7.12it/s][2026-01-05 11:43:06 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 67,
[2026-01-05 11:43:06 TP0] Decode batch, #running-req: 32, #token: 31872, token usage: 0.03, cpu graph: True, gen throughput (token/s): 603.93, #queue-req: 67,
[2026-01-05 11:43:07] INFO:     127.0.0.1:36404 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 1221/1319 [04:12<00:17,  5.66it/s][2026-01-05 11:43:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 66,
[2026-01-05 11:43:07] INFO:     127.0.0.1:36426 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 1222/1319 [04:13<00:22,  4.26it/s][2026-01-05 11:43:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 65,
[2026-01-05 11:43:07] INFO:     127.0.0.1:36470 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 1223/1319 [04:13<00:26,  3.62it/s][2026-01-05 11:43:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 64,
[2026-01-05 11:43:08] INFO:     127.0.0.1:36644 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 1224/1319 [04:13<00:24,  3.85it/s][2026-01-05 11:43:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 63,
[2026-01-05 11:43:08 TP0] Decode batch, #running-req: 32, #token: 32640, token usage: 0.03, cpu graph: True, gen throughput (token/s): 924.29, #queue-req: 63,
[2026-01-05 11:43:08] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 1225/1319 [04:14<00:25,  3.62it/s][2026-01-05 11:43:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 62,
[2026-01-05 11:43:08] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 1226/1319 [04:14<00:26,  3.55it/s][2026-01-05 11:43:08 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 61,
[2026-01-05 11:43:09] INFO:     127.0.0.1:36554 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 1227/1319 [04:14<00:26,  3.51it/s][2026-01-05 11:43:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 60,
[2026-01-05 11:43:09] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 1228/1319 [04:15<00:29,  3.07it/s][2026-01-05 11:43:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 59,
[2026-01-05 11:43:09] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 58,
[2026-01-05 11:43:09] INFO:     127.0.0.1:36578 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 1230/1319 [04:15<00:21,  4.08it/s][2026-01-05 11:43:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 57,
[2026-01-05 11:43:09 TP0] Decode batch, #running-req: 32, #token: 32896, token usage: 0.03, cpu graph: True, gen throughput (token/s): 824.57, #queue-req: 57,
[2026-01-05 11:43:10] INFO:     127.0.0.1:36472 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 1231/1319 [04:15<00:23,  3.82it/s][2026-01-05 11:43:10] INFO:     127.0.0.1:36590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 56,
[2026-01-05 11:43:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 55,
[2026-01-05 11:43:10] INFO:     127.0.0.1:36580 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 1233/1319 [04:16<00:21,  4.06it/s][2026-01-05 11:43:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 54,
[2026-01-05 11:43:10] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 1234/1319 [04:16<00:18,  4.60it/s][2026-01-05 11:43:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 53,
[2026-01-05 11:43:10] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 1235/1319 [04:16<00:16,  5.01it/s][2026-01-05 11:43:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 52,
[2026-01-05 11:43:10] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 1236/1319 [04:16<00:16,  4.96it/s][2026-01-05 11:43:10] INFO:     127.0.0.1:36550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 51,
[2026-01-05 11:43:11] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 1238/1319 [04:16<00:11,  7.07it/s][2026-01-05 11:43:11 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 49,
[2026-01-05 11:43:11] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:11] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 1239/1319 [04:17<00:12,  6.24it/s][2026-01-05 11:43:11 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 47,
[2026-01-05 11:43:11] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 1241/1319 [04:17<00:09,  7.82it/s][2026-01-05 11:43:11] INFO:     127.0.0.1:55356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 46,
[2026-01-05 11:43:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 45,
[2026-01-05 11:43:11 TP0] Decode batch, #running-req: 32, #token: 32256, token usage: 0.03, cpu graph: True, gen throughput (token/s): 687.37, #queue-req: 45,
[2026-01-05 11:43:11] INFO:     127.0.0.1:36600 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:11] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 1243/1319 [04:17<00:12,  5.94it/s]
 94%|█████████▍| 1244/1319 [04:17<00:12,  6.09it/s][2026-01-05 11:43:11 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 43,
[2026-01-05 11:43:12] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 1245/1319 [04:17<00:12,  5.81it/s][2026-01-05 11:43:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 42,
[2026-01-05 11:43:12] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 1246/1319 [04:18<00:12,  5.74it/s][2026-01-05 11:43:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 41,
[2026-01-05 11:43:12] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▍| 1247/1319 [04:18<00:13,  5.40it/s][2026-01-05 11:43:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 40,
[2026-01-05 11:43:12] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▍| 1248/1319 [04:18<00:14,  5.02it/s][2026-01-05 11:43:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 39,
[2026-01-05 11:43:13] INFO:     127.0.0.1:36676 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▍| 1249/1319 [04:18<00:14,  4.75it/s][2026-01-05 11:43:13] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 38,
[2026-01-05 11:43:13] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▍| 1251/1319 [04:18<00:10,  6.78it/s][2026-01-05 11:43:13 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 36,
[2026-01-05 11:43:13] INFO:     127.0.0.1:55450 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▍| 1252/1319 [04:19<00:10,  6.46it/s][2026-01-05 11:43:13 TP0] Decode batch, #running-req: 32, #token: 30976, token usage: 0.03, cpu graph: True, gen throughput (token/s): 774.52, #queue-req: 36,
[2026-01-05 11:43:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 35,
[2026-01-05 11:43:13] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▍| 1253/1319 [04:19<00:10,  6.36it/s][2026-01-05 11:43:13 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 34,
[2026-01-05 11:43:14] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 1254/1319 [04:19<00:17,  3.82it/s][2026-01-05 11:43:14] INFO:     127.0.0.1:36692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 33,
[2026-01-05 11:43:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 32,
[2026-01-05 11:43:14 TP0] Decode batch, #running-req: 32, #token: 31872, token usage: 0.03, cpu graph: True, gen throughput (token/s): 918.79, #queue-req: 32,
[2026-01-05 11:43:14] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 1256/1319 [04:20<00:18,  3.36it/s][2026-01-05 11:43:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 31,
[2026-01-05 11:43:14] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 1257/1319 [04:20<00:17,  3.58it/s][2026-01-05 11:43:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 30,
[2026-01-05 11:43:15] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 1258/1319 [04:20<00:16,  3.72it/s][2026-01-05 11:43:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 29,
[2026-01-05 11:43:15] INFO:     127.0.0.1:55562 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 1259/1319 [04:21<00:13,  4.36it/s][2026-01-05 11:43:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 28,
[2026-01-05 11:43:15] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 1260/1319 [04:21<00:11,  5.08it/s][2026-01-05 11:43:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 27,
[2026-01-05 11:43:15] INFO:     127.0.0.1:55694 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 1261/1319 [04:21<00:10,  5.77it/s][2026-01-05 11:43:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 26,
[2026-01-05 11:43:15] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 1262/1319 [04:21<00:09,  6.00it/s][2026-01-05 11:43:15] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 25,
[2026-01-05 11:43:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 24,
[2026-01-05 11:43:15] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 1264/1319 [04:21<00:07,  6.97it/s][2026-01-05 11:43:15 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 23,
[2026-01-05 11:43:16] INFO:     127.0.0.1:55630 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 1265/1319 [04:21<00:07,  7.41it/s][2026-01-05 11:43:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 22,
[2026-01-05 11:43:16] INFO:     127.0.0.1:55544 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 1266/1319 [04:22<00:08,  6.27it/s][2026-01-05 11:43:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 21,
[2026-01-05 11:43:16] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 1267/1319 [04:22<00:09,  5.75it/s][2026-01-05 11:43:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 20,
[2026-01-05 11:43:16] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 1268/1319 [04:22<00:10,  4.83it/s][2026-01-05 11:43:16 TP0] Decode batch, #running-req: 32, #token: 30976, token usage: 0.03, cpu graph: True, gen throughput (token/s): 619.07, #queue-req: 20,
[2026-01-05 11:43:16 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 19,
[2026-01-05 11:43:16] INFO:     127.0.0.1:55654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:16] INFO:     127.0.0.1:55670 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 1269/1319 [04:22<00:10,  4.79it/s]
 96%|█████████▋| 1270/1319 [04:22<00:07,  6.13it/s][2026-01-05 11:43:17 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 17,
[2026-01-05 11:43:17] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 1271/1319 [04:22<00:07,  6.13it/s][2026-01-05 11:43:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 16,
[2026-01-05 11:43:17] INFO:     127.0.0.1:55638 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 1272/1319 [04:23<00:09,  4.92it/s][2026-01-05 11:43:17] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 15,
[2026-01-05 11:43:17] INFO:     127.0.0.1:55506 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:17 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 13,
[2026-01-05 11:43:17] INFO:     127.0.0.1:55602 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 1275/1319 [04:23<00:07,  5.53it/s][2026-01-05 11:43:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 12,
[2026-01-05 11:43:18] INFO:     127.0.0.1:55682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:18] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 1276/1319 [04:24<00:08,  4.79it/s]
 97%|█████████▋| 1277/1319 [04:24<00:08,  5.24it/s][2026-01-05 11:43:18 TP0] Prefill batch, #new-seq: 2, #new-token: 1792, #cached-token: 0, token usage: 0.02, #running-req: 30, #queue-req: 10,
[2026-01-05 11:43:18 TP0] Decode batch, #running-req: 32, #token: 31744, token usage: 0.03, cpu graph: True, gen throughput (token/s): 778.72, #queue-req: 10,
[2026-01-05 11:43:18] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 1278/1319 [04:24<00:07,  5.27it/s][2026-01-05 11:43:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 9,
[2026-01-05 11:43:18] INFO:     127.0.0.1:55816 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 1279/1319 [04:24<00:07,  5.12it/s][2026-01-05 11:43:18] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 8,
[2026-01-05 11:43:18 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 7,
[2026-01-05 11:43:19] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 1281/1319 [04:24<00:07,  4.94it/s][2026-01-05 11:43:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 6,
[2026-01-05 11:43:19] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 1282/1319 [04:25<00:08,  4.62it/s][2026-01-05 11:43:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 5,
[2026-01-05 11:43:19] INFO:     127.0.0.1:55516 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 1283/1319 [04:25<00:08,  4.25it/s][2026-01-05 11:43:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 4,
[2026-01-05 11:43:19] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 1284/1319 [04:25<00:07,  4.87it/s][2026-01-05 11:43:19 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 3,
[2026-01-05 11:43:20 TP0] Decode batch, #running-req: 32, #token: 32000, token usage: 0.03, cpu graph: True, gen throughput (token/s): 782.72, #queue-req: 3,
[2026-01-05 11:43:20] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 1285/1319 [04:25<00:08,  3.95it/s][2026-01-05 11:43:20] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.02, #running-req: 31, #queue-req: 2,
[2026-01-05 11:43:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 1,
[2026-01-05 11:43:20] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 1287/1319 [04:26<00:06,  4.66it/s][2026-01-05 11:43:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.03, #running-req: 31, #queue-req: 0,
[2026-01-05 11:43:20] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 1288/1319 [04:26<00:06,  4.77it/s][2026-01-05 11:43:20] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:20] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 1290/1319 [04:26<00:04,  6.85it/s][2026-01-05 11:43:20] INFO:     127.0.0.1:54732 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:20] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 1292/1319 [04:26<00:03,  8.89it/s][2026-01-05 11:43:21] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:21] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 1294/1319 [04:26<00:02,  8.64it/s][2026-01-05 11:43:21 TP0] Decode batch, #running-req: 25, #token: 25472, token usage: 0.02, cpu graph: True, gen throughput (token/s): 874.45, #queue-req: 0,
[2026-01-05 11:43:21] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:21] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 1296/1319 [04:27<00:02,  7.94it/s][2026-01-05 11:43:21] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:21] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 1298/1319 [04:27<00:02,  9.72it/s][2026-01-05 11:43:21] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:21] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▊| 1300/1319 [04:27<00:02,  8.74it/s][2026-01-05 11:43:21] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:22] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▊| 1302/1319 [04:27<00:02,  7.43it/s][2026-01-05 11:43:22 TP0] Decode batch, #running-req: 17, #token: 17920, token usage: 0.01, cpu graph: True, gen throughput (token/s): 706.64, #queue-req: 0,
[2026-01-05 11:43:22] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 1303/1319 [04:28<00:02,  6.02it/s][2026-01-05 11:43:22] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 1304/1319 [04:28<00:02,  6.03it/s][2026-01-05 11:43:22] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:22] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:22] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 1307/1319 [04:28<00:01,  8.83it/s][2026-01-05 11:43:22] INFO:     127.0.0.1:55774 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:23] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:23] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 1309/1319 [04:28<00:01,  7.53it/s]
 99%|█████████▉| 1310/1319 [04:28<00:01,  7.89it/s][2026-01-05 11:43:23] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:23] INFO:     127.0.0.1:54674 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 1312/1319 [04:29<00:00,  8.31it/s][2026-01-05 11:43:23 TP0] Decode batch, #running-req: 7, #token: 7424, token usage: 0.01, cpu graph: True, gen throughput (token/s): 420.40, #queue-req: 0,
[2026-01-05 11:43:23] INFO:     127.0.0.1:54698 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 1313/1319 [04:29<00:00,  6.71it/s][2026-01-05 11:43:23] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 1314/1319 [04:29<00:00,  6.19it/s][2026-01-05 11:43:24] INFO:     127.0.0.1:54778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:43:24] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 1315/1319 [04:30<00:00,  4.68it/s]
100%|█████████▉| 1316/1319 [04:30<00:00,  4.76it/s][2026-01-05 11:43:24 TP0] Decode batch, #running-req: 5, #token: 3328, token usage: 0.00, cpu graph: True, gen throughput (token/s): 291.07, #queue-req: 0,
[2026-01-05 11:43:25 TP0] Decode batch, #running-req: 3, #token: 3584, token usage: 0.00, cpu graph: True, gen throughput (token/s): 159.99, #queue-req: 0,
[2026-01-05 11:43:25] INFO:     127.0.0.1:54748 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 1317/1319 [04:31<00:00,  2.58it/s][2026-01-05 11:43:25] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 1318/1319 [04:31<00:00,  2.76it/s][2026-01-05 11:43:25 TP0] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cpu graph: True, gen throughput (token/s): 126.72, #queue-req: 0,
[2026-01-05 11:43:25] INFO:     127.0.0.1:54724 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 1319/1319 [04:31<00:00,  3.12it/s]
100%|██████████| 1319/1319 [04:31<00:00,  4.86it/s]
/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 178612 is still running
  _warn("subprocess %s is still running" % self.pid,
.Accuracy: 0.941
Invalid: 0.000
Latency: 271.727 s
Output throughput: 730.836 token/s
[CI Test Method] TestAscendTp4Bf16.test_b_throughput
##=== Testing throughput: /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507 ===##
command=python3 -m sglang.bench_offline_throughput --num-prompts 1 --dataset-name random --random-input-len 256 --random-output-len 256 --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507 --trust-remote-code --mem-fraction-static 0.7 --max-running-requests 32 --attention-backend ascend --disable-radix-cache --cuda-graph-max-bs 32 --tp-size 4 --base-gpu-id 4
Output: [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
#Input tokens: 38
#Output tokens: 236
#Input tokens: 256
#Output tokens: 16

====== Offline Throughput Benchmark Result =======
Backend:                                 engine
Successful requests:                     1
Benchmark duration (s):                  3.20
Total input tokens:                      38
Total generated tokens:                  236
Last generation throughput (tok/s):      75.94
Request throughput (req/s):              0.31
Input token throughput (tok/s):          11.88
Output token throughput (tok/s):         73.78
Total token throughput (tok/s):          85.66
==================================================

Error: /usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:43:39] INFO engine.py:220: server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.7, max_running_requests=32, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=385826132, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=4, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:43:39] server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.7, max_running_requests=32, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=385826132, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=4, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:43:39] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:43:49 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:43:49 TP3] Init torch distributed begin.
[2026-01-05 11:43:49 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:43:49 TP0] Init torch distributed begin.
[2026-01-05 11:43:51 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:43:51 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:43:51 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 11:43:51 TP0] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:43:52 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:43:52 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:43:52 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 11:43:52 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:43:52 TP2] Load weight begin. avail mem=60.88 GB
[2026-01-05 11:43:52 TP0] Load weight begin. avail mem=60.87 GB
[2026-01-05 11:43:53 TP1] Load weight begin. avail mem=61.10 GB
[2026-01-05 11:43:53 TP3] Load weight begin. avail mem=61.10 GB

Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   6% Completed | 1/16 [00:05<01:20,  5.35s/it]

Loading safetensors checkpoint shards:  12% Completed | 2/16 [00:10<01:16,  5.49s/it]

Loading safetensors checkpoint shards:  19% Completed | 3/16 [00:16<01:12,  5.57s/it]

Loading safetensors checkpoint shards:  25% Completed | 4/16 [00:22<01:07,  5.60s/it]

Loading safetensors checkpoint shards:  31% Completed | 5/16 [00:27<01:00,  5.46s/it]

Loading safetensors checkpoint shards:  38% Completed | 6/16 [00:28<00:39,  3.98s/it]

Loading safetensors checkpoint shards:  44% Completed | 7/16 [00:34<00:40,  4.46s/it]

Loading safetensors checkpoint shards:  50% Completed | 8/16 [00:39<00:38,  4.77s/it]

Loading safetensors checkpoint shards:  56% Completed | 9/16 [00:45<00:35,  5.02s/it]

Loading safetensors checkpoint shards:  62% Completed | 10/16 [00:50<00:30,  5.13s/it]

Loading safetensors checkpoint shards:  69% Completed | 11/16 [00:55<00:26,  5.25s/it]

Loading safetensors checkpoint shards:  75% Completed | 12/16 [01:01<00:21,  5.29s/it]

Loading safetensors checkpoint shards:  81% Completed | 13/16 [01:07<00:16,  5.42s/it]

Loading safetensors checkpoint shards:  88% Completed | 14/16 [01:12<00:10,  5.50s/it]

Loading safetensors checkpoint shards:  94% Completed | 15/16 [01:18<00:05,  5.53s/it]

Loading safetensors checkpoint shards: 100% Completed | 16/16 [01:26<00:00,  6.27s/it]

Loading safetensors checkpoint shards: 100% Completed | 16/16 [01:26<00:00,  5.39s/it]

[2026-01-05 11:45:19 TP2] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=46.35 GB, mem usage=14.53 GB.
[2026-01-05 11:45:19 TP1] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=46.57 GB, mem usage=14.53 GB.
[2026-01-05 11:45:19 TP0] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=46.34 GB, mem usage=14.53 GB.
[2026-01-05 11:45:19 TP3] Load weight end. type=Qwen3MoeForCausalLM, dtype=torch.bfloat16, avail mem=46.57 GB, mem usage=14.53 GB.
[2026-01-05 11:45:19 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:45:19 TP0] The available memory for KV cache is 28.07 GB.
[2026-01-05 11:45:19 TP3] The available memory for KV cache is 28.07 GB.
[2026-01-05 11:45:19 TP2] The available memory for KV cache is 28.07 GB.
[2026-01-05 11:45:19 TP1] The available memory for KV cache is 28.07 GB.
[2026-01-05 11:45:19 TP2] KV Cache is allocated. #tokens: 1226496, K size: 14.04 GB, V size: 14.04 GB
[2026-01-05 11:45:19 TP0] KV Cache is allocated. #tokens: 1226496, K size: 14.04 GB, V size: 14.04 GB
[2026-01-05 11:45:19 TP3] KV Cache is allocated. #tokens: 1226496, K size: 14.04 GB, V size: 14.04 GB
[2026-01-05 11:45:19 TP1] KV Cache is allocated. #tokens: 1226496, K size: 14.04 GB, V size: 14.04 GB
[2026-01-05 11:45:19 TP0] Memory pool end. avail mem=18.24 GB
[2026-01-05 11:45:19 TP2] Memory pool end. avail mem=18.24 GB
[2026-01-05 11:45:19 TP3] Memory pool end. avail mem=18.46 GB
[2026-01-05 11:45:19 TP1] Memory pool end. avail mem=18.46 GB
[2026-01-05 11:45:19 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=18.24 GB
[2026-01-05 11:45:19 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32]
[2026-01-05 11:45:19 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=18.46 GB
[2026-01-05 11:45:19 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=18.46 GB
[2026-01-05 11:45:19 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=18.24 GB

  0%|          | 0/8 [00:00<?, ?it/s]
Capturing batches (bs=32 avail_mem=18.20 GB):   0%|          | 0/8 [00:00<?, ?it/s][rank3]:[W105 11:45:23.841744118 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank1]:[W105 11:45:23.841750318 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank0]:[W105 11:45:23.841749658 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank2]:[W105 11:45:23.841748488 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())
[rank0]:[W105 11:45:23.069986414 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank3]:[W105 11:45:23.069991024 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank1]:[W105 11:45:23.069990464 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank2]:[W105 11:45:23.070012305 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())

Capturing batches (bs=32 avail_mem=18.20 GB):  12%|█▎        | 1/8 [00:04<00:31,  4.47s/it]
Capturing batches (bs=24 avail_mem=17.55 GB):  12%|█▎        | 1/8 [00:04<00:31,  4.47s/it]
Capturing batches (bs=24 avail_mem=17.55 GB):  25%|██▌       | 2/8 [00:06<00:18,  3.11s/it]
Capturing batches (bs=16 avail_mem=17.53 GB):  25%|██▌       | 2/8 [00:06<00:18,  3.11s/it]
Capturing batches (bs=16 avail_mem=17.53 GB):  38%|███▊      | 3/8 [00:08<00:13,  2.68s/it]
Capturing batches (bs=12 avail_mem=17.52 GB):  38%|███▊      | 3/8 [00:08<00:13,  2.68s/it]
Capturing batches (bs=12 avail_mem=17.52 GB):  50%|█████     | 4/8 [00:11<00:09,  2.49s/it]
Capturing batches (bs=8 avail_mem=17.51 GB):  50%|█████     | 4/8 [00:11<00:09,  2.49s/it]
Capturing batches (bs=8 avail_mem=17.51 GB):  62%|██████▎   | 5/8 [00:13<00:07,  2.38s/it]
Capturing batches (bs=4 avail_mem=17.51 GB):  62%|██████▎   | 5/8 [00:13<00:07,  2.38s/it]
Capturing batches (bs=4 avail_mem=17.51 GB):  75%|███████▌  | 6/8 [00:15<00:04,  2.31s/it]
Capturing batches (bs=2 avail_mem=17.50 GB):  75%|███████▌  | 6/8 [00:15<00:04,  2.31s/it]
Capturing batches (bs=2 avail_mem=17.50 GB):  88%|████████▊ | 7/8 [00:17<00:02,  2.27s/it]
Capturing batches (bs=1 avail_mem=17.49 GB):  88%|████████▊ | 7/8 [00:17<00:02,  2.27s/it]
Capturing batches (bs=1 avail_mem=17.49 GB): 100%|██████████| 8/8 [00:20<00:00,  2.56s/it]
Capturing batches (bs=1 avail_mem=17.49 GB): 100%|██████████| 8/8 [00:20<00:00,  2.59s/it]
[2026-01-05 11:45:41 TP3] Capture cuda graph end. Time elapsed: 21.78 s. mem usage=0.75 GB. avail mem=17.71 GB.
[2026-01-05 11:45:41 TP1] Capture cuda graph end. Time elapsed: 21.90 s. mem usage=0.75 GB. avail mem=17.71 GB.
[2026-01-05 11:45:41 TP2] Capture cuda graph end. Time elapsed: 21.91 s. mem usage=0.75 GB. avail mem=17.49 GB.
[2026-01-05 11:45:41 TP0] Capture cuda graph end. Time elapsed: 21.95 s. mem usage=0.75 GB. avail mem=17.49 GB.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:45:42 TP0] max_total_num_tokens=1226496, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=32, context_len=262144, available_gpu_mem=17.49 GB
[2026-01-05 11:45:56]
Warmup...
[2026-01-05 11:45:56 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank3]:[W105 11:45:56.242920914 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank1]:[W105 11:45:56.242920844 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank0]:[W105 11:45:56.242934114 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank2]:[W105 11:45:56.242948975 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[rank1]:[W105 11:46:19.784197022 compiler_depend.ts:3136] Warning: The indexFromRank 1is not equal indexFromCurDevice 5 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[rank0]:[W105 11:46:19.786078062 compiler_depend.ts:3136] Warning: The indexFromRank 0is not equal indexFromCurDevice 4 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[rank3]:[W105 11:46:19.786195616 compiler_depend.ts:3136] Warning: The indexFromRank 3is not equal indexFromCurDevice 7 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[rank2]:[W105 11:46:19.786686684 compiler_depend.ts:3136] Warning: The indexFromRank 2is not equal indexFromCurDevice 6 , which might be normal if the number of devices on your collective communication server is inconsistent.Otherwise, you need to check if the current device is correct when calling the interface.If it's incorrect, it might have introduced an error. (function operator())
[2026-01-05 11:46:22]
Benchmark...
[2026-01-05 11:46:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:46:22 TP0] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 0.26, #queue-req: 0,
[2026-01-05 11:46:23 TP0] Decode batch, #running-req: 1, #token: 128, token usage: 0.00, cpu graph: True, gen throughput (token/s): 76.34, #queue-req: 0,
[2026-01-05 11:46:23 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, cpu graph: True, gen throughput (token/s): 75.85, #queue-req: 0,
[2026-01-05 11:46:24 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, cpu graph: True, gen throughput (token/s): 74.33, #queue-req: 0,
[2026-01-05 11:46:24 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, cpu graph: True, gen throughput (token/s): 76.48, #queue-req: 0,
[2026-01-05 11:46:25 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, cpu graph: True, gen throughput (token/s): 75.94, #queue-req: 0,

.
----------------------------------------------------------------------
Ran 2 tests in 609.967s

OK
##=== /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-30B-A3B-Instruct-2507 throughput: 75.94 ===##
.
.
End (21/43):
filename='ascend/llm_models/test_ascend_tp4_bf16.py', elapsed=621, estimated_time=400
.
.

.
.
Begin (22/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_mimo_7b_rl.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:46:53] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-7B-RL', tokenizer_path='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-7B-RL', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=209256460, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-7B-RL', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:46:54] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:47:03] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:47:04] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:47:04] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:47:05] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:47:05] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:10<00:30, 10.09s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:33<00:35, 17.88s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:40<00:13, 13.17s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:49<00:00, 11.52s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:49<00:00, 12.49s/it]

[2026-01-05 11:47:56] Load weight end. type=MiMoForCausalLM, dtype=torch.bfloat16, avail mem=46.60 GB, mem usage=14.22 GB.
[2026-01-05 11:47:56] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:47:56] The available memory for KV cache is 34.43 GB.
[2026-01-05 11:47:56] KV Cache is allocated. #tokens: 250624, K size: 17.22 GB, V size: 17.22 GB
[2026-01-05 11:47:56] Memory pool end. avail mem=11.68 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:47:57] max_total_num_tokens=250624, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=3917, context_len=32768, available_gpu_mem=11.68 GB
[2026-01-05 11:47:57] INFO:     Started server process [245853]
[2026-01-05 11:47:57] INFO:     Waiting for application startup.
[2026-01-05 11:47:57] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 11:47:57] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 11:47:57] INFO:     Application startup complete.
[2026-01-05 11:47:57] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:47:58] INFO:     127.0.0.1:56950 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:47:59] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:47:58.677364937 compiler_depend.ts:198] Warning: Driver Version: "՚" is invalid or not supported yet. (function operator())
[2026-01-05 11:48:04] INFO:     127.0.0.1:56970 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:48:11] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:11] The server is fired up and ready to roll!
[2026-01-05 11:48:14] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:48:15] INFO:     127.0.0.1:39708 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:48:15] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:48:15] INFO:     127.0.0.1:39722 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:48:15] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:48:15] INFO:     127.0.0.1:39738 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-7B-RL --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:48:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:48:15] Prefill batch, #new-seq: 12, #new-token: 1792, #cached-token: 9216, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 11:48:15] Prefill batch, #new-seq: 27, #new-token: 3456, #cached-token: 20736, token usage: 0.01, #running-req: 13, #queue-req: 0,
[2026-01-05 11:48:15] Prefill batch, #new-seq: 21, #new-token: 2816, #cached-token: 16128, token usage: 0.02, #running-req: 40, #queue-req: 0,
[2026-01-05 11:48:15] Prefill batch, #new-seq: 56, #new-token: 7424, #cached-token: 43008, token usage: 0.04, #running-req: 61, #queue-req: 0,
[2026-01-05 11:48:15] Prefill batch, #new-seq: 11, #new-token: 1536, #cached-token: 8448, token usage: 0.07, #running-req: 117, #queue-req: 0,
[2026-01-05 11:48:16] Decode batch, #running-req: 128, #token: 21376, token usage: 0.09, cpu graph: False, gen throughput (token/s): 54.05, #queue-req: 0,
[2026-01-05 11:48:17] INFO:     127.0.0.1:39754 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:17] INFO:     127.0.0.1:39776 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:02<07:05,  2.14s/it]
  1%|          | 2/200 [00:02<04:26,  1.35s/it][2026-01-05 11:48:17] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 126, #queue-req: 0,
[2026-01-05 11:48:17] INFO:     127.0.0.1:40390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:17] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:17] INFO:     127.0.0.1:40186 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:02<02:22,  1.37it/s][2026-01-05 11:48:17] INFO:     127.0.0.1:40254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:17] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:17] INFO:     127.0.0.1:40332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:17] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:17] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-05 11:48:17] INFO:     127.0.0.1:39740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:17] INFO:     127.0.0.1:40750 - "POST /generate HTTP/1.1" 200 OK

  4%|▎         | 7/200 [00:02<01:13,  2.63it/s]
  4%|▍         | 8/200 [00:02<00:41,  4.65it/s][2026-01-05 11:48:17] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 11:48:17] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:17] INFO:     127.0.0.1:40612 - "POST /generate HTTP/1.1" 200 OK

  5%|▌         | 10/200 [00:02<00:32,  5.77it/s][2026-01-05 11:48:17] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 11:48:17] INFO:     127.0.0.1:40868 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:17] INFO:     127.0.0.1:40906 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:02<00:26,  7.06it/s][2026-01-05 11:48:17] INFO:     127.0.0.1:39902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:17] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 11:48:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:18] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:02<00:22,  8.27it/s][2026-01-05 11:48:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:18] Decode batch, #running-req: 127, #token: 31104, token usage: 0.12, cpu graph: False, gen throughput (token/s): 3683.38, #queue-req: 0,
[2026-01-05 11:48:18] INFO:     127.0.0.1:40448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] INFO:     127.0.0.1:40302 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,

  8%|▊         | 16/200 [00:03<00:19,  9.40it/s][2026-01-05 11:48:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:18] INFO:     127.0.0.1:40094 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] INFO:     127.0.0.1:40452 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:03<00:16, 10.98it/s][2026-01-05 11:48:18] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] INFO:     127.0.0.1:40242 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] INFO:     127.0.0.1:40708 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 11:48:18] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:18] INFO:     127.0.0.1:40422 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 131, #queue-req: 0,
[2026-01-05 11:48:18] INFO:     127.0.0.1:40518 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:03<00:11, 15.84it/s][2026-01-05 11:48:18] INFO:     127.0.0.1:40144 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:18] INFO:     127.0.0.1:40560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] INFO:     127.0.0.1:40608 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:18] INFO:     127.0.0.1:40840 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 130, #queue-req: 0,
[2026-01-05 11:48:18] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 29/200 [00:03<00:08, 20.04it/s][2026-01-05 11:48:18] INFO:     127.0.0.1:39970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:18] INFO:     127.0.0.1:40536 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:18] INFO:     127.0.0.1:40694 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:03<00:09, 17.69it/s][2026-01-05 11:48:18] INFO:     127.0.0.1:40736 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:19] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:19] INFO:     127.0.0.1:39938 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK

 17%|█▋        | 34/200 [00:03<00:09, 16.96it/s]
 18%|█▊        | 35/200 [00:03<00:09, 18.11it/s][2026-01-05 11:48:19] INFO:     127.0.0.1:39806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 11:48:19] INFO:     127.0.0.1:40222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:19] INFO:     127.0.0.1:40676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-05 11:48:19] INFO:     127.0.0.1:40064 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:04<00:08, 19.80it/s][2026-01-05 11:48:19] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:19] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:19] INFO:     127.0.0.1:40404 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] INFO:     127.0.0.1:40354 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:19] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] INFO:     127.0.0.1:40748 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:04<00:09, 17.39it/s]
 22%|██▏       | 44/200 [00:04<00:08, 18.95it/s]
 22%|██▏       | 44/200 [00:04<00:08, 18.95it/s][2026-01-05 11:48:19] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] INFO:     127.0.0.1:40080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:19] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 131, #queue-req: 0,
[2026-01-05 11:48:19] INFO:     127.0.0.1:40024 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] INFO:     127.0.0.1:40316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] INFO:     127.0.0.1:40380 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:04<00:08, 17.61it/s]
 25%|██▌       | 50/200 [00:04<00:06, 22.62it/s]
 25%|██▌       | 50/200 [00:04<00:06, 22.62it/s]
 25%|██▌       | 50/200 [00:04<00:06, 22.62it/s][2026-01-05 11:48:19] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.11, #running-req: 124, #queue-req: 0,
[2026-01-05 11:48:19] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:19] INFO:     127.0.0.1:40480 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:19] INFO:     127.0.0.1:39778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:04<00:07, 20.12it/s]
 28%|██▊       | 55/200 [00:04<00:06, 21.03it/s]
 28%|██▊       | 55/200 [00:04<00:06, 21.03it/s][2026-01-05 11:48:19] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-05 11:48:19] INFO:     127.0.0.1:40042 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] INFO:     127.0.0.1:40848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] INFO:     127.0.0.1:40882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:19] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-05 11:48:20] Decode batch, #running-req: 125, #token: 28160, token usage: 0.11, cpu graph: False, gen throughput (token/s): 2719.27, #queue-req: 0,
[2026-01-05 11:48:20] INFO:     127.0.0.1:39832 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:04<00:05, 23.62it/s][2026-01-05 11:48:20] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:20] INFO:     127.0.0.1:40160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-05 11:48:20] INFO:     127.0.0.1:40856 - "POST /generate HTTP/1.1" 200 OK

 31%|███       | 62/200 [00:05<00:06, 21.83it/s][2026-01-05 11:48:20] INFO:     127.0.0.1:40504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:20] INFO:     127.0.0.1:40716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:20] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-05 11:48:20] INFO:     127.0.0.1:40326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:05<00:06, 19.71it/s]
 34%|███▎      | 67/200 [00:05<00:06, 21.60it/s]
 34%|███▎      | 67/200 [00:05<00:06, 21.60it/s][2026-01-05 11:48:20] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-05 11:48:20] INFO:     127.0.0.1:52806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:20] INFO:     127.0.0.1:40374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] INFO:     127.0.0.1:40580 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:05<00:05, 22.35it/s][2026-01-05 11:48:20] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-05 11:48:20] INFO:     127.0.0.1:40336 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] INFO:     127.0.0.1:40822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-05 11:48:20] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-05 11:48:20] INFO:     127.0.0.1:39888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] INFO:     127.0.0.1:39996 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:05<00:07, 17.61it/s]
 37%|███▋      | 74/200 [00:05<00:07, 16.44it/s][2026-01-05 11:48:20] INFO:     127.0.0.1:40288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] INFO:     127.0.0.1:52682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:20] INFO:     127.0.0.1:40264 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:40224 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:05<00:07, 16.14it/s][2026-01-05 11:48:21] INFO:     127.0.0.1:40396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:52648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:52712 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 81/200 [00:06<00:06, 17.12it/s][2026-01-05 11:48:21] INFO:     127.0.0.1:40108 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:40648 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:40680 - "POST /generate HTTP/1.1" 200 OK

 43%|████▎     | 86/200 [00:06<00:05, 22.19it/s][2026-01-05 11:48:21] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] Decode batch, #running-req: 112, #token: 28672, token usage: 0.11, cpu graph: False, gen throughput (token/s): 3387.43, #queue-req: 0,
[2026-01-05 11:48:21] INFO:     127.0.0.1:40430 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:06<00:05, 20.88it/s][2026-01-05 11:48:21] INFO:     127.0.0.1:40378 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:40884 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:06<00:05, 18.07it/s][2026-01-05 11:48:21] INFO:     127.0.0.1:40596 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:52662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:06<00:04, 23.28it/s][2026-01-05 11:48:21] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:39874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:40466 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:21] INFO:     127.0.0.1:52950 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:06<00:04, 21.63it/s]
 50%|█████     | 101/200 [00:06<00:04, 22.25it/s][2026-01-05 11:48:22] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:06<00:04, 23.35it/s][2026-01-05 11:48:22] INFO:     127.0.0.1:39828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:40116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:40182 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:07<00:04, 22.57it/s][2026-01-05 11:48:22] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:40890 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:40918 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:40012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 112/200 [00:07<00:03, 25.66it/s]
 56%|█████▋    | 113/200 [00:07<00:02, 30.45it/s][2026-01-05 11:48:22] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:39912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:39926 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:07<00:02, 32.09it/s]
 59%|█████▉    | 118/200 [00:07<00:02, 35.83it/s][2026-01-05 11:48:22] Decode batch, #running-req: 82, #token: 25344, token usage: 0.10, cpu graph: False, gen throughput (token/s): 3556.76, #queue-req: 0,
[2026-01-05 11:48:22] INFO:     127.0.0.1:52750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:53082 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK

 61%|██████    | 122/200 [00:07<00:02, 30.34it/s]
 62%|██████▏   | 123/200 [00:07<00:02, 29.23it/s][2026-01-05 11:48:22] INFO:     127.0.0.1:52932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:53192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:40808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▎   | 127/200 [00:07<00:02, 25.56it/s][2026-01-05 11:48:22] INFO:     127.0.0.1:40420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:22] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:52996 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:07<00:02, 25.15it/s][2026-01-05 11:48:23] INFO:     127.0.0.1:52652 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:08<00:02, 26.77it/s][2026-01-05 11:48:23] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:40900 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:08<00:02, 26.05it/s][2026-01-05 11:48:23] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 140/200 [00:08<00:02, 25.52it/s]
 70%|███████   | 141/200 [00:08<00:02, 27.37it/s][2026-01-05 11:48:23] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:08<00:02, 23.78it/s]
 72%|███████▎  | 145/200 [00:08<00:02, 23.57it/s][2026-01-05 11:48:23] Decode batch, #running-req: 57, #token: 19584, token usage: 0.08, cpu graph: False, gen throughput (token/s): 2712.39, #queue-req: 0,
[2026-01-05 11:48:23] INFO:     127.0.0.1:53028 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:39860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 148/200 [00:08<00:03, 15.46it/s][2026-01-05 11:48:23] INFO:     127.0.0.1:52634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:53154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:23] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:24] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:09<00:03, 15.94it/s][2026-01-05 11:48:24] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:24] Decode batch, #running-req: 47, #token: 18560, token usage: 0.07, cpu graph: False, gen throughput (token/s): 2159.36, #queue-req: 0,
[2026-01-05 11:48:24] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK

 77%|███████▋  | 154/200 [00:09<00:04,  9.89it/s][2026-01-05 11:48:25] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:25] INFO:     127.0.0.1:53142 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:10<00:06,  7.10it/s][2026-01-05 11:48:25] Decode batch, #running-req: 44, #token: 20352, token usage: 0.08, cpu graph: False, gen throughput (token/s): 1995.99, #queue-req: 0,
[2026-01-05 11:48:25] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:25] INFO:     127.0.0.1:53220 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:10<00:05,  7.41it/s][2026-01-05 11:48:25] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:25] INFO:     127.0.0.1:52696 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:10<00:04,  8.04it/s][2026-01-05 11:48:26] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:26] INFO:     127.0.0.1:40284 - "POST /generate HTTP/1.1" 200 OK

 81%|████████  | 162/200 [00:11<00:06,  6.01it/s][2026-01-05 11:48:26] Decode batch, #running-req: 38, #token: 18816, token usage: 0.08, cpu graph: False, gen throughput (token/s): 1766.11, #queue-req: 0,
[2026-01-05 11:48:26] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:11<00:07,  5.25it/s][2026-01-05 11:48:27] Decode batch, #running-req: 37, #token: 19328, token usage: 0.08, cpu graph: False, gen throughput (token/s): 1648.99, #queue-req: 0,
[2026-01-05 11:48:28] Decode batch, #running-req: 37, #token: 22272, token usage: 0.09, cpu graph: False, gen throughput (token/s): 1605.91, #queue-req: 0,
[2026-01-05 11:48:28] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:13<00:17,  2.12it/s][2026-01-05 11:48:28] INFO:     127.0.0.1:40026 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:13<00:14,  2.42it/s][2026-01-05 11:48:29] Decode batch, #running-req: 35, #token: 21888, token usage: 0.09, cpu graph: False, gen throughput (token/s): 1526.54, #queue-req: 0,
[2026-01-05 11:48:30] INFO:     127.0.0.1:39790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:39798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:39808 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:39848 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:39940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:39956 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] Decode batch, #running-req: 35, #token: 8448, token usage: 0.03, cpu graph: False, gen throughput (token/s): 1497.11, #queue-req: 0,
[2026-01-05 11:48:30] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40106 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40158 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40168 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40170 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40576 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40584 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40712 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:30] INFO:     127.0.0.1:40762 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:14<00:23,  1.47it/s]
 94%|█████████▎| 187/200 [00:14<00:02,  6.21it/s]
 94%|█████████▎| 187/200 [00:14<00:02,  6.21it/s]
 94%|█████████▎| 187/200 [00:14<00:02,  6.21it/s]
 94%|█████████▎| 187/200 [00:14<00:02,  6.21it/s]
 94%|█████████▎| 187/200 [00:14<00:02,  6.21it/s]
 94%|█████████▎| 187/200 [00:14<00:02,  6.21it/s]
 94%|█████████▎| 187/200 [00:14<00:02,  6.21it/s]
 94%|█████████▎| 187/200 [00:14<00:02,  6.21it/s]
 94%|█████████▎| 187/200 [00:14<00:02,  6.21it/s]
 94%|█████████▎| 187/200 [00:14<00:02,  6.21it/s]
 94%|█████████▎| 187/200 [00:14<00:02,  6.21it/s][2026-01-05 11:48:30] Decode batch, #running-req: 13, #token: 8960, token usage: 0.04, cpu graph: False, gen throughput (token/s): 595.92, #queue-req: 0,
[2026-01-05 11:48:31] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:31] INFO:     127.0.0.1:52708 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:16<00:02,  4.58it/s]
 94%|█████████▍| 189/200 [00:16<00:03,  3.62it/s][2026-01-05 11:48:31] INFO:     127.0.0.1:52736 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:16<00:02,  3.76it/s][2026-01-05 11:48:31] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:31] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:31] Decode batch, #running-req: 8, #token: 4608, token usage: 0.02, cpu graph: False, gen throughput (token/s): 546.03, #queue-req: 0,
[2026-01-05 11:48:31] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:31] INFO:     127.0.0.1:52852 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:16<00:01,  4.66it/s]
 97%|█████████▋| 194/200 [00:16<00:00,  6.28it/s][2026-01-05 11:48:32] INFO:     127.0.0.1:52920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:32] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:17<00:00,  6.07it/s][2026-01-05 11:48:32] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:32] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:17<00:00,  6.22it/s][2026-01-05 11:48:32] Decode batch, #running-req: 2, #token: 2048, token usage: 0.01, cpu graph: False, gen throughput (token/s): 186.46, #queue-req: 0,
[2026-01-05 11:48:32] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:48:32] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:17<00:00,  6.07it/s]
100%|██████████| 200/200 [00:17<00:00, 11.24it/s]
.
----------------------------------------------------------------------
Ran 1 test in 109.092s

OK
Accuracy: 0.790
Invalid: 0.000
Latency: 17.861 s
Output throughput: 2142.814 token/s
.
.
End (22/43):
filename='ascend/llm_models/test_ascend_mimo_7b_rl.py', elapsed=126, estimated_time=400
.
.

.
.
Begin (23/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_mistral_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:48:58] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/mistralai/Mistral-7B-Instruct-v0.2', tokenizer_path='/root/.cache/modelscope/hub/models/mistralai/Mistral-7B-Instruct-v0.2', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=948628933, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/mistralai/Mistral-7B-Instruct-v0.2', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 11:48:58] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:49:08] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 11:49:08] Init torch distributed ends. mem usage=-0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:49:09] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:49:09] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 11:49:09] Load weight begin. avail mem=60.82 GB

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:09<00:19,  9.87s/it]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:21<00:10, 10.74s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:32<00:00, 10.96s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:32<00:00, 10.81s/it]

[2026-01-05 11:49:43] Load weight end. type=MistralForCausalLM, dtype=torch.bfloat16, avail mem=47.30 GB, mem usage=13.52 GB.
[2026-01-05 11:49:43] Using KV cache dtype: torch.bfloat16
[2026-01-05 11:49:43] The available memory for KV cache is 35.14 GB.
[2026-01-05 11:49:43] KV Cache is allocated. #tokens: 287744, K size: 17.57 GB, V size: 17.57 GB
[2026-01-05 11:49:43] Memory pool end. avail mem=11.66 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 11:49:43] max_total_num_tokens=287744, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=11.66 GB
[2026-01-05 11:49:43] INFO:     Started server process [248582]
[2026-01-05 11:49:43] INFO:     Waiting for application startup.
[2026-01-05 11:49:43] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 11:49:43] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 11:49:43] INFO:     Application startup complete.
[2026-01-05 11:49:43] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 11:49:44] INFO:     127.0.0.1:33064 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 11:49:44] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 11:49:44.645003863 compiler_depend.ts:198] Warning: Driver Version: "G" is invalid or not supported yet. (function operator())
[2026-01-05 11:49:49] INFO:     127.0.0.1:41358 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:49:59] INFO:     127.0.0.1:52806 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 11:50:02] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:02] The server is fired up and ready to roll!
[2026-01-05 11:50:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:50:10] INFO:     127.0.0.1:45634 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-05 11:50:10] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-05 11:50:10] INFO:     127.0.0.1:45642 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-05 11:50:10] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:50:10] INFO:     127.0.0.1:45656 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/mistralai/Mistral-7B-Instruct-v0.2 --trust-remote-code --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-05 11:50:10] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 11:50:10] Prefill batch, #new-seq: 18, #new-token: 4096, #cached-token: 13824, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-05 11:50:10] Prefill batch, #new-seq: 40, #new-token: 8192, #cached-token: 30720, token usage: 0.02, #running-req: 19, #queue-req: 1,
[2026-01-05 11:50:10] Prefill batch, #new-seq: 41, #new-token: 8192, #cached-token: 30720, token usage: 0.05, #running-req: 58, #queue-req: 24,
[2026-01-05 11:50:11] Prefill batch, #new-seq: 29, #new-token: 6400, #cached-token: 22272, token usage: 0.07, #running-req: 99, #queue-req: 0,
[2026-01-05 11:50:14] Decode batch, #running-req: 128, #token: 33536, token usage: 0.12, cpu graph: False, gen throughput (token/s): 59.71, #queue-req: 0,
[2026-01-05 11:50:14] INFO:     127.0.0.1:45672 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:04<14:08,  4.27s/it][2026-01-05 11:50:14] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:14] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:14] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:15] INFO:     127.0.0.1:45684 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 3/200 [00:04<03:57,  1.20s/it][2026-01-05 11:50:15] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:15] INFO:     127.0.0.1:45894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:15] INFO:     127.0.0.1:45662 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:15] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:04<02:06,  1.55it/s]
  3%|▎         | 6/200 [00:04<01:07,  2.86it/s][2026-01-05 11:50:15] INFO:     127.0.0.1:45942 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:15] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 11:50:15] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-05 11:50:15] INFO:     127.0.0.1:46176 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 9/200 [00:04<00:42,  4.46it/s][2026-01-05 11:50:15] INFO:     127.0.0.1:45878 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:15] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:15] INFO:     127.0.0.1:46752 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:15] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:15] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-05 11:50:15] INFO:     127.0.0.1:46490 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 12/200 [00:05<00:32,  5.74it/s][2026-01-05 11:50:15] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:15] INFO:     127.0.0.1:46284 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:15] INFO:     127.0.0.1:46628 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:15] INFO:     127.0.0.1:46874 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:05<00:26,  7.00it/s]
  8%|▊         | 15/200 [00:05<00:19,  9.58it/s][2026-01-05 11:50:15] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:15] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.12, #running-req: 125, #queue-req: 0,
[2026-01-05 11:50:15] Decode batch, #running-req: 125, #token: 33152, token usage: 0.12, cpu graph: False, gen throughput (token/s): 3541.41, #queue-req: 0,
[2026-01-05 11:50:15] INFO:     127.0.0.1:46530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:15] INFO:     127.0.0.1:46780 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:15] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:15] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:46822 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:05<00:15, 11.96it/s][2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:46004 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:05<00:15, 11.89it/s][2026-01-05 11:50:16] INFO:     127.0.0.1:46562 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:46796 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:05<00:14, 12.48it/s][2026-01-05 11:50:16] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:46840 - "POST /generate HTTP/1.1" 200 OK

 13%|█▎        | 26/200 [00:06<00:12, 14.07it/s][2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:46360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:46604 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:06<00:11, 14.77it/s][2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:46002 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:06<00:09, 17.49it/s][2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:45802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] INFO:     127.0.0.1:46736 - "POST /generate HTTP/1.1" 200 OK

 16%|█▋        | 33/200 [00:06<00:09, 17.36it/s]
 18%|█▊        | 35/200 [00:06<00:07, 23.48it/s]
 18%|█▊        | 35/200 [00:06<00:07, 23.48it/s][2026-01-05 11:50:16] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] Prefill batch, #new-seq: 4, #new-token: 896, #cached-token: 3072, token usage: 0.12, #running-req: 124, #queue-req: 0,
[2026-01-05 11:50:16] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-05 11:50:17] INFO:     127.0.0.1:46242 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:06<00:07, 20.44it/s][2026-01-05 11:50:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:17] INFO:     127.0.0.1:46870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] INFO:     127.0.0.1:46420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:17] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 129, #queue-req: 0,
[2026-01-05 11:50:17] INFO:     127.0.0.1:46306 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:06<00:08, 18.72it/s]
 21%|██        | 42/200 [00:06<00:08, 19.44it/s][2026-01-05 11:50:17] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.12, #running-req: 126, #queue-req: 0,
[2026-01-05 11:50:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:17] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] INFO:     127.0.0.1:46398 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.12, #running-req: 129, #queue-req: 0,
[2026-01-05 11:50:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.12, #running-req: 131, #queue-req: 0,
[2026-01-05 11:50:17] INFO:     127.0.0.1:46264 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:06<00:07, 21.27it/s][2026-01-05 11:50:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.13, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:17] INFO:     127.0.0.1:45908 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] INFO:     127.0.0.1:46036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] INFO:     127.0.0.1:46508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:07<00:06, 22.26it/s]
 26%|██▌       | 51/200 [00:07<00:05, 25.14it/s][2026-01-05 11:50:17] Prefill batch, #new-seq: 4, #new-token: 768, #cached-token: 3072, token usage: 0.12, #running-req: 124, #queue-req: 0,
[2026-01-05 11:50:17] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:17] INFO:     127.0.0.1:46126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.12, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:17] INFO:     127.0.0.1:46400 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:07<00:07, 19.82it/s][2026-01-05 11:50:17] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.13, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:17] Decode batch, #running-req: 128, #token: 37760, token usage: 0.13, cpu graph: False, gen throughput (token/s): 2524.65, #queue-req: 0,
[2026-01-05 11:50:17] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] INFO:     127.0.0.1:46860 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.13, #running-req: 126, #queue-req: 0,

 28%|██▊       | 57/200 [00:07<00:07, 19.19it/s][2026-01-05 11:50:17] INFO:     127.0.0.1:46456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:17] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 129, #queue-req: 0,
[2026-01-05 11:50:18] INFO:     127.0.0.1:45694 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] INFO:     127.0.0.1:46804 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:07<00:07, 18.77it/s][2026-01-05 11:50:18] INFO:     127.0.0.1:46148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.13, #running-req: 126, #queue-req: 0,
[2026-01-05 11:50:18] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] INFO:     127.0.0.1:45718 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.13, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 130, #queue-req: 0,
[2026-01-05 11:50:18] INFO:     127.0.0.1:45708 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 64/200 [00:07<00:06, 20.07it/s][2026-01-05 11:50:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:18] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] INFO:     127.0.0.1:46574 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] Prefill batch, #new-seq: 2, #new-token: 512, #cached-token: 1536, token usage: 0.13, #running-req: 126, #queue-req: 0,
[2026-01-05 11:50:18] INFO:     127.0.0.1:46978 - "POST /generate HTTP/1.1" 200 OK

 34%|███▎      | 67/200 [00:07<00:06, 19.17it/s][2026-01-05 11:50:18] INFO:     127.0.0.1:45746 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.13, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:18] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:08<00:08, 16.30it/s][2026-01-05 11:50:18] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.14, #running-req: 127, #queue-req: 0,
[2026-01-05 11:50:18] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.14, #running-req: 128, #queue-req: 0,
[2026-01-05 11:50:18] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.14, #running-req: 129, #queue-req: 0,
[2026-01-05 11:50:18] INFO:     127.0.0.1:46134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] INFO:     127.0.0.1:46690 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:08<00:07, 16.80it/s]
 36%|███▋      | 73/200 [00:08<00:06, 18.89it/s][2026-01-05 11:50:18] INFO:     127.0.0.1:46014 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.13, #running-req: 126, #queue-req: 0,
[2026-01-05 11:50:18] INFO:     127.0.0.1:45928 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:18] INFO:     127.0.0.1:46900 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:08<00:05, 22.56it/s][2026-01-05 11:50:19] INFO:     127.0.0.1:34592 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:08<00:05, 22.15it/s]
 40%|████      | 81/200 [00:08<00:04, 23.88it/s][2026-01-05 11:50:19] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46932 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK

 42%|████▏     | 84/200 [00:08<00:04, 24.67it/s][2026-01-05 11:50:19] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46322 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46820 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:08<00:04, 23.70it/s][2026-01-05 11:50:19] INFO:     127.0.0.1:45734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46478 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46492 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46710 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] Decode batch, #running-req: 112, #token: 33664, token usage: 0.12, cpu graph: False, gen throughput (token/s): 3238.55, #queue-req: 0,
[2026-01-05 11:50:19] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:09<00:03, 28.28it/s][2026-01-05 11:50:19] INFO:     127.0.0.1:45976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:47020 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:34400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:09<00:03, 29.86it/s]
 50%|████▉     | 99/200 [00:09<00:03, 33.35it/s][2026-01-05 11:50:19] INFO:     127.0.0.1:46112 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46990 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:34698 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:09<00:02, 34.39it/s][2026-01-05 11:50:19] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:34504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:47022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:09<00:02, 37.51it/s][2026-01-05 11:50:19] INFO:     127.0.0.1:34778 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:19] INFO:     127.0.0.1:45834 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▋    | 113/200 [00:09<00:02, 37.39it/s][2026-01-05 11:50:20] INFO:     127.0.0.1:45922 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:46744 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:46160 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:46020 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 117/200 [00:09<00:02, 37.47it/s][2026-01-05 11:50:20] INFO:     127.0.0.1:34516 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:34750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:46590 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:09<00:02, 35.17it/s][2026-01-05 11:50:20] INFO:     127.0.0.1:34674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:34578 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:09<00:02, 33.92it/s][2026-01-05 11:50:20] INFO:     127.0.0.1:45776 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:46182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:46034 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] Decode batch, #running-req: 71, #token: 24704, token usage: 0.09, cpu graph: False, gen throughput (token/s): 3385.35, #queue-req: 0,
[2026-01-05 11:50:20] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:10<00:02, 33.66it/s][2026-01-05 11:50:20] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:46660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK

 67%|██████▋   | 134/200 [00:10<00:01, 33.24it/s][2026-01-05 11:50:20] INFO:     127.0.0.1:46096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:34560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:45958 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:47028 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:10<00:02, 25.71it/s]
 70%|██████▉   | 139/200 [00:10<00:02, 23.68it/s][2026-01-05 11:50:20] INFO:     127.0.0.1:46088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:34396 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:45770 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:20] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:45682 - "POST /generate HTTP/1.1" 200 OK

 73%|███████▎  | 146/200 [00:10<00:01, 30.84it/s][2026-01-05 11:50:21] INFO:     127.0.0.1:46052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:46942 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:47076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:34606 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 152/200 [00:10<00:01, 35.09it/s][2026-01-05 11:50:21] INFO:     127.0.0.1:34714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:46894 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:34856 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:10<00:01, 29.21it/s][2026-01-05 11:50:21] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:34842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] Decode batch, #running-req: 44, #token: 16256, token usage: 0.06, cpu graph: False, gen throughput (token/s): 2350.48, #queue-req: 0,
[2026-01-05 11:50:21] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:34818 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:11<00:01, 21.11it/s][2026-01-05 11:50:21] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:34486 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:11<00:01, 21.90it/s][2026-01-05 11:50:21] INFO:     127.0.0.1:34424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:34564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:34742 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:11<00:01, 22.61it/s][2026-01-05 11:50:21] INFO:     127.0.0.1:45970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:21] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:22] INFO:     127.0.0.1:46198 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:22] INFO:     127.0.0.1:34538 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:11<00:01, 23.11it/s]
 86%|████████▌ | 171/200 [00:11<00:01, 25.26it/s][2026-01-05 11:50:22] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:22] INFO:     127.0.0.1:34866 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:22] INFO:     127.0.0.1:34626 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:11<00:01, 21.52it/s][2026-01-05 11:50:22] INFO:     127.0.0.1:34638 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:22] Decode batch, #running-req: 25, #token: 11264, token usage: 0.04, cpu graph: False, gen throughput (token/s): 1488.98, #queue-req: 0,
[2026-01-05 11:50:22] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:22] INFO:     127.0.0.1:34588 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:12<00:01, 15.82it/s][2026-01-05 11:50:22] INFO:     127.0.0.1:34484 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:22] INFO:     127.0.0.1:46766 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:22] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:12<00:01, 17.58it/s][2026-01-05 11:50:22] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:22] INFO:     127.0.0.1:34806 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:22] INFO:     127.0.0.1:47066 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:12<00:01, 16.60it/s][2026-01-05 11:50:23] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:23] INFO:     127.0.0.1:34788 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:12<00:00, 16.76it/s][2026-01-05 11:50:23] INFO:     127.0.0.1:46076 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:23] INFO:     127.0.0.1:45818 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:12<00:00, 15.56it/s][2026-01-05 11:50:23] Decode batch, #running-req: 13, #token: 6656, token usage: 0.02, cpu graph: False, gen throughput (token/s): 856.07, #queue-req: 0,
[2026-01-05 11:50:23] INFO:     127.0.0.1:34772 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:23] INFO:     127.0.0.1:34794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:23] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:13<00:00, 13.78it/s][2026-01-05 11:50:23] INFO:     127.0.0.1:47012 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:23] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:13<00:00,  9.47it/s][2026-01-05 11:50:23] INFO:     127.0.0.1:45790 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:24] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:13<00:00,  9.85it/s][2026-01-05 11:50:24] Decode batch, #running-req: 6, #token: 3328, token usage: 0.01, cpu graph: False, gen throughput (token/s): 417.99, #queue-req: 0,
[2026-01-05 11:50:24] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:24] INFO:     127.0.0.1:34710 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:13<00:00, 10.80it/s][2026-01-05 11:50:24] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:24] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:13<00:00, 10.24it/s][2026-01-05 11:50:25] Decode batch, #running-req: 2, #token: 1792, token usage: 0.01, cpu graph: False, gen throughput (token/s): 117.96, #queue-req: 0,
[2026-01-05 11:50:25] Decode batch, #running-req: 2, #token: 1920, token usage: 0.01, cpu graph: False, gen throughput (token/s): 92.87, #queue-req: 0,
[2026-01-05 11:50:26] Decode batch, #running-req: 2, #token: 2048, token usage: 0.01, cpu graph: False, gen throughput (token/s): 92.78, #queue-req: 0,
[2026-01-05 11:50:27] Decode batch, #running-req: 2, #token: 2048, token usage: 0.01, cpu graph: False, gen throughput (token/s): 92.74, #queue-req: 0,
[2026-01-05 11:50:27] INFO:     127.0.0.1:34720 - "POST /generate HTTP/1.1" 200 OK
[2026-01-05 11:50:28] INFO:     127.0.0.1:34632 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [00:17<00:00,  1.65it/s]
100%|██████████| 200/200 [00:17<00:00, 11.24it/s]
.
----------------------------------------------------------------------
Ran 1 test in 99.183s

OK
Accuracy: 0.385
Invalid: 0.005
Latency: 17.876 s
Output throughput: 1563.515 token/s
.
.
End (23/43):
filename='ascend/llm_models/test_ascend_mistral_7b.py', elapsed=110, estimated_time=400
.
.

.
.
Begin (24/43):
python3 /data/c30044170/newHDK/ascend/llm_models/test_ascend_embedding_models.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[CI Test Method] TestEmbeddingModels.test_prefill_logits
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:50:56] INFO SentenceTransformer.py:219: Use pytorch device_name: npu
[2026-01-05 11:50:56] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: /root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct
`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.73s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.73s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.03s/it]
[2026-01-05 11:51:04] INFO SentenceTransformer.py:378: 1 prompt is loaded, with the key: query

Batches:   0%|          | 0/1 [00:00<?, ?it/s]
Batches: 100%|██████████| 1/1 [00:01<00:00,  1.49s/it]
Batches: 100%|██████████| 1/1 [00:01<00:00,  1.49s/it]
[2026-01-05 11:51:06] INFO model_config.py:1010: Downcasting torch.float32 to torch.float16.
[2026-01-05 11:51:06] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-05 11:51:06] INFO engine.py:220: server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.65, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=10061939, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.03it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.55s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.46s/it]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[rank0]:[W105 11:51:22.437324420 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return fn(*args, **kwargs)
.
----------------------------------------------------------------------
Ran 1 test in 39.443s

OK
similarity diff tensor(1.5795e-05)
prefill_tolerance=1e-05
similarity diff tensor(1.4305e-06)
prefill_tolerance=1e-05
similarity diff tensor(1.1921e-06)
prefill_tolerance=1e-05
similarity diff tensor(1.3113e-06)
prefill_tolerance=1e-05
similarity diff tensor(0.1605)
prefill_tolerance=1e-05
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffc9f58c8a0>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (24/43):
filename='ascend/llm_models/test_ascend_embedding_models.py', elapsed=62, estimated_time=400
.
.

.
.
Begin (25/43):
python3 /data/c30044170/newHDK/ascend/embedding_models/test_ascend_embedding_models_e5.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[CI Test Method] TestEmbeddingModels.test_prefill_logits
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:51:51] INFO SentenceTransformer.py:219: Use pytorch device_name: npu
[2026-01-05 11:51:51] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/intfloat/e5-mistral-7b-instruct
`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.77it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.66it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.48it/s]
[2026-01-05 11:52:15] INFO SentenceTransformer.py:380: 4 prompts are loaded, with the keys: ['web_search_query', 'sts_query', 'summarization_query', 'bitext_query']

Batches:   0%|          | 0/1 [00:00<?, ?it/s]
Batches: 100%|██████████| 1/1 [00:04<00:00,  4.09s/it]
Batches: 100%|██████████| 1/1 [00:04<00:00,  4.09s/it]
[2026-01-05 11:52:20] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-05 11:52:20] INFO engine.py:220: server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/intfloat/e5-mistral-7b-instruct', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/intfloat/e5-mistral-7b-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.65, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=607383316, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/intfloat/e5-mistral-7b-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.75it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.99it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.95it/s]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[rank0]:[W105 11:52:32.441226076 compiler_depend.ts:198] Warning: Driver Version: "&" is invalid or not supported yet. (function operator())
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return fn(*args, **kwargs)
.
----------------------------------------------------------------------
Ran 1 test in 56.841s

OK
similarity diff tensor(0.1948)
similarity diff tensor(1.6093e-06)
similarity diff tensor(1.7881e-06)
similarity diff tensor(2.2650e-06)
similarity diff tensor(0.0225)
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfff7c1c7c050>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (25/43):
filename='ascend/embedding_models/test_ascend_embedding_models_e5.py', elapsed=71, estimated_time=400
.
.

.
.
Begin (26/43):
python3 /data/c30044170/newHDK/ascend/embedding_models/test_embedding_models_clip_vit_large_patch14_336.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Token indices sequence length is longer than the specified maximum sequence length for this model (6404 > 77). Running this sequence through the model will result in indexing errors
[CI Test Method] TestEmbeddingModels.test_prefill_logits
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[W105 11:53:08.487996694 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[2026-01-05 11:53:09] INFO model_config.py:1010: Downcasting torch.float32 to torch.float16.
[2026-01-05 11:53:09] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-05 11:53:09] INFO engine.py:220: server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/AI-ModelScope/clip-vit-large-patch14-336', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/AI-ModelScope/clip-vit-large-patch14-336', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.65, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=1056699645, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/AI-ModelScope/clip-vit-large-patch14-336', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(

Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]

Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[rank0]:[W105 11:53:22.869531371 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return fn(*args, **kwargs)
.
----------------------------------------------------------------------
Ran 1 test in 31.291s

OK
similarity diff tensor(4.1723e-07)
similarity diff tensor(5.9605e-07)
similarity diff tensor(0.1140)
similarity diff tensor(0.2194)
similarity diff tensor(0.1198)
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfff71aa43d20>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (26/43):
filename='ascend/embedding_models/test_embedding_models_clip_vit_large_patch14_336.py', elapsed=47, estimated_time=400
.
.

.
.
Begin (27/43):
python3 /data/c30044170/newHDK/ascend/embedding_models/test_ascend_embedding_models.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[CI Test Method] TestEmbeddingModels.test_prefill_logits
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:53:50] INFO SentenceTransformer.py:219: Use pytorch device_name: npu
[2026-01-05 11:53:50] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: /root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct
`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.58it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.31it/s]
[2026-01-05 11:53:52] INFO SentenceTransformer.py:378: 1 prompt is loaded, with the key: query

Batches:   0%|          | 0/1 [00:00<?, ?it/s]
Batches: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]
Batches: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]
[2026-01-05 11:53:54] INFO model_config.py:1010: Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 11:53:54] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-05 11:53:54] INFO engine.py:220: server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.65, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=334281656, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.08it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.35it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.30it/s]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[rank0]:[W105 11:54:08.443934899 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return fn(*args, **kwargs)
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/embedding_models/test_ascend_embedding_models.py", line 102, in test_prefill_logits
    self.assert_close_prefill_logits(
  File "/data/c30044170/newHDK/ascend/embedding_models/test_ascend_embedding_models.py", line 93, in assert_close_prefill_logits
    assert torch.all(
           ^^^^^^^^^^
AssertionError: embeddings are not all close
E
======================================================================
ERROR: test_prefill_logits (__main__.TestEmbeddingModels.test_prefill_logits)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: embeddings are not all close

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 28.921s

FAILED (errors=1)
similarity diff tensor(0.0003)
similarity diff tensor(9.4175e-05)
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffcaf4bc8a0>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (27/43):
filename='ascend/embedding_models/test_ascend_embedding_models.py', elapsed=45, estimated_time=400
.
.


[CI Retry] ascend/embedding_models/test_ascend_embedding_models.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/embedding_models/test_ascend_embedding_models.py

.
.
Begin (27/43):
python3 /data/c30044170/newHDK/ascend/embedding_models/test_ascend_embedding_models.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[CI Test Method] TestEmbeddingModels.test_prefill_logits
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:55:42] INFO SentenceTransformer.py:219: Use pytorch device_name: npu
[2026-01-05 11:55:42] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: /root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct
`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.83s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]
[2026-01-05 11:55:50] INFO SentenceTransformer.py:378: 1 prompt is loaded, with the key: query

Batches:   0%|          | 0/1 [00:00<?, ?it/s]
Batches: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]
Batches: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]
[2026-01-05 11:55:52] INFO model_config.py:1010: Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 11:55:52] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-05 11:55:52] INFO engine.py:220: server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.65, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=516589388, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/iic/gte_Qwen2-1.5B-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.12it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.59s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.49s/it]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[rank0]:[W105 11:56:08.229355716 compiler_depend.ts:198] Warning: Driver Version: "u" is invalid or not supported yet. (function operator())
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return fn(*args, **kwargs)
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/embedding_models/test_ascend_embedding_models.py", line 102, in test_prefill_logits
    self.assert_close_prefill_logits(
  File "/data/c30044170/newHDK/ascend/embedding_models/test_ascend_embedding_models.py", line 93, in assert_close_prefill_logits
    assert torch.all(
           ^^^^^^^^^^
AssertionError: embeddings are not all close
E
======================================================================
ERROR: test_prefill_logits (__main__.TestEmbeddingModels.test_prefill_logits)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: embeddings are not all close

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 37.296s

FAILED (errors=1)
similarity diff tensor(0.0003)
similarity diff tensor(9.4175e-05)
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffc9eeb4910>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (27/43):
filename='ascend/embedding_models/test_ascend_embedding_models.py', elapsed=60, estimated_time=400
.
.


✗ FAILED: ascend/embedding_models/test_ascend_embedding_models.py returned exit code 1

.
.
Begin (28/43):
python3 /data/c30044170/newHDK/ascend/reward_models/test_ascned_llama_3_1_8b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[CI Test Method] TestRewardModels.test_reward_scores
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.36s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.33s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.27s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.24s/it]
[2026-01-05 11:56:50] WARNING model_config.py:1014: Casting torch.bfloat16 to torch.float16.
[2026-01-05 11:56:50] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-05 11:56:50] INFO engine.py:220: server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/huggingface/hub/models--Skywork--Skywork-Reward-Llama-3.1-8B-v0.2/snapshots/d4117fbfd81b72f41b96341238baa1e3e90a4ce1/', tokenizer_path='/data/ascend-ci-share-pkking-sglang/huggingface/hub/models--Skywork--Skywork-Reward-Llama-3.1-8B-v0.2/snapshots/d4117fbfd81b72f41b96341238baa1e3e90a4ce1/', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.65, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=270899685, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/ascend-ci-share-pkking-sglang/huggingface/hub/models--Skywork--Skywork-Reward-Llama-3.1-8B-v0.2/snapshots/d4117fbfd81b72f41b96341238baa1e3e90a4ce1/', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.03s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.27s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.12it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.04it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.01it/s]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[rank0]:[W105 11:57:07.656836720 compiler_depend.ts:198] Warning: Driver Version: "d" is invalid or not supported yet. (function operator())
.
----------------------------------------------------------------------
Ran 1 test in 43.341s

OK
hf_scores=tensor([-24.2500,   1.0918])
srt_scores=tensor([-24.2344,   1.0918])
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffc88f3fd20>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (28/43):
filename='ascend/reward_models/test_ascned_llama_3_1_8b.py', elapsed=59, estimated_time=400
.
.

.
.
Begin (29/43):
python3 /data/c30044170/newHDK/ascend/reward_models/test_ascend_qwen2_5_math_rm_72b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 11:57:25] WARNING model_config.py:1014: Casting torch.bfloat16 to torch.float16.
[2026-01-05 11:57:25] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-05 11:57:25] INFO engine.py:220: server_args=ServerArgs(model_path='/data/l30079981/weights/Qwen2.5-Math-RM-72B', tokenizer_path='/data/l30079981/weights/Qwen2.5-Math-RM-72B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=4438212, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/l30079981/weights/Qwen2.5-Math-RM-72B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[CI Test Method] TestInternlm2_7bReward.test_assert_close_reward_scores
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffac2373470>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffad6c8f470>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffae86eb470>
  self.init_sockets(server_args, port_args)
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(

Loading safetensors checkpoint shards:   0% Completed | 0/37 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   5% Completed | 2/37 [00:00<00:01, 19.00it/s]

Loading safetensors checkpoint shards:  14% Completed | 5/37 [00:00<00:01, 19.89it/s]

Loading safetensors checkpoint shards:  19% Completed | 7/37 [00:00<00:01, 19.37it/s]

Loading safetensors checkpoint shards:  24% Completed | 9/37 [00:00<00:01, 18.94it/s]

Loading safetensors checkpoint shards:  30% Completed | 11/37 [00:00<00:01, 17.66it/s]

Loading safetensors checkpoint shards:  35% Completed | 13/37 [00:00<00:01, 17.32it/s]

Loading safetensors checkpoint shards:  41% Completed | 15/37 [00:00<00:01, 17.74it/s]

Loading safetensors checkpoint shards:  46% Completed | 17/37 [00:00<00:01, 17.49it/s]

Loading safetensors checkpoint shards:  54% Completed | 20/37 [00:01<00:00, 18.87it/s]

Loading safetensors checkpoint shards:  59% Completed | 22/37 [00:01<00:00, 18.11it/s]

Loading safetensors checkpoint shards:  65% Completed | 24/37 [00:01<00:00, 18.31it/s]

Loading safetensors checkpoint shards:  70% Completed | 26/37 [00:01<00:00, 18.66it/s]

Loading safetensors checkpoint shards:  76% Completed | 28/37 [00:01<00:00, 17.69it/s]

Loading safetensors checkpoint shards:  81% Completed | 30/37 [00:01<00:00, 17.85it/s]

Loading safetensors checkpoint shards:  89% Completed | 33/37 [00:01<00:00, 19.22it/s]

Loading safetensors checkpoint shards:  97% Completed | 36/37 [00:01<00:00, 19.55it/s]

Loading safetensors checkpoint shards: 100% Completed | 37/37 [00:01<00:00, 18.52it/s]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[rank3]:[W105 12:00:20.520096867 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank0]:[W105 12:00:20.520113507 compiler_depend.ts:198] Warning: Driver Version: "N" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:00:20.520116098 compiler_depend.ts:198] Warning: Driver Version: "l" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:00:20.520115347 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
.
----------------------------------------------------------------------
Ran 1 test in 179.276s

OK
accuracy: tensor([-2.2480, -0.6484])
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffcc1267690>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (29/43):
filename='ascend/reward_models/test_ascend_qwen2_5_math_rm_72b.py', elapsed=194, estimated_time=400
.
.

.
.
Begin (30/43):
python3 /data/c30044170/newHDK/ascend/reward_models/test_ascend_reward_models.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:00:46] INFO model_config.py:1010: Downcasting torch.float32 to torch.float16.
[2026-01-05 12:00:46] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-05 12:00:46] INFO engine.py:220: server_args=ServerArgs(model_path='/data/l30079981/weights/Qwen2.5-1.5B-apeach/', tokenizer_path='/data/l30079981/weights/Qwen2.5-1.5B-apeach/', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=561647509, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/l30079981/weights/Qwen2.5-1.5B-apeach/', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[CI Test Method] TestInternlm2_7bReward.test_assert_close_reward_scores
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffac6a98fb0>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffabe844fb0>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffadaa98fb0>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.16it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.65it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.30it/s]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[rank2]:[W105 12:01:07.404182677 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:01:07.404218859 compiler_depend.ts:198] Warning: Driver Version: "׮" is invalid or not supported yet. (function operator())
[rank0]:[W105 12:01:07.404249810 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:01:07.404278091 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
.
----------------------------------------------------------------------
Ran 1 test in 24.506s

OK
accuracy: tensor([-2.8984, -2.9297])
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffca0c73e70>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (30/43):
filename='ascend/reward_models/test_ascend_reward_models.py', elapsed=47, estimated_time=400
.
.

.
.
Begin (31/43):
python3 /data/c30044170/newHDK/ascend/reward_models/test_ascend_reward_internlm2_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:01:26] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-05 12:01:26] INFO engine.py:220: server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b-reward', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b-reward', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=4186692, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b-reward', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[CI Test Method] TestInternlm2_7bReward.test_assert_close_reward_scores
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffac5987e90>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffae6857e90>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffae15e3e90>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(

Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:01<00:11,  1.64s/it]

Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:03<00:11,  1.90s/it]

Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:05<00:10,  2.06s/it]

Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:08<00:08,  2.16s/it]

Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:09<00:05,  1.80s/it]

Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:11<00:03,  1.82s/it]

Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:13<00:02,  2.10s/it]

Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:16<00:00,  2.08s/it]

Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:16<00:00,  2.00s/it]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[rank0]:[W105 12:01:58.231355888 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:01:58.231497873 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:01:58.231540015 compiler_depend.ts:198] Warning: Driver Version: "u" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:01:58.231542635 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
.
----------------------------------------------------------------------
Ran 1 test in 35.441s

OK
accuracy: tensor([-0.8408,  0.1798])
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffc82490980>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (31/43):
filename='ascend/reward_models/test_ascend_reward_internlm2_7b.py', elapsed=51, estimated_time=400
.
.

.
.
Begin (32/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_vlm_models_mistral-small_3_1_24b_instruct2503.py
.
.

Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/vlm_models/test_vlm_models_mistral-small_3_1_24b_instruct2503.py", line 11, in <module>
    from sglang.test.test_vlm_utils import TestVLMModels
ModuleNotFoundError: No module named 'sglang.test.test_vlm_utils'
.
.
End (32/43):
filename='ascend/vlm_models/test_vlm_models_mistral-small_3_1_24b_instruct2503.py', elapsed=1, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_vlm_models_mistral-small_3_1_24b_instruct2503.py failed with non-retriable error: ModuleNotFoundError - not retrying


✗ FAILED: ascend/vlm_models/test_vlm_models_mistral-small_3_1_24b_instruct2503.py returned exit code 1

.
.
Begin (33/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_janus_pro_1b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:02:25] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B', tokenizer_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=686274010, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-05 12:02:27] Inferred chat template from model path: janus-pro
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:02:35 TP3] Init torch distributed begin.
[2026-01-05 12:02:35 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:02:36 TP1] Init torch distributed begin.
[2026-01-05 12:02:36 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:02:37 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:02:37 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:02:37 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:02:37 TP2] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:02:37 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:02:37 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:02:37 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:02:37 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:02:37 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:02:38 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:02:38 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:02:38 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:02:38 TP2] Load weight begin. avail mem=60.89 GB
[2026-01-05 12:02:38 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:02:38 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:02:38 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:02:38 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:02:38 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:02:38 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:02:38 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:02:38 TP2] Using sdpa as multimodal attention backend.

Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading pt checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.36s/it]

Loading pt checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.36s/it]

[2026-01-05 12:02:52 TP1] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.59 GB, mem usage=1.54 GB.
[2026-01-05 12:02:52 TP3] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.58 GB, mem usage=1.54 GB.
[2026-01-05 12:02:52 TP2] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.35 GB, mem usage=1.54 GB.
[2026-01-05 12:02:52 TP0] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.28 GB, mem usage=1.54 GB.
[2026-01-05 12:02:52 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:02:52 TP0] The available memory for KV cache is 19.72 GB.
[2026-01-05 12:02:52 TP3] The available memory for KV cache is 19.72 GB.
[2026-01-05 12:02:52 TP2] The available memory for KV cache is 19.72 GB.
[2026-01-05 12:02:52 TP1] The available memory for KV cache is 19.72 GB.
[2026-01-05 12:02:53 TP0] KV Cache is allocated. #tokens: 430848, K size: 9.86 GB, V size: 9.86 GB
[2026-01-05 12:02:53 TP2] KV Cache is allocated. #tokens: 430848, K size: 9.86 GB, V size: 9.86 GB
[2026-01-05 12:02:53 TP3] KV Cache is allocated. #tokens: 430848, K size: 9.86 GB, V size: 9.86 GB
[2026-01-05 12:02:53 TP1] KV Cache is allocated. #tokens: 430848, K size: 9.86 GB, V size: 9.86 GB
[2026-01-05 12:02:53 TP0] Memory pool end. avail mem=39.27 GB
[2026-01-05 12:02:53 TP2] Memory pool end. avail mem=39.35 GB
[2026-01-05 12:02:53 TP3] Memory pool end. avail mem=39.58 GB
[2026-01-05 12:02:53 TP1] Memory pool end. avail mem=39.58 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:02:53 TP0] max_total_num_tokens=430848, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=16384, available_gpu_mem=39.27 GB
[2026-01-05 12:02:54] INFO:     Started server process [271920]
[2026-01-05 12:02:54] INFO:     Waiting for application startup.
[2026-01-05 12:02:54] INFO:     Application startup complete.
[2026-01-05 12:02:54] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:02:55] INFO:     127.0.0.1:50508 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:02:55 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:02:55.520970257 compiler_depend.ts:198] Warning: Driver Version: "/" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:02:55.522116929 compiler_depend.ts:198] Warning: Driver Version: "ϯ" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:02:55.522373929 compiler_depend.ts:198] Warning: Driver Version: "P" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:02:55.522461092 compiler_depend.ts:198] Warning: Driver Version: "-" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:02:56] INFO:     127.0.0.1:50538 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:03:06] INFO:     127.0.0.1:33896 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:03:16] INFO:     127.0.0.1:59326 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:03:19] INFO:     127.0.0.1:50524 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:03:19] The server is fired up and ready to roll!
[2026-01-05 12:03:26 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:03:27] INFO:     127.0.0.1:48060 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/vlm_models/vlm_utils.py", line 147, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/data/c30044170/newHDK/ascend/vlm_models/vlm_utils.py", line 93, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/__w/sglang/sglang/test/registered/ascend/vlm_models/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_janus_pro_1b.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/data/c30044170/newHDK/ascend/vlm_models/vlm_utils.py", line 177, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/__w/sglang/sglang/test/registered/ascend/vlm_models/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 271920 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/vlm_models/vlm_utils.py", line 147, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/data/c30044170/newHDK/ascend/vlm_models/vlm_utils.py", line 93, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/__w/sglang/sglang/test/registered/ascend/vlm_models/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/__w/sglang/sglang/test/registered/ascend/vlm_models/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 71.595s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/__w/sglang/sglang/test/registered/ascend/vlm_models/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 271920
.
.
End (33/43):
filename='ascend/vlm_models/test_ascend_janus_pro_1b.py', elapsed=81, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_janus_pro_1b.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_janus_pro_1b.py

.
.
Begin (33/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_janus_pro_1b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:04:47] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B', tokenizer_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=616733459, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-05 12:04:49] Inferred chat template from model path: janus-pro
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:04:56 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-05 12:04:57 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:04:57 TP3] Init torch distributed begin.
[2026-01-05 12:04:57 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:04:58 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:04:58 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:04:58 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:04:58 TP3] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:04:58 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:04:59 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:04:59 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:04:59 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:04:59 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:04:59 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:04:59 TP2] Load weight begin. avail mem=60.90 GB
[2026-01-05 12:04:59 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:04:59 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:04:59 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:04:59 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:04:59 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:04:59 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:04:59 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:04:59 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:04:59 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:04:59 TP3] Using sdpa as multimodal attention backend.

Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[2026-01-05 12:05:00 TP1] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.59 GB, mem usage=1.54 GB.

Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.21it/s]

Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.21it/s]

[2026-01-05 12:05:00 TP2] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.35 GB, mem usage=1.54 GB.
[2026-01-05 12:05:00 TP0] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.28 GB, mem usage=1.54 GB.
[2026-01-05 12:05:01 TP3] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.58 GB, mem usage=1.54 GB.
[2026-01-05 12:05:01 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:05:01 TP0] The available memory for KV cache is 19.72 GB.
[2026-01-05 12:05:01 TP2] The available memory for KV cache is 19.72 GB.
[2026-01-05 12:05:01 TP3] The available memory for KV cache is 19.72 GB.
[2026-01-05 12:05:01 TP1] The available memory for KV cache is 19.72 GB.
[2026-01-05 12:05:01 TP0] KV Cache is allocated. #tokens: 430848, K size: 9.86 GB, V size: 9.86 GB
[2026-01-05 12:05:01 TP2] KV Cache is allocated. #tokens: 430848, K size: 9.86 GB, V size: 9.86 GB
[2026-01-05 12:05:01 TP3] KV Cache is allocated. #tokens: 430848, K size: 9.86 GB, V size: 9.86 GB
[2026-01-05 12:05:01 TP1] KV Cache is allocated. #tokens: 430848, K size: 9.86 GB, V size: 9.86 GB
[2026-01-05 12:05:01 TP0] Memory pool end. avail mem=39.27 GB
[2026-01-05 12:05:01 TP2] Memory pool end. avail mem=39.35 GB
[2026-01-05 12:05:01 TP3] Memory pool end. avail mem=39.58 GB
[2026-01-05 12:05:01 TP1] Memory pool end. avail mem=39.59 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:05:02 TP0] max_total_num_tokens=430848, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=16384, available_gpu_mem=39.27 GB
[2026-01-05 12:05:02] INFO:     Started server process [279842]
[2026-01-05 12:05:02] INFO:     Waiting for application startup.
[2026-01-05 12:05:02] INFO:     Application startup complete.
[2026-01-05 12:05:02] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:05:03] INFO:     127.0.0.1:53806 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:05:04 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:05:04.775323841 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:05:04.775337952 compiler_depend.ts:198] Warning: Driver Version: "1" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:05:04.775345182 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:05:04.775371353 compiler_depend.ts:198] Warning: Driver Version: "g" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:05:07] INFO:     127.0.0.1:41366 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:05:17] INFO:     127.0.0.1:33090 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:05:27] INFO:     127.0.0.1:56712 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:05:29] INFO:     127.0.0.1:53822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:05:29] The server is fired up and ready to roll!
[2026-01-05 12:05:37 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:05:38] INFO:     127.0.0.1:45062 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/vlm_models/vlm_utils.py", line 147, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/data/c30044170/newHDK/ascend/vlm_models/vlm_utils.py", line 93, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/__w/sglang/sglang/test/registered/ascend/vlm_models/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_janus_pro_1b.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/data/c30044170/newHDK/ascend/vlm_models/vlm_utils.py", line 177, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/__w/sglang/sglang/test/registered/ascend/vlm_models/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 279842 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/vlm_models/vlm_utils.py", line 147, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/data/c30044170/newHDK/ascend/vlm_models/vlm_utils.py", line 93, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/__w/sglang/sglang/test/registered/ascend/vlm_models/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/__w/sglang/sglang/test/registered/ascend/vlm_models/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 61.623s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/__w/sglang/sglang/test/registered/ascend/vlm_models/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 279842
.
.
End (33/43):
filename='ascend/vlm_models/test_ascend_janus_pro_1b.py', elapsed=72, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_janus_pro_1b.py returned exit code 1

.
.
Begin (34/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_235.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:06:02] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=162850902, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:06:04] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:06:12 TP3] Init torch distributed begin.
[2026-01-05 12:06:12 TP0] Init torch distributed begin.
[2026-01-05 12:06:12 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:06:13 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:06:14 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:06:14 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:06:14 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:06:14 TP0] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:06:15 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:06:15 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:06:15 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:06:15 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:06:15 TP0] Load weight begin. avail mem=60.80 GB
[2026-01-05 12:06:15 TP3] Load weight begin. avail mem=61.11 GB
[2026-01-05 12:06:15 TP2] Load weight begin. avail mem=60.88 GB
[2026-01-05 12:06:15 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:06:15 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:06:15 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:06:15 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:06:15 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:06:15 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:06:15 TP1] Load weight begin. avail mem=61.11 GB
[2026-01-05 12:06:16 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:06:16 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:06:16 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2907, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 317, in __init__
    self.init_model_worker()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 455, in init_model_worker
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py", line 253, in __init__
    self._model_runner = ModelRunner(
                         ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 385, in __init__
    self.initialize(min_per_gpu_memory)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 468, in initialize
    self.load_model()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 941, in load_model
    self.model = self.loader.load_model(
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_loader/loader.py", line 625, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_loader/loader.py", line 276, in _initialize_model
    return model_class(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_vl_moe.py", line 148, in __init__
    super().__init__(config, quant_config, prefix, language_model_cls)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_vl.py", line 723, in __init__
    self.model = language_model_cls(
                 ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_vl_moe.py", line 47, in __init__
    super().__init__(config=config, quant_config=quant_config, prefix=prefix)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_moe.py", line 890, in __init__
    super().__init__(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen2_moe.py", line 573, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 592, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 594, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen2_moe.py", line 575, in <lambda>
    lambda idx, prefix: decoder_layer_type(
                        ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_moe.py", line 743, in __init__
    self.mlp = Qwen3MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_moe.py", line 233, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 256, in __init__
    self.quant_method.create_weights(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/unquant.py", line 203, in create_weights
    torch.empty(num_experts, w2_weight_n, w2_weight_k, dtype=params_dtype),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py", line 182, in decorated
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
RuntimeError: NPU out of memory. Tried to allocate 386.00 MiB (NPU 3; 61.28 GiB total capacity; 60.60 GiB already allocated; 60.60 GiB current active; 66.61 MiB free; 60.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.

[2026-01-05 12:06:16] Received sigquit from a child process. It usually means the child failed.
[2026-01-05 12:06:16 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2907, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 317, in __init__
    self.init_model_worker()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 455, in init_model_worker
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py", line 253, in __init__
    self._model_runner = ModelRunner(
                         ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 385, in __init__
    self.initialize(min_per_gpu_memory)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 468, in initialize
    self.load_model()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py", line 941, in load_model
    self.model = self.loader.load_model(
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_loader/loader.py", line 625, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/model_loader/loader.py", line 276, in _initialize_model
    return model_class(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_vl_moe.py", line 148, in __init__
    super().__init__(config, quant_config, prefix, language_model_cls)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_vl.py", line 723, in __init__
    self.model = language_model_cls(
                 ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_vl_moe.py", line 47, in __init__
    super().__init__(config=config, quant_config=quant_config, prefix=prefix)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_moe.py", line 890, in __init__
    super().__init__(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen2_moe.py", line 573, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 592, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 594, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen2_moe.py", line 575, in <lambda>
    lambda idx, prefix: decoder_layer_type(
                        ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_moe.py", line 743, in __init__
    self.mlp = Qwen3MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/models/qwen3_moe.py", line 233, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 256, in __init__
    self.quant_method.create_weights(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/unquant.py", line 203, in create_weights
    torch.empty(num_experts, w2_weight_n, w2_weight_k, dtype=params_dtype),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py", line 182, in decorated
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_device.py", line 103, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
RuntimeError: NPU out of memory. Tried to allocate 386.00 MiB (NPU 1; 61.28 GiB total capacity; 60.60 GiB already allocated; 60.60 GiB current active; 66.42 MiB free; 60.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.

[2026-01-05 12:06:16] Received sigquit from a child process. It usually means the child failed.
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 135, in _run_vlm_mmmu_test
    process = popen_launch_server(
              ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code -9. Check server logs for errors.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_235.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct: Server process exited with code -9. Check server logs for errors.
E
======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 135, in _run_vlm_mmmu_test
    process = popen_launch_server(
              ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code -9. Check server logs for errors.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct: Server process exited with code -9. Check server logs for errors.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 30.008s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct: Server process exited with code -9. Check server logs for errors.
.
.
End (34/43):
filename='ascend/vlm_models/test_ascend_qwen3_vl_235.py', elapsed=41, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_qwen3_vl_235.py failed with non-retriable error: RuntimeError - not retrying


✗ FAILED: ascend/vlm_models/test_ascend_qwen3_vl_235.py returned exit code 1

.
.
Begin (35/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_vlm_models_glm_4_5v.py
.
.

Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/vlm_models/test_vlm_models_glm_4_5v.py", line 11, in <module>
    from sglang.test.test_vlm_utils import TestVLMModels
ModuleNotFoundError: No module named 'sglang.test.test_vlm_utils'
.
.
End (35/43):
filename='ascend/vlm_models/test_vlm_models_glm_4_5v.py', elapsed=1, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_vlm_models_glm_4_5v.py failed with non-retriable error: ModuleNotFoundError - not retrying


✗ FAILED: ascend/vlm_models/test_vlm_models_glm_4_5v.py returned exit code 1

.
.
Begin (36/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_minicpm_o_2_6.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 12:06:41] INFO configuration_minicpm.py:187: vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:06:41] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6', tokenizer_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=748872237, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:06:43] vision_config is None, using default vision config
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-05 12:06:44] Inferred chat template from model path: minicpmo
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-05 12:06:51 TP3] vision_config is None, using default vision config
[2026-01-05 12:06:51 TP3] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-05 12:06:51 TP0] vision_config is None, using default vision config
[2026-01-05 12:06:51 TP0] vision_config is None, using default vision config
[2026-01-05 12:06:51 TP1] vision_config is None, using default vision config
[2026-01-05 12:06:51 TP1] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:06:51 TP2] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 12:06:51 TP2] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:06:51 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:06:52 TP0] Init torch distributed begin.
[2026-01-05 12:06:52 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:06:52 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:06:53 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:06:53 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:06:53 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:06:53 TP3] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:06:53 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:06:54 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:06:54 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:06:54 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:06:54 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:06:54 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:06:54 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:06:54 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:06:54 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:06:54 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:06:54 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:06:55 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:06:55 TP2] Load weight begin. avail mem=60.90 GB
[2026-01-05 12:06:55 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:06:55 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:06:55 TP0] TTS is disabled for now
[2026-01-05 12:06:55 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:06:55 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:06:55 TP1] TTS is disabled for now

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2026-01-05 12:06:55 TP3] TTS is disabled for now
[2026-01-05 12:06:55 TP2] TTS is disabled for now

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.01s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.04s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:12<00:04,  4.17s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.23s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.18s/it]

[2026-01-05 12:07:12 TP0] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=51.72 GB, mem usage=9.10 GB.
[2026-01-05 12:07:12 TP2] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=51.79 GB, mem usage=9.10 GB.
[2026-01-05 12:07:12 TP3] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=52.03 GB, mem usage=9.10 GB.
[2026-01-05 12:07:12 TP1] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=52.03 GB, mem usage=9.10 GB.
[2026-01-05 12:07:12 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:07:12 TP0] The available memory for KV cache is 12.18 GB.
[2026-01-05 12:07:12 TP3] The available memory for KV cache is 12.18 GB.
[2026-01-05 12:07:12 TP1] The available memory for KV cache is 12.18 GB.
[2026-01-05 12:07:12 TP2] The available memory for KV cache is 12.18 GB.
[2026-01-05 12:07:12 TP0] KV Cache is allocated. #tokens: 912512, K size: 6.09 GB, V size: 6.09 GB
[2026-01-05 12:07:12 TP2] KV Cache is allocated. #tokens: 912512, K size: 6.09 GB, V size: 6.09 GB
[2026-01-05 12:07:12 TP3] KV Cache is allocated. #tokens: 912512, K size: 6.09 GB, V size: 6.09 GB
[2026-01-05 12:07:12 TP0] Memory pool end. avail mem=39.03 GB
[2026-01-05 12:07:12 TP1] KV Cache is allocated. #tokens: 912512, K size: 6.09 GB, V size: 6.09 GB
[2026-01-05 12:07:12 TP2] Memory pool end. avail mem=39.11 GB
[2026-01-05 12:07:12 TP3] Memory pool end. avail mem=39.34 GB
[2026-01-05 12:07:12 TP1] Memory pool end. avail mem=39.34 GB
[2026-01-05 12:07:12 TP0] vision_config is None, using default vision config
[2026-01-05 12:07:12 TP3] vision_config is None, using default vision config
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
[2026-01-05 12:07:12 TP1] vision_config is None, using default vision config
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
[2026-01-05 12:07:12 TP2] vision_config is None, using default vision config
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:07:13 TP0] max_total_num_tokens=912512, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=39.03 GB
[2026-01-05 12:07:14] INFO:     Started server process [288481]
[2026-01-05 12:07:14] INFO:     Waiting for application startup.
[2026-01-05 12:07:14] INFO:     Application startup complete.
[2026-01-05 12:07:14] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:07:15] INFO:     127.0.0.1:40226 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:07:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:07:15.900594539 compiler_depend.ts:198] Warning: Driver Version: "+" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:07:15.903305539 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:07:15.903898962 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:07:15.904450662 compiler_depend.ts:198] Warning: Driver Version: "̔" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:07:22] INFO:     127.0.0.1:52926 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:07:32] INFO:     127.0.0.1:33600 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:07:32] INFO:     127.0.0.1:40232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:07:32] The server is fired up and ready to roll!
[2026-01-05 12:07:42 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:07:43] INFO:     127.0.0.1:34506 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_minicpm_o_2_6.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 288481 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 71.638s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6 --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 288481
.
.
End (36/43):
filename='ascend/vlm_models/test_ascend_minicpm_o_2_6.py', elapsed=82, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_minicpm_o_2_6.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_minicpm_o_2_6.py

.
.
Begin (36/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_minicpm_o_2_6.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 12:09:03] INFO configuration_minicpm.py:187: vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:09:03] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6', tokenizer_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=513494649, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:09:04] vision_config is None, using default vision config
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-05 12:09:05] Inferred chat template from model path: minicpmo
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:09:12 TP0] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:09:12 TP0] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 12:09:13 TP2] vision_config is None, using default vision config
[2026-01-05 12:09:13 TP2] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:09:13 TP3] vision_config is None, using default vision config
[2026-01-05 12:09:13 TP3] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:09:13 TP1] vision_config is None, using default vision config
[2026-01-05 12:09:13 TP1] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:09:13 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:09:13 TP2] Init torch distributed begin.
[2026-01-05 12:09:14 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:09:14 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:09:15 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:09:15 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:09:15 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:09:15 TP0] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:09:15 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:09:15 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:09:15 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:09:15 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:09:15 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:09:16 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:09:16 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:09:16 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:09:16 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:09:16 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:09:16 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:09:16 TP2] Load weight begin. avail mem=60.90 GB
[2026-01-05 12:09:16 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:09:16 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:09:16 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:09:16 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:09:16 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:09:16 TP0] TTS is disabled for now

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2026-01-05 12:09:16 TP3] TTS is disabled for now

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  3.54it/s]
[2026-01-05 12:09:16 TP2] TTS is disabled for now
[2026-01-05 12:09:17 TP1] TTS is disabled for now

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  3.07it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.82it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.87it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.93it/s]

[2026-01-05 12:09:18 TP0] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=51.72 GB, mem usage=9.10 GB.
[2026-01-05 12:09:18 TP3] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=52.03 GB, mem usage=9.10 GB.
[2026-01-05 12:09:18 TP2] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=51.79 GB, mem usage=9.10 GB.
[2026-01-05 12:09:18 TP1] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=52.03 GB, mem usage=9.10 GB.
[2026-01-05 12:09:18 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:09:18 TP0] The available memory for KV cache is 12.19 GB.
[2026-01-05 12:09:18 TP2] The available memory for KV cache is 12.19 GB.
[2026-01-05 12:09:18 TP3] The available memory for KV cache is 12.19 GB.
[2026-01-05 12:09:18 TP1] The available memory for KV cache is 12.19 GB.
[2026-01-05 12:09:18 TP2] KV Cache is allocated. #tokens: 912640, K size: 6.09 GB, V size: 6.09 GB
[2026-01-05 12:09:18 TP3] KV Cache is allocated. #tokens: 912640, K size: 6.09 GB, V size: 6.09 GB
[2026-01-05 12:09:18 TP0] KV Cache is allocated. #tokens: 912640, K size: 6.09 GB, V size: 6.09 GB
[2026-01-05 12:09:18 TP1] KV Cache is allocated. #tokens: 912640, K size: 6.09 GB, V size: 6.09 GB
[2026-01-05 12:09:18 TP2] Memory pool end. avail mem=39.10 GB
[2026-01-05 12:09:18 TP0] Memory pool end. avail mem=39.03 GB
[2026-01-05 12:09:18 TP3] Memory pool end. avail mem=39.34 GB
[2026-01-05 12:09:18 TP1] Memory pool end. avail mem=39.34 GB
[2026-01-05 12:09:18 TP0] vision_config is None, using default vision config
[2026-01-05 12:09:18 TP3] vision_config is None, using default vision config
[2026-01-05 12:09:18 TP2] vision_config is None, using default vision config
[2026-01-05 12:09:18 TP1] vision_config is None, using default vision config
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:09:19 TP0] max_total_num_tokens=912640, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=39.03 GB
[2026-01-05 12:09:20] INFO:     Started server process [294794]
[2026-01-05 12:09:20] INFO:     Waiting for application startup.
[2026-01-05 12:09:20] INFO:     Application startup complete.
[2026-01-05 12:09:20] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:09:21] INFO:     127.0.0.1:33232 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:09:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:09:21.832776201 compiler_depend.ts:198] Warning: Driver Version: "h" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:09:21.836049762 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:09:21.836416116 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:09:21.836671605 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:09:23] INFO:     127.0.0.1:33244 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:09:33] INFO:     127.0.0.1:57548 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:09:38] INFO:     127.0.0.1:33234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:09:38] The server is fired up and ready to roll!
[2026-01-05 12:09:43 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:09:44] INFO:     127.0.0.1:46494 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_minicpm_o_2_6.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 294794 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 51.593s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6 --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 294794
.
.
End (36/43):
filename='ascend/vlm_models/test_ascend_minicpm_o_2_6.py', elapsed=62, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_minicpm_o_2_6.py returned exit code 1

.
.
Begin (37/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:10:09] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=1019442406, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-05 12:10:12] Inferred chat template from model path: phi-4-mm
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:10:25 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:10:26 TP0] Init torch distributed begin.
[2026-01-05 12:10:26 TP2] Init torch distributed begin.
[2026-01-05 12:10:26 TP3] Init torch distributed begin.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:10:27 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:10:27 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:10:27 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:10:27 TP0] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:10:27 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:10:28 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:10:28 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:10:28 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:10:28 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:10:29 TP2] Load weight begin. avail mem=60.89 GB
[2026-01-05 12:10:29 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:10:29 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:10:29 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:10:29 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:10:29 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:10:29 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:10:29 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:10:29 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:10:29 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:10:29 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:10:29 TP2] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.22s/it]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:07<00:04,  4.04s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:12<00:00,  4.58s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:12<00:00,  4.15s/it]

[2026-01-05 12:10:42 TP3] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=58.04 GB, mem usage=3.09 GB.
[2026-01-05 12:10:42 TP2] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=57.80 GB, mem usage=3.09 GB.
[2026-01-05 12:10:42 TP0] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=57.73 GB, mem usage=3.09 GB.
[2026-01-05 12:10:42 TP1] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=58.04 GB, mem usage=3.09 GB.
[2026-01-05 12:10:42 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:10:42 TP0] The available memory for KV cache is 18.20 GB.
[2026-01-05 12:10:42 TP3] The available memory for KV cache is 18.20 GB.
[2026-01-05 12:10:42 TP1] The available memory for KV cache is 18.20 GB.
[2026-01-05 12:10:42 TP2] The available memory for KV cache is 18.20 GB.
[2026-01-05 12:10:42 TP0] KV Cache is allocated. #tokens: 596224, K size: 9.10 GB, V size: 9.10 GB
[2026-01-05 12:10:42 TP2] KV Cache is allocated. #tokens: 596224, K size: 9.10 GB, V size: 9.10 GB
[2026-01-05 12:10:42 TP3] KV Cache is allocated. #tokens: 596224, K size: 9.10 GB, V size: 9.10 GB
[2026-01-05 12:10:42 TP1] KV Cache is allocated. #tokens: 596224, K size: 9.10 GB, V size: 9.10 GB
[2026-01-05 12:10:42 TP0] Memory pool end. avail mem=38.39 GB
[2026-01-05 12:10:42 TP2] Memory pool end. avail mem=38.46 GB
[2026-01-05 12:10:42 TP3] Memory pool end. avail mem=38.70 GB
[2026-01-05 12:10:42 TP1] Memory pool end. avail mem=38.70 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:10:43 TP0] max_total_num_tokens=596224, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2329, context_len=131072, available_gpu_mem=38.39 GB
[2026-01-05 12:10:44] INFO:     Started server process [301107]
[2026-01-05 12:10:44] INFO:     Waiting for application startup.
[2026-01-05 12:10:44] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:10:44] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:10:44] INFO:     Application startup complete.
[2026-01-05 12:10:44] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:10:45] INFO:     127.0.0.1:45296 - "GET /model_info HTTP/1.1" 200 OK
[2026-01-05 12:10:45] INFO:     127.0.0.1:45300 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:10:46 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:10:46.727331851 compiler_depend.ts:198] Warning: Driver Version: "۳" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:10:46.727348981 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:10:46.727764127 compiler_depend.ts:198] Warning: Driver Version: "p" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:10:46.728318857 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:10:55] INFO:     127.0.0.1:50628 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:11:05] INFO:     127.0.0.1:51502 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:11:06] INFO:     127.0.0.1:45312 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:11:06] The server is fired up and ready to roll!
[2026-01-05 12:11:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:11:16] INFO:     127.0.0.1:51394 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 301107 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 81.505s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 301107
.
.
End (37/43):
filename='ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py', elapsed=92, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py

.
.
Begin (37/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:12:37] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=569545798, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-05 12:12:40] Inferred chat template from model path: phi-4-mm
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:12:48 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:12:48 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:12:48 TP2] Init torch distributed begin.
[2026-01-05 12:12:48 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:12:49 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:12:49 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:12:49 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:12:49 TP2] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:12:49 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:12:50 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:12:50 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:12:50 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:12:50 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:12:51 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:12:51 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:12:51 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:12:51 TP2] Load weight begin. avail mem=60.89 GB
[2026-01-05 12:12:51 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:12:51 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:12:51 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:12:51 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:12:51 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:12:51 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:12:51 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:12:51 TP2] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:00<00:00,  3.91it/s]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:00<00:00,  3.91it/s]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:00<00:00,  3.91it/s]

[2026-01-05 12:12:52 TP0] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=57.73 GB, mem usage=3.09 GB.
[2026-01-05 12:12:52 TP3] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=58.04 GB, mem usage=3.09 GB.
[2026-01-05 12:12:52 TP1] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=58.04 GB, mem usage=3.09 GB.
[2026-01-05 12:12:52 TP2] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=57.81 GB, mem usage=3.09 GB.
[2026-01-05 12:12:52 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:12:52 TP0] The available memory for KV cache is 18.20 GB.
[2026-01-05 12:12:52 TP3] The available memory for KV cache is 18.20 GB.
[2026-01-05 12:12:52 TP1] The available memory for KV cache is 18.20 GB.
[2026-01-05 12:12:52 TP2] The available memory for KV cache is 18.20 GB.
[2026-01-05 12:12:52 TP0] KV Cache is allocated. #tokens: 596224, K size: 9.10 GB, V size: 9.10 GB
[2026-01-05 12:12:52 TP2] KV Cache is allocated. #tokens: 596224, K size: 9.10 GB, V size: 9.10 GB
[2026-01-05 12:12:52 TP3] KV Cache is allocated. #tokens: 596224, K size: 9.10 GB, V size: 9.10 GB
[2026-01-05 12:12:52 TP1] KV Cache is allocated. #tokens: 596224, K size: 9.10 GB, V size: 9.10 GB
[2026-01-05 12:12:52 TP0] Memory pool end. avail mem=38.39 GB
[2026-01-05 12:12:52 TP2] Memory pool end. avail mem=38.47 GB
[2026-01-05 12:12:52 TP3] Memory pool end. avail mem=38.70 GB
[2026-01-05 12:12:52 TP1] Memory pool end. avail mem=38.70 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:12:54 TP0] max_total_num_tokens=596224, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2329, context_len=131072, available_gpu_mem=38.39 GB
[2026-01-05 12:12:55] INFO:     Started server process [307420]
[2026-01-05 12:12:55] INFO:     Waiting for application startup.
[2026-01-05 12:12:55] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:12:55] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:12:55] INFO:     Application startup complete.
[2026-01-05 12:12:55] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:12:56] INFO:     127.0.0.1:55638 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:12:56 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:12:56.881839585 compiler_depend.ts:198] Warning: Driver Version: "n" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:12:56.884744603 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:12:56.887402882 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:12:56.887707244 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:12:58] INFO:     127.0.0.1:47936 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:13:08] INFO:     127.0.0.1:51402 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:13:13] INFO:     127.0.0.1:55650 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:13:13] The server is fired up and ready to roll!
[2026-01-05 12:13:18 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:13:19] INFO:     127.0.0.1:42526 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 307420 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 51.671s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 307420
.
.
End (37/43):
filename='ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py', elapsed=62, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py returned exit code 1

.
.
Begin (38/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_janus_pro_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:13:40] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B', tokenizer_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=756425482, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-05 12:13:42] Inferred chat template from model path: janus-pro
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:13:50 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:13:50 TP1] Init torch distributed begin.
[2026-01-05 12:13:50 TP0] Init torch distributed begin.
[2026-01-05 12:13:50 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:13:52 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:13:52 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:13:52 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:13:52 TP1] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:13:52 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:13:53 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:13:53 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:13:53 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:13:53 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:13:53 TP2] Load weight begin. avail mem=60.89 GB
[2026-01-05 12:13:53 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:13:53 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:13:53 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:13:53 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:13:53 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:13:53 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:13:53 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:13:53 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:13:53 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:13:53 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:13:53 TP0] Using sdpa as multimodal attention backend.

Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading pt checkpoint shards:  50% Completed | 1/2 [00:17<00:17, 17.13s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:52<00:00, 27.78s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:52<00:00, 26.19s/it]

[2026-01-05 12:14:46 TP2] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.63 GB, mem usage=4.26 GB.
[2026-01-05 12:14:46 TP0] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.56 GB, mem usage=4.26 GB.
[2026-01-05 12:14:46 TP1] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.87 GB, mem usage=4.26 GB.
[2026-01-05 12:14:46 TP3] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.87 GB, mem usage=4.26 GB.
[2026-01-05 12:14:46 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:14:46 TP0] The available memory for KV cache is 17.03 GB.
[2026-01-05 12:14:46 TP3] The available memory for KV cache is 17.03 GB.
[2026-01-05 12:14:46 TP2] The available memory for KV cache is 17.03 GB.
[2026-01-05 12:14:46 TP1] The available memory for KV cache is 17.03 GB.
[2026-01-05 12:14:46 TP0] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-05 12:14:46 TP2] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-05 12:14:46 TP3] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-05 12:14:46 TP1] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-05 12:14:46 TP0] Memory pool end. avail mem=39.27 GB
[2026-01-05 12:14:46 TP2] Memory pool end. avail mem=39.35 GB
[2026-01-05 12:14:46 TP3] Memory pool end. avail mem=39.58 GB
[2026-01-05 12:14:46 TP1] Memory pool end. avail mem=39.58 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:14:47 TP0] max_total_num_tokens=148736, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=16384, available_gpu_mem=39.25 GB
[2026-01-05 12:14:48] INFO:     Started server process [313731]
[2026-01-05 12:14:48] INFO:     Waiting for application startup.
[2026-01-05 12:14:48] INFO:     Application startup complete.
[2026-01-05 12:14:48] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:14:49] INFO:     127.0.0.1:48708 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:14:49 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:14:49.302623956 compiler_depend.ts:198] Warning: Driver Version: "j" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:14:49.303439207 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:14:49.305326317 compiler_depend.ts:198] Warning: Driver Version: "b" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:14:49.305327387 compiler_depend.ts:198] Warning: Driver Version: "r" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:14:50] INFO:     127.0.0.1:48724 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:15:00] INFO:     127.0.0.1:48320 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:15:09] INFO:     127.0.0.1:48722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:15:09] The server is fired up and ready to roll!
[2026-01-05 12:15:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:15:11] INFO:     127.0.0.1:55552 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_janus_pro_7b.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 313731 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestJanusPro7B.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 101.653s

FAILED (errors=1)
[CI Test Method] TestJanusPro7B.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 313731
.
.
End (38/43):
filename='ascend/vlm_models/test_ascend_janus_pro_7b.py', elapsed=113, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_janus_pro_7b.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_janus_pro_7b.py

.
.
Begin (38/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_janus_pro_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:16:39] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B', tokenizer_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=571742283, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-05 12:16:41] Inferred chat template from model path: janus-pro
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-05 12:16:48 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:16:49 TP1] Init torch distributed begin.
[2026-01-05 12:16:49 TP0] Init torch distributed begin.
[2026-01-05 12:16:49 TP3] Init torch distributed begin.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:16:51 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:16:51 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:16:51 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:16:51 TP2] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:16:51 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:16:52 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:16:52 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:16:52 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:16:52 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:16:52 TP2] Load weight begin. avail mem=60.89 GB
[2026-01-05 12:16:52 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:16:52 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:16:52 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:16:52 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:16:52 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:16:52 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:16:52 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:16:52 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:16:52 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:16:52 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:16:52 TP2] Using sdpa as multimodal attention backend.

Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading pt checkpoint shards:  50% Completed | 1/2 [00:15<00:15, 15.41s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:46<00:00, 24.82s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [00:46<00:00, 23.41s/it]

[2026-01-05 12:17:39 TP1] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.87 GB, mem usage=4.26 GB.
[2026-01-05 12:17:39 TP3] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.87 GB, mem usage=4.26 GB.
[2026-01-05 12:17:39 TP2] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.63 GB, mem usage=4.26 GB.
[2026-01-05 12:17:39 TP0] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.56 GB, mem usage=4.26 GB.
[2026-01-05 12:17:39 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:17:39 TP0] The available memory for KV cache is 17.03 GB.
[2026-01-05 12:17:39 TP3] The available memory for KV cache is 17.03 GB.
[2026-01-05 12:17:39 TP2] The available memory for KV cache is 17.03 GB.
[2026-01-05 12:17:39 TP1] The available memory for KV cache is 17.03 GB.
[2026-01-05 12:17:39 TP0] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-05 12:17:39 TP2] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-05 12:17:39 TP3] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-05 12:17:39 TP1] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-05 12:17:39 TP0] Memory pool end. avail mem=39.27 GB
[2026-01-05 12:17:39 TP2] Memory pool end. avail mem=39.34 GB
[2026-01-05 12:17:39 TP3] Memory pool end. avail mem=39.58 GB
[2026-01-05 12:17:39 TP1] Memory pool end. avail mem=39.58 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:17:40 TP0] max_total_num_tokens=148736, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=16384, available_gpu_mem=39.25 GB
[2026-01-05 12:17:41] INFO:     Started server process [320384]
[2026-01-05 12:17:41] INFO:     Waiting for application startup.
[2026-01-05 12:17:41] INFO:     Application startup complete.
[2026-01-05 12:17:41] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:17:42] INFO:     127.0.0.1:57742 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:17:42 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:17:42.341884723 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:17:42.342496786 compiler_depend.ts:198] Warning: Driver Version: "ֲ" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:17:42.342763746 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:17:42.343811535 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:17:49] INFO:     127.0.0.1:49888 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:17:58] INFO:     127.0.0.1:57750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:17:58] The server is fired up and ready to roll!
[2026-01-05 12:17:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:18:00] INFO:     127.0.0.1:48808 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_janus_pro_7b.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 320384 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestJanusPro7B.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 91.597s

FAILED (errors=1)
[CI Test Method] TestJanusPro7B.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 320384
.
.
End (38/43):
filename='ascend/vlm_models/test_ascend_janus_pro_7b.py', elapsed=108, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_janus_pro_7b.py returned exit code 1

.
.
Begin (39/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:18:19] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=429466810, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:18:21] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:18:29 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:18:30 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:18:30 TP0] Init torch distributed begin.
[2026-01-05 12:18:30 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:18:31 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:18:31 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:18:31 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:18:31 TP2] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:18:31 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:18:32 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:18:32 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:18:32 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:18:32 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:18:32 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:18:32 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:18:32 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:18:32 TP2] Load weight begin. avail mem=60.89 GB
[2026-01-05 12:18:33 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:18:33 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:18:33 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:18:33 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:18:33 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:18:33 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:18:33 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:18:33 TP3] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.13s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.94s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.97s/it]

[2026-01-05 12:18:41 TP1] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=59.25 GB, mem usage=1.88 GB.
[2026-01-05 12:18:41 TP0] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=58.94 GB, mem usage=1.88 GB.
[2026-01-05 12:18:41 TP3] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=59.25 GB, mem usage=1.88 GB.
[2026-01-05 12:18:41 TP2] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=59.02 GB, mem usage=1.88 GB.
[2026-01-05 12:18:41 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:18:41 TP0] The available memory for KV cache is 19.38 GB.
[2026-01-05 12:18:41 TP3] The available memory for KV cache is 19.38 GB.
[2026-01-05 12:18:41 TP2] The available memory for KV cache is 19.38 GB.
[2026-01-05 12:18:41 TP1] The available memory for KV cache is 19.38 GB.
[2026-01-05 12:18:41 TP0] KV Cache is allocated. #tokens: 1128704, K size: 9.69 GB, V size: 9.69 GB
[2026-01-05 12:18:41 TP1] KV Cache is allocated. #tokens: 1128704, K size: 9.69 GB, V size: 9.69 GB
[2026-01-05 12:18:41 TP3] KV Cache is allocated. #tokens: 1128704, K size: 9.69 GB, V size: 9.69 GB
[2026-01-05 12:18:41 TP0] Memory pool end. avail mem=37.57 GB
[2026-01-05 12:18:41 TP1] Memory pool end. avail mem=37.89 GB
[2026-01-05 12:18:41 TP3] Memory pool end. avail mem=37.89 GB
[2026-01-05 12:18:41 TP2] KV Cache is allocated. #tokens: 1128704, K size: 9.69 GB, V size: 9.69 GB
[2026-01-05 12:18:41 TP2] Memory pool end. avail mem=37.65 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:18:42 TP0] max_total_num_tokens=1128704, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=128000, available_gpu_mem=37.57 GB
[2026-01-05 12:18:43] INFO:     Started server process [326697]
[2026-01-05 12:18:43] INFO:     Waiting for application startup.
[2026-01-05 12:18:43] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 1e-06, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:18:43] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 1e-06, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:18:43] INFO:     Application startup complete.
[2026-01-05 12:18:43] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:18:44] INFO:     127.0.0.1:47444 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:18:44 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:18:44.078583812 compiler_depend.ts:198] Warning: Driver Version: "]" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:18:44.080927379 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:18:44.081169228 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:18:44.081200879 compiler_depend.ts:198] Warning: Driver Version: "k" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:18:50] INFO:     127.0.0.1:38902 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:19:00] INFO:     127.0.0.1:35678 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:19:01] INFO:     127.0.0.1:47452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:19:01] The server is fired up and ready to roll!
[2026-01-05 12:19:10 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:19:11] INFO:     127.0.0.1:58382 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 326697 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 61.643s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 326697
.
.
End (39/43):
filename='ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py', elapsed=72, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py

.
.
Begin (39/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:20:40] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=369133607, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:20:43] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:20:50 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:20:50 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:20:51 TP2] Init torch distributed begin.
[2026-01-05 12:20:51 TP3] Init torch distributed begin.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:20:52 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:20:52 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:20:52 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:20:52 TP1] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:20:52 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:20:52 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:20:52 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:20:52 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:20:52 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:20:52 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:20:52 TP2] Load weight begin. avail mem=60.90 GB
[2026-01-05 12:20:52 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:20:52 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:20:53 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:20:53 TP0] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[2026-01-05 12:20:53 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:20:53 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:20:53 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:20:53 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:20:53 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:20:53 TP2] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.60s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.53s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.54s/it]

[2026-01-05 12:21:00 TP1] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=59.25 GB, mem usage=1.88 GB.
[2026-01-05 12:21:00 TP0] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=58.94 GB, mem usage=1.88 GB.
[2026-01-05 12:21:00 TP3] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=59.25 GB, mem usage=1.88 GB.
[2026-01-05 12:21:00 TP2] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=59.02 GB, mem usage=1.88 GB.
[2026-01-05 12:21:00 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:21:00 TP0] The available memory for KV cache is 19.38 GB.
[2026-01-05 12:21:00 TP3] The available memory for KV cache is 19.38 GB.
[2026-01-05 12:21:00 TP2] The available memory for KV cache is 19.38 GB.
[2026-01-05 12:21:00 TP1] The available memory for KV cache is 19.38 GB.
[2026-01-05 12:21:01 TP1] KV Cache is allocated. #tokens: 1128704, K size: 9.69 GB, V size: 9.69 GB
[2026-01-05 12:21:01 TP0] KV Cache is allocated. #tokens: 1128704, K size: 9.69 GB, V size: 9.69 GB
[2026-01-05 12:21:01 TP3] KV Cache is allocated. #tokens: 1128704, K size: 9.69 GB, V size: 9.69 GB
[2026-01-05 12:21:01 TP2] KV Cache is allocated. #tokens: 1128704, K size: 9.69 GB, V size: 9.69 GB
[2026-01-05 12:21:01 TP0] Memory pool end. avail mem=37.58 GB
[2026-01-05 12:21:01 TP1] Memory pool end. avail mem=37.89 GB
[2026-01-05 12:21:01 TP3] Memory pool end. avail mem=37.89 GB
[2026-01-05 12:21:01 TP2] Memory pool end. avail mem=37.65 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:21:01 TP0] max_total_num_tokens=1128704, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=128000, available_gpu_mem=37.58 GB
[2026-01-05 12:21:02] INFO:     Started server process [332419]
[2026-01-05 12:21:02] INFO:     Waiting for application startup.
[2026-01-05 12:21:02] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 1e-06, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:21:02] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 1e-06, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:21:02] INFO:     Application startup complete.
[2026-01-05 12:21:02] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:21:03] INFO:     127.0.0.1:44206 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:21:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:21:03.437269425 compiler_depend.ts:198] Warning: Driver Version: ";" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:21:03.437421651 compiler_depend.ts:198] Warning: Driver Version: "ݳ" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:21:03.437446882 compiler_depend.ts:198] Warning: Driver Version: "ϭ" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:21:03.437495493 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:21:09] INFO:     127.0.0.1:39554 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:21:19] INFO:     127.0.0.1:45542 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:21:22] INFO:     127.0.0.1:44216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:21:22] The server is fired up and ready to roll!
[2026-01-05 12:21:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:21:30] INFO:     127.0.0.1:56516 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 332419 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 61.581s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 332419
.
.
End (39/43):
filename='ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py', elapsed=78, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py returned exit code 1

.
.
Begin (40/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_minicpm_v_2_6.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 12:21:50] INFO configuration_minicpm.py:92: vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:21:50] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6', tokenizer_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=776921366, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:21:52] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-05 12:21:53] Inferred chat template from model path: minicpmv
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-05 12:21:59 TP1] vision_config is None, using default vision config
[2026-01-05 12:21:59 TP1] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 12:22:00 TP0] vision_config is None, using default vision config
[2026-01-05 12:22:00 TP0] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:22:00 TP2] vision_config is None, using default vision config
[2026-01-05 12:22:00 TP2] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:22:00 TP3] vision_config is None, using default vision config
[2026-01-05 12:22:00 TP3] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:22:00 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:22:00 TP0] Init torch distributed begin.
[2026-01-05 12:22:01 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:22:01 TP3] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:22:02 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:22:02 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:22:02 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:22:02 TP1] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:22:02 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:22:02 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:22:02 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:22:02 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:22:02 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:22:03 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:22:03 TP2] Load weight begin. avail mem=60.90 GB
[2026-01-05 12:22:03 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:22:03 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:22:03 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:22:03 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:22:03 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:22:03 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:22:03 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:22:03 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:22:03 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:22:03 TP1] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.06s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.01s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:10<00:03,  3.11s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.59s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.60s/it]

[2026-01-05 12:22:18 TP0] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.67 GB, mem usage=4.15 GB.
[2026-01-05 12:22:18 TP1] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.98 GB, mem usage=4.15 GB.
[2026-01-05 12:22:18 TP3] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.98 GB, mem usage=4.15 GB.
[2026-01-05 12:22:18 TP2] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.74 GB, mem usage=4.15 GB.
[2026-01-05 12:22:18 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:22:18 TP0] The available memory for KV cache is 17.13 GB.
[2026-01-05 12:22:18 TP3] The available memory for KV cache is 17.13 GB.
[2026-01-05 12:22:18 TP1] The available memory for KV cache is 17.13 GB.
[2026-01-05 12:22:18 TP2] The available memory for KV cache is 17.13 GB.
[2026-01-05 12:22:18 TP0] KV Cache is allocated. #tokens: 1283328, K size: 8.57 GB, V size: 8.57 GB
[2026-01-05 12:22:18 TP2] KV Cache is allocated. #tokens: 1283328, K size: 8.57 GB, V size: 8.57 GB
[2026-01-05 12:22:18 TP3] KV Cache is allocated. #tokens: 1283328, K size: 8.57 GB, V size: 8.57 GB
[2026-01-05 12:22:18 TP1] KV Cache is allocated. #tokens: 1283328, K size: 8.57 GB, V size: 8.57 GB
[2026-01-05 12:22:18 TP0] Memory pool end. avail mem=39.03 GB
[2026-01-05 12:22:18 TP2] Memory pool end. avail mem=39.10 GB
[2026-01-05 12:22:18 TP3] Memory pool end. avail mem=39.34 GB
[2026-01-05 12:22:18 TP1] Memory pool end. avail mem=39.34 GB
[2026-01-05 12:22:18 TP1] vision_config is None, using default vision config
[2026-01-05 12:22:18 TP0] vision_config is None, using default vision config
[2026-01-05 12:22:18 TP3] vision_config is None, using default vision config
[2026-01-05 12:22:18 TP2] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:22:19 TP0] max_total_num_tokens=1283328, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=39.03 GB
[2026-01-05 12:22:20] INFO:     Started server process [338023]
[2026-01-05 12:22:20] INFO:     Waiting for application startup.
[2026-01-05 12:22:20] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:22:20] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:22:20] INFO:     Application startup complete.
[2026-01-05 12:22:20] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:22:21] INFO:     127.0.0.1:51188 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:22:21] INFO:     127.0.0.1:51202 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:22:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:22:21.990136757 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:22:21.992915630 compiler_depend.ts:198] Warning: Driver Version: "f" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:22:21.994058893 compiler_depend.ts:198] Warning: Driver Version: "6" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:22:21.994987767 compiler_depend.ts:198] Warning: Driver Version: "g" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:22:31] INFO:     127.0.0.1:37510 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:22:37] INFO:     127.0.0.1:51210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:22:37] The server is fired up and ready to roll!
[2026-01-05 12:22:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:22:42] INFO:     127.0.0.1:42302 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_minicpm_v_2_6.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 338023 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 61.615s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6 --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 338023
.
.
End (40/43):
filename='ascend/vlm_models/test_ascend_minicpm_v_2_6.py', elapsed=72, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_minicpm_v_2_6.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_minicpm_v_2_6.py

.
.
Begin (40/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_minicpm_v_2_6.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 12:24:02] INFO configuration_minicpm.py:92: vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:24:02] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6', tokenizer_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=737826701, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:24:03] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-05 12:24:04] Inferred chat template from model path: minicpmv
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:24:11 TP3] vision_config is None, using default vision config
[2026-01-05 12:24:11 TP3] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:24:12 TP0] vision_config is None, using default vision config
[2026-01-05 12:24:12 TP0] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:24:12 TP1] vision_config is None, using default vision config
[2026-01-05 12:24:12 TP2] vision_config is None, using default vision config
[2026-01-05 12:24:12 TP1] vision_config is None, using default vision config
[2026-01-05 12:24:12 TP2] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:24:12 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:24:12 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:24:13 TP2] Init torch distributed begin.
[2026-01-05 12:24:13 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:24:14 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:24:14 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:24:14 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:24:14 TP1] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:24:14 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:24:15 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:24:15 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:24:15 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:24:15 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:24:15 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:24:15 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:24:15 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:24:15 TP2] Load weight begin. avail mem=60.90 GB
[2026-01-05 12:24:15 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:24:15 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:24:15 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:24:15 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:24:15 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:24:15 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:24:15 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:24:15 TP3] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  3.91it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  3.41it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  3.66it/s]
[2026-01-05 12:24:17 TP1] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.98 GB, mem usage=4.15 GB.

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.52it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.56it/s]

[2026-01-05 12:24:17 TP2] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.74 GB, mem usage=4.15 GB.
[2026-01-05 12:24:17 TP0] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.67 GB, mem usage=4.15 GB.
[2026-01-05 12:24:17 TP3] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.98 GB, mem usage=4.15 GB.
[2026-01-05 12:24:17 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:24:17 TP0] The available memory for KV cache is 17.14 GB.
[2026-01-05 12:24:17 TP3] The available memory for KV cache is 17.14 GB.
[2026-01-05 12:24:17 TP2] The available memory for KV cache is 17.14 GB.
[2026-01-05 12:24:17 TP1] The available memory for KV cache is 17.14 GB.
[2026-01-05 12:24:17 TP0] KV Cache is allocated. #tokens: 1283328, K size: 8.57 GB, V size: 8.57 GB
[2026-01-05 12:24:17 TP2] KV Cache is allocated. #tokens: 1283328, K size: 8.57 GB, V size: 8.57 GB
[2026-01-05 12:24:17 TP1] KV Cache is allocated. #tokens: 1283328, K size: 8.57 GB, V size: 8.57 GB
[2026-01-05 12:24:17 TP3] KV Cache is allocated. #tokens: 1283328, K size: 8.57 GB, V size: 8.57 GB
[2026-01-05 12:24:17 TP0] Memory pool end. avail mem=39.03 GB
[2026-01-05 12:24:17 TP2] Memory pool end. avail mem=39.10 GB
[2026-01-05 12:24:17 TP1] Memory pool end. avail mem=39.34 GB
[2026-01-05 12:24:17 TP3] Memory pool end. avail mem=39.34 GB
[2026-01-05 12:24:17 TP0] vision_config is None, using default vision config
[2026-01-05 12:24:17 TP2] vision_config is None, using default vision config
[2026-01-05 12:24:17 TP1] vision_config is None, using default vision config
[2026-01-05 12:24:17 TP3] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:24:18 TP0] max_total_num_tokens=1283328, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=39.03 GB
[2026-01-05 12:24:19] INFO:     Started server process [344338]
[2026-01-05 12:24:19] INFO:     Waiting for application startup.
[2026-01-05 12:24:19] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:24:19] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:24:19] INFO:     Application startup complete.
[2026-01-05 12:24:19] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:24:20] INFO:     127.0.0.1:55976 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:24:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:24:20.031220110 compiler_depend.ts:198] Warning: Driver Version: "H" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:24:20.034010733 compiler_depend.ts:198] Warning: Driver Version: "ֿ" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:24:20.035828731 compiler_depend.ts:198] Warning: Driver Version: "ڽ" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:24:20.035877173 compiler_depend.ts:198] Warning: Driver Version: "<" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:24:22] INFO:     127.0.0.1:56000 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:24:32] INFO:     127.0.0.1:37118 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:24:36] INFO:     127.0.0.1:55992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:24:36] The server is fired up and ready to roll!
[2026-01-05 12:24:42 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:24:43] INFO:     127.0.0.1:48820 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_minicpm_v_2_6.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 344338 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 51.595s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6 --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 344338
.
.
End (40/43):
filename='ascend/vlm_models/test_ascend_minicpm_v_2_6.py', elapsed=62, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_minicpm_v_2_6.py returned exit code 1

.
.
Begin (41/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_deepseek_vl2.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:25:04] server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.95, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=793406109, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-05 12:25:07] Inferred chat template from model path: deepseek-vl2
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:25:21] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 12:25:21] Init torch distributed ends. mem usage=-0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:25:22] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:25:22] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:25:22] Load weight begin. avail mem=56.81 GB
[2026-01-05 12:25:23] Config not support fused shared expert(s). Shared experts fusion optimization is disabled.

Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:03<00:21,  3.12s/it]

Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:06<00:19,  3.17s/it]

Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:07<00:10,  2.07s/it]

Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:10<00:09,  2.43s/it]

Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:12<00:07,  2.56s/it]

Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:15<00:05,  2.69s/it]

Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:18<00:02,  2.74s/it]

Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:19<00:00,  2.02s/it]

Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:19<00:00,  2.39s/it]

[2026-01-05 12:27:04] Load weight end. type=DeepseekVL2ForCausalLM, dtype=torch.bfloat16, avail mem=4.95 GB, mem usage=51.87 GB.
[2026-01-05 12:27:04] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:27:04] The available memory for KV cache is 2.11 GB.
[2026-01-05 12:27:04] KV Cache is allocated. #tokens: 65408, KV size: 2.11 GB
[2026-01-05 12:27:04] Memory pool end. avail mem=2.77 GB
[2026-01-05 12:27:05] Capture cuda graph begin. This can take up to several minutes. avail mem=2.77 GB
[2026-01-05 12:27:05] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64]

  0%|          | 0/12 [00:00<?, ?it/s]
Capturing batches (bs=64 avail_mem=2.73 GB):   0%|          | 0/12 [00:00<?, ?it/s][rank0]:[W105 12:27:06.294101091 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())

Capturing batches (bs=64 avail_mem=2.73 GB):   8%|▊         | 1/12 [00:01<00:14,  1.30s/it]
Capturing batches (bs=56 avail_mem=2.44 GB):   8%|▊         | 1/12 [00:01<00:14,  1.30s/it]
Capturing batches (bs=56 avail_mem=2.44 GB):  17%|█▋        | 2/12 [00:01<00:06,  1.47it/s]
Capturing batches (bs=48 avail_mem=2.41 GB):  17%|█▋        | 2/12 [00:01<00:06,  1.47it/s]
Capturing batches (bs=48 avail_mem=2.41 GB):  25%|██▌       | 3/12 [00:01<00:04,  2.09it/s]
Capturing batches (bs=40 avail_mem=2.40 GB):  25%|██▌       | 3/12 [00:01<00:04,  2.09it/s]
Capturing batches (bs=40 avail_mem=2.40 GB):  33%|███▎      | 4/12 [00:02<00:03,  2.60it/s]
Capturing batches (bs=32 avail_mem=2.39 GB):  33%|███▎      | 4/12 [00:02<00:03,  2.60it/s]
Capturing batches (bs=32 avail_mem=2.39 GB):  42%|████▏     | 5/12 [00:02<00:02,  3.01it/s]
Capturing batches (bs=24 avail_mem=2.38 GB):  42%|████▏     | 5/12 [00:02<00:02,  3.01it/s]
Capturing batches (bs=24 avail_mem=2.38 GB):  50%|█████     | 6/12 [00:02<00:01,  3.33it/s]
Capturing batches (bs=16 avail_mem=2.38 GB):  50%|█████     | 6/12 [00:02<00:01,  3.33it/s]
Capturing batches (bs=16 avail_mem=2.38 GB):  58%|█████▊    | 7/12 [00:02<00:01,  3.57it/s]
Capturing batches (bs=12 avail_mem=2.37 GB):  58%|█████▊    | 7/12 [00:02<00:01,  3.57it/s]
Capturing batches (bs=12 avail_mem=2.37 GB):  67%|██████▋   | 8/12 [00:02<00:01,  3.75it/s]
Capturing batches (bs=8 avail_mem=2.36 GB):  67%|██████▋   | 8/12 [00:02<00:01,  3.75it/s]
Capturing batches (bs=8 avail_mem=2.36 GB):  75%|███████▌  | 9/12 [00:03<00:00,  3.88it/s]
Capturing batches (bs=4 avail_mem=2.35 GB):  75%|███████▌  | 9/12 [00:03<00:00,  3.88it/s]
Capturing batches (bs=4 avail_mem=2.35 GB):  83%|████████▎ | 10/12 [00:03<00:00,  3.91it/s]
Capturing batches (bs=2 avail_mem=2.35 GB):  83%|████████▎ | 10/12 [00:03<00:00,  3.91it/s]
Capturing batches (bs=2 avail_mem=2.35 GB):  92%|█████████▏| 11/12 [00:03<00:00,  3.92it/s]
Capturing batches (bs=1 avail_mem=2.34 GB):  92%|█████████▏| 11/12 [00:03<00:00,  3.92it/s]
Capturing batches (bs=1 avail_mem=2.34 GB): 100%|██████████| 12/12 [00:03<00:00,  3.97it/s]
Capturing batches (bs=1 avail_mem=2.34 GB): 100%|██████████| 12/12 [00:03<00:00,  3.02it/s]
[2026-01-05 12:27:10] Capture cuda graph end. Time elapsed: 4.91 s. mem usage=0.44 GB. avail mem=2.34 GB.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:27:10] max_total_num_tokens=65408, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4200, available_gpu_mem=2.34 GB
[2026-01-05 12:27:11] INFO:     Started server process [350649]
[2026-01-05 12:27:11] INFO:     Waiting for application startup.
[2026-01-05 12:27:11] INFO:     Application startup complete.
[2026-01-05 12:27:11] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:27:12] INFO:     127.0.0.1:44400 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:27:12] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:27:12.144226855 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:27:14] INFO:     127.0.0.1:44430 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:27:24] INFO:     127.0.0.1:53468 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:27:29] INFO:     127.0.0.1:44416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:27:29] The server is fired up and ready to roll!
[2026-01-05 12:27:34] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:27:35] INFO:     127.0.0.1:33570 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/vlm_models/mmmu_vlm_mixin.py", line 155, in _run_vlm_mmmu_test
    self.run_mmmu_eval(model.model, output_path)
  File "/data/c30044170/newHDK/ascend/vlm_models/mmmu_vlm_mixin.py", line 84, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2",tp=1', '--tasks', 'mmmu_val', '--batch_size', '64', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_deepseek_vl2.py", line 33, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test(model, "./logs")
  File "/data/c30044170/newHDK/ascend/vlm_models/mmmu_vlm_mixin.py", line 193, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {model.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2",tp=1', '--tasks', 'mmmu_val', '--batch_size', '64', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 350649 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestVLMModels.test_vlm_mmmu_benchmark)
Test VLM models against MMMU benchmark.
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/vlm_models/mmmu_vlm_mixin.py", line 155, in _run_vlm_mmmu_test
    self.run_mmmu_eval(model.model, output_path)
  File "/data/c30044170/newHDK/ascend/vlm_models/mmmu_vlm_mixin.py", line 84, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2",tp=1', '--tasks', 'mmmu_val', '--batch_size', '64', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2",tp=1', '--tasks', 'mmmu_val', '--batch_size', '64', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 161.282s

FAILED (errors=1)
[CI Test Method] TestVLMModels.test_vlm_mmmu_benchmark

Testing model: /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2 --trust-remote-code --cuda-graph-max-bs 64 --enable-multimodal --mem-fraction-static 0.95 --log-level info --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2",tp=1', '--tasks', 'mmmu_val', '--batch_size', '64', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs']' returned non-zero exit status 1.
Cleaning up process 350649
.
.
End (41/43):
filename='ascend/vlm_models/test_ascend_deepseek_vl2.py', elapsed=172, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_deepseek_vl2.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_deepseek_vl2.py

.
.
Begin (41/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_deepseek_vl2.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:28:55] server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.95, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=744760404, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-05 12:28:57] Inferred chat template from model path: deepseek-vl2
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:29:05] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-05 12:29:06] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:29:06] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:29:07] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:29:07] Load weight begin. avail mem=56.82 GB
[2026-01-05 12:29:08] Config not support fused shared expert(s). Shared experts fusion optimization is disabled.

Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:00<00:00, 33.49it/s]

Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:00<00:00, 33.41it/s]

Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:00<00:00, 33.40it/s]

[2026-01-05 12:29:24] Load weight end. type=DeepseekVL2ForCausalLM, dtype=torch.bfloat16, avail mem=4.95 GB, mem usage=51.87 GB.
[2026-01-05 12:29:24] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:29:24] The available memory for KV cache is 2.11 GB.
[2026-01-05 12:29:25] KV Cache is allocated. #tokens: 65408, KV size: 2.11 GB
[2026-01-05 12:29:25] Memory pool end. avail mem=2.77 GB
[2026-01-05 12:29:25] Capture cuda graph begin. This can take up to several minutes. avail mem=2.77 GB
[2026-01-05 12:29:25] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64]

  0%|          | 0/12 [00:00<?, ?it/s]
Capturing batches (bs=64 avail_mem=2.73 GB):   0%|          | 0/12 [00:00<?, ?it/s][rank0]:[W105 12:29:26.931346170 compiler_depend.ts:28] Warning: The oprator of MoeInitRouting will be removed from Pytorch and switch to AscendSpeed after 630. (function operator())

Capturing batches (bs=64 avail_mem=2.73 GB):   8%|▊         | 1/12 [00:00<00:09,  1.18it/s]
Capturing batches (bs=56 avail_mem=2.44 GB):   8%|▊         | 1/12 [00:00<00:09,  1.18it/s]
Capturing batches (bs=56 avail_mem=2.44 GB):  17%|█▋        | 2/12 [00:01<00:04,  2.03it/s]
Capturing batches (bs=48 avail_mem=2.41 GB):  17%|█▋        | 2/12 [00:01<00:04,  2.03it/s]
Capturing batches (bs=48 avail_mem=2.41 GB):  25%|██▌       | 3/12 [00:01<00:03,  2.65it/s]
Capturing batches (bs=40 avail_mem=2.40 GB):  25%|██▌       | 3/12 [00:01<00:03,  2.65it/s]
Capturing batches (bs=40 avail_mem=2.40 GB):  33%|███▎      | 4/12 [00:01<00:02,  3.10it/s]
Capturing batches (bs=32 avail_mem=2.39 GB):  33%|███▎      | 4/12 [00:01<00:02,  3.10it/s]
Capturing batches (bs=32 avail_mem=2.39 GB):  42%|████▏     | 5/12 [00:01<00:02,  3.41it/s]
Capturing batches (bs=24 avail_mem=2.38 GB):  42%|████▏     | 5/12 [00:01<00:02,  3.41it/s]
Capturing batches (bs=24 avail_mem=2.38 GB):  50%|█████     | 6/12 [00:02<00:01,  3.64it/s]
Capturing batches (bs=16 avail_mem=2.38 GB):  50%|█████     | 6/12 [00:02<00:01,  3.64it/s]
Capturing batches (bs=16 avail_mem=2.38 GB):  58%|█████▊    | 7/12 [00:02<00:01,  3.80it/s]
Capturing batches (bs=12 avail_mem=2.37 GB):  58%|█████▊    | 7/12 [00:02<00:01,  3.80it/s]
Capturing batches (bs=12 avail_mem=2.37 GB):  67%|██████▋   | 8/12 [00:02<00:01,  3.91it/s]
Capturing batches (bs=8 avail_mem=2.36 GB):  67%|██████▋   | 8/12 [00:02<00:01,  3.91it/s]
Capturing batches (bs=8 avail_mem=2.36 GB):  75%|███████▌  | 9/12 [00:02<00:00,  3.94it/s]
Capturing batches (bs=4 avail_mem=2.35 GB):  75%|███████▌  | 9/12 [00:02<00:00,  3.94it/s]
Capturing batches (bs=4 avail_mem=2.35 GB):  83%|████████▎ | 10/12 [00:03<00:00,  3.96it/s]
Capturing batches (bs=2 avail_mem=2.35 GB):  83%|████████▎ | 10/12 [00:03<00:00,  3.96it/s]
Capturing batches (bs=2 avail_mem=2.35 GB):  92%|█████████▏| 11/12 [00:03<00:00,  3.97it/s]
Capturing batches (bs=1 avail_mem=2.34 GB):  92%|█████████▏| 11/12 [00:03<00:00,  3.97it/s]
Capturing batches (bs=1 avail_mem=2.34 GB): 100%|██████████| 12/12 [00:03<00:00,  4.01it/s]
Capturing batches (bs=1 avail_mem=2.34 GB): 100%|██████████| 12/12 [00:03<00:00,  3.41it/s]
[2026-01-05 12:29:29] Capture cuda graph end. Time elapsed: 4.55 s. mem usage=0.44 GB. avail mem=2.33 GB.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:29:30] max_total_num_tokens=65408, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4200, available_gpu_mem=2.33 GB
[2026-01-05 12:29:30] INFO:     Started server process [359830]
[2026-01-05 12:29:30] INFO:     Waiting for application startup.
[2026-01-05 12:29:31] INFO:     Application startup complete.
[2026-01-05 12:29:31] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:29:32] INFO:     127.0.0.1:52464 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:29:32] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:29:32.759862930 compiler_depend.ts:198] Warning: Driver Version: "છ" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:29:36] INFO:     127.0.0.1:52482 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:29:44] INFO:     127.0.0.1:52466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:29:44] The server is fired up and ready to roll!
[2026-01-05 12:29:46] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:29:47] INFO:     127.0.0.1:36682 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/vlm_models/mmmu_vlm_mixin.py", line 155, in _run_vlm_mmmu_test
    self.run_mmmu_eval(model.model, output_path)
  File "/data/c30044170/newHDK/ascend/vlm_models/mmmu_vlm_mixin.py", line 84, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2",tp=1', '--tasks', 'mmmu_val', '--batch_size', '64', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_deepseek_vl2.py", line 33, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test(model, "./logs")
  File "/data/c30044170/newHDK/ascend/vlm_models/mmmu_vlm_mixin.py", line 193, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {model.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2",tp=1', '--tasks', 'mmmu_val', '--batch_size', '64', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 359830 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestVLMModels.test_vlm_mmmu_benchmark)
Test VLM models against MMMU benchmark.
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/newHDK/ascend/vlm_models/mmmu_vlm_mixin.py", line 155, in _run_vlm_mmmu_test
    self.run_mmmu_eval(model.model, output_path)
  File "/data/c30044170/newHDK/ascend/vlm_models/mmmu_vlm_mixin.py", line 84, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2",tp=1', '--tasks', 'mmmu_val', '--batch_size', '64', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2",tp=1', '--tasks', 'mmmu_val', '--batch_size', '64', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 61.198s

FAILED (errors=1)
[CI Test Method] TestVLMModels.test_vlm_mmmu_benchmark

Testing model: /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2 --trust-remote-code --cuda-graph-max-bs 64 --enable-multimodal --mem-fraction-static 0.95 --log-level info --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/deepseek-ai/deepseek-vl2",tp=1', '--tasks', 'mmmu_val', '--batch_size', '64', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs']' returned non-zero exit status 1.
Cleaning up process 359830
.
.
End (41/43):
filename='ascend/vlm_models/test_ascend_deepseek_vl2.py', elapsed=72, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_deepseek_vl2.py returned exit code 1

.
.
Begin (42/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_gemma_3_4b_it.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 12:30:11] INFO model_config.py:981: For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:30:11] INFO model_config.py:1010: Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:30:11] WARNING server_args.py:1247: Disable hybrid SWA memory for Gemma3ForConditionalGeneration as it is not yet supported.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:30:12] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', tokenizer_path='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=True, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=78725673, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 12:30:12] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:30:12] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:30:17] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:30:26 TP1] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:30:26 TP1] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:30:26 TP3] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:30:26 TP3] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:30:26 TP2] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:30:26 TP0] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:30:26 TP2] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:30:26 TP0] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:30:29 TP1] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:30:29 TP1] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:30:29 TP1] Init torch distributed begin.
[2026-01-05 12:30:29 TP0] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:30:29 TP0] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:30:29 TP0] Init torch distributed begin.
[2026-01-05 12:30:29 TP3] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:30:29 TP3] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:30:29 TP3] Init torch distributed begin.
[2026-01-05 12:30:29 TP2] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:30:29 TP2] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:30:29 TP2] Init torch distributed begin.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:30:31 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:30:31 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:30:31 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:30:31 TP0] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:30:31 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:30:31 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:30:31 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:30:31 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:30:31 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:30:31 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:30:31 TP2] Load weight begin. avail mem=60.90 GB
[2026-01-05 12:30:31 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:30:31 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:30:32 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:30:32 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:30:32 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:30:32 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:30:32 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:30:32 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:30:32 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:30:32 TP3] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:06<00:06,  6.57s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  4.94s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.18s/it]

[2026-01-05 12:30:43 TP2] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.92 GB, mem usage=3.97 GB.
[2026-01-05 12:30:43 TP3] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.16 GB, mem usage=3.97 GB.
[2026-01-05 12:30:43 TP1] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.16 GB, mem usage=3.97 GB.
[2026-01-05 12:30:43 TP0] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.85 GB, mem usage=3.97 GB.
[2026-01-05 12:30:43 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:30:43 TP0] The available memory for KV cache is 17.32 GB.
[2026-01-05 12:30:43 TP3] The available memory for KV cache is 17.32 GB.
[2026-01-05 12:30:43 TP1] The available memory for KV cache is 17.32 GB.
[2026-01-05 12:30:43 TP2] The available memory for KV cache is 17.32 GB.
[2026-01-05 12:30:43 TP2] KV Cache is allocated. #tokens: 534016, K size: 8.66 GB, V size: 8.66 GB
[2026-01-05 12:30:43 TP1] KV Cache is allocated. #tokens: 534016, K size: 8.66 GB, V size: 8.66 GB
[2026-01-05 12:30:43 TP0] KV Cache is allocated. #tokens: 534016, K size: 8.66 GB, V size: 8.66 GB
[2026-01-05 12:30:43 TP3] KV Cache is allocated. #tokens: 534016, K size: 8.66 GB, V size: 8.66 GB
[2026-01-05 12:30:43 TP2] Memory pool end. avail mem=31.60 GB
[2026-01-05 12:30:43 TP1] Memory pool end. avail mem=31.84 GB
[2026-01-05 12:30:43 TP0] Memory pool end. avail mem=31.53 GB
[2026-01-05 12:30:43 TP3] Memory pool end. avail mem=31.84 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:30:46 TP0] max_total_num_tokens=534016, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=1048576, available_gpu_mem=31.53 GB
[2026-01-05 12:30:48] INFO:     Started server process [368950]
[2026-01-05 12:30:48] INFO:     Waiting for application startup.
[2026-01-05 12:30:48] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 64, 'top_p': 0.95}
[2026-01-05 12:30:48] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 64, 'top_p': 0.95}
[2026-01-05 12:30:48] INFO:     Application startup complete.
[2026-01-05 12:30:48] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:30:49] INFO:     127.0.0.1:48554 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:30:49 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:30:49.659475688 compiler_depend.ts:198] Warning: Driver Version: "]" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:30:49.660937563 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:30:49.660991655 compiler_depend.ts:198] Warning: Driver Version: "|" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:30:49.662081535 compiler_depend.ts:198] Warning: Driver Version: "6" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:30:57] INFO:     127.0.0.1:39920 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:31:07] INFO:     127.0.0.1:52676 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:31:08] INFO:     127.0.0.1:48558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:31:08] The server is fired up and ready to roll!
[2026-01-05 12:31:17 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:31:18] INFO:     127.0.0.1:47548 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/google/gemma-3-4b-it",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_gemma_3_4b_it.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/google/gemma-3-4b-it: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/google/gemma-3-4b-it",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 368950 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/google/gemma-3-4b-it",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/google/gemma-3-4b-it: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/google/gemma-3-4b-it",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 81.579s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/google/gemma-3-4b-it
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/google/gemma-3-4b-it --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/google/gemma-3-4b-it: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/google/gemma-3-4b-it",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 368950
.
.
End (42/43):
filename='ascend/vlm_models/test_ascend_gemma_3_4b_it.py', elapsed=92, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_gemma_3_4b_it.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_gemma_3_4b_it.py

.
.
Begin (42/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_gemma_3_4b_it.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 12:32:40] INFO model_config.py:981: For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:32:40] INFO model_config.py:1010: Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:32:40] WARNING server_args.py:1247: Disable hybrid SWA memory for Gemma3ForConditionalGeneration as it is not yet supported.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:32:41] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', tokenizer_path='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=True, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=670959397, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-05 12:32:41] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:32:41] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:32:45] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:32:50 TP2] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:32:50 TP2] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-05 12:32:50 TP0] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:32:50 TP0] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:32:50 TP3] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:32:50 TP3] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:32:50 TP1] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:32:50 TP1] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:32:53 TP2] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:32:53 TP2] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:32:53 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:32:53 TP0] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:32:53 TP0] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:32:53 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:32:53 TP3] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:32:53 TP3] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:32:53 TP3] Init torch distributed begin.
[2026-01-05 12:32:53 TP1] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-05 12:32:53 TP1] Downcasting torch.float32 to torch.bfloat16.
[2026-01-05 12:32:53 TP1] Init torch distributed begin.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:32:54 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:32:54 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:32:54 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:32:54 TP2] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:32:55 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:32:55 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:32:55 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:32:55 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:32:55 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:32:55 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:32:55 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:32:55 TP2] Load weight begin. avail mem=60.89 GB
[2026-01-05 12:32:55 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:32:55 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:32:55 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:32:55 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:32:55 TP1] Using sdpa as multimodal attention backend.
[2026-01-05 12:32:55 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:32:55 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:32:55 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:32:55 TP3] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.26it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]

[2026-01-05 12:32:58 TP0] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.85 GB, mem usage=3.97 GB.
[2026-01-05 12:32:58 TP1] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.16 GB, mem usage=3.97 GB.
[2026-01-05 12:32:58 TP2] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.92 GB, mem usage=3.97 GB.
[2026-01-05 12:32:58 TP3] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.16 GB, mem usage=3.97 GB.
[2026-01-05 12:32:58 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:32:58 TP0] The available memory for KV cache is 17.32 GB.
[2026-01-05 12:32:58 TP3] The available memory for KV cache is 17.32 GB.
[2026-01-05 12:32:58 TP1] The available memory for KV cache is 17.32 GB.
[2026-01-05 12:32:58 TP2] The available memory for KV cache is 17.32 GB.
[2026-01-05 12:32:58 TP0] KV Cache is allocated. #tokens: 534016, K size: 8.66 GB, V size: 8.66 GB
[2026-01-05 12:32:58 TP1] KV Cache is allocated. #tokens: 534016, K size: 8.66 GB, V size: 8.66 GB
[2026-01-05 12:32:58 TP2] KV Cache is allocated. #tokens: 534016, K size: 8.66 GB, V size: 8.66 GB
[2026-01-05 12:32:58 TP3] KV Cache is allocated. #tokens: 534016, K size: 8.66 GB, V size: 8.66 GB
[2026-01-05 12:32:58 TP0] Memory pool end. avail mem=31.53 GB
[2026-01-05 12:32:58 TP2] Memory pool end. avail mem=31.60 GB
[2026-01-05 12:32:58 TP1] Memory pool end. avail mem=31.84 GB
[2026-01-05 12:32:58 TP3] Memory pool end. avail mem=31.84 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:33:01 TP0] max_total_num_tokens=534016, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=1048576, available_gpu_mem=31.53 GB
[2026-01-05 12:33:03] INFO:     Started server process [374950]
[2026-01-05 12:33:03] INFO:     Waiting for application startup.
[2026-01-05 12:33:03] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 64, 'top_p': 0.95}
[2026-01-05 12:33:03] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 64, 'top_p': 0.95}
[2026-01-05 12:33:03] INFO:     Application startup complete.
[2026-01-05 12:33:03] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:33:04] INFO:     127.0.0.1:39446 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:33:04 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:33:04.276077404 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:33:04.279885995 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:33:04.281513316 compiler_depend.ts:198] Warning: Driver Version: "u" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:33:04.283182458 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:33:10] INFO:     127.0.0.1:39772 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:33:20] INFO:     127.0.0.1:39456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:33:20] The server is fired up and ready to roll!
[2026-01-05 12:33:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:33:21] INFO:     127.0.0.1:47842 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/google/gemma-3-4b-it",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_gemma_3_4b_it.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/google/gemma-3-4b-it: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/google/gemma-3-4b-it",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 374950 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/google/gemma-3-4b-it",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/google/gemma-3-4b-it: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/google/gemma-3-4b-it",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 51.505s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/google/gemma-3-4b-it
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/google/gemma-3-4b-it --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/google/gemma-3-4b-it: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/google/gemma-3-4b-it",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 374950
.
.
End (42/43):
filename='ascend/vlm_models/test_ascend_gemma_3_4b_it.py', elapsed=63, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_gemma_3_4b_it.py returned exit code 1

.
.
Begin (43/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:33:42] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL', tokenizer_path='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=616672882, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:33:45] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:33:52 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:33:53 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:33:53 TP2] Init torch distributed begin.
[2026-01-05 12:33:53 TP1] Init torch distributed begin.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:33:54 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:33:54 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:33:54 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:33:54 TP1] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:33:54 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:33:55 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:33:55 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:33:55 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:33:55 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:33:55 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:33:55 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:33:55 TP2] Load weight begin. avail mem=60.89 GB
[2026-01-05 12:33:55 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:33:55 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:33:55 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:33:56 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:33:56 TP0] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2026-01-05 12:33:56 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:33:56 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:33:56 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:33:56 TP1] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.49s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.86s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:11<00:03,  3.43s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  3.94s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.01s/it]

[2026-01-05 12:34:12 TP0] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.74 GB, mem usage=4.08 GB.
[2026-01-05 12:34:12 TP1] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.05 GB, mem usage=4.08 GB.
[2026-01-05 12:34:12 TP2] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.82 GB, mem usage=4.08 GB.
[2026-01-05 12:34:12 TP3] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.05 GB, mem usage=4.08 GB.
[2026-01-05 12:34:12 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:34:12 TP0] The available memory for KV cache is 17.18 GB.
[2026-01-05 12:34:12 TP3] The available memory for KV cache is 17.18 GB.
[2026-01-05 12:34:12 TP2] The available memory for KV cache is 17.18 GB.
[2026-01-05 12:34:12 TP1] The available memory for KV cache is 17.18 GB.
[2026-01-05 12:34:12 TP0] KV Cache is allocated. #tokens: 500224, K size: 8.59 GB, V size: 8.59 GB
[2026-01-05 12:34:12 TP2] KV Cache is allocated. #tokens: 500224, K size: 8.59 GB, V size: 8.59 GB
[2026-01-05 12:34:12 TP3] KV Cache is allocated. #tokens: 500224, K size: 8.59 GB, V size: 8.59 GB
[2026-01-05 12:34:12 TP1] KV Cache is allocated. #tokens: 500224, K size: 8.59 GB, V size: 8.59 GB
[2026-01-05 12:34:12 TP0] Memory pool end. avail mem=38.55 GB
[2026-01-05 12:34:12 TP2] Memory pool end. avail mem=38.63 GB
[2026-01-05 12:34:12 TP3] Memory pool end. avail mem=38.86 GB
[2026-01-05 12:34:12 TP1] Memory pool end. avail mem=38.86 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:34:13 TP0] max_total_num_tokens=500224, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=128000, available_gpu_mem=38.55 GB
[2026-01-05 12:34:14] INFO:     Started server process [380948]
[2026-01-05 12:34:14] INFO:     Waiting for application startup.
[2026-01-05 12:34:14] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:34:14] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:34:14] INFO:     Application startup complete.
[2026-01-05 12:34:14] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:34:15] INFO:     127.0.0.1:46258 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:34:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:34:15.240736867 compiler_depend.ts:198] Warning: Driver Version: "!" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:34:15.243495599 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:34:15.243597343 compiler_depend.ts:198] Warning: Driver Version: "n" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:34:15.243938156 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:34:23] INFO:     127.0.0.1:57716 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:34:31] INFO:     127.0.0.1:46274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:34:31] The server is fired up and ready to roll!
[2026-01-05 12:34:33 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:34:34] INFO:     127.0.0.1:36940 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 380948 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 61.574s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 380948
.
.
End (43/43):
filename='ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py', elapsed=73, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py

.
.
Begin (43/43):
python3 /data/c30044170/newHDK/ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:36:03] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL', tokenizer_path='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=292814513, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:36:06] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:36:13 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:36:14 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:36:14 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-05 12:36:14 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-05 12:36:15 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:36:15 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:36:15 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-05 12:36:15 TP3] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:36:15 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:36:16 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:36:16 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:36:16 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-05 12:36:16 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2026-01-05 12:36:16 TP0] Load weight begin. avail mem=60.82 GB
[2026-01-05 12:36:16 TP2] Load weight begin. avail mem=60.90 GB
[2026-01-05 12:36:16 TP3] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:36:16 TP1] Load weight begin. avail mem=61.13 GB
[2026-01-05 12:36:16 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:36:16 TP2] Using sdpa as multimodal attention backend.
[2026-01-05 12:36:16 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:36:16 TP3] Using sdpa as multimodal attention backend.
[2026-01-05 12:36:16 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:36:16 TP0] Using sdpa as multimodal attention backend.
[2026-01-05 12:36:16 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-05 12:36:16 TP1] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.42s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.66s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:11<00:03,  3.36s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.85s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.91s/it]

[2026-01-05 12:36:32 TP2] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.82 GB, mem usage=4.08 GB.
[2026-01-05 12:36:33 TP0] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.74 GB, mem usage=4.08 GB.
[2026-01-05 12:36:33 TP1] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.05 GB, mem usage=4.08 GB.
[2026-01-05 12:36:33 TP3] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.05 GB, mem usage=4.08 GB.
[2026-01-05 12:36:33 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-05 12:36:33 TP0] The available memory for KV cache is 17.18 GB.
[2026-01-05 12:36:33 TP3] The available memory for KV cache is 17.18 GB.
[2026-01-05 12:36:33 TP1] The available memory for KV cache is 17.18 GB.
[2026-01-05 12:36:33 TP2] The available memory for KV cache is 17.18 GB.
[2026-01-05 12:36:33 TP0] KV Cache is allocated. #tokens: 500224, K size: 8.59 GB, V size: 8.59 GB
[2026-01-05 12:36:33 TP1] KV Cache is allocated. #tokens: 500224, K size: 8.59 GB, V size: 8.59 GB
[2026-01-05 12:36:33 TP2] KV Cache is allocated. #tokens: 500224, K size: 8.59 GB, V size: 8.59 GB
[2026-01-05 12:36:33 TP3] KV Cache is allocated. #tokens: 500224, K size: 8.59 GB, V size: 8.59 GB
[2026-01-05 12:36:33 TP0] Memory pool end. avail mem=38.55 GB
[2026-01-05 12:36:33 TP1] Memory pool end. avail mem=38.86 GB
[2026-01-05 12:36:33 TP2] Memory pool end. avail mem=38.63 GB
[2026-01-05 12:36:33 TP3] Memory pool end. avail mem=38.86 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-05 12:36:34 TP0] max_total_num_tokens=500224, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=128000, available_gpu_mem=38.55 GB
[2026-01-05 12:36:34] INFO:     Started server process [386556]
[2026-01-05 12:36:34] INFO:     Waiting for application startup.
[2026-01-05 12:36:34] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:36:34] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-05 12:36:34] INFO:     Application startup complete.
[2026-01-05 12:36:34] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-05 12:36:35] INFO:     127.0.0.1:34866 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-05 12:36:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[rank0]:[W105 12:36:35.547898071 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank3]:[W105 12:36:35.547910561 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank2]:[W105 12:36:35.547907841 compiler_depend.ts:198] Warning: Driver Version: ")" is invalid or not supported yet. (function operator())
[rank1]:[W105 12:36:35.548241764 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-05 12:36:42] INFO:     127.0.0.1:47672 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:36:52] INFO:     127.0.0.1:45040 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-05 12:36:54] INFO:     127.0.0.1:34878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-05 12:36:54] The server is fired up and ready to roll!
[2026-01-05 12:37:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-05 12:37:03] INFO:     127.0.0.1:34082 - "GET /health_generate HTTP/1.1" 200 OK
/usr/local/python3.11.13/bin/python3: No module named lmms_eval
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/newHDK/ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 178, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 386556 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 148, in _run_vlm_mmmu_test
    self.run_mmmu_eval(self.model, output_path, limit)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/ascend/vlm_utils.py", line 94, in run_mmmu_eval
    subprocess.run(
  File "/usr/local/python3.11.13/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1729, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1728, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 71.568s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL: Command '['python3', '-m', 'lmms_eval', '--model', 'openai_compatible', '--model_args', 'model_version="/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL",tp=1', '--tasks', 'mmmu_val', '--batch_size', '2', '--log_samples', '--log_samples_suffix', 'openai_compatible', '--output_path', './logs', '--limit', '50', '--config', '/data/l30081563/IS/sglang/python/sglang/test/mmmu-val.yaml']' returned non-zero exit status 1.
Cleaning up process 386556
.
.
End (43/43):
filename='ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py', elapsed=88, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py returned exit code 1

Fail. Time elapsed: 7035.34s

============================================================
Test Summary: 27/44 passed
Retries: 13 test(s) were retried
============================================================
✓ PASSED:
  ascend/llm_models/test_ascend_internlm2_7b.py
  ascend/llm_models/test_ascend_granite_3_1_8b.py
  ascend/llm_models/test_ascend_chatglm2_6b.py
  ascend/llm_models/test_ascend_gemma_3_4b_it_llm.py
  ascend/llm_models/test_ascend_smollm_1_7b.py
  ascend/llm_models/test_ascend_exaone_3.py
  ascend/llm_models/test_ascend_phi_4_multimodal.py
  ascend/llm_models/test_ascend_c4ai_command_r_v01.py
  ascend/llm_models/test_ascend_llm_models_XVERSE.py
  ascend/llm_models/test_ascend_pp_single_node.py
  ascend/llm_models/test_ascend_glm4_9b_chat.py
  ascend/llm_models/test_ascend_llm_models_stablelm-2-1_6b.py
  ascend/llm_models/test_ascend_persimmon_8b_chat.py
  ascend/llm_models/test_ascend_llama_2_7b.py
  ascend/llm_models/test_ascend_ling_lite.py
  ascend/llm_models/test_ascend_afm_4_5b.py
  ascend/llm_models/test_ascend_granite_3_0_3b_a800m.py
  ascend/llm_models/test_ascend_tp4_bf16.py
  ascend/llm_models/test_ascend_mimo_7b_rl.py
  ascend/llm_models/test_ascend_mistral_7b.py
  ascend/llm_models/test_ascend_embedding_models.py
  ascend/embedding_models/test_ascend_embedding_models_e5.py
  ascend/embedding_models/test_embedding_models_clip_vit_large_patch14_336.py
  ascend/reward_models/test_ascned_llama_3_1_8b.py
  ascend/reward_models/test_ascend_qwen2_5_math_rm_72b.py
  ascend/reward_models/test_ascend_reward_models.py
  ascend/reward_models/test_ascend_reward_internlm2_7b.py

✗ FAILED (with reason):
  ascend/llm_models/test_ascend_llm_models_grok.py (exit code 1)
  ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py (exit code 1)
  ascend/llm_models/test_qwen3_30B_models.py (exit code 1)
  ascend/llm_models/test_ascend_baichuan2_13b_chat.py (exit code 1)
  ascend/embedding_models/test_ascend_embedding_models.py (exit code 1)
  ascend/vlm_models/test_vlm_models_mistral-small_3_1_24b_instruct2503.py (exit code 1)
  ascend/vlm_models/test_ascend_janus_pro_1b.py (exit code 1)
  ascend/vlm_models/test_ascend_qwen3_vl_235.py (exit code 1)
  ascend/vlm_models/test_vlm_models_glm_4_5v.py (exit code 1)
  ascend/vlm_models/test_ascend_minicpm_o_2_6.py (exit code 1)
  ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py (exit code 1)
  ascend/vlm_models/test_ascend_janus_pro_7b.py (exit code 1)
  ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py (exit code 1)
  ascend/vlm_models/test_ascend_minicpm_v_2_6.py (exit code 1)
  ascend/vlm_models/test_ascend_deepseek_vl2.py (exit code 1)
  ascend/vlm_models/test_ascend_gemma_3_4b_it.py (exit code 1)
  ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py (exit code 1)

↻ RETRIED:
  ascend/llm_models/test_ascend_llm_models_grok.py (2 attempts, failed)
  ascend/llm_models/test_ascend_internlm2_7b.py (2 attempts, passed)
  ascend/llm_models/test_ascend_baichuan2_13b_chat.py (2 attempts, failed)
  ascend/embedding_models/test_ascend_embedding_models.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_janus_pro_1b.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_minicpm_o_2_6.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_janus_pro_7b.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_minicpm_v_2_6.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_deepseek_vl2.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_gemma_3_4b_it.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py (2 attempts, failed)
============================================================


============================================================
Failed Tests List (names only):
============================================================
  1. ascend/llm_models/test_ascend_llm_models_grok.py
  2. ascend/llm_models/test_ascend_llm_models_olmoe_1b_7b.py
  3. ascend/llm_models/test_qwen3_30B_models.py
  4. ascend/llm_models/test_ascend_baichuan2_13b_chat.py
  5. ascend/embedding_models/test_ascend_embedding_models.py
  6. ascend/vlm_models/test_vlm_models_mistral-small_3_1_24b_instruct2503.py
  7. ascend/vlm_models/test_ascend_janus_pro_1b.py
  8. ascend/vlm_models/test_ascend_qwen3_vl_235.py
  9. ascend/vlm_models/test_vlm_models_glm_4_5v.py
  10. ascend/vlm_models/test_ascend_minicpm_o_2_6.py
  11. ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py
  12. ascend/vlm_models/test_ascend_janus_pro_7b.py
  13. ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py
  14. ascend/vlm_models/test_ascend_minicpm_v_2_6.py
  15. ascend/vlm_models/test_ascend_deepseek_vl2.py
  16. ascend/vlm_models/test_ascend_gemma_3_4b_it.py
  17. ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py
============================================================

2026-01-05 12:37:06,673 - INFO - Test suite finished with exit code: -1
Test suite finished with exit code: -1
2026-01-05 12:37:06,674 - INFO - All logs saved to: /home/c30044170/newHDK/log/all/20260105_103950
All logs saved to: /home/c30044170/newHDK/log/all/20260105_103950
