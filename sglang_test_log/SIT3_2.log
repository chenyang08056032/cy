2026-01-17 11:53:31,975 - INFO - Starting Ascend test suite: per-commit-1-npu-a3
Starting Ascend test suite: per-commit-1-npu-a3
2026-01-17 11:53:31,975 - INFO - Command args: Namespace(timeout_per_file=3600, suite='per-commit-1-npu-a3', auto_partition_id=None, auto_partition_size=None, continue_on_error=True, enable_retry=True, max_attempts=2, retry_wait_seconds=60, log_dir='/data/c30044170/log')
Command args: Namespace(timeout_per_file=3600, suite='per-commit-1-npu-a3', auto_partition_id=None, auto_partition_size=None, continue_on_error=True, enable_retry=True, max_attempts=2, retry_wait_seconds=60, log_dir='/data/c30044170/log')
2026-01-17 11:53:31,975 - INFO - Log directory: /data/c30044170/log/per-commit-1-npu-a3/20260117_115331
Log directory: /data/c30044170/log/per-commit-1-npu-a3/20260117_115331
2026-01-17 11:53:31,979 - INFO - ✅ Suite sanity check passed
✅ Suite sanity check passed
2026-01-17 11:53:31,979 - INFO - Total test files: 24
Total test files: 24
.
.
Begin (0/23):
python3 /data/c30044170/code/newHDK/ascend/rerank_models/test_ascend_cross_encoder_models.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[CI Test Method] TestCrossEncoderModels.test_prefill_logits
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 11:53:55] INFO model_config.py:1016: Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 11:53:55] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-17 11:53:55] INFO engine.py:220: server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/BAAI/bge-reranker-v2-m3', tokenizer_path='/root/.cache/modelscope/hub/models/BAAI/bge-reranker-v2-m3', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.65, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=-1, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=14100144, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/BAAI/bge-reranker-v2-m3', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.46it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.46it/s]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
/usr/local/python3.11.13/lib/python3.11/multiprocessing/resource_tracker.py:123: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
  warnings.warn('resource_tracker: process died unexpectedly, '
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
`torch_dtype` is deprecated! Use `dtype` instead!
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.40it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
.
----------------------------------------------------------------------
Ran 1 test in 62.734s

OK
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffc9d235470>
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffc9d236ba0>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (0/23):
filename='ascend/rerank_models/test_ascend_cross_encoder_models.py', elapsed=77, estimated_time=400
.
.

.
.
Begin (1/23):
python3 /data/c30044170/code/newHDK/ascend/reward_models/test_ascend_reward_internlm2_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 11:54:58] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-17 11:54:58] INFO engine.py:220: server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b-reward', tokenizer_path='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b-reward', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=821008732, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Shanghai_AI_Laboratory/internlm2-7b-reward', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[CI Test Method] TestInternlm2_7bReward.test_assert_close_reward_scores
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffae4cae630>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffad195a630>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffaebec6630>
  self.init_sockets(server_args, port_args)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:02<00:15,  2.21s/it]

Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:04<00:14,  2.50s/it]

Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:07<00:13,  2.73s/it]

Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:11<00:11,  2.90s/it]

Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:12<00:07,  2.46s/it]

Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:15<00:04,  2.49s/it]

Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:18<00:02,  2.66s/it]

Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:20<00:00,  2.64s/it]

Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:20<00:00,  2.61s/it]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
.
----------------------------------------------------------------------
Ran 1 test in 43.899s

OK
accuracy: tensor([-0.8408,  0.1798])
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffc89541390>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (1/23):
filename='ascend/reward_models/test_ascend_reward_internlm2_7b.py', elapsed=59, estimated_time=400
.
.

.
.
Begin (2/23):
python3 /data/c30044170/code/newHDK/ascend/reward_models/test_ascend_llama_3_1_8b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[CI Test Method] TestRewardModels.test_reward_scores
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.11s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.30s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.24s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.22s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.96s/it]
[2026-01-17 11:56:27] WARNING model_config.py:1020: Casting torch.bfloat16 to torch.float16.
[2026-01-17 11:56:27] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-17 11:56:27] INFO engine.py:220: server_args=ServerArgs(model_path='/root/.cache/huggingface/hub/models--Skywork--Skywork-Reward-Llama-3.1-8B-v0.2/snapshots/d4117fbfd81b72f41b96341238baa1e3e90a4ce1/', tokenizer_path='/root/.cache/huggingface/hub/models--Skywork--Skywork-Reward-Llama-3.1-8B-v0.2/snapshots/d4117fbfd81b72f41b96341238baa1e3e90a4ce1/', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.65, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=79614900, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/huggingface/hub/models--Skywork--Skywork-Reward-Llama-3.1-8B-v0.2/snapshots/d4117fbfd81b72f41b96341238baa1e3e90a4ce1/', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.20s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.87s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.24s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.20s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.29s/it]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
.
----------------------------------------------------------------------
Ran 1 test in 51.643s

OK
hf_scores=tensor([-24.2500,   1.0918])
srt_scores=tensor([-24.2344,   1.0918])
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffc7c494ad0>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (2/23):
filename='ascend/reward_models/test_ascend_llama_3_1_8b.py', elapsed=67, estimated_time=400
.
.

.
.
Begin (3/23):
python3 /data/c30044170/code/newHDK/ascend/reward_models/test_ascend_qwen2_5_math_rm_72b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 11:57:04] WARNING model_config.py:1020: Casting torch.bfloat16 to torch.float16.
[2026-01-17 11:57:04] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-17 11:57:04] INFO engine.py:220: server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-Math-RM-72B', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-Math-RM-72B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=832854640, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-Math-RM-72B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[CI Test Method] TestInternlm2_7bReward.test_assert_close_reward_scores
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffabb9fa8d0>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffadaa6a8d0>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffab5b568d0>
  self.init_sockets(server_args, port_args)
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

Loading safetensors checkpoint shards:   0% Completed | 0/37 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   8% Completed | 3/37 [00:00<00:01, 20.00it/s]

Loading safetensors checkpoint shards:  16% Completed | 6/37 [00:00<00:01, 20.97it/s]

Loading safetensors checkpoint shards:  24% Completed | 9/37 [00:00<00:01, 21.04it/s]

Loading safetensors checkpoint shards:  32% Completed | 12/37 [00:00<00:01, 20.42it/s]

Loading safetensors checkpoint shards:  41% Completed | 15/37 [00:00<00:01, 20.01it/s]

Loading safetensors checkpoint shards:  49% Completed | 18/37 [00:00<00:00, 21.60it/s]

Loading safetensors checkpoint shards:  57% Completed | 21/37 [00:01<00:00, 20.85it/s]

Loading safetensors checkpoint shards:  65% Completed | 24/37 [00:01<00:00, 20.11it/s]

Loading safetensors checkpoint shards:  73% Completed | 27/37 [00:01<00:00, 20.49it/s]

Loading safetensors checkpoint shards:  81% Completed | 30/37 [00:01<00:00, 19.98it/s]

Loading safetensors checkpoint shards:  89% Completed | 33/37 [00:01<00:00, 21.20it/s]

Loading safetensors checkpoint shards:  97% Completed | 36/37 [00:01<00:00, 21.44it/s]

Loading safetensors checkpoint shards: 100% Completed | 37/37 [00:01<00:00, 20.80it/s]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
.
----------------------------------------------------------------------
Ran 1 test in 233.769s

OK
accuracy: tensor([-2.2480, -0.6484])
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffcdb84fe00>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (3/23):
filename='ascend/reward_models/test_ascend_qwen2_5_math_rm_72b.py', elapsed=248, estimated_time=400
.
.

.
.
Begin (4/23):
python3 /data/c30044170/code/newHDK/ascend/reward_models/test_ascend_Qwen2_5_1_5B_apeach.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:01:12] INFO model_config.py:1016: Downcasting torch.float32 to torch.float16.
[2026-01-17 12:01:12] WARNING server_args.py:1438: Overlap scheduler is disabled for embedding models.
[2026-01-17 12:01:12] INFO engine.py:220: server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Howeee/Qwen2.5-1.5B-apeach', tokenizer_path='/root/.cache/modelscope/hub/models/Howeee/Qwen2.5-1.5B-apeach', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=True, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=20000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=316789536, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='error', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Howeee/Qwen2.5-1.5B-apeach', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=4, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method=None, kt_cpuinfer=None, kt_threadpool_count=None, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 4], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config=None, enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[CI Test Method] TestInternlm2_7bReward.test_assert_close_reward_scores
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffac233e630>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffac3926630>
  self.init_sockets(server_args, port_args)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py:304: ResourceWarning: Unclosed context <zmq.Context() at 0xfffab679a630>
  self.init_sockets(server_args, port_args)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.19it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.85it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.45it/s]

/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
.
----------------------------------------------------------------------
Ran 1 test in 27.811s

OK
accuracy: tensor([-2.9277, -2.9570])
sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.DEALER) at 0xfffca3774b40>
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
.
.
End (4/23):
filename='ascend/reward_models/test_ascend_Qwen2_5_1_5B_apeach.py', elapsed=42, estimated_time=400
.
.

.
.
Begin (5/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_gemma_3_4b_it.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:02:02] INFO model_config.py:987: For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 12:02:02] INFO model_config.py:1016: Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 12:02:02] WARNING server_args.py:1247: Disable hybrid SWA memory for Gemma3ForConditionalGeneration as it is not yet supported.
[2026-01-17 12:02:02] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', tokenizer_path='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=True, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=105809464, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/google/gemma-3-4b-it', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 12:02:02] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 12:02:02] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:02:06] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-17 12:02:12 TP1] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 12:02:12 TP1] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:02:12 TP0] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 12:02:12 TP0] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:02:13 TP2] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 12:02:13 TP2] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 12:02:13 TP3] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 12:02:13 TP3] Downcasting torch.float32 to torch.bfloat16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:02:15 TP1] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 12:02:15 TP1] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 12:02:15 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:02:15 TP0] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 12:02:15 TP0] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 12:02:15 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:02:15 TP2] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 12:02:15 TP2] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 12:02:15 TP2] Init torch distributed begin.
[2026-01-17 12:02:15 TP3] For Gemma 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.
[2026-01-17 12:02:15 TP3] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 12:02:15 TP3] Init torch distributed begin.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 12:02:16 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:02:16 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:02:16 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:02:16 TP2] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 12:02:16 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 12:02:17 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:02:17 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:02:17 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:02:17 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:02:17 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:02:17 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 12:02:17 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:02:17 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:02:17 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:02:17 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 12:02:17 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:02:17 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 12:02:17 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:02:17 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 12:02:17 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:02:17 TP0] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:07<00:07,  7.83s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:12<00:00,  5.89s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:12<00:00,  6.19s/it]

[2026-01-17 12:02:31 TP2] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.89 GB, mem usage=3.97 GB.
[2026-01-17 12:02:31 TP3] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.17 GB, mem usage=3.97 GB.
[2026-01-17 12:02:31 TP0] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.84 GB, mem usage=3.97 GB.
[2026-01-17 12:02:31 TP1] Load weight end. type=Gemma3ForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.17 GB, mem usage=3.97 GB.
[2026-01-17 12:02:31 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 12:02:31 TP3] The available memory for KV cache is 17.31 GB.
[2026-01-17 12:02:31 TP0] The available memory for KV cache is 17.31 GB.
[2026-01-17 12:02:31 TP2] The available memory for KV cache is 17.31 GB.
[2026-01-17 12:02:31 TP1] The available memory for KV cache is 17.31 GB.
[2026-01-17 12:02:32 TP0] KV Cache is allocated. #tokens: 533888, K size: 8.66 GB, V size: 8.66 GB
[2026-01-17 12:02:32 TP0] Memory pool end. avail mem=31.52 GB
[2026-01-17 12:02:32 TP3] KV Cache is allocated. #tokens: 533888, K size: 8.66 GB, V size: 8.66 GB
[2026-01-17 12:02:32 TP1] KV Cache is allocated. #tokens: 533888, K size: 8.66 GB, V size: 8.66 GB
[2026-01-17 12:02:32 TP3] Memory pool end. avail mem=31.85 GB
[2026-01-17 12:02:32 TP2] KV Cache is allocated. #tokens: 533888, K size: 8.66 GB, V size: 8.66 GB
[2026-01-17 12:02:32 TP1] Memory pool end. avail mem=31.85 GB
[2026-01-17 12:02:32 TP2] Memory pool end. avail mem=31.57 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 12:02:35 TP0] max_total_num_tokens=533888, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=1048576, available_gpu_mem=31.52 GB
[2026-01-17 12:02:37] INFO:     Started server process [24569]
[2026-01-17 12:02:37] INFO:     Waiting for application startup.
[2026-01-17 12:02:37] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 64, 'top_p': 0.95}
[2026-01-17 12:02:37] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 64, 'top_p': 0.95}
[2026-01-17 12:02:37] INFO:     Application startup complete.
[2026-01-17 12:02:37] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 12:02:38] INFO:     127.0.0.1:55932 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 12:02:38 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:02:42] INFO:     127.0.0.1:55946 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:02:52] INFO:     127.0.0.1:50300 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:03:00] INFO:     127.0.0.1:55936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 12:03:00] The server is fired up and ready to roll!
[2026-01-17 12:03:02 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:04] INFO:     127.0.0.1:41062 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 12:03:12.640[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 12:03:15.883[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 12:03:17.162[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 12:03:17.162[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 12:03:17.163[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 12:03:17.170[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 12:03:25.113[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 7879.59it/s]
[32m2026-01-17 12:03:25.120[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 12:03:25 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:25] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:00<00:30,  1.63it/s][2026-01-17 12:03:25 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:26] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:00<00:20,  2.40it/s][2026-01-17 12:03:26 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:26] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:01<00:30,  1.53it/s][2026-01-17 12:03:27 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:27 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.53, #queue-req: 0,
[2026-01-17 12:03:27] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:02<00:35,  1.30it/s][2026-01-17 12:03:27 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:28] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:03<00:37,  1.20it/s][2026-01-17 12:03:28 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:29] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:04<00:28,  1.54it/s][2026-01-17 12:03:29 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:30] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:04<00:31,  1.35it/s][2026-01-17 12:03:30 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:30 TP0] Decode batch, #running-req: 1, #token: 512, token usage: 0.00, npu graph: False, gen throughput (token/s): 15.13, #queue-req: 0,
[2026-01-17 12:03:31] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:05<00:33,  1.24it/s][2026-01-17 12:03:31 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:31] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:06<00:26,  1.57it/s][2026-01-17 12:03:31 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:32] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:07<00:29,  1.37it/s][2026-01-17 12:03:32 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:32 TP0] Decode batch, #running-req: 1, #token: 512, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.41, #queue-req: 0,
[2026-01-17 12:03:33] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:08<00:31,  1.26it/s][2026-01-17 12:03:33 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:33] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:08<00:24,  1.57it/s][2026-01-17 12:03:33 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:33] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:08<00:19,  1.89it/s][2026-01-17 12:03:33 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:34] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:08<00:16,  2.16it/s][2026-01-17 12:03:34 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:34] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:09<00:14,  2.43it/s][2026-01-17 12:03:34 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:34] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:09<00:12,  2.71it/s][2026-01-17 12:03:34 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:34] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:09<00:11,  2.92it/s][2026-01-17 12:03:34 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:35] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:10<00:16,  1.92it/s][2026-01-17 12:03:35 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:36] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:10<00:13,  2.33it/s][2026-01-17 12:03:36 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:36 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.41, #queue-req: 0,
[2026-01-17 12:03:36] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:11<00:17,  1.73it/s][2026-01-17 12:03:36 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:37] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:12<00:19,  1.48it/s][2026-01-17 12:03:37 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:38 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 18.41, #queue-req: 0,
[2026-01-17 12:03:38] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:13<00:20,  1.36it/s][2026-01-17 12:03:38 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:38] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:13<00:16,  1.68it/s][2026-01-17 12:03:39 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:39] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:14<00:17,  1.47it/s][2026-01-17 12:03:39 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:40] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:15<00:14,  1.77it/s][2026-01-17 12:03:40 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:40 TP0] Decode batch, #running-req: 1, #token: 512, token usage: 0.00, npu graph: False, gen throughput (token/s): 15.96, #queue-req: 0,
[2026-01-17 12:03:41] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:15<00:16,  1.48it/s][2026-01-17 12:03:41 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:41] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:16<00:12,  1.81it/s][2026-01-17 12:03:41 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:41] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:16<00:10,  2.14it/s][2026-01-17 12:03:41 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:41] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:16<00:08,  2.48it/s][2026-01-17 12:03:41 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:42] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:17<00:07,  2.80it/s][2026-01-17 12:03:44 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:44] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:19<00:20,  1.07s/it][2026-01-17 12:03:45 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:45] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:20<00:17,  1.01it/s][2026-01-17 12:03:47 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:47] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:22<00:20,  1.23s/it][2026-01-17 12:03:50 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:50] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:25<00:28,  1.79s/it][2026-01-17 12:03:50 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:51] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:25<00:20,  1.39s/it][2026-01-17 12:03:51 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:51] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:26<00:14,  1.06s/it][2026-01-17 12:03:51 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:51] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:26<00:11,  1.14it/s][2026-01-17 12:03:53 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:54] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:28<00:15,  1.31s/it][2026-01-17 12:03:56 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:56] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [00:31<00:17,  1.60s/it][2026-01-17 12:03:56 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:56] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [00:31<00:12,  1.21s/it][2026-01-17 12:03:58 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:58] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [00:33<00:11,  1.32s/it][2026-01-17 12:03:58 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:03:58 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 2.23, #queue-req: 0,
[2026-01-17 12:03:59] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [00:34<00:10,  1.25s/it][2026-01-17 12:04:01 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:04:01] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [00:36<00:11,  1.59s/it][2026-01-17 12:04:03 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:04:04] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [00:39<00:11,  1.87s/it][2026-01-17 12:04:06 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:04:06] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [00:41<00:10,  2.03s/it][2026-01-17 12:04:07 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:04:07] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [00:42<00:07,  1.76s/it][2026-01-17 12:04:09 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:04:09] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [00:44<00:05,  1.70s/it][2026-01-17 12:04:11 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:04:11] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [00:46<00:03,  1.77s/it][2026-01-17 12:04:12 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:04:12] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [00:47<00:01,  1.54s/it][2026-01-17 12:04:14 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:04:14] INFO:     127.0.0.1:48152 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [00:49<00:00,  1.75s/it]
Model Responding: 100%|██████████| 50/50 [00:49<00:00,  1.01it/s]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 4766.36it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.36667}, 'Accounting': {'num': 30, 'acc': 0.36667}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.35}, 'Agriculture': {'num': 20, 'acc': 0.35}, 'Overall': {'num': 50, 'acc': 0.36}}
[32m2026-01-17 12:04:14.529[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 12:04:14.535[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/google/gemma-3-4b-it",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  | 0.36|±  |   N/A|

/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 24569 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 144.975s

OK
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/google/gemma-3-4b-it
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/google/gemma-3-4b-it --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.2, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffc9dcd8cc0>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffc9dcd9bc0>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffc9dcdac00>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffc9dcdbe20>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260113_212507', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 22423.166117323, 'end_time': 22509.248523342, 'total_evaluation_time_seconds': '86.08240601900252'}
Model /root/.cache/modelscope/hub/models/google/gemma-3-4b-it achieved accuracy: 0.2000
Cleaning up process 24569
.
.
End (5/23):
filename='ascend/vlm_models/test_ascend_gemma_3_4b_it.py', elapsed=155, estimated_time=400
.
.

.
.
Begin (6/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_janus_pro_7b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:04:37] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B', tokenizer_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=302829432, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-17 12:04:39] Inferred chat template from model path: janus-pro
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:04:47 TP1] Init torch distributed begin.
[2026-01-17 12:04:48 TP3] Init torch distributed begin.
[2026-01-17 12:04:48 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:04:48 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 12:04:49 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:04:49 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:04:49 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:04:49 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:04:49 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 12:04:50 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:04:50 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:04:50 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:04:50 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:04:50 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:04:50 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:04:50 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 12:04:50 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:04:50 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:04:50 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 12:04:50 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:04:50 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:04:50 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 12:04:50 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 12:04:50 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:04:50 TP2] Using sdpa as multimodal attention backend.

Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading pt checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.84s/it]
[2026-01-17 12:05:51 TP1] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.88 GB, mem usage=4.26 GB.
[2026-01-17 12:05:51 TP3] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.88 GB, mem usage=4.26 GB.

Loading pt checkpoint shards: 100% Completed | 2/2 [01:01<00:00, 32.50s/it]

Loading pt checkpoint shards: 100% Completed | 2/2 [01:01<00:00, 30.60s/it]

[2026-01-17 12:05:52 TP2] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.60 GB, mem usage=4.26 GB.
[2026-01-17 12:05:52 TP0] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=56.55 GB, mem usage=4.26 GB.
[2026-01-17 12:05:52 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 12:05:52 TP0] The available memory for KV cache is 17.02 GB.
[2026-01-17 12:05:52 TP3] The available memory for KV cache is 17.02 GB.
[2026-01-17 12:05:52 TP2] The available memory for KV cache is 17.02 GB.
[2026-01-17 12:05:52 TP1] The available memory for KV cache is 17.02 GB.
[2026-01-17 12:05:52 TP0] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-17 12:05:52 TP0] Memory pool end. avail mem=39.26 GB
[2026-01-17 12:05:52 TP2] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-17 12:05:52 TP1] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-17 12:05:52 TP2] Memory pool end. avail mem=39.31 GB
[2026-01-17 12:05:52 TP1] Memory pool end. avail mem=39.59 GB
[2026-01-17 12:05:52 TP3] KV Cache is allocated. #tokens: 148736, K size: 8.52 GB, V size: 8.52 GB
[2026-01-17 12:05:52 TP3] Memory pool end. avail mem=39.59 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 12:05:53 TP0] max_total_num_tokens=148736, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=16384, available_gpu_mem=39.24 GB
[2026-01-17 12:05:54] INFO:     Started server process [33125]
[2026-01-17 12:05:54] INFO:     Waiting for application startup.
[2026-01-17 12:05:54] INFO:     Application startup complete.
[2026-01-17 12:05:54] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 12:05:55] INFO:     127.0.0.1:35560 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 12:05:55 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:05:57] INFO:     127.0.0.1:58730 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:06:07] INFO:     127.0.0.1:59718 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:06:17] INFO:     127.0.0.1:60776 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:06:22] INFO:     127.0.0.1:35574 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 12:06:22] The server is fired up and ready to roll!
[2026-01-17 12:06:27 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:28] INFO:     127.0.0.1:43922 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 12:06:36.685[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 12:06:38.777[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 12:06:39.534[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 12:06:39.534[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 12:06:39.535[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 12:06:39.538[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 12:06:45.798[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9085.66it/s]
[32m2026-01-17 12:06:45.804[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 12:06:46 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:47] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:01<01:02,  1.28s/it][2026-01-17 12:06:47 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:48 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.33, #queue-req: 0,
[2026-01-17 12:06:48] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:02<00:56,  1.19s/it][2026-01-17 12:06:48 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:49] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:03<00:54,  1.15s/it][2026-01-17 12:06:49 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:50] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:04<00:52,  1.14s/it][2026-01-17 12:06:50 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:51 TP0] Decode batch, #running-req: 1, #token: 896, token usage: 0.01, npu graph: False, gen throughput (token/s): 13.21, #queue-req: 0,
[2026-01-17 12:06:51] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:05<00:49,  1.11s/it][2026-01-17 12:06:51 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:52] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:06<00:47,  1.09s/it][2026-01-17 12:06:52 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:53 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 17.30, #queue-req: 0,
[2026-01-17 12:06:53] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:07<00:45,  1.06s/it][2026-01-17 12:06:53 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:54] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:08<00:43,  1.05s/it][2026-01-17 12:06:55 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:55] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:09<00:42,  1.04s/it][2026-01-17 12:06:56 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:56 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.01, npu graph: False, gen throughput (token/s): 14.25, #queue-req: 0,
[2026-01-17 12:06:56] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:10<00:41,  1.03s/it][2026-01-17 12:06:57 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:57] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:11<00:40,  1.03s/it][2026-01-17 12:06:58 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:58 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 17.37, #queue-req: 0,
[2026-01-17 12:06:58] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:12<00:39,  1.03s/it][2026-01-17 12:06:59 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:06:59] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:13<00:38,  1.03s/it][2026-01-17 12:07:00 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:00] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:14<00:37,  1.03s/it][2026-01-17 12:07:01 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:01 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.01, npu graph: False, gen throughput (token/s): 13.90, #queue-req: 0,
[2026-01-17 12:07:01] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:15<00:36,  1.04s/it][2026-01-17 12:07:02 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:02] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:17<00:35,  1.04s/it][2026-01-17 12:07:03 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:03 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 17.32, #queue-req: 0,
[2026-01-17 12:07:03] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:18<00:34,  1.03s/it][2026-01-17 12:07:04 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:04] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:19<00:32,  1.03s/it][2026-01-17 12:07:05 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:05] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:20<00:31,  1.02s/it][2026-01-17 12:07:06 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:08 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.01, npu graph: False, gen throughput (token/s): 8.68, #queue-req: 0,
[2026-01-17 12:07:08] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:22<00:47,  1.57s/it][2026-01-17 12:07:09 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:09] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:23<00:41,  1.42s/it][2026-01-17 12:07:10 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:10 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.48, #queue-req: 0,
[2026-01-17 12:07:10] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:25<00:37,  1.32s/it][2026-01-17 12:07:11 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:11] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:26<00:33,  1.25s/it][2026-01-17 12:07:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:13] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:27<00:31,  1.20s/it][2026-01-17 12:07:13 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:13 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.01, npu graph: False, gen throughput (token/s): 13.30, #queue-req: 0,
[2026-01-17 12:07:14] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:28<00:29,  1.17s/it][2026-01-17 12:07:14 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:15] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:29<00:27,  1.15s/it][2026-01-17 12:07:15 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:16 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.11, #queue-req: 0,
[2026-01-17 12:07:16] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:30<00:26,  1.14s/it][2026-01-17 12:07:16 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:17] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:31<00:24,  1.13s/it][2026-01-17 12:07:18 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:18] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:32<00:23,  1.13s/it][2026-01-17 12:07:19 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:19 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.01, npu graph: False, gen throughput (token/s): 13.03, #queue-req: 0,
[2026-01-17 12:07:19] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:33<00:22,  1.12s/it][2026-01-17 12:07:22 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:23] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:37<00:34,  1.83s/it][2026-01-17 12:07:24 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:24 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 7.37, #queue-req: 0,
[2026-01-17 12:07:24] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:39<00:32,  1.78s/it][2026-01-17 12:07:26 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:27] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:41<00:34,  2.03s/it][2026-01-17 12:07:30 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:31] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:45<00:41,  2.62s/it][2026-01-17 12:07:32 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:32 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.01, npu graph: False, gen throughput (token/s): 5.22, #queue-req: 0,
[2026-01-17 12:07:32] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:46<00:33,  2.22s/it][2026-01-17 12:07:33 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:33] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:48<00:26,  1.89s/it][2026-01-17 12:07:34 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:35] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 12:07:35 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 15.26, #queue-req: 0,

Model Responding:  74%|███████▍  | 37/50 [00:49<00:22,  1.70s/it][2026-01-17 12:07:37 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:38] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:52<00:25,  2.15s/it][2026-01-17 12:07:40 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:41] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [00:55<00:26,  2.44s/it][2026-01-17 12:07:41 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:42 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.01, npu graph: False, gen throughput (token/s): 5.55, #queue-req: 0,
[2026-01-17 12:07:42] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [00:56<00:20,  2.04s/it][2026-01-17 12:07:44 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:44] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [00:59<00:19,  2.16s/it][2026-01-17 12:07:45 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:46 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 10.25, #queue-req: 0,
[2026-01-17 12:07:46] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [01:00<00:15,  1.89s/it][2026-01-17 12:07:48 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:49] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [01:03<00:16,  2.29s/it][2026-01-17 12:07:52 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:52] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:07<00:15,  2.61s/it][2026-01-17 12:07:55 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:55 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.01, npu graph: False, gen throughput (token/s): 4.15, #queue-req: 0,
[2026-01-17 12:07:56] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [01:10<00:14,  2.81s/it][2026-01-17 12:07:57 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:07:58] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:12<00:10,  2.55s/it][2026-01-17 12:07:59 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:08:00 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 8.75, #queue-req: 0,
[2026-01-17 12:08:00] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:14<00:07,  2.51s/it][2026-01-17 12:08:02 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:08:03] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [01:17<00:05,  2.58s/it][2026-01-17 12:08:04 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:08:05] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [01:19<00:02,  2.36s/it][2026-01-17 12:08:07 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:08:07 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.01, npu graph: False, gen throughput (token/s): 5.36, #queue-req: 0,
[2026-01-17 12:08:08] INFO:     127.0.0.1:35410 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [01:22<00:00,  2.58s/it]
Model Responding: 100%|██████████| 50/50 [01:22<00:00,  1.65s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 1758.06it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.36667}, 'Accounting': {'num': 30, 'acc': 0.36667}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.35}, 'Agriculture': {'num': 20, 'acc': 0.35}, 'Overall': {'num': 50, 'acc': 0.36}}
[32m2026-01-17 12:08:08.196[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 12:08:08.201[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  | 0.36|±  |   N/A|

/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 33125 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 224.031s

OK
[CI Test Method] TestJanusPro7B.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.2, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffc9dcd8cc0>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffc9dcd9bc0>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffc9dcdac00>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffc9dcdbe20>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260113_212507', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 22423.166117323, 'end_time': 22509.248523342, 'total_evaluation_time_seconds': '86.08240601900252'}
Model /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B achieved accuracy: 0.2000
Cleaning up process 33125
.
.
End (6/23):
filename='ascend/vlm_models/test_ascend_janus_pro_7b.py', elapsed=235, estimated_time=400
.
.

.
.
Begin (7/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:08:32] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL', tokenizer_path='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=752425831, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:08:34] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:08:42 TP3] Init torch distributed begin.
[2026-01-17 12:08:42 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:08:43 TP0] Init torch distributed begin.
[2026-01-17 12:08:43 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 12:08:45 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:08:45 TP0] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 12:08:45 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:08:45 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:08:45 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 12:08:45 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:08:45 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:08:45 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:08:45 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:08:46 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:08:46 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:08:46 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 12:08:46 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:08:46 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:08:46 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 12:08:46 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:08:46 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 12:08:46 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:08:46 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 12:08:46 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:08:46 TP2] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:06<00:19,  6.38s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:12<00:12,  6.40s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:14<00:04,  4.45s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  5.08s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  5.24s/it]

[2026-01-17 12:09:07 TP1] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.06 GB, mem usage=4.08 GB.
[2026-01-17 12:09:07 TP2] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.78 GB, mem usage=4.08 GB.
[2026-01-17 12:09:07 TP0] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.73 GB, mem usage=4.08 GB.
[2026-01-17 12:09:07 TP3] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=57.06 GB, mem usage=4.08 GB.
[2026-01-17 12:09:07 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 12:09:07 TP0] The available memory for KV cache is 17.17 GB.
[2026-01-17 12:09:07 TP3] The available memory for KV cache is 17.17 GB.
[2026-01-17 12:09:07 TP2] The available memory for KV cache is 17.17 GB.
[2026-01-17 12:09:07 TP1] The available memory for KV cache is 17.17 GB.
[2026-01-17 12:09:08 TP0] KV Cache is allocated. #tokens: 500096, K size: 8.59 GB, V size: 8.59 GB
[2026-01-17 12:09:08 TP3] KV Cache is allocated. #tokens: 500096, K size: 8.59 GB, V size: 8.59 GB
[2026-01-17 12:09:08 TP0] Memory pool end. avail mem=38.54 GB
[2026-01-17 12:09:08 TP3] Memory pool end. avail mem=38.88 GB
[2026-01-17 12:09:08 TP1] KV Cache is allocated. #tokens: 500096, K size: 8.59 GB, V size: 8.59 GB
[2026-01-17 12:09:08 TP2] KV Cache is allocated. #tokens: 500096, K size: 8.59 GB, V size: 8.59 GB
[2026-01-17 12:09:08 TP1] Memory pool end. avail mem=38.88 GB
[2026-01-17 12:09:08 TP2] Memory pool end. avail mem=38.60 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 12:09:09 TP0] max_total_num_tokens=500096, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=128000, available_gpu_mem=38.54 GB
[2026-01-17 12:09:10] INFO:     Started server process [41892]
[2026-01-17 12:09:10] INFO:     Waiting for application startup.
[2026-01-17 12:09:10] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 12:09:10] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 12:09:10] INFO:     Application startup complete.
[2026-01-17 12:09:10] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 12:09:11] INFO:     127.0.0.1:56070 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 12:09:11 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:09:12] INFO:     127.0.0.1:56080 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:09:22] INFO:     127.0.0.1:48578 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:09:29] INFO:     127.0.0.1:56078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 12:09:29] The server is fired up and ready to roll!
[2026-01-17 12:09:32 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:09:33] INFO:     127.0.0.1:48708 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 12:09:41.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 12:09:43.521[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 12:09:44.276[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 12:09:44.276[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 12:09:44.277[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 12:09:44.279[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 12:09:51.197[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9149.88it/s]
[32m2026-01-17 12:09:51.203[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 12:09:51 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:09:52] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:01<00:49,  1.01s/it][2026-01-17 12:09:52 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:09:54 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.56, #queue-req: 0,
[2026-01-17 12:09:54] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:03<01:18,  1.64s/it][2026-01-17 12:09:54 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:09:54] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:03<00:56,  1.20s/it][2026-01-17 12:09:55 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:09:55] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:04<00:46,  1.02s/it][2026-01-17 12:09:55 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:09:56 TP0] Decode batch, #running-req: 1, #token: 640, token usage: 0.00, npu graph: False, gen throughput (token/s): 21.07, #queue-req: 0,
[2026-01-17 12:09:56] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:05<00:41,  1.08it/s][2026-01-17 12:09:56 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:09:57] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:06<00:38,  1.15it/s][2026-01-17 12:09:57 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:09:57 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 23.38, #queue-req: 0,
[2026-01-17 12:09:57] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:06<00:34,  1.24it/s][2026-01-17 12:09:57 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:09:58] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:07<00:32,  1.29it/s][2026-01-17 12:09:58 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:09:59] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:08<00:30,  1.33it/s][2026-01-17 12:09:59 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:09:59 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 21.88, #queue-req: 0,
[2026-01-17 12:10:00] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:08<00:29,  1.36it/s][2026-01-17 12:10:00 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:00] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:09<00:28,  1.38it/s][2026-01-17 12:10:00 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:01 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 23.52, #queue-req: 0,
[2026-01-17 12:10:01] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:10<00:27,  1.38it/s][2026-01-17 12:10:01 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:02] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:10<00:26,  1.40it/s][2026-01-17 12:10:02 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:02] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:11<00:23,  1.55it/s][2026-01-17 12:10:02 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:03 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 20.48, #queue-req: 0,
[2026-01-17 12:10:03] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:12<00:23,  1.46it/s][2026-01-17 12:10:03 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:04] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:12<00:23,  1.46it/s][2026-01-17 12:10:04 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:04] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:13<00:22,  1.45it/s][2026-01-17 12:10:04 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:05 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 22.16, #queue-req: 0,
[2026-01-17 12:10:05] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:14<00:22,  1.44it/s][2026-01-17 12:10:05 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:06] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:14<00:21,  1.47it/s][2026-01-17 12:10:06 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:06 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 24.39, #queue-req: 0,
[2026-01-17 12:10:06] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:15<00:20,  1.46it/s][2026-01-17 12:10:06 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:07] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:16<00:20,  1.44it/s][2026-01-17 12:10:07 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:08] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:17<00:19,  1.41it/s][2026-01-17 12:10:08 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:08 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 21.47, #queue-req: 0,
[2026-01-17 12:10:08] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:17<00:18,  1.43it/s][2026-01-17 12:10:09 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:09] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:18<00:18,  1.42it/s][2026-01-17 12:10:09 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:10 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 24.35, #queue-req: 0,
[2026-01-17 12:10:10] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:19<00:17,  1.45it/s][2026-01-17 12:10:10 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:11] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:19<00:17,  1.37it/s][2026-01-17 12:10:11 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:11] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:20<00:15,  1.51it/s][2026-01-17 12:10:11 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:12 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 19.20, #queue-req: 0,
[2026-01-17 12:10:12] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:21<00:15,  1.45it/s][2026-01-17 12:10:12 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:13] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:21<00:14,  1.46it/s][2026-01-17 12:10:13 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:13] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:22<00:13,  1.45it/s][2026-01-17 12:10:17 TP0] Prefill batch, #new-seq: 1, #new-token: 7040, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:28 TP0] Decode batch, #running-req: 1, #token: 7040, token usage: 0.01, npu graph: False, gen throughput (token/s): 2.45, #queue-req: 0,
[2026-01-17 12:10:29] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:37<01:36,  5.05s/it][2026-01-17 12:10:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:32] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:41<01:22,  4.61s/it][2026-01-17 12:10:34 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:39 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.89, #queue-req: 0,
[2026-01-17 12:10:39] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:47<01:27,  5.16s/it][2026-01-17 12:10:42 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:52] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [01:00<02:00,  7.56s/it][2026-01-17 12:10:52 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:53] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [01:01<01:23,  5.59s/it][2026-01-17 12:10:53 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:53 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 2.73, #queue-req: 0,
[2026-01-17 12:10:53] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [01:02<00:57,  4.14s/it][2026-01-17 12:10:54 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:10:54] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [01:03<00:41,  3.20s/it][2026-01-17 12:10:57 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:11:06 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.12, #queue-req: 0,
[2026-01-17 12:11:06] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [01:15<01:08,  5.70s/it][2026-01-17 12:11:09 TP0] Prefill batch, #new-seq: 1, #new-token: 6528, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:11:18] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [01:26<01:21,  7.45s/it][2026-01-17 12:11:18 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:11:18] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [01:27<00:54,  5.44s/it][2026-01-17 12:11:20 TP0] Prefill batch, #new-seq: 1, #new-token: 5632, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:11:27 TP0] Decode batch, #running-req: 1, #token: 5760, token usage: 0.01, npu graph: False, gen throughput (token/s): 1.91, #queue-req: 0,
[2026-01-17 12:11:27] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [01:36<00:58,  6.48s/it][2026-01-17 12:11:27 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:11:28] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [01:37<00:38,  4.85s/it][2026-01-17 12:11:31 TP0] Prefill batch, #new-seq: 1, #new-token: 6528, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:11:40 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.05, #queue-req: 0,
[2026-01-17 12:11:40] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [01:49<00:48,  6.92s/it][2026-01-17 12:11:43 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:11:51] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [02:00<00:49,  8.26s/it][2026-01-17 12:11:54 TP0] Prefill batch, #new-seq: 1, #new-token: 5760, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:12:01] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [02:10<00:43,  8.78s/it][2026-01-17 12:12:03 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:12:06 TP0] Decode batch, #running-req: 1, #token: 2560, token usage: 0.01, npu graph: False, gen throughput (token/s): 1.56, #queue-req: 0,
[2026-01-17 12:12:06] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [02:15<00:30,  7.51s/it][2026-01-17 12:12:08 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:12:12] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [02:21<00:21,  7.14s/it][2026-01-17 12:12:14 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:12:19 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.08, #queue-req: 0,
[2026-01-17 12:12:19] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [02:27<00:13,  6.93s/it][2026-01-17 12:12:20 TP0] Prefill batch, #new-seq: 1, #new-token: 2688, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:12:22] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [02:30<00:05,  5.76s/it][2026-01-17 12:12:25 TP0] Prefill batch, #new-seq: 1, #new-token: 6912, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:12:35] INFO:     127.0.0.1:36404 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [02:44<00:00,  8.07s/it]
Model Responding: 100%|██████████| 50/50 [02:44<00:00,  3.29s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 2362.99it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.3}, 'Accounting': {'num': 30, 'acc': 0.3}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.3}, 'Agriculture': {'num': 20, 'acc': 0.3}, 'Overall': {'num': 50, 'acc': 0.3}}
[32m2026-01-17 12:12:35.651[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 12:12:35.658[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  |  0.3|±  |   N/A|

/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 41892 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 256.464s

OK
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.2, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffc9dcd8cc0>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffc9dcd9bc0>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffc9dcdac00>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffc9dcdbe20>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260113_212507', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 22423.166117323, 'end_time': 22509.248523342, 'total_evaluation_time_seconds': '86.08240601900252'}
Model /root/.cache/modelscope/hub/models/XiaomiMiMo/MiMo-VL-7B-RL achieved accuracy: 0.2000
Cleaning up process 41892
.
.
End (7/23):
filename='ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py', elapsed=267, estimated_time=400
.
.

.
.
Begin (8/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_janus_pro_1b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:12:59] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B', tokenizer_path='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=925490835, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-17 12:13:01] Inferred chat template from model path: janus-pro
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:13:09 TP3] Init torch distributed begin.
[2026-01-17 12:13:09 TP0] Init torch distributed begin.
[2026-01-17 12:13:09 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:13:09 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 12:13:11 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:13:11 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:13:11 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:13:11 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:13:11 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 12:13:11 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:13:11 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:13:11 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:13:11 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:13:11 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 12:13:11 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:13:11 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:13:11 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:13:12 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:13:12 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 12:13:12 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:13:12 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:13:12 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 12:13:12 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 12:13:12 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:13:12 TP1] Using sdpa as multimodal attention backend.

Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[2026-01-17 12:13:22 TP1] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.60 GB, mem usage=1.54 GB.

Loading pt checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.63s/it]

Loading pt checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.63s/it]

[2026-01-17 12:13:22 TP3] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.60 GB, mem usage=1.54 GB.
[2026-01-17 12:13:22 TP2] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.32 GB, mem usage=1.54 GB.
[2026-01-17 12:13:22 TP0] Load weight end. type=MultiModalityCausalLM, dtype=torch.bfloat16, avail mem=59.27 GB, mem usage=1.54 GB.
[2026-01-17 12:13:22 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 12:13:22 TP0] The available memory for KV cache is 19.72 GB.
[2026-01-17 12:13:22 TP3] The available memory for KV cache is 19.72 GB.
[2026-01-17 12:13:22 TP2] The available memory for KV cache is 19.72 GB.
[2026-01-17 12:13:22 TP1] The available memory for KV cache is 19.72 GB.
[2026-01-17 12:13:22 TP1] KV Cache is allocated. #tokens: 430720, K size: 9.86 GB, V size: 9.86 GB
[2026-01-17 12:13:22 TP3] KV Cache is allocated. #tokens: 430720, K size: 9.86 GB, V size: 9.86 GB
[2026-01-17 12:13:22 TP1] Memory pool end. avail mem=39.60 GB
[2026-01-17 12:13:22 TP0] KV Cache is allocated. #tokens: 430720, K size: 9.86 GB, V size: 9.86 GB
[2026-01-17 12:13:22 TP3] Memory pool end. avail mem=39.60 GB
[2026-01-17 12:13:22 TP2] KV Cache is allocated. #tokens: 430720, K size: 9.86 GB, V size: 9.86 GB
[2026-01-17 12:13:22 TP0] Memory pool end. avail mem=39.27 GB
[2026-01-17 12:13:22 TP2] Memory pool end. avail mem=39.32 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 12:13:23 TP0] max_total_num_tokens=430720, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=16384, available_gpu_mem=39.27 GB
[2026-01-17 12:13:24] INFO:     Started server process [48857]
[2026-01-17 12:13:24] INFO:     Waiting for application startup.
[2026-01-17 12:13:24] INFO:     Application startup complete.
[2026-01-17 12:13:24] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 12:13:25] INFO:     127.0.0.1:59830 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 12:13:25 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:13:29] INFO:     127.0.0.1:59354 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:13:39] INFO:     127.0.0.1:38528 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:13:44] INFO:     127.0.0.1:59844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 12:13:44] The server is fired up and ready to roll!
[2026-01-17 12:13:49 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:13:50] INFO:     127.0.0.1:40408 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 12:13:58.740[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 12:14:00.817[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 12:14:01.606[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 12:14:01.607[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 12:14:01.608[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 12:14:01.610[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 12:14:10.369[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9062.89it/s]
[32m2026-01-17 12:14:10.376[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 12:14:11 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:11] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:01<00:55,  1.13s/it][2026-01-17 12:14:11 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:12 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.63, #queue-req: 0,
[2026-01-17 12:14:12] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:02<00:48,  1.02s/it][2026-01-17 12:14:12 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:13] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:02<00:45,  1.02it/s][2026-01-17 12:14:13 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:14] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:03<00:44,  1.04it/s][2026-01-17 12:14:14 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:15 TP0] Decode batch, #running-req: 1, #token: 896, token usage: 0.00, npu graph: False, gen throughput (token/s): 15.41, #queue-req: 0,
[2026-01-17 12:14:15] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:04<00:43,  1.05it/s][2026-01-17 12:14:15 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:16] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:05<00:41,  1.05it/s][2026-01-17 12:14:16 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:17 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 19.24, #queue-req: 0,
[2026-01-17 12:14:17] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:06<00:40,  1.06it/s][2026-01-17 12:14:17 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:18] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:07<00:39,  1.07it/s][2026-01-17 12:14:18 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:18] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:08<00:37,  1.08it/s][2026-01-17 12:14:19 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:19 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 15.77, #queue-req: 0,
[2026-01-17 12:14:19] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:09<00:36,  1.08it/s][2026-01-17 12:14:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:20] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:10<00:35,  1.08it/s][2026-01-17 12:14:21 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:21 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 19.12, #queue-req: 0,
[2026-01-17 12:14:21] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:11<00:35,  1.07it/s][2026-01-17 12:14:22 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:22] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:12<00:34,  1.07it/s][2026-01-17 12:14:23 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:23] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:13<00:34,  1.06it/s][2026-01-17 12:14:24 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:24 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.98, #queue-req: 0,
[2026-01-17 12:14:24] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:14<00:33,  1.05it/s][2026-01-17 12:14:25 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:25] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:15<00:32,  1.05it/s][2026-01-17 12:14:26 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:26 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 19.16, #queue-req: 0,
[2026-01-17 12:14:26] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:16<00:31,  1.06it/s][2026-01-17 12:14:26 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:27] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:17<00:30,  1.05it/s][2026-01-17 12:14:27 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:28] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:18<00:29,  1.05it/s][2026-01-17 12:14:28 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:30 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 9.63, #queue-req: 0,
[2026-01-17 12:14:30] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:20<00:42,  1.40s/it][2026-01-17 12:14:31 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:31] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:21<00:36,  1.26s/it][2026-01-17 12:14:32 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:32 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 19.39, #queue-req: 0,
[2026-01-17 12:14:32] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:22<00:32,  1.16s/it][2026-01-17 12:14:33 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:33] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:23<00:29,  1.09s/it][2026-01-17 12:14:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:34] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:24<00:27,  1.04s/it][2026-01-17 12:14:35 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:35 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 15.54, #queue-req: 0,
[2026-01-17 12:14:35] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:25<00:25,  1.01s/it][2026-01-17 12:14:35 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:36] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:26<00:24,  1.00s/it][2026-01-17 12:14:36 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:37 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 18.31, #queue-req: 0,
[2026-01-17 12:14:37] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:27<00:22,  1.00it/s][2026-01-17 12:14:37 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:38] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:28<00:22,  1.00s/it][2026-01-17 12:14:38 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:39] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:29<00:20,  1.00it/s][2026-01-17 12:14:39 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:40 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.59, #queue-req: 0,
[2026-01-17 12:14:40] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:30<00:19,  1.00it/s][2026-01-17 12:14:43 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:43] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:33<00:32,  1.69s/it][2026-01-17 12:14:44 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:45 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 7.92, #queue-req: 0,
[2026-01-17 12:14:45] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:34<00:29,  1.63s/it][2026-01-17 12:14:47 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:47] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:37<00:31,  1.88s/it][2026-01-17 12:14:51 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:51] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:40<00:37,  2.37s/it][2026-01-17 12:14:51 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:52] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:41<00:28,  1.93s/it][2026-01-17 12:14:52 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:52 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 5.22, #queue-req: 0,
[2026-01-17 12:14:53] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:42<00:22,  1.64s/it][2026-01-17 12:14:53 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:54] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:43<00:18,  1.45s/it][2026-01-17 12:14:56 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:57] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:46<00:22,  1.91s/it][2026-01-17 12:14:59 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:14:59 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 5.84, #queue-req: 0,
[2026-01-17 12:15:00] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [00:49<00:24,  2.22s/it][2026-01-17 12:15:00 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:15:01] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [00:50<00:18,  1.84s/it][2026-01-17 12:15:02 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:15:03 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.74, #queue-req: 0,
[2026-01-17 12:15:03] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [00:52<00:17,  1.96s/it][2026-01-17 12:15:03 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:15:04] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [00:53<00:13,  1.71s/it][2026-01-17 12:15:06 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:15:07] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [00:57<00:14,  2.11s/it][2026-01-17 12:15:10 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:15:10 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 5.59, #queue-req: 0,
[2026-01-17 12:15:10] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:00<00:14,  2.44s/it][2026-01-17 12:15:13 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:15:13] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [01:03<00:13,  2.63s/it][2026-01-17 12:15:14 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:15:15 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 7.93, #queue-req: 0,
[2026-01-17 12:15:15] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:05<00:09,  2.37s/it][2026-01-17 12:15:17 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:15:17] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:07<00:06,  2.32s/it][2026-01-17 12:15:19 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:15:20] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [01:09<00:04,  2.39s/it][2026-01-17 12:15:21 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:15:21 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 6.39, #queue-req: 0,
[2026-01-17 12:15:21] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [01:11<00:02,  2.16s/it][2026-01-17 12:15:24 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:15:24] INFO:     127.0.0.1:34146 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [01:14<00:00,  2.40s/it]
Model Responding: 100%|██████████| 50/50 [01:14<00:00,  1.49s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 3505.18it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.2}, 'Accounting': {'num': 30, 'acc': 0.2}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.2}, 'Agriculture': {'num': 20, 'acc': 0.2}, 'Overall': {'num': 50, 'acc': 0.2}}
[32m2026-01-17 12:15:24.845[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 12:15:24.850[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  |  0.2|±  |   N/A|

/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 48857 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 158.783s

OK
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.2, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffc9dcd8cc0>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffc9dcd9bc0>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffc9dcdac00>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffc9dcdbe20>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260113_212507', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 22423.166117323, 'end_time': 22509.248523342, 'total_evaluation_time_seconds': '86.08240601900252'}
Model /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B achieved accuracy: 0.2000
Cleaning up process 48857
.
.
End (8/23):
filename='ascend/vlm_models/test_ascend_janus_pro_1b.py', elapsed=169, estimated_time=400
.
.

.
.
Begin (9/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_minicpm_v_2_6.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:15:48] INFO configuration_minicpm.py:92: vision_config is None, using default vision config
[2026-01-17 12:15:48] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6', tokenizer_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=521936561, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:15:50] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-17 12:15:50] Inferred chat template from model path: minicpmv
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2026-01-17 12:15:58 TP3] vision_config is None, using default vision config
[2026-01-17 12:15:58 TP3] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:15:59 TP0] vision_config is None, using default vision config
[2026-01-17 12:15:59 TP0] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:15:59 TP3] Init torch distributed begin.
[2026-01-17 12:15:59 TP1] vision_config is None, using default vision config
[2026-01-17 12:15:59 TP2] vision_config is None, using default vision config
[2026-01-17 12:15:59 TP2] vision_config is None, using default vision config
[2026-01-17 12:15:59 TP1] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:15:59 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:16:00 TP2] Init torch distributed begin.
[2026-01-17 12:16:00 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 12:16:01 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:16:01 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:16:01 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:16:01 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:16:01 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 12:16:02 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:16:02 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:16:02 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:16:02 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:16:02 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:16:02 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:16:02 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:16:02 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 12:16:02 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:16:02 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 12:16:02 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:16:02 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 12:16:02 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:16:02 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 12:16:02 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:16:02 TP0] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:15,  5.08s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:10<00:10,  5.08s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:12<00:03,  3.77s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:17<00:00,  4.36s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:17<00:00,  4.41s/it]

[2026-01-17 12:16:21 TP3] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.99 GB, mem usage=4.15 GB.
[2026-01-17 12:16:21 TP0] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.66 GB, mem usage=4.15 GB.
[2026-01-17 12:16:21 TP2] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.71 GB, mem usage=4.15 GB.
[2026-01-17 12:16:21 TP1] Load weight end. type=MiniCPMV2_6, dtype=torch.bfloat16, avail mem=56.99 GB, mem usage=4.15 GB.
[2026-01-17 12:16:21 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 12:16:21 TP0] The available memory for KV cache is 17.13 GB.
[2026-01-17 12:16:21 TP3] The available memory for KV cache is 17.13 GB.
[2026-01-17 12:16:21 TP1] The available memory for KV cache is 17.13 GB.
[2026-01-17 12:16:21 TP2] The available memory for KV cache is 17.13 GB.
[2026-01-17 12:16:21 TP0] KV Cache is allocated. #tokens: 1283072, K size: 8.57 GB, V size: 8.57 GB
[2026-01-17 12:16:21 TP3] KV Cache is allocated. #tokens: 1283072, K size: 8.57 GB, V size: 8.57 GB
[2026-01-17 12:16:21 TP0] Memory pool end. avail mem=39.02 GB
[2026-01-17 12:16:21 TP1] KV Cache is allocated. #tokens: 1283072, K size: 8.57 GB, V size: 8.57 GB
[2026-01-17 12:16:21 TP3] Memory pool end. avail mem=39.35 GB
[2026-01-17 12:16:21 TP1] Memory pool end. avail mem=39.35 GB
[2026-01-17 12:16:21 TP2] KV Cache is allocated. #tokens: 1283072, K size: 8.57 GB, V size: 8.57 GB
[2026-01-17 12:16:21 TP2] Memory pool end. avail mem=39.07 GB
[2026-01-17 12:16:21 TP1] vision_config is None, using default vision config
[2026-01-17 12:16:21 TP0] vision_config is None, using default vision config
[2026-01-17 12:16:22 TP3] vision_config is None, using default vision config
[2026-01-17 12:16:22 TP2] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 12:16:22 TP0] max_total_num_tokens=1283072, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=39.02 GB
[2026-01-17 12:16:23] INFO:     Started server process [55877]
[2026-01-17 12:16:23] INFO:     Waiting for application startup.
[2026-01-17 12:16:23] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 12:16:23] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 12:16:23] INFO:     Application startup complete.
[2026-01-17 12:16:23] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 12:16:24] INFO:     127.0.0.1:45412 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 12:16:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:16:28] INFO:     127.0.0.1:52026 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:16:38] INFO:     127.0.0.1:34938 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:16:40] INFO:     127.0.0.1:45424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 12:16:40] The server is fired up and ready to roll!
[2026-01-17 12:16:48 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:16:49] INFO:     127.0.0.1:41250 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 12:16:57.846[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 12:16:59.947[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 12:17:00.715[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 12:17:00.716[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 12:17:00.716[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 12:17:00.719[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 12:17:10.150[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9214.61it/s]
[32m2026-01-17 12:17:10.156[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 12:17:10 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:10] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:00<00:18,  2.63it/s][2026-01-17 12:17:10 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:10] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:00<00:12,  3.78it/s][2026-01-17 12:17:10 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:10] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:00<00:10,  4.51it/s][2026-01-17 12:17:11 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:11] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:01<00:15,  2.91it/s][2026-01-17 12:17:11 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:11] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:01<00:16,  2.70it/s][2026-01-17 12:17:11 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:12] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:02<00:15,  2.82it/s][2026-01-17 12:17:12 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:12] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:02<00:12,  3.43it/s][2026-01-17 12:17:12 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:12] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:02<00:10,  3.91it/s][2026-01-17 12:17:12 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:12] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:02<00:09,  4.35it/s][2026-01-17 12:17:12 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:12] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:02<00:08,  4.71it/s][2026-01-17 12:17:12 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:13] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:02<00:07,  5.00it/s][2026-01-17 12:17:13 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:13] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:03<00:07,  5.20it/s][2026-01-17 12:17:13 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:13] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:03<00:06,  5.38it/s][2026-01-17 12:17:13 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:13] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:03<00:08,  4.35it/s][2026-01-17 12:17:13 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:14 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.53, #queue-req: 0,
[2026-01-17 12:17:14] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:03<00:09,  3.76it/s][2026-01-17 12:17:14 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:14] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:04<00:08,  4.16it/s][2026-01-17 12:17:14 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:14] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:04<00:07,  4.46it/s][2026-01-17 12:17:14 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:14] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:04<00:06,  4.79it/s][2026-01-17 12:17:14 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:14] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:04<00:05,  5.39it/s][2026-01-17 12:17:14 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:14] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:04<00:06,  4.76it/s][2026-01-17 12:17:15 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:15] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:05<00:05,  5.02it/s][2026-01-17 12:17:15 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:15] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:05<00:05,  5.21it/s][2026-01-17 12:17:15 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:15] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:05<00:05,  5.30it/s][2026-01-17 12:17:15 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:15] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:05<00:04,  5.43it/s][2026-01-17 12:17:15 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:15] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:05<00:04,  5.52it/s][2026-01-17 12:17:16 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:16] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:06<00:05,  4.22it/s][2026-01-17 12:17:16 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:16] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:06<00:05,  3.99it/s][2026-01-17 12:17:16 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:16] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:06<00:05,  3.95it/s][2026-01-17 12:17:16 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:16] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:06<00:04,  4.36it/s][2026-01-17 12:17:16 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:17] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:06<00:04,  4.72it/s][2026-01-17 12:17:19 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:20] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:09<00:19,  1.03s/it][2026-01-17 12:17:20 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:21] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:10<00:18,  1.02s/it][2026-01-17 12:17:22 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:23] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:12<00:22,  1.34s/it][2026-01-17 12:17:26 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:26] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:16<00:31,  1.97s/it][2026-01-17 12:17:26 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:27 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.09, #queue-req: 0,
[2026-01-17 12:17:27] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:16<00:22,  1.51s/it][2026-01-17 12:17:27 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:27] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:17<00:15,  1.11s/it][2026-01-17 12:17:27 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:27] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:17<00:12,  1.08it/s][2026-01-17 12:17:30 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:30] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:20<00:17,  1.44s/it][2026-01-17 12:17:32 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:32] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [00:22<00:19,  1.78s/it][2026-01-17 12:17:32 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:33] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [00:22<00:13,  1.30s/it][2026-01-17 12:17:34 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:34] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [00:24<00:13,  1.46s/it][2026-01-17 12:17:35 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:35] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [00:25<00:09,  1.17s/it][2026-01-17 12:17:37 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:38] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [00:28<00:11,  1.65s/it][2026-01-17 12:17:40 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:41] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [00:30<00:12,  2.02s/it][2026-01-17 12:17:43 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:43] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [00:33<00:11,  2.23s/it][2026-01-17 12:17:44 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:45] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [00:34<00:07,  1.97s/it][2026-01-17 12:17:46 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:47] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [00:36<00:05,  1.94s/it][2026-01-17 12:17:48 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:49] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [00:39<00:04,  2.03s/it][2026-01-17 12:17:50 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:50] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [00:40<00:01,  1.81s/it][2026-01-17 12:17:52 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:17:53] INFO:     127.0.0.1:55046 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [00:42<00:00,  2.04s/it]
Model Responding: 100%|██████████| 50/50 [00:42<00:00,  1.16it/s]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 4157.47it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.33333}, 'Accounting': {'num': 30, 'acc': 0.33333}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.25}, 'Agriculture': {'num': 20, 'acc': 0.25}, 'Overall': {'num': 50, 'acc': 0.3}}
[32m2026-01-17 12:17:53.158[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 12:17:53.165[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  |  0.3|±  |   N/A|

/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 55877 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 137.020s

OK
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6 --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.2, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffc9dcd8cc0>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffc9dcd9bc0>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffc9dcdac00>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffc9dcdbe20>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260113_212507', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 22423.166117323, 'end_time': 22509.248523342, 'total_evaluation_time_seconds': '86.08240601900252'}
Model /root/.cache/modelscope/hub/models/openbmb/MiniCPM-V-2_6 achieved accuracy: 0.2000
Cleaning up process 55877
.
.
End (9/23):
filename='ascend/vlm_models/test_ascend_minicpm_v_2_6.py', elapsed=147, estimated_time=400
.
.

.
.
Begin (10/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_minicpm_o_2_6.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:18:15] INFO configuration_minicpm.py:187: vision_config is None, using default vision config
[2026-01-17 12:18:16] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6', tokenizer_path='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=816768537, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:18:17] vision_config is None, using default vision config
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-17 12:18:18] Inferred chat template from model path: minicpmo
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:18:25 TP3] vision_config is None, using default vision config
[2026-01-17 12:18:25 TP1] vision_config is None, using default vision config
[2026-01-17 12:18:25 TP3] vision_config is None, using default vision config
[2026-01-17 12:18:25 TP1] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:18:26 TP0] vision_config is None, using default vision config
[2026-01-17 12:18:26 TP0] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-17 12:18:26 TP2] vision_config is None, using default vision config
[2026-01-17 12:18:26 TP2] vision_config is None, using default vision config
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:18:26 TP3] Init torch distributed begin.
[2026-01-17 12:18:26 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:18:27 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:18:27 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 12:18:28 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:18:28 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:18:28 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:18:28 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:18:28 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 12:18:29 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:18:29 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:18:29 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:18:29 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:18:29 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 12:18:29 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:18:29 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:18:29 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:18:29 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:18:29 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 12:18:29 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:18:29 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 12:18:29 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:18:29 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 12:18:29 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:18:29 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 12:18:30 TP0] TTS is disabled for now

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[2026-01-17 12:18:30 TP2] TTS is disabled for now
[2026-01-17 12:18:30 TP1] TTS is disabled for now
[2026-01-17 12:18:30 TP3] TTS is disabled for now

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:15,  5.21s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:10<00:10,  5.42s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:15<00:05,  5.09s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:21<00:00,  5.45s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:21<00:00,  5.37s/it]

[2026-01-17 12:18:51 TP0] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=51.71 GB, mem usage=9.10 GB.
[2026-01-17 12:18:52 TP3] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=52.04 GB, mem usage=9.10 GB.
[2026-01-17 12:18:52 TP1] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=52.04 GB, mem usage=9.10 GB.
[2026-01-17 12:18:52 TP2] Load weight end. type=MiniCPMO, dtype=torch.bfloat16, avail mem=51.76 GB, mem usage=9.10 GB.
[2026-01-17 12:18:52 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 12:18:52 TP0] The available memory for KV cache is 12.18 GB.
[2026-01-17 12:18:52 TP3] The available memory for KV cache is 12.18 GB.
[2026-01-17 12:18:52 TP1] The available memory for KV cache is 12.18 GB.
[2026-01-17 12:18:52 TP2] The available memory for KV cache is 12.18 GB.
[2026-01-17 12:18:52 TP3] KV Cache is allocated. #tokens: 912256, K size: 6.09 GB, V size: 6.09 GB
[2026-01-17 12:18:52 TP2] KV Cache is allocated. #tokens: 912256, K size: 6.09 GB, V size: 6.09 GB
[2026-01-17 12:18:52 TP1] KV Cache is allocated. #tokens: 912256, K size: 6.09 GB, V size: 6.09 GB
[2026-01-17 12:18:52 TP0] KV Cache is allocated. #tokens: 912256, K size: 6.09 GB, V size: 6.09 GB
[2026-01-17 12:18:52 TP3] Memory pool end. avail mem=39.35 GB
[2026-01-17 12:18:52 TP2] Memory pool end. avail mem=39.07 GB
[2026-01-17 12:18:52 TP1] Memory pool end. avail mem=39.35 GB
[2026-01-17 12:18:52 TP0] Memory pool end. avail mem=39.02 GB
[2026-01-17 12:18:52 TP3] vision_config is None, using default vision config
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
[2026-01-17 12:18:52 TP1] vision_config is None, using default vision config
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
[2026-01-17 12:18:52 TP2] vision_config is None, using default vision config
[2026-01-17 12:18:52 TP0] vision_config is None, using default vision config
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
<unknown>:313: DeprecationWarning: invalid escape sequence '\('
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 12:18:53 TP0] max_total_num_tokens=912256, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=39.02 GB
[2026-01-17 12:18:54] INFO:     Started server process [62032]
[2026-01-17 12:18:54] INFO:     Waiting for application startup.
[2026-01-17 12:18:54] INFO:     Application startup complete.
[2026-01-17 12:18:54] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 12:18:55] INFO:     127.0.0.1:51104 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 12:18:55 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:18:56] INFO:     127.0.0.1:51130 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:19:06] INFO:     127.0.0.1:41550 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:19:12] INFO:     127.0.0.1:51118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 12:19:12] The server is fired up and ready to roll!
[2026-01-17 12:19:16 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:17] INFO:     127.0.0.1:53764 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 12:19:24.938[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 12:19:27.013[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 12:19:27.769[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 12:19:27.769[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 12:19:27.770[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 12:19:27.772[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 12:19:37.140[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9238.15it/s]
[32m2026-01-17 12:19:37.146[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 12:19:37 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:37] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:00<00:18,  2.65it/s][2026-01-17 12:19:37 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:37] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:00<00:12,  3.79it/s][2026-01-17 12:19:37 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:37] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:00<00:10,  4.49it/s][2026-01-17 12:19:38 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:38] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:01<00:15,  2.90it/s][2026-01-17 12:19:38 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:39] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:02<00:23,  1.95it/s][2026-01-17 12:19:39 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:39 TP0] Decode batch, #running-req: 1, #token: 512, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.54, #queue-req: 0,
[2026-01-17 12:19:39] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:02<00:25,  1.73it/s][2026-01-17 12:19:39 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:40] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:02<00:19,  2.23it/s][2026-01-17 12:19:40 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:40] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:03<00:15,  2.76it/s][2026-01-17 12:19:40 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:40] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:03<00:12,  3.21it/s][2026-01-17 12:19:40 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:40] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:03<00:11,  3.62it/s][2026-01-17 12:19:40 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:40] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:03<00:09,  3.98it/s][2026-01-17 12:19:40 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:41] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:03<00:08,  4.35it/s][2026-01-17 12:19:41 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:41] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:04<00:07,  4.67it/s][2026-01-17 12:19:41 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:41] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:04<00:09,  3.87it/s][2026-01-17 12:19:41 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:41] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:04<00:09,  3.70it/s][2026-01-17 12:19:41 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:42] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:04<00:08,  4.13it/s][2026-01-17 12:19:42 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:42 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.24, #queue-req: 0,
[2026-01-17 12:19:42] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:05<00:11,  2.95it/s][2026-01-17 12:19:42 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:42] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:05<00:09,  3.46it/s][2026-01-17 12:19:42 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:42] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:05<00:07,  4.13it/s][2026-01-17 12:19:42 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:43] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:06<00:07,  3.93it/s][2026-01-17 12:19:43 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:43] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:06<00:06,  4.29it/s][2026-01-17 12:19:43 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:43] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:06<00:09,  3.04it/s][2026-01-17 12:19:43 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:44] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:06<00:07,  3.57it/s][2026-01-17 12:19:44 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:44] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:07<00:06,  4.04it/s][2026-01-17 12:19:44 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:44] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:07<00:05,  4.33it/s][2026-01-17 12:19:44 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:44] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:07<00:07,  3.40it/s][2026-01-17 12:19:45 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:45 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.66, #queue-req: 0,
[2026-01-17 12:19:45] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:08<00:09,  2.46it/s][2026-01-17 12:19:45 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:45] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:08<00:08,  2.74it/s][2026-01-17 12:19:45 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:46] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:08<00:06,  3.27it/s][2026-01-17 12:19:46 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:46] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:09<00:05,  3.79it/s][2026-01-17 12:19:48 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:49] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:12<00:21,  1.12s/it][2026-01-17 12:19:50 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:50] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:13<00:19,  1.10s/it][2026-01-17 12:19:52 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:52] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:15<00:24,  1.43s/it][2026-01-17 12:19:55 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:56] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:18<00:32,  2.05s/it][2026-01-17 12:19:56 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:56] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:19<00:23,  1.58s/it][2026-01-17 12:19:56 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:56] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:19<00:16,  1.16s/it][2026-01-17 12:19:57 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:57] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:20<00:12,  1.03it/s][2026-01-17 12:19:59 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:19:59] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:22<00:17,  1.49s/it][2026-01-17 12:20:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:02] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [00:25<00:20,  1.83s/it][2026-01-17 12:20:02 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:02] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [00:25<00:13,  1.34s/it][2026-01-17 12:20:04 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:04 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 2.06, #queue-req: 0,
[2026-01-17 12:20:04] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [00:27<00:13,  1.52s/it][2026-01-17 12:20:04 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:05] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [00:28<00:09,  1.22s/it][2026-01-17 12:20:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:08] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [00:31<00:12,  1.81s/it][2026-01-17 12:20:10 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:11] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [00:34<00:12,  2.13s/it][2026-01-17 12:20:13 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:14] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [00:36<00:11,  2.33s/it][2026-01-17 12:20:15 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:15] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [00:38<00:08,  2.07s/it][2026-01-17 12:20:17 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:17] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [00:40<00:06,  2.04s/it][2026-01-17 12:20:19 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:19] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [00:42<00:04,  2.10s/it][2026-01-17 12:20:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:21] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [00:44<00:01,  1.88s/it][2026-01-17 12:20:23 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:20:23] INFO:     127.0.0.1:41942 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [00:46<00:00,  2.11s/it]
Model Responding: 100%|██████████| 50/50 [00:46<00:00,  1.07it/s]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 4505.55it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.26667}, 'Accounting': {'num': 30, 'acc': 0.26667}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.35}, 'Agriculture': {'num': 20, 'acc': 0.35}, 'Overall': {'num': 50, 'acc': 0.3}}
[32m2026-01-17 12:20:23.818[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 12:20:23.825[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  |  0.3|±  |   N/A|

/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 62032 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 140.410s

OK
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6 --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.2, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffc9dcd8cc0>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffc9dcd9bc0>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffc9dcdac00>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffc9dcdbe20>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260113_212507', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 22423.166117323, 'end_time': 22509.248523342, 'total_evaluation_time_seconds': '86.08240601900252'}
Model /root/.cache/modelscope/hub/models/openbmb/MiniCPM-o-2_6 achieved accuracy: 0.2000
Cleaning up process 62032
.
.
End (10/23):
filename='ascend/vlm_models/test_ascend_minicpm_o_2_6.py', elapsed=150, estimated_time=400
.
.

.
.
Begin (11/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:20:46] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=246225593, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:20:48] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:20:56 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:20:57 TP1] Init torch distributed begin.
[2026-01-17 12:20:57 TP3] Init torch distributed begin.
[2026-01-17 12:20:57 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 12:20:58 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:20:58 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:20:58 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:20:58 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:20:58 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 12:20:59 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:20:59 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:20:59 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:20:59 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:20:59 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:20:59 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:20:59 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:20:59 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 12:20:59 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:20:59 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 12:20:59 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:20:59 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 12:20:59 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:20:59 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 12:20:59 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:20:59 TP0] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:05<00:05,  5.02s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.78s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.82s/it]

[2026-01-17 12:21:09 TP1] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=59.26 GB, mem usage=1.88 GB.
[2026-01-17 12:21:09 TP0] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=58.93 GB, mem usage=1.88 GB.
[2026-01-17 12:21:09 TP3] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=59.26 GB, mem usage=1.88 GB.
[2026-01-17 12:21:09 TP2] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=58.98 GB, mem usage=1.88 GB.
[2026-01-17 12:21:09 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 12:21:09 TP0] The available memory for KV cache is 19.37 GB.
[2026-01-17 12:21:09 TP2] The available memory for KV cache is 19.37 GB.
[2026-01-17 12:21:09 TP3] The available memory for KV cache is 19.37 GB.
[2026-01-17 12:21:09 TP1] The available memory for KV cache is 19.37 GB.
[2026-01-17 12:21:10 TP0] KV Cache is allocated. #tokens: 1128448, K size: 9.69 GB, V size: 9.69 GB
[2026-01-17 12:21:10 TP0] Memory pool end. avail mem=37.57 GB
[2026-01-17 12:21:10 TP3] KV Cache is allocated. #tokens: 1128448, K size: 9.69 GB, V size: 9.69 GB
[2026-01-17 12:21:10 TP1] KV Cache is allocated. #tokens: 1128448, K size: 9.69 GB, V size: 9.69 GB
[2026-01-17 12:21:10 TP3] Memory pool end. avail mem=37.90 GB
[2026-01-17 12:21:10 TP2] KV Cache is allocated. #tokens: 1128448, K size: 9.69 GB, V size: 9.69 GB
[2026-01-17 12:21:10 TP1] Memory pool end. avail mem=37.90 GB
[2026-01-17 12:21:10 TP2] Memory pool end. avail mem=37.62 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 12:21:11 TP0] max_total_num_tokens=1128448, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=128000, available_gpu_mem=37.57 GB
[2026-01-17 12:21:12] INFO:     Started server process [68187]
[2026-01-17 12:21:12] INFO:     Waiting for application startup.
[2026-01-17 12:21:12] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 1e-06, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 12:21:12] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 1e-06, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 12:21:12] INFO:     Application startup complete.
[2026-01-17 12:21:12] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 12:21:13] INFO:     127.0.0.1:37746 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 12:21:13 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:21:16] INFO:     127.0.0.1:37774 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:21:26] INFO:     127.0.0.1:60052 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:21:29] INFO:     127.0.0.1:37760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 12:21:29] The server is fired up and ready to roll!
[2026-01-17 12:21:36 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:21:37] INFO:     127.0.0.1:38412 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 12:21:45.430[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 12:21:47.512[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 12:21:48.279[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 12:21:48.279[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 12:21:48.280[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 12:21:48.283[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 12:21:59.242[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9177.10it/s]
[32m2026-01-17 12:21:59.248[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 12:21:59 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:21:59] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:00<00:27,  1.77it/s][2026-01-17 12:21:59 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:00] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:00<00:17,  2.75it/s][2026-01-17 12:22:00 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:00] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:00<00:13,  3.49it/s][2026-01-17 12:22:00 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:00] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:01<00:13,  3.44it/s][2026-01-17 12:22:00 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:00] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:01<00:12,  3.60it/s][2026-01-17 12:22:00 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:01] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:01<00:12,  3.63it/s][2026-01-17 12:22:01 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:01] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:02<00:10,  4.01it/s][2026-01-17 12:22:01 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:01] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:02<00:10,  4.09it/s][2026-01-17 12:22:01 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:01] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:02<00:09,  4.13it/s][2026-01-17 12:22:01 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:01] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:02<00:09,  4.29it/s][2026-01-17 12:22:01 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:02] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:02<00:08,  4.40it/s][2026-01-17 12:22:02 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:02] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:03<00:08,  4.49it/s][2026-01-17 12:22:02 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:02] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:03<00:08,  4.26it/s][2026-01-17 12:22:02 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:02 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.60, #queue-req: 0,
[2026-01-17 12:22:02] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:03<00:08,  4.09it/s][2026-01-17 12:22:02 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:03] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:03<00:08,  3.89it/s][2026-01-17 12:22:03 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:03] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:04<00:08,  3.89it/s][2026-01-17 12:22:03 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:03] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:04<00:07,  4.14it/s][2026-01-17 12:22:03 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:03] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:04<00:07,  4.29it/s][2026-01-17 12:22:03 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:04] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:04<00:06,  4.68it/s][2026-01-17 12:22:04 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:04] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:04<00:06,  4.79it/s][2026-01-17 12:22:04 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:04] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:05<00:05,  4.83it/s][2026-01-17 12:22:04 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:04] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:05<00:06,  4.66it/s][2026-01-17 12:22:04 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:04] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:05<00:05,  4.85it/s][2026-01-17 12:22:04 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:05] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:05<00:05,  4.92it/s][2026-01-17 12:22:05 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:05] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:05<00:04,  5.05it/s][2026-01-17 12:22:05 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:05] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:06<00:05,  4.24it/s][2026-01-17 12:22:05 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:05] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:06<00:05,  3.99it/s][2026-01-17 12:22:05 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:06] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:06<00:05,  3.90it/s][2026-01-17 12:22:06 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:06] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:07<00:04,  4.24it/s][2026-01-17 12:22:06 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:06] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:07<00:04,  4.20it/s][2026-01-17 12:22:09 TP0] Prefill batch, #new-seq: 1, #new-token: 7040, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:20] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:21<01:22,  4.32s/it][2026-01-17 12:22:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:21 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 2.10, #queue-req: 0,
[2026-01-17 12:22:21] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:22<01:02,  3.50s/it][2026-01-17 12:22:24 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:27] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:28<01:09,  4.10s/it][2026-01-17 12:22:31 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:39] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:40<01:44,  6.53s/it][2026-01-17 12:22:39 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:40] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:40<01:10,  4.71s/it][2026-01-17 12:22:40 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:40] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:41<00:47,  3.36s/it][2026-01-17 12:22:40 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:40] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:41<00:32,  2.49s/it][2026-01-17 12:22:43 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:22:52] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:52<01:01,  5.11s/it][2026-01-17 12:22:54 TP0] Prefill batch, #new-seq: 1, #new-token: 6528, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:23:03] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [01:03<01:16,  6.93s/it][2026-01-17 12:23:03 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:23:03] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [01:04<00:49,  4.92s/it][2026-01-17 12:23:05 TP0] Prefill batch, #new-seq: 1, #new-token: 5632, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:23:12] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [01:13<00:55,  6.19s/it][2026-01-17 12:23:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:23:13] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [01:13<00:36,  4.51s/it][2026-01-17 12:23:16 TP0] Prefill batch, #new-seq: 1, #new-token: 6528, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:23:24] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [01:25<00:45,  6.51s/it][2026-01-17 12:23:27 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:23:35] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:36<00:47,  7.91s/it][2026-01-17 12:23:38 TP0] Prefill batch, #new-seq: 1, #new-token: 5760, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:23:46] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [01:47<00:43,  8.75s/it][2026-01-17 12:23:47 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:23:49] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:49<00:27,  6.99s/it][2026-01-17 12:23:51 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:23:54] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:55<00:19,  6.50s/it][2026-01-17 12:23:56 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:23:59] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [02:00<00:12,  6.16s/it][2026-01-17 12:24:00 TP0] Prefill batch, #new-seq: 1, #new-token: 2688, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:24:02] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [02:03<00:05,  5.06s/it][2026-01-17 12:24:05 TP0] Prefill batch, #new-seq: 1, #new-token: 6912, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:24:15] INFO:     127.0.0.1:40302 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [02:16<00:00,  7.44s/it]
Model Responding: 100%|██████████| 50/50 [02:16<00:00,  2.72s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 5595.84it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.5}, 'Accounting': {'num': 30, 'acc': 0.5}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.4}, 'Agriculture': {'num': 20, 'acc': 0.4}, 'Overall': {'num': 50, 'acc': 0.46}}
[32m2026-01-17 12:24:15.376[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 12:24:15.383[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  | 0.46|±  |   N/A|

/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 68187 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 222.276s

OK
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.2, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffc9dcd8cc0>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffc9dcd9bc0>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffc9dcdac00>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffc9dcdbe20>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-1B",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260113_212507', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 22423.166117323, 'end_time': 22509.248523342, 'total_evaluation_time_seconds': '86.08240601900252'}
Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-3B-Instruct achieved accuracy: 0.2000
Cleaning up process 68187
.
.
End (11/23):
filename='ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py', elapsed=233, estimated_time=400
.
.

.
.
Begin (12/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:24:39] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct', tokenizer_path='/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=1037030535, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
[2026-01-17 12:24:42] Inferred chat template from model path: phi-4-mm
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:24:50 TP0] Init torch distributed begin.
[2026-01-17 12:24:50 TP1] Init torch distributed begin.
[2026-01-17 12:24:50 TP3] Init torch distributed begin.
[2026-01-17 12:24:51 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 12:24:52 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:24:52 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:24:52 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:24:52 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:24:52 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 12:24:53 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:24:53 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:24:53 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:24:53 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:24:53 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:24:53 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:24:53 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:24:53 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:24:53 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 12:24:53 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:24:53 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 12:24:53 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 12:24:53 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:24:53 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 12:24:53 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:24:53 TP0] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.16s/it]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:08<00:04,  4.79s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:14<00:00,  5.31s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:14<00:00,  4.81s/it]

[2026-01-17 12:25:08 TP1] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=58.05 GB, mem usage=3.09 GB.
[2026-01-17 12:25:08 TP2] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=57.77 GB, mem usage=3.09 GB.
[2026-01-17 12:25:08 TP0] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=57.72 GB, mem usage=3.09 GB.
[2026-01-17 12:25:08 TP3] Load weight end. type=Phi4MMForCausalLM, dtype=torch.bfloat16, avail mem=58.05 GB, mem usage=3.09 GB.
[2026-01-17 12:25:08 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 12:25:08 TP0] The available memory for KV cache is 18.19 GB.
[2026-01-17 12:25:08 TP3] The available memory for KV cache is 18.19 GB.
[2026-01-17 12:25:08 TP1] The available memory for KV cache is 18.19 GB.
[2026-01-17 12:25:08 TP2] The available memory for KV cache is 18.19 GB.
[2026-01-17 12:25:09 TP0] KV Cache is allocated. #tokens: 596096, K size: 9.10 GB, V size: 9.10 GB
[2026-01-17 12:25:09 TP0] Memory pool end. avail mem=38.38 GB
[2026-01-17 12:25:09 TP1] KV Cache is allocated. #tokens: 596096, K size: 9.10 GB, V size: 9.10 GB
[2026-01-17 12:25:09 TP1] Memory pool end. avail mem=38.72 GB
[2026-01-17 12:25:09 TP3] KV Cache is allocated. #tokens: 596096, K size: 9.10 GB, V size: 9.10 GB
[2026-01-17 12:25:09 TP2] KV Cache is allocated. #tokens: 596096, K size: 9.10 GB, V size: 9.10 GB
[2026-01-17 12:25:09 TP3] Memory pool end. avail mem=38.71 GB
[2026-01-17 12:25:09 TP2] Memory pool end. avail mem=38.43 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 12:25:11 TP0] max_total_num_tokens=596096, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2328, context_len=131072, available_gpu_mem=38.38 GB
[2026-01-17 12:25:12] INFO:     Started server process [73982]
[2026-01-17 12:25:12] INFO:     Waiting for application startup.
[2026-01-17 12:25:12] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 12:25:12] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 12:25:12] INFO:     Application startup complete.
[2026-01-17 12:25:12] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 12:25:13] INFO:     127.0.0.1:40306 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 12:25:13 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:25:19] INFO:     127.0.0.1:39736 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:25:29] INFO:     127.0.0.1:56574 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:25:29] INFO:     127.0.0.1:40310 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 12:25:29] The server is fired up and ready to roll!
[2026-01-17 12:25:39 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:25:40] INFO:     127.0.0.1:35660 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 12:25:48.118[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 12:25:50.194[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 12:25:50.953[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 12:25:50.953[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 12:25:50.954[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 12:25:50.956[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 12:25:59.769[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9178.71it/s]
[32m2026-01-17 12:25:59.775[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 12:26:00 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:00] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:01<00:55,  1.13s/it][2026-01-17 12:26:00 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:01 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.55, #queue-req: 0,
[2026-01-17 12:26:01] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:02<00:47,  1.00it/s][2026-01-17 12:26:01 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:02] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:02<00:44,  1.04it/s][2026-01-17 12:26:02 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:03] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:03<00:36,  1.25it/s][2026-01-17 12:26:03 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:04] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:04<00:38,  1.17it/s][2026-01-17 12:26:04 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.38, #queue-req: 0,
[2026-01-17 12:26:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:05] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:05<00:38,  1.15it/s][2026-01-17 12:26:05 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:06] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:06<00:37,  1.14it/s][2026-01-17 12:26:06 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:06 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.47, #queue-req: 0,
[2026-01-17 12:26:06] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:07<00:37,  1.12it/s][2026-01-17 12:26:07 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:07] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:08<00:36,  1.11it/s][2026-01-17 12:26:07 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:08] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:09<00:36,  1.10it/s][2026-01-17 12:26:08 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:09 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.75, #queue-req: 0,
[2026-01-17 12:26:09] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:09<00:31,  1.23it/s][2026-01-17 12:26:09 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:10] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:10<00:28,  1.32it/s][2026-01-17 12:26:10 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:10] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:11<00:30,  1.23it/s][2026-01-17 12:26:11 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:11] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:11<00:26,  1.37it/s][2026-01-17 12:26:11 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.33, #queue-req: 0,
[2026-01-17 12:26:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:12] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:12<00:28,  1.22it/s][2026-01-17 12:26:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:13] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:13<00:28,  1.17it/s][2026-01-17 12:26:13 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:13] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:14<00:24,  1.36it/s][2026-01-17 12:26:13 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:14 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.89, #queue-req: 0,
[2026-01-17 12:26:14] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:15<00:25,  1.25it/s][2026-01-17 12:26:14 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:15] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:16<00:25,  1.21it/s][2026-01-17 12:26:15 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:16] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:16<00:19,  1.52it/s][2026-01-17 12:26:16 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:16 TP0] Decode batch, #running-req: 1, #token: 640, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.78, #queue-req: 0,
[2026-01-17 12:26:16] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:17<00:21,  1.35it/s][2026-01-17 12:26:17 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:17] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:18<00:22,  1.25it/s][2026-01-17 12:26:17 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:18] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:18<00:17,  1.58it/s][2026-01-17 12:26:18 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:19 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.50, #queue-req: 0,
[2026-01-17 12:26:19] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:19<00:18,  1.37it/s][2026-01-17 12:26:19 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:19] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:19<00:17,  1.47it/s][2026-01-17 12:26:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1408, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:20] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:20<00:18,  1.32it/s][2026-01-17 12:26:20 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:21] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:21<00:15,  1.44it/s][2026-01-17 12:26:21 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:21 TP0] Decode batch, #running-req: 1, #token: 896, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.08, #queue-req: 0,
[2026-01-17 12:26:22] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:22<00:17,  1.28it/s][2026-01-17 12:26:22 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:22] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:22<00:13,  1.60it/s][2026-01-17 12:26:22 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:22] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:22<00:10,  1.85it/s][2026-01-17 12:26:25 TP0] Prefill batch, #new-seq: 1, #new-token: 8192, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:26] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:26<00:28,  1.50s/it][2026-01-17 12:26:27 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:27] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:27<00:25,  1.42s/it][2026-01-17 12:26:29 TP0] Prefill batch, #new-seq: 1, #new-token: 5248, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:30] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:30<00:29,  1.75s/it][2026-01-17 12:26:33 TP0] Prefill batch, #new-seq: 1, #new-token: 7424, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:34] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:34<00:39,  2.45s/it][2026-01-17 12:26:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:34] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:35<00:28,  1.90s/it][2026-01-17 12:26:34 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:35 TP0] Decode batch, #running-req: 1, #token: 640, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.00, #queue-req: 0,
[2026-01-17 12:26:35] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:35<00:20,  1.47s/it][2026-01-17 12:26:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:36] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:36<00:16,  1.23s/it][2026-01-17 12:26:38 TP0] Prefill batch, #new-seq: 1, #new-token: 7424, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:39] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:39<00:22,  1.89s/it][2026-01-17 12:26:42 TP0] Prefill batch, #new-seq: 1, #new-token: 7424, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:42] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [00:43<00:26,  2.37s/it][2026-01-17 12:26:43 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:43] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [00:43<00:17,  1.76s/it][2026-01-17 12:26:45 TP0] Prefill batch, #new-seq: 1, #new-token: 6656, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:45] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [00:46<00:18,  2.01s/it][2026-01-17 12:26:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1408, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:46] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [00:46<00:13,  1.64s/it][2026-01-17 12:26:49 TP0] Prefill batch, #new-seq: 1, #new-token: 7424, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:50 TP0] Decode batch, #running-req: 1, #token: 7424, token usage: 0.01, npu graph: False, gen throughput (token/s): 2.67, #queue-req: 0,
[2026-01-17 12:26:50] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [00:51<00:16,  2.39s/it][2026-01-17 12:26:53 TP0] Prefill batch, #new-seq: 1, #new-token: 7424, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:54] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [00:54<00:16,  2.78s/it][2026-01-17 12:26:57 TP0] Prefill batch, #new-seq: 1, #new-token: 6656, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:58] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [00:58<00:15,  3.07s/it][2026-01-17 12:26:59 TP0] Prefill batch, #new-seq: 1, #new-token: 3584, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:26:59] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:00<00:10,  2.63s/it][2026-01-17 12:27:01 TP0] Prefill batch, #new-seq: 1, #new-token: 5248, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:27:02 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.29, #queue-req: 0,
[2026-01-17 12:27:02] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:02<00:07,  2.58s/it][2026-01-17 12:27:04 TP0] Prefill batch, #new-seq: 1, #new-token: 5248, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:27:05] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [01:05<00:05,  2.67s/it][2026-01-17 12:27:06 TP0] Prefill batch, #new-seq: 1, #new-token: 3584, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:27:07] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [01:07<00:02,  2.42s/it][2026-01-17 12:27:09 TP0] Prefill batch, #new-seq: 1, #new-token: 8064, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:27:10] INFO:     127.0.0.1:48748 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [01:11<00:00,  2.83s/it]
Model Responding: 100%|██████████| 50/50 [01:11<00:00,  1.42s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 4039.35it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.26667}, 'Accounting': {'num': 30, 'acc': 0.26667}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.35}, 'Agriculture': {'num': 20, 'acc': 0.35}, 'Overall': {'num': 50, 'acc': 0.3}}
[32m2026-01-17 12:27:10.866[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 12:27:10.872[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  |  0.3|±  |   N/A|

/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 73982 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 164.874s

OK
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct achieved accuracy: 0.3000
Cleaning up process 73982
.
.
End (12/23):
filename='ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py', elapsed=175, estimated_time=400
.
.

.
.
Begin (13/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_235.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:27:34] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=393412771, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:27:36] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2026-01-17 12:27:47 TP7] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:27:47 TP5] Init torch distributed begin.
[2026-01-17 12:27:47 TP6] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:27:47 TP12] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:27:48 TP13] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:27:48 TP8] Init torch distributed begin.
[2026-01-17 12:27:48 TP9] Init torch distributed begin.
[2026-01-17 12:27:48 TP11] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:27:48 TP14] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:27:48 TP10] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:27:49 TP2] Init torch distributed begin.
[2026-01-17 12:27:49 TP1] Init torch distributed begin.
[2026-01-17 12:27:49 TP4] Init torch distributed begin.
[2026-01-17 12:27:49 TP0] Init torch distributed begin.
[2026-01-17 12:27:49 TP3] Init torch distributed begin.
[2026-01-17 12:27:49 TP15] Init torch distributed begin.
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[2026-01-17 12:27:50 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP15] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP13] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP14] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP11] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP12] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP9] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP8] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP10] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 12:27:50 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP7] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 12:27:50 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:50 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:27:51 TP12] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP11] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP10] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP13] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP14] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP15] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP8] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP9] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:51 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:27:52 TP6] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:27:52 TP12] Load weight begin. avail mem=60.85 GB
[2026-01-17 12:27:52 TP8] Load weight begin. avail mem=60.85 GB
[2026-01-17 12:27:52 TP10] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:27:52 TP6] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP6] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP11] Load weight begin. avail mem=61.11 GB
[2026-01-17 12:27:52 TP5] Load weight begin. avail mem=61.12 GB
[2026-01-17 12:27:52 TP12] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP12] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP8] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP8] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP10] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP10] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP11] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP11] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP1] Load weight begin. avail mem=61.12 GB
[2026-01-17 12:27:52 TP5] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP5] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP4] Load weight begin. avail mem=60.85 GB
[2026-01-17 12:27:52 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP14] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:27:52 TP2] Load weight begin. avail mem=60.84 GB
[2026-01-17 12:27:52 TP4] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP4] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP15] Load weight begin. avail mem=61.11 GB
[2026-01-17 12:27:52 TP14] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP14] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP3] Load weight begin. avail mem=61.12 GB
[2026-01-17 12:27:52 TP0] Load weight begin. avail mem=60.79 GB
[2026-01-17 12:27:52 TP15] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP15] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP13] Load weight begin. avail mem=61.12 GB
[2026-01-17 12:27:52 TP9] Load weight begin. avail mem=61.12 GB
[2026-01-17 12:27:52 TP13] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP13] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP7] Load weight begin. avail mem=61.11 GB
[2026-01-17 12:27:52 TP9] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP9] Using sdpa as multimodal attention backend.
[2026-01-17 12:27:52 TP7] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:27:52 TP7] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/96 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   1% Completed | 1/96 [00:08<14:14,  9.00s/it]

Loading safetensors checkpoint shards:   2% Completed | 2/96 [00:19<15:05,  9.63s/it]

Loading safetensors checkpoint shards:   3% Completed | 3/96 [00:29<15:39, 10.10s/it]

Loading safetensors checkpoint shards:   4% Completed | 4/96 [00:40<16:03, 10.48s/it]

Loading safetensors checkpoint shards:   5% Completed | 5/96 [00:51<15:52, 10.47s/it]

Loading safetensors checkpoint shards:   6% Completed | 6/96 [01:01<15:28, 10.32s/it]

Loading safetensors checkpoint shards:   7% Completed | 7/96 [01:11<15:05, 10.18s/it]

Loading safetensors checkpoint shards:   8% Completed | 8/96 [01:20<14:40, 10.01s/it]

Loading safetensors checkpoint shards:   9% Completed | 9/96 [01:31<14:39, 10.11s/it]

Loading safetensors checkpoint shards:  10% Completed | 10/96 [01:41<14:41, 10.25s/it]

Loading safetensors checkpoint shards:  11% Completed | 11/96 [01:52<14:34, 10.29s/it]

Loading safetensors checkpoint shards:  12% Completed | 12/96 [02:02<14:39, 10.47s/it]

Loading safetensors checkpoint shards:  14% Completed | 13/96 [02:12<14:13, 10.29s/it]

Loading safetensors checkpoint shards:  15% Completed | 14/96 [02:22<13:57, 10.21s/it]

Loading safetensors checkpoint shards:  16% Completed | 15/96 [02:32<13:32, 10.03s/it]

Loading safetensors checkpoint shards:  17% Completed | 16/96 [02:42<13:15,  9.95s/it]

Loading safetensors checkpoint shards:  18% Completed | 17/96 [02:51<12:52,  9.78s/it]

Loading safetensors checkpoint shards:  19% Completed | 18/96 [03:01<12:46,  9.82s/it]

Loading safetensors checkpoint shards:  20% Completed | 19/96 [03:11<12:36,  9.83s/it]

Loading safetensors checkpoint shards:  21% Completed | 20/96 [03:21<12:30,  9.88s/it]

Loading safetensors checkpoint shards:  22% Completed | 21/96 [03:30<12:12,  9.77s/it]

Loading safetensors checkpoint shards:  23% Completed | 22/96 [03:41<12:12,  9.90s/it]

Loading safetensors checkpoint shards:  24% Completed | 23/96 [03:50<11:51,  9.75s/it]

Loading safetensors checkpoint shards:  25% Completed | 24/96 [04:00<11:45,  9.80s/it]

Loading safetensors checkpoint shards:  26% Completed | 25/96 [04:09<11:20,  9.58s/it]

Loading safetensors checkpoint shards:  27% Completed | 26/96 [04:19<11:20,  9.72s/it]

Loading safetensors checkpoint shards:  28% Completed | 27/96 [04:28<11:04,  9.62s/it]

Loading safetensors checkpoint shards:  29% Completed | 28/96 [04:38<10:59,  9.70s/it]

Loading safetensors checkpoint shards:  30% Completed | 29/96 [04:48<10:58,  9.82s/it]

Loading safetensors checkpoint shards:  31% Completed | 30/96 [04:58<10:42,  9.73s/it]

Loading safetensors checkpoint shards:  32% Completed | 31/96 [05:01<08:27,  7.80s/it]

Loading safetensors checkpoint shards:  33% Completed | 32/96 [05:11<08:56,  8.39s/it]

Loading safetensors checkpoint shards:  34% Completed | 33/96 [05:21<09:20,  8.90s/it]

Loading safetensors checkpoint shards:  35% Completed | 34/96 [05:31<09:29,  9.19s/it]

Loading safetensors checkpoint shards:  36% Completed | 35/96 [05:41<09:38,  9.48s/it]

Loading safetensors checkpoint shards:  38% Completed | 36/96 [05:52<09:45,  9.76s/it]

Loading safetensors checkpoint shards:  39% Completed | 37/96 [06:02<09:46,  9.94s/it]

Loading safetensors checkpoint shards:  40% Completed | 38/96 [06:12<09:38,  9.97s/it]

Loading safetensors checkpoint shards:  41% Completed | 39/96 [06:21<09:21,  9.85s/it]

Loading safetensors checkpoint shards:  42% Completed | 40/96 [06:32<09:26, 10.12s/it]

Loading safetensors checkpoint shards:  43% Completed | 41/96 [06:43<09:24, 10.27s/it]

Loading safetensors checkpoint shards:  44% Completed | 42/96 [06:53<09:17, 10.33s/it]

Loading safetensors checkpoint shards:  45% Completed | 43/96 [07:03<09:01, 10.21s/it]

Loading safetensors checkpoint shards:  46% Completed | 44/96 [07:13<08:49, 10.19s/it]

Loading safetensors checkpoint shards:  47% Completed | 45/96 [07:23<08:36, 10.13s/it]

Loading safetensors checkpoint shards:  48% Completed | 46/96 [07:33<08:26, 10.13s/it]

Loading safetensors checkpoint shards:  49% Completed | 47/96 [07:43<08:12, 10.05s/it]

Loading safetensors checkpoint shards:  50% Completed | 48/96 [07:54<08:10, 10.21s/it]

Loading safetensors checkpoint shards:  51% Completed | 49/96 [08:04<07:58, 10.18s/it]

Loading safetensors checkpoint shards:  52% Completed | 50/96 [08:14<07:43, 10.08s/it]

Loading safetensors checkpoint shards:  53% Completed | 51/96 [08:24<07:32, 10.05s/it]

Loading safetensors checkpoint shards:  54% Completed | 52/96 [08:34<07:18,  9.96s/it]

Loading safetensors checkpoint shards:  55% Completed | 53/96 [08:44<07:07,  9.93s/it]

Loading safetensors checkpoint shards:  56% Completed | 54/96 [08:53<06:56,  9.91s/it]

Loading safetensors checkpoint shards:  57% Completed | 55/96 [09:02<06:33,  9.61s/it]

Loading safetensors checkpoint shards:  58% Completed | 56/96 [09:12<06:22,  9.56s/it]

Loading safetensors checkpoint shards:  59% Completed | 57/96 [09:21<06:04,  9.34s/it]

Loading safetensors checkpoint shards:  60% Completed | 58/96 [09:30<05:57,  9.41s/it]
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 134, in _run_vlm_mmmu_test
    process = popen_launch_server(
              ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 688, in popen_launch_server
    raise TimeoutError("Server failed to start within the timeout period.")
TimeoutError: Server failed to start within the timeout period.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_235.py", line 30, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 177, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct: Server failed to start within the timeout period.
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 80137 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 134, in _run_vlm_mmmu_test
    process = popen_launch_server(
              ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 688, in popen_launch_server
    raise TimeoutError("Server failed to start within the timeout period.")
TimeoutError: Server failed to start within the timeout period.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct: Server failed to start within the timeout period.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 600.767s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.8 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 16 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Error testing /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct: Server failed to start within the timeout period.
.
.
End (13/23):
filename='ascend/vlm_models/test_ascend_qwen3_vl_235.py', elapsed=611, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_qwen3_vl_235.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_qwen3_vl_235.py

.
.
Begin (13/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_235.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:38:46] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=176800764, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:38:48] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:38:58 TP5] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:38:59 TP2] Init torch distributed begin.
[2026-01-17 12:38:59 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:38:59 TP0] Init torch distributed begin.
[2026-01-17 12:38:59 TP8] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:38:59 TP14] Init torch distributed begin.
[2026-01-17 12:38:59 TP4] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:38:59 TP15] Init torch distributed begin.
[2026-01-17 12:38:59 TP1] Init torch distributed begin.
[2026-01-17 12:38:59 TP13] Init torch distributed begin.
[2026-01-17 12:38:59 TP11] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:38:59 TP6] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:39:00 TP12] Init torch distributed begin.
[2026-01-17 12:39:00 TP7] Init torch distributed begin.
[2026-01-17 12:39:00 TP9] Init torch distributed begin.
[2026-01-17 12:39:00 TP10] Init torch distributed begin.
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[2026-01-17 12:39:01 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP15] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 12:39:01 TP13] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP14] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP12] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP11] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP10] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP9] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP8] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:01 TP4] Init torch distributed ends. mem usage=-0.00 GB
[2026-01-17 12:39:01 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:39:02 TP11] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP8] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP15] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP13] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP14] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP12] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP10] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:02 TP9] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:39:03 TP11] Load weight begin. avail mem=61.11 GB
[2026-01-17 12:39:03 TP15] Load weight begin. avail mem=61.11 GB
[2026-01-17 12:39:03 TP11] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP11] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP0] Load weight begin. avail mem=60.79 GB
[2026-01-17 12:39:03 TP8] Load weight begin. avail mem=60.85 GB
[2026-01-17 12:39:03 TP15] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP15] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP3] Load weight begin. avail mem=61.12 GB
[2026-01-17 12:39:03 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP8] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP8] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP6] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:39:03 TP4] Load weight begin. avail mem=60.85 GB
[2026-01-17 12:39:03 TP2] Load weight begin. avail mem=60.84 GB
[2026-01-17 12:39:03 TP1] Load weight begin. avail mem=61.12 GB
[2026-01-17 12:39:03 TP6] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP6] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP4] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP4] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP13] Load weight begin. avail mem=61.12 GB
[2026-01-17 12:39:03 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP13] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP13] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP5] Load weight begin. avail mem=61.12 GB
[2026-01-17 12:39:03 TP7] Load weight begin. avail mem=61.11 GB
[2026-01-17 12:39:03 TP5] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP5] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP12] Load weight begin. avail mem=60.85 GB
[2026-01-17 12:39:03 TP10] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:39:03 TP9] Load weight begin. avail mem=61.12 GB
[2026-01-17 12:39:03 TP14] Load weight begin. avail mem=60.86 GB
[2026-01-17 12:39:03 TP7] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP7] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP12] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP12] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP10] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP10] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP9] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP9] Using sdpa as multimodal attention backend.
[2026-01-17 12:39:03 TP14] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:39:03 TP14] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/96 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   1% Completed | 1/96 [00:00<01:33,  1.02it/s]

Loading safetensors checkpoint shards:   2% Completed | 2/96 [00:02<01:40,  1.07s/it]

Loading safetensors checkpoint shards:   3% Completed | 3/96 [00:03<01:36,  1.04s/it]

Loading safetensors checkpoint shards:   4% Completed | 4/96 [00:04<01:42,  1.11s/it]

Loading safetensors checkpoint shards:   5% Completed | 5/96 [00:05<01:43,  1.13s/it]

Loading safetensors checkpoint shards:   6% Completed | 6/96 [00:06<01:32,  1.03s/it]

Loading safetensors checkpoint shards:   7% Completed | 7/96 [00:07<01:28,  1.00it/s]

Loading safetensors checkpoint shards:   8% Completed | 8/96 [00:08<01:30,  1.03s/it]

Loading safetensors checkpoint shards:   9% Completed | 9/96 [00:09<01:31,  1.05s/it]

Loading safetensors checkpoint shards:  10% Completed | 10/96 [00:10<01:30,  1.05s/it]

Loading safetensors checkpoint shards:  11% Completed | 11/96 [00:11<01:29,  1.05s/it]

Loading safetensors checkpoint shards:  12% Completed | 12/96 [00:12<01:29,  1.06s/it]

Loading safetensors checkpoint shards:  14% Completed | 13/96 [00:13<01:28,  1.07s/it]

Loading safetensors checkpoint shards:  15% Completed | 14/96 [00:14<01:26,  1.06s/it]

Loading safetensors checkpoint shards:  16% Completed | 15/96 [00:15<01:26,  1.07s/it]

Loading safetensors checkpoint shards:  17% Completed | 16/96 [00:16<01:24,  1.06s/it]

Loading safetensors checkpoint shards:  18% Completed | 17/96 [00:17<01:23,  1.05s/it]

Loading safetensors checkpoint shards:  19% Completed | 18/96 [00:18<01:21,  1.05s/it]

Loading safetensors checkpoint shards:  20% Completed | 19/96 [00:19<01:19,  1.04s/it]

Loading safetensors checkpoint shards:  21% Completed | 20/96 [00:21<01:18,  1.03s/it]

Loading safetensors checkpoint shards:  22% Completed | 21/96 [00:22<01:17,  1.04s/it]

Loading safetensors checkpoint shards:  23% Completed | 22/96 [00:23<01:16,  1.04s/it]

Loading safetensors checkpoint shards:  24% Completed | 23/96 [00:24<01:17,  1.06s/it]

Loading safetensors checkpoint shards:  25% Completed | 24/96 [00:25<01:16,  1.07s/it]

Loading safetensors checkpoint shards:  26% Completed | 25/96 [00:26<01:15,  1.07s/it]

Loading safetensors checkpoint shards:  27% Completed | 26/96 [00:27<01:14,  1.06s/it]

Loading safetensors checkpoint shards:  28% Completed | 27/96 [00:28<01:13,  1.06s/it]

Loading safetensors checkpoint shards:  29% Completed | 28/96 [00:29<01:12,  1.07s/it]

Loading safetensors checkpoint shards:  30% Completed | 29/96 [00:30<01:11,  1.07s/it]

Loading safetensors checkpoint shards:  31% Completed | 30/96 [00:31<01:10,  1.07s/it]

Loading safetensors checkpoint shards:  32% Completed | 31/96 [00:31<00:54,  1.19it/s]

Loading safetensors checkpoint shards:  33% Completed | 32/96 [00:32<00:55,  1.16it/s]

Loading safetensors checkpoint shards:  34% Completed | 33/96 [00:33<00:58,  1.08it/s]

Loading safetensors checkpoint shards:  35% Completed | 34/96 [00:35<01:00,  1.02it/s]

Loading safetensors checkpoint shards:  36% Completed | 35/96 [00:36<01:01,  1.00s/it]

Loading safetensors checkpoint shards:  38% Completed | 36/96 [00:37<01:01,  1.02s/it]

Loading safetensors checkpoint shards:  39% Completed | 37/96 [00:38<01:00,  1.03s/it]

Loading safetensors checkpoint shards:  40% Completed | 38/96 [00:39<01:00,  1.05s/it]

Loading safetensors checkpoint shards:  41% Completed | 39/96 [00:40<00:59,  1.05s/it]

Loading safetensors checkpoint shards:  42% Completed | 40/96 [00:41<00:59,  1.06s/it]

Loading safetensors checkpoint shards:  43% Completed | 41/96 [00:42<00:59,  1.07s/it]

Loading safetensors checkpoint shards:  44% Completed | 42/96 [00:43<00:57,  1.07s/it]

Loading safetensors checkpoint shards:  45% Completed | 43/96 [00:44<00:56,  1.06s/it]

Loading safetensors checkpoint shards:  46% Completed | 44/96 [00:45<00:55,  1.06s/it]

Loading safetensors checkpoint shards:  47% Completed | 45/96 [00:46<00:53,  1.05s/it]

Loading safetensors checkpoint shards:  48% Completed | 46/96 [00:47<00:52,  1.04s/it]

Loading safetensors checkpoint shards:  49% Completed | 47/96 [00:48<00:51,  1.05s/it]

Loading safetensors checkpoint shards:  50% Completed | 48/96 [00:49<00:50,  1.05s/it]

Loading safetensors checkpoint shards:  51% Completed | 49/96 [00:50<00:49,  1.05s/it]

Loading safetensors checkpoint shards:  52% Completed | 50/96 [00:52<00:48,  1.05s/it]

Loading safetensors checkpoint shards:  53% Completed | 51/96 [00:53<00:47,  1.06s/it]

Loading safetensors checkpoint shards:  54% Completed | 52/96 [00:54<00:46,  1.06s/it]

Loading safetensors checkpoint shards:  55% Completed | 53/96 [00:55<00:45,  1.06s/it]

Loading safetensors checkpoint shards:  56% Completed | 54/96 [00:56<00:44,  1.07s/it]

Loading safetensors checkpoint shards:  57% Completed | 55/96 [00:57<00:44,  1.07s/it]

Loading safetensors checkpoint shards:  58% Completed | 56/96 [00:58<00:42,  1.07s/it]

Loading safetensors checkpoint shards:  59% Completed | 57/96 [00:59<00:42,  1.08s/it]

Loading safetensors checkpoint shards:  60% Completed | 58/96 [01:00<00:41,  1.09s/it]

Loading safetensors checkpoint shards:  61% Completed | 59/96 [01:06<01:35,  2.58s/it]

Loading safetensors checkpoint shards:  62% Completed | 60/96 [01:17<02:56,  4.91s/it]

Loading safetensors checkpoint shards:  64% Completed | 61/96 [01:27<03:48,  6.53s/it]

Loading safetensors checkpoint shards:  65% Completed | 62/96 [01:37<04:14,  7.49s/it]

Loading safetensors checkpoint shards:  66% Completed | 63/96 [01:46<04:30,  8.19s/it]

Loading safetensors checkpoint shards:  67% Completed | 64/96 [01:57<04:41,  8.79s/it]

Loading safetensors checkpoint shards:  68% Completed | 65/96 [02:07<04:48,  9.31s/it]

Loading safetensors checkpoint shards:  69% Completed | 66/96 [02:17<04:42,  9.41s/it]

Loading safetensors checkpoint shards:  70% Completed | 67/96 [02:27<04:41,  9.70s/it]

Loading safetensors checkpoint shards:  71% Completed | 68/96 [02:37<04:31,  9.71s/it]

Loading safetensors checkpoint shards:  72% Completed | 69/96 [02:47<04:27,  9.92s/it]

Loading safetensors checkpoint shards:  73% Completed | 70/96 [02:57<04:15,  9.83s/it]

Loading safetensors checkpoint shards:  74% Completed | 71/96 [03:07<04:06,  9.84s/it]

Loading safetensors checkpoint shards:  75% Completed | 72/96 [03:16<03:53,  9.72s/it]

Loading safetensors checkpoint shards:  76% Completed | 73/96 [03:25<03:38,  9.52s/it]

Loading safetensors checkpoint shards:  77% Completed | 74/96 [03:36<03:34,  9.74s/it]

Loading safetensors checkpoint shards:  78% Completed | 75/96 [03:45<03:22,  9.65s/it]

Loading safetensors checkpoint shards:  79% Completed | 76/96 [03:54<03:08,  9.45s/it]

Loading safetensors checkpoint shards:  80% Completed | 77/96 [04:03<02:54,  9.19s/it]

Loading safetensors checkpoint shards:  81% Completed | 78/96 [04:13<02:50,  9.48s/it]

Loading safetensors checkpoint shards:  82% Completed | 79/96 [04:22<02:41,  9.51s/it]

Loading safetensors checkpoint shards:  83% Completed | 80/96 [04:32<02:33,  9.60s/it]

Loading safetensors checkpoint shards:  84% Completed | 81/96 [04:42<02:24,  9.66s/it]

Loading safetensors checkpoint shards:  85% Completed | 82/96 [04:51<02:14,  9.60s/it]

Loading safetensors checkpoint shards:  86% Completed | 83/96 [05:01<02:04,  9.58s/it]

Loading safetensors checkpoint shards:  88% Completed | 84/96 [05:11<01:56,  9.70s/it]

Loading safetensors checkpoint shards:  89% Completed | 85/96 [05:20<01:45,  9.58s/it]

Loading safetensors checkpoint shards:  90% Completed | 86/96 [05:30<01:35,  9.59s/it]

Loading safetensors checkpoint shards:  91% Completed | 87/96 [05:39<01:25,  9.47s/it]

Loading safetensors checkpoint shards:  92% Completed | 88/96 [05:48<01:14,  9.35s/it]

Loading safetensors checkpoint shards:  93% Completed | 89/96 [05:57<01:05,  9.33s/it]

Loading safetensors checkpoint shards:  94% Completed | 90/96 [06:08<00:58,  9.69s/it]

Loading safetensors checkpoint shards:  95% Completed | 91/96 [06:18<00:48,  9.68s/it]

Loading safetensors checkpoint shards:  96% Completed | 92/96 [06:27<00:38,  9.66s/it]

Loading safetensors checkpoint shards:  97% Completed | 93/96 [06:37<00:29,  9.75s/it]

Loading safetensors checkpoint shards:  98% Completed | 94/96 [06:39<00:14,  7.47s/it]

Loading safetensors checkpoint shards:  99% Completed | 95/96 [06:48<00:07,  7.75s/it]

Loading safetensors checkpoint shards: 100% Completed | 96/96 [06:57<00:00,  8.28s/it]

Loading safetensors checkpoint shards: 100% Completed | 96/96 [06:57<00:00,  4.35s/it]

[2026-01-17 12:46:16 TP1] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.89 GB, mem usage=28.23 GB.
[2026-01-17 12:46:17 TP8] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.62 GB, mem usage=28.23 GB.
[2026-01-17 12:46:17 TP15] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.88 GB, mem usage=28.23 GB.
[2026-01-17 12:46:18 TP13] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.89 GB, mem usage=28.23 GB.
[2026-01-17 12:46:19 TP2] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.61 GB, mem usage=28.23 GB.
[2026-01-17 12:46:19 TP11] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.88 GB, mem usage=28.23 GB.
[2026-01-17 12:46:19 TP5] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.89 GB, mem usage=28.23 GB.
[2026-01-17 12:46:19 TP6] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.63 GB, mem usage=28.23 GB.
[2026-01-17 12:46:20 TP12] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.62 GB, mem usage=28.23 GB.
[2026-01-17 12:46:20 TP0] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.56 GB, mem usage=28.23 GB.
[2026-01-17 12:46:20 TP14] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.63 GB, mem usage=28.23 GB.
[2026-01-17 12:46:21 TP10] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.63 GB, mem usage=28.23 GB.
[2026-01-17 12:46:21 TP4] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.62 GB, mem usage=28.23 GB.
[2026-01-17 12:46:21 TP7] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.88 GB, mem usage=28.23 GB.
[2026-01-17 12:46:21 TP3] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.89 GB, mem usage=28.23 GB.
[2026-01-17 12:46:22 TP9] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=32.90 GB, mem usage=28.23 GB.
[2026-01-17 12:46:22 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 12:46:22 TP0] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP14] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP15] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP13] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP11] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP12] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP10] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP9] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP8] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP7] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP5] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP6] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP4] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP2] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP3] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:22 TP1] The available memory for KV cache is 20.32 GB.
[2026-01-17 12:46:23 TP2] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP10] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP2] Memory pool end. avail mem=10.21 GB
[2026-01-17 12:46:23 TP10] Memory pool end. avail mem=10.22 GB
[2026-01-17 12:46:23 TP0] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP15] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP8] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP5] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP0] Memory pool end. avail mem=10.15 GB
[2026-01-17 12:46:23 TP15] Memory pool end. avail mem=10.48 GB
[2026-01-17 12:46:23 TP3] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP8] Memory pool end. avail mem=10.21 GB
[2026-01-17 12:46:23 TP13] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP5] Memory pool end. avail mem=10.49 GB
[2026-01-17 12:46:23 TP4] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP1] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP14] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP3] Memory pool end. avail mem=10.49 GB
[2026-01-17 12:46:23 TP4] Memory pool end. avail mem=10.21 GB
[2026-01-17 12:46:23 TP12] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP9] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP1] Memory pool end. avail mem=10.49 GB
[2026-01-17 12:46:23 TP14] Memory pool end. avail mem=10.22 GB
[2026-01-17 12:46:23 TP13] Memory pool end. avail mem=10.49 GB
[2026-01-17 12:46:23 TP9] Memory pool end. avail mem=10.49 GB
[2026-01-17 12:46:23 TP12] Memory pool end. avail mem=10.21 GB
[2026-01-17 12:46:23 TP11] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP6] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP11] Memory pool end. avail mem=10.48 GB
[2026-01-17 12:46:23 TP7] KV Cache is allocated. #tokens: 453248, K size: 10.16 GB, V size: 10.16 GB
[2026-01-17 12:46:23 TP6] Memory pool end. avail mem=10.22 GB
[2026-01-17 12:46:23 TP7] Memory pool end. avail mem=10.48 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 12:46:24 TP0] max_total_num_tokens=453248, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=262144, available_gpu_mem=10.15 GB
[2026-01-17 12:46:25] INFO:     Started server process [85697]
[2026-01-17 12:46:25] INFO:     Waiting for application startup.
[2026-01-17 12:46:25] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 12:46:25] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 12:46:25] INFO:     Application startup complete.
[2026-01-17 12:46:25] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 12:46:26] INFO:     127.0.0.1:40710 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:46:26] INFO:     127.0.0.1:40718 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 12:46:26 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
.....('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:46:36] INFO:     127.0.0.1:58708 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
................[2026-01-17 12:46:46] INFO:     127.0.0.1:41372 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 12:46:52] INFO:     127.0.0.1:40728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 12:46:52] The server is fired up and ready to roll!
[2026-01-17 12:46:56 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:46:57] INFO:     127.0.0.1:55140 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 12:47:05.279[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 12:47:08.431[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 12:47:09.643[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 12:47:09.643[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 12:47:09.644[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 12:47:09.650[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 12:47:17.536[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9182.73it/s]
[32m2026-01-17 12:47:17.542[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 12:47:17 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:18] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:00<00:47,  1.04it/s][2026-01-17 12:47:18 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:20] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:03<01:22,  1.71s/it][2026-01-17 12:47:20 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:21] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:03<00:58,  1.24s/it][2026-01-17 12:47:21 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:22] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:04<00:47,  1.03s/it][2026-01-17 12:47:22 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:22] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:05<00:39,  1.13it/s][2026-01-17 12:47:22 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:28] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:10<01:46,  2.42s/it][2026-01-17 12:47:28 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:28] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:11<01:20,  1.86s/it][2026-01-17 12:47:28 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:31 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.08, #queue-req: 0,
[2026-01-17 12:47:31] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:14<01:34,  2.25s/it][2026-01-17 12:47:31 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:32] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:14<01:10,  1.71s/it][2026-01-17 12:47:32 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:33] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:15<00:54,  1.37s/it][2026-01-17 12:47:33 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:33] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:16<00:44,  1.14s/it][2026-01-17 12:47:33 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:34] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:16<00:37,  1.02it/s][2026-01-17 12:47:34 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:34] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:17<00:31,  1.17it/s][2026-01-17 12:47:34 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:35] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:18<00:29,  1.22it/s][2026-01-17 12:47:35 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:36] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:18<00:27,  1.30it/s][2026-01-17 12:47:36 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:36] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:19<00:24,  1.40it/s][2026-01-17 12:47:36 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:37] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:19<00:23,  1.42it/s][2026-01-17 12:47:37 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:38] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:20<00:22,  1.43it/s][2026-01-17 12:47:38 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:38] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:21<00:21,  1.47it/s][2026-01-17 12:47:38 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:39] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:22<00:20,  1.46it/s][2026-01-17 12:47:39 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:40] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:22<00:19,  1.47it/s][2026-01-17 12:47:40 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:40] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:23<00:18,  1.49it/s][2026-01-17 12:47:40 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:41] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:24<00:18,  1.49it/s][2026-01-17 12:47:41 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:42] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:24<00:17,  1.48it/s][2026-01-17 12:47:42 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:42 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.38, #queue-req: 0,
[2026-01-17 12:47:42] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:25<00:16,  1.49it/s][2026-01-17 12:47:43 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:43] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:26<00:16,  1.44it/s][2026-01-17 12:47:43 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:44] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:26<00:16,  1.43it/s][2026-01-17 12:47:44 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:45] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:27<00:15,  1.45it/s][2026-01-17 12:47:45 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:45] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:28<00:13,  1.50it/s][2026-01-17 12:47:45 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:46] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:28<00:13,  1.51it/s][2026-01-17 12:47:49 TP0] Prefill batch, #new-seq: 1, #new-token: 5504, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:54 TP14] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP12] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP13] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP11] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP10] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP15] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP8] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP9] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:54 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:47:55] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:37<01:00,  3.17s/it][2026-01-17 12:47:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:47:57] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:39<00:49,  2.74s/it][2026-01-17 12:47:59 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:01] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:43<00:55,  3.25s/it][2026-01-17 12:48:05 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:09 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP15] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP14] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP9] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP11] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP13] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP12] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP10] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:09 TP8] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:10] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:52<01:17,  4.85s/it][2026-01-17 12:48:10 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:10] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:53<00:55,  3.68s/it][2026-01-17 12:48:11 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:11] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:54<00:38,  2.75s/it][2026-01-17 12:48:11 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:12] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:54<00:28,  2.21s/it][2026-01-17 12:48:15 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:19 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP9] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP11] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP15] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP14] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP13] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP12] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP10] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19 TP8] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:19] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [01:02<00:45,  3.77s/it][2026-01-17 12:48:22 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:26 TP15] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP10] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP9] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP14] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP12] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP13] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP11] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:26 TP8] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:27] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [01:09<00:54,  4.92s/it][2026-01-17 12:48:27 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:28] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [01:10<00:36,  3.63s/it][2026-01-17 12:48:30 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:32 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP10] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP15] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP9] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP8] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP14] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP12] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP13] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:32 TP11] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:33] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [01:15<00:37,  4.12s/it][2026-01-17 12:48:33 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:34] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [01:16<00:25,  3.15s/it][2026-01-17 12:48:37 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:41 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP15] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP9] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP11] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP10] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP12] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP13] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP8] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP14] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:41 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:42] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [01:24<00:32,  4.62s/it][2026-01-17 12:48:45 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:49 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP11] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP15] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP9] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP13] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP14] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP12] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP10] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:49 TP8] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:50] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:32<00:33,  5.64s/it][2026-01-17 12:48:53 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:55 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP10] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP13] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP15] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP11] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP9] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP14] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP12] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP8] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:55 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:48:56 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.54, #queue-req: 0,
[2026-01-17 12:48:56] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [01:39<00:29,  5.81s/it][2026-01-17 12:48:57 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:48:59] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:41<00:19,  4.80s/it][2026-01-17 12:49:00 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:49:03] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:45<00:13,  4.59s/it][2026-01-17 12:49:05 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:49:07] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [01:49<00:08,  4.47s/it][2026-01-17 12:49:08 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:49:09] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [01:52<00:03,  3.85s/it][2026-01-17 12:49:12 TP0] Prefill batch, #new-seq: 1, #new-token: 5248, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:49:16 TP13] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP9] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP10] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP15] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP11] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP14] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP8] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP12] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 12:49:16] INFO:     127.0.0.1:51626 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [01:59<00:00,  4.87s/it]
Model Responding: 100%|██████████| 50/50 [01:59<00:00,  2.39s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 2530.23it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.56667}, 'Accounting': {'num': 30, 'acc': 0.56667}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.75}, 'Agriculture': {'num': 20, 'acc': 0.75}, 'Overall': {'num': 50, 'acc': 0.64}}
[32m2026-01-17 12:49:17.003[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 12:49:17.008[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  | 0.64|±  |   N/A|

/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 85697 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 645.425s

OK
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.8 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 16 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-235B-A22B-Instruct achieved accuracy: 0.3000
Cleaning up process 85697
.
.
End (13/23):
filename='ascend/vlm_models/test_ascend_qwen3_vl_235.py', elapsed=656, estimated_time=400
.
.


✓ PASSED on retry (attempt 2): ascend/vlm_models/test_ascend_qwen3_vl_235.py

.
.
Begin (14/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_deepseek_vl2.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:49:42] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/deepseek-ai/deepseek-vl2', tokenizer_path='/root/.cache/modelscope/hub/models/deepseek-ai/deepseek-vl2', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.95, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=71562656, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/deepseek-ai/deepseek-vl2', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2026-01-17 12:49:45] Inferred chat template from model path: deepseek-vl2
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:49:53] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2026-01-17 12:49:53] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:49:53] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 12:49:54] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:49:54] Load weight begin. avail mem=56.80 GB
[2026-01-17 12:49:55] Config not support fused shared expert(s). Shared experts fusion optimization is disabled.

Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:02<00:20,  2.87s/it]

Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:05<00:17,  2.95s/it]

Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:06<00:09,  1.92s/it]

Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:09<00:09,  2.33s/it]

Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:12<00:07,  2.48s/it]

Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:15<00:05,  2.64s/it]

Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:18<00:02,  2.75s/it]

Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:18<00:00,  2.03s/it]

Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:18<00:00,  2.34s/it]

[2026-01-17 12:52:01] Load weight end. type=DeepseekVL2ForCausalLM, dtype=torch.bfloat16, avail mem=4.43 GB, mem usage=52.37 GB.
[2026-01-17 12:52:01] Using KV cache dtype: torch.bfloat16
[2026-01-17 12:52:01] The available memory for KV cache is 1.59 GB.
[2026-01-17 12:52:01] KV Cache is allocated. #tokens: 49408, KV size: 1.59 GB
[2026-01-17 12:52:01] Memory pool end. avail mem=3.01 GB
[2026-01-17 12:52:02] Capture npu graph begin. This can take up to several minutes. avail mem=3.02 GB
[2026-01-17 12:52:02] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64]

  0%|          | 0/12 [00:00<?, ?it/s]
Capturing batches (bs=64 avail_mem=3.00 GB):   0%|          | 0/12 [00:00<?, ?it/s]
Capturing batches (bs=64 avail_mem=3.00 GB):   8%|▊         | 1/12 [00:00<00:09,  1.11it/s]
Capturing batches (bs=56 avail_mem=2.71 GB):   8%|▊         | 1/12 [00:00<00:09,  1.11it/s]
Capturing batches (bs=56 avail_mem=2.71 GB):  17%|█▋        | 2/12 [00:01<00:05,  1.92it/s]
Capturing batches (bs=48 avail_mem=2.68 GB):  17%|█▋        | 2/12 [00:01<00:05,  1.92it/s]
Capturing batches (bs=48 avail_mem=2.68 GB):  25%|██▌       | 3/12 [00:01<00:03,  2.52it/s]
Capturing batches (bs=40 avail_mem=2.66 GB):  25%|██▌       | 3/12 [00:01<00:03,  2.52it/s]
Capturing batches (bs=40 avail_mem=2.66 GB):  33%|███▎      | 4/12 [00:01<00:02,  2.95it/s]
Capturing batches (bs=32 avail_mem=2.65 GB):  33%|███▎      | 4/12 [00:01<00:02,  2.95it/s]
Capturing batches (bs=32 avail_mem=2.65 GB):  42%|████▏     | 5/12 [00:01<00:02,  3.26it/s]
Capturing batches (bs=24 avail_mem=2.65 GB):  42%|████▏     | 5/12 [00:01<00:02,  3.26it/s]
Capturing batches (bs=24 avail_mem=2.65 GB):  50%|█████     | 6/12 [00:02<00:01,  3.48it/s]
Capturing batches (bs=16 avail_mem=2.64 GB):  50%|█████     | 6/12 [00:02<00:01,  3.48it/s]
Capturing batches (bs=16 avail_mem=2.64 GB):  58%|█████▊    | 7/12 [00:02<00:01,  3.64it/s]
Capturing batches (bs=12 avail_mem=2.63 GB):  58%|█████▊    | 7/12 [00:02<00:01,  3.64it/s]
Capturing batches (bs=12 avail_mem=2.63 GB):  67%|██████▋   | 8/12 [00:02<00:01,  3.75it/s]
Capturing batches (bs=8 avail_mem=2.62 GB):  67%|██████▋   | 8/12 [00:02<00:01,  3.75it/s]
Capturing batches (bs=8 avail_mem=2.62 GB):  75%|███████▌  | 9/12 [00:02<00:00,  3.83it/s]
Capturing batches (bs=4 avail_mem=2.62 GB):  75%|███████▌  | 9/12 [00:02<00:00,  3.83it/s]
Capturing batches (bs=4 avail_mem=2.62 GB):  83%|████████▎ | 10/12 [00:03<00:00,  3.88it/s]
Capturing batches (bs=2 avail_mem=2.61 GB):  83%|████████▎ | 10/12 [00:03<00:00,  3.88it/s]
Capturing batches (bs=2 avail_mem=2.61 GB):  92%|█████████▏| 11/12 [00:03<00:00,  3.92it/s]
Capturing batches (bs=1 avail_mem=2.60 GB):  92%|█████████▏| 11/12 [00:03<00:00,  3.92it/s]
Capturing batches (bs=1 avail_mem=2.60 GB): 100%|██████████| 12/12 [00:03<00:00,  3.99it/s]
Capturing batches (bs=1 avail_mem=2.60 GB): 100%|██████████| 12/12 [00:03<00:00,  3.29it/s]
[2026-01-17 12:52:06] Capture npu graph end. Time elapsed: 4.88 s. mem usage=0.42 GB. avail mem=2.60 GB.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 12:52:07] max_total_num_tokens=49408, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=4200, available_gpu_mem=2.60 GB
[2026-01-17 12:52:08] INFO:     Started server process [114820]
[2026-01-17 12:52:08] INFO:     Waiting for application startup.
[2026-01-17 12:52:08] INFO:     Application startup complete.
[2026-01-17 12:52:08] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 12:52:09] INFO:     127.0.0.1:42282 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 12:52:09] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
.('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 12:52:11] INFO:     127.0.0.1:42292 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 12:52:11] The server is fired up and ready to roll!
[2026-01-17 12:52:12] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:13] INFO:     127.0.0.1:42304 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 12:52:21.310[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 12:52:23.367[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 12:52:24.110[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 12:52:24.111[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 12:52:24.114[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 12:52:32.582[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/900 [00:00<?, ?it/s]
100%|██████████| 900/900 [00:00<00:00, 10151.55it/s]
[32m2026-01-17 12:52:32.672[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/900 [00:00<?, ?it/s][2026-01-17 12:52:32] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   0%|          | 1/900 [00:00<06:08,  2.44it/s][2026-01-17 12:52:33] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   0%|          | 2/900 [00:00<04:08,  3.62it/s][2026-01-17 12:52:33] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   0%|          | 3/900 [00:00<03:22,  4.43it/s][2026-01-17 12:52:33] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   0%|          | 4/900 [00:01<03:42,  4.03it/s][2026-01-17 12:52:33] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   1%|          | 5/900 [00:01<03:50,  3.88it/s][2026-01-17 12:52:34] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   1%|          | 6/900 [00:01<03:57,  3.77it/s][2026-01-17 12:52:34] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   1%|          | 7/900 [00:01<03:35,  4.15it/s][2026-01-17 12:52:34] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   1%|          | 8/900 [00:01<03:23,  4.38it/s][2026-01-17 12:52:34] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   1%|          | 9/900 [00:02<03:07,  4.75it/s][2026-01-17 12:52:34] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   1%|          | 10/900 [00:02<03:03,  4.84it/s][2026-01-17 12:52:35] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   1%|          | 11/900 [00:02<03:08,  4.71it/s][2026-01-17 12:52:35] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   1%|▏         | 12/900 [00:02<03:03,  4.83it/s][2026-01-17 12:52:35] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   1%|▏         | 13/900 [00:02<03:00,  4.91it/s][2026-01-17 12:52:35] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 14/900 [00:03<03:19,  4.45it/s][2026-01-17 12:52:36] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 15/900 [00:03<03:33,  4.14it/s][2026-01-17 12:52:36] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:36] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 0.24, #queue-req: 0,
[2026-01-17 12:52:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 16/900 [00:03<03:21,  4.39it/s][2026-01-17 12:52:36] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 17/900 [00:03<03:06,  4.75it/s][2026-01-17 12:52:36] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 18/900 [00:04<03:01,  4.86it/s][2026-01-17 12:52:36] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 19/900 [00:04<03:22,  4.35it/s][2026-01-17 12:52:37] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 20/900 [00:04<03:18,  4.43it/s][2026-01-17 12:52:37] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 21/900 [00:04<03:08,  4.65it/s][2026-01-17 12:52:37] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 22/900 [00:04<03:01,  4.85it/s][2026-01-17 12:52:37] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   3%|▎         | 23/900 [00:05<02:55,  5.01it/s][2026-01-17 12:52:37] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   3%|▎         | 24/900 [00:05<02:51,  5.10it/s][2026-01-17 12:52:38] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   3%|▎         | 25/900 [00:05<02:48,  5.20it/s][2026-01-17 12:52:38] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   3%|▎         | 26/900 [00:05<03:12,  4.55it/s][2026-01-17 12:52:38] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   3%|▎         | 27/900 [00:06<03:14,  4.50it/s][2026-01-17 12:52:38] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   3%|▎         | 28/900 [00:06<03:09,  4.61it/s][2026-01-17 12:52:38] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   3%|▎         | 29/900 [00:06<02:59,  4.84it/s][2026-01-17 12:52:39] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   3%|▎         | 30/900 [00:06<02:48,  5.18it/s][2026-01-17 12:52:41] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   3%|▎         | 31/900 [00:09<14:00,  1.03it/s][2026-01-17 12:52:42] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▎         | 32/900 [00:10<13:52,  1.04it/s][2026-01-17 12:52:44] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▎         | 33/900 [00:12<18:10,  1.26s/it][2026-01-17 12:52:47] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:48] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 34/900 [00:15<26:55,  1.87s/it][2026-01-17 12:52:48] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:48] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 35/900 [00:15<20:35,  1.43s/it][2026-01-17 12:52:48] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:48] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 3.23, #queue-req: 0,
[2026-01-17 12:52:48] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 36/900 [00:16<15:11,  1.05s/it][2026-01-17 12:52:49] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 37/900 [00:16<12:35,  1.14it/s][2026-01-17 12:52:51] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 38/900 [00:19<19:37,  1.37s/it][2026-01-17 12:52:53] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 39/900 [00:21<24:10,  1.69s/it][2026-01-17 12:52:54] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 40/900 [00:21<17:40,  1.23s/it][2026-01-17 12:52:55] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   5%|▍         | 41/900 [00:23<19:40,  1.37s/it][2026-01-17 12:52:56] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   5%|▍         | 42/900 [00:23<16:09,  1.13s/it][2026-01-17 12:52:58] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:52:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   5%|▍         | 43/900 [00:26<22:22,  1.57s/it][2026-01-17 12:53:01] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   5%|▍         | 44/900 [00:29<27:04,  1.90s/it][2026-01-17 12:53:04] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   5%|▌         | 45/900 [00:31<29:59,  2.10s/it][2026-01-17 12:53:05] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   5%|▌         | 46/900 [00:33<26:16,  1.85s/it][2026-01-17 12:53:07] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   5%|▌         | 47/900 [00:34<25:48,  1.82s/it][2026-01-17 12:53:09] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   5%|▌         | 48/900 [00:36<26:55,  1.90s/it][2026-01-17 12:53:10] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   5%|▌         | 49/900 [00:38<23:48,  1.68s/it][2026-01-17 12:53:12] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 50/900 [00:40<26:56,  1.90s/it][2026-01-17 12:53:13] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 51/900 [00:40<20:17,  1.43s/it][2026-01-17 12:53:15] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 52/900 [00:43<25:33,  1.81s/it][2026-01-17 12:53:16] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 53/900 [00:44<22:21,  1.58s/it][2026-01-17 12:53:19] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 54/900 [00:47<27:04,  1.92s/it][2026-01-17 12:53:20] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 55/900 [00:48<22:26,  1.59s/it][2026-01-17 12:53:20] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:21] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 1.24, #queue-req: 0,
[2026-01-17 12:53:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 56/900 [00:48<16:43,  1.19s/it][2026-01-17 12:53:21] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▋         | 57/900 [00:48<14:14,  1.01s/it][2026-01-17 12:53:23] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▋         | 58/900 [00:51<18:47,  1.34s/it][2026-01-17 12:53:25] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   7%|▋         | 59/900 [00:53<22:42,  1.62s/it][2026-01-17 12:53:27] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   7%|▋         | 60/900 [00:55<24:50,  1.77s/it][2026-01-17 12:53:28] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   7%|▋         | 61/900 [00:55<18:11,  1.30s/it][2026-01-17 12:53:28] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   7%|▋         | 62/900 [00:55<13:31,  1.03it/s][2026-01-17 12:53:28] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   7%|▋         | 63/900 [00:56<10:15,  1.36it/s][2026-01-17 12:53:28] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   7%|▋         | 64/900 [00:56<07:58,  1.75it/s][2026-01-17 12:53:28] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   7%|▋         | 65/900 [00:56<06:17,  2.21it/s][2026-01-17 12:53:29] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   7%|▋         | 66/900 [00:56<05:15,  2.64it/s][2026-01-17 12:53:29] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   7%|▋         | 67/900 [00:56<04:35,  3.03it/s][2026-01-17 12:53:29] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 68/900 [00:57<04:01,  3.45it/s][2026-01-17 12:53:29] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 69/900 [00:57<03:35,  3.86it/s][2026-01-17 12:53:29] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 70/900 [00:57<03:12,  4.32it/s][2026-01-17 12:53:30] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 71/900 [00:57<03:08,  4.40it/s][2026-01-17 12:53:30] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 72/900 [00:57<03:11,  4.32it/s][2026-01-17 12:53:30] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 73/900 [00:58<03:02,  4.53it/s][2026-01-17 12:53:30] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:30] Decode batch, #running-req: 1, #token: 768, token usage: 0.02, npu graph: True, gen throughput (token/s): 4.03, #queue-req: 0,
[2026-01-17 12:53:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 74/900 [00:58<03:12,  4.28it/s][2026-01-17 12:53:31] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 75/900 [00:58<03:03,  4.50it/s][2026-01-17 12:53:31] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 76/900 [00:58<02:54,  4.72it/s][2026-01-17 12:53:31] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   9%|▊         | 77/900 [00:58<02:55,  4.69it/s][2026-01-17 12:53:31] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   9%|▊         | 78/900 [00:59<03:03,  4.48it/s][2026-01-17 12:53:31] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   9%|▉         | 79/900 [00:59<03:11,  4.29it/s][2026-01-17 12:53:32] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   9%|▉         | 80/900 [00:59<03:18,  4.14it/s][2026-01-17 12:53:32] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   9%|▉         | 81/900 [00:59<03:22,  4.04it/s][2026-01-17 12:53:32] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   9%|▉         | 82/900 [01:00<03:22,  4.04it/s][2026-01-17 12:53:32] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   9%|▉         | 83/900 [01:00<03:22,  4.04it/s][2026-01-17 12:53:33] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   9%|▉         | 84/900 [01:00<03:38,  3.73it/s][2026-01-17 12:53:33] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   9%|▉         | 85/900 [01:00<03:39,  3.71it/s][2026-01-17 12:53:33] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|▉         | 86/900 [01:01<03:31,  3.84it/s][2026-01-17 12:53:33] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|▉         | 87/900 [01:01<03:15,  4.15it/s][2026-01-17 12:53:34] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|▉         | 88/900 [01:01<03:12,  4.23it/s][2026-01-17 12:53:34] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|▉         | 89/900 [01:01<03:14,  4.18it/s][2026-01-17 12:53:34] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 90/900 [01:02<03:16,  4.12it/s][2026-01-17 12:53:35] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 91/900 [01:02<04:16,  3.16it/s][2026-01-17 12:53:35] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 92/900 [01:03<05:12,  2.59it/s][2026-01-17 12:53:36] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 93/900 [01:04<06:58,  1.93it/s][2026-01-17 12:53:36] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 6.95, #queue-req: 0,
[2026-01-17 12:53:37] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 94/900 [01:04<07:24,  1.81it/s][2026-01-17 12:53:37] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  11%|█         | 95/900 [01:05<08:04,  1.66it/s][2026-01-17 12:53:38] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  11%|█         | 96/900 [01:06<08:39,  1.55it/s][2026-01-17 12:53:39] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  11%|█         | 97/900 [01:06<08:17,  1.61it/s][2026-01-17 12:53:39] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  11%|█         | 98/900 [01:07<07:31,  1.77it/s][2026-01-17 12:53:40] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  11%|█         | 99/900 [01:07<07:25,  1.80it/s][2026-01-17 12:53:40] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  11%|█         | 100/900 [01:07<06:27,  2.06it/s][2026-01-17 12:53:41] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  11%|█         | 101/900 [01:08<07:57,  1.67it/s][2026-01-17 12:53:41] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  11%|█▏        | 102/900 [01:09<08:03,  1.65it/s][2026-01-17 12:53:42] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  11%|█▏        | 103/900 [01:09<07:44,  1.72it/s][2026-01-17 12:53:43] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:43] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 104/900 [01:11<09:28,  1.40it/s][2026-01-17 12:53:44] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 105/900 [01:11<09:26,  1.40it/s][2026-01-17 12:53:44] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 106/900 [01:12<08:27,  1.56it/s][2026-01-17 12:53:45] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 107/900 [01:12<08:42,  1.52it/s][2026-01-17 12:53:46] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 108/900 [01:13<08:49,  1.50it/s][2026-01-17 12:53:46] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 109/900 [01:14<08:13,  1.60it/s][2026-01-17 12:53:47] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:47] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 110/900 [01:15<10:11,  1.29it/s][2026-01-17 12:53:48] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:48] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 111/900 [01:15<10:05,  1.30it/s][2026-01-17 12:53:49] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 112/900 [01:16<09:26,  1.39it/s][2026-01-17 12:53:49] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  13%|█▎        | 113/900 [01:17<10:00,  1.31it/s][2026-01-17 12:53:50] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 2.98, #queue-req: 0,
[2026-01-17 12:53:50] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  13%|█▎        | 114/900 [01:17<08:30,  1.54it/s][2026-01-17 12:53:50] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  13%|█▎        | 115/900 [01:18<07:19,  1.79it/s][2026-01-17 12:53:51] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  13%|█▎        | 116/900 [01:18<08:11,  1.59it/s][2026-01-17 12:53:52] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  13%|█▎        | 117/900 [01:19<09:26,  1.38it/s][2026-01-17 12:53:53] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  13%|█▎        | 118/900 [01:20<09:30,  1.37it/s][2026-01-17 12:53:53] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  13%|█▎        | 119/900 [01:21<08:42,  1.49it/s][2026-01-17 12:53:54] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  13%|█▎        | 120/900 [01:22<10:21,  1.25it/s][2026-01-17 12:53:56] Prefill batch, #new-seq: 1, #new-token: 3968, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  13%|█▎        | 121/900 [01:24<15:11,  1.17s/it][2026-01-17 12:53:57] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▎        | 122/900 [01:25<13:31,  1.04s/it][2026-01-17 12:53:58] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▎        | 123/900 [01:25<11:57,  1.08it/s][2026-01-17 12:53:58] Prefill batch, #new-seq: 1, #new-token: 2944, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:53:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 124/900 [01:26<11:34,  1.12it/s][2026-01-17 12:54:00] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 125/900 [01:27<12:33,  1.03it/s][2026-01-17 12:54:00] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 126/900 [01:28<11:51,  1.09it/s][2026-01-17 12:54:01] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 127/900 [01:29<12:16,  1.05it/s][2026-01-17 12:54:02] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 128/900 [01:30<10:41,  1.20it/s][2026-01-17 12:54:02] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 129/900 [01:30<08:39,  1.48it/s][2026-01-17 12:54:03] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 130/900 [01:30<08:25,  1.52it/s][2026-01-17 12:54:03] Prefill batch, #new-seq: 1, #new-token: 2944, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  15%|█▍        | 131/900 [01:31<08:15,  1.55it/s][2026-01-17 12:54:05] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  15%|█▍        | 132/900 [01:32<10:02,  1.27it/s][2026-01-17 12:54:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1792, token usage: 0.04, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  15%|█▍        | 133/900 [01:33<09:56,  1.29it/s][2026-01-17 12:54:06] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 2.50, #queue-req: 0,
[2026-01-17 12:54:06] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  15%|█▍        | 134/900 [01:34<10:24,  1.23it/s][2026-01-17 12:54:07] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1920, token usage: 0.04, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  15%|█▌        | 135/900 [01:35<11:03,  1.15it/s][2026-01-17 12:54:08] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 2816, token usage: 0.06, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  15%|█▌        | 136/900 [01:35<09:42,  1.31it/s][2026-01-17 12:54:08] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  15%|█▌        | 137/900 [01:36<09:08,  1.39it/s][2026-01-17 12:54:09] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  15%|█▌        | 138/900 [01:37<09:09,  1.39it/s][2026-01-17 12:54:10] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  15%|█▌        | 139/900 [01:37<08:57,  1.42it/s][2026-01-17 12:54:11] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 140/900 [01:38<09:01,  1.40it/s][2026-01-17 12:54:11] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 141/900 [01:39<08:30,  1.49it/s][2026-01-17 12:54:12] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:12] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 142/900 [01:39<08:30,  1.48it/s][2026-01-17 12:54:12] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 143/900 [01:40<08:22,  1.51it/s][2026-01-17 12:54:13] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 144/900 [01:41<07:51,  1.60it/s][2026-01-17 12:54:13] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 145/900 [01:41<07:08,  1.76it/s][2026-01-17 12:54:14] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1408, token usage: 0.03, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 146/900 [01:42<07:24,  1.70it/s][2026-01-17 12:54:15] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▋        | 147/900 [01:42<07:40,  1.64it/s][2026-01-17 12:54:15] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▋        | 148/900 [01:43<08:18,  1.51it/s][2026-01-17 12:54:16] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  17%|█▋        | 149/900 [01:43<07:24,  1.69it/s][2026-01-17 12:54:17] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  17%|█▋        | 150/900 [01:44<08:03,  1.55it/s][2026-01-17 12:54:17] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  17%|█▋        | 151/900 [01:45<07:46,  1.60it/s][2026-01-17 12:54:18] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  17%|█▋        | 152/900 [01:45<06:35,  1.89it/s][2026-01-17 12:54:18] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  17%|█▋        | 153/900 [01:45<05:30,  2.26it/s][2026-01-17 12:54:18] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 3.22, #queue-req: 0,
[2026-01-17 12:54:18] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  17%|█▋        | 154/900 [01:46<04:48,  2.59it/s][2026-01-17 12:54:18] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  17%|█▋        | 155/900 [01:46<04:23,  2.82it/s][2026-01-17 12:54:19] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  17%|█▋        | 156/900 [01:46<04:54,  2.53it/s][2026-01-17 12:54:19] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  17%|█▋        | 157/900 [01:47<04:29,  2.75it/s][2026-01-17 12:54:19] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 158/900 [01:47<04:06,  3.01it/s][2026-01-17 12:54:20] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 159/900 [01:47<03:51,  3.21it/s][2026-01-17 12:54:20] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 160/900 [01:47<03:40,  3.36it/s][2026-01-17 12:54:20] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 161/900 [01:48<03:28,  3.54it/s][2026-01-17 12:54:20] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 162/900 [01:48<03:21,  3.67it/s][2026-01-17 12:54:21] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 163/900 [01:48<03:18,  3.71it/s][2026-01-17 12:54:21] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 164/900 [01:49<04:10,  2.94it/s][2026-01-17 12:54:21] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 165/900 [01:49<03:53,  3.14it/s][2026-01-17 12:54:22] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 166/900 [01:49<03:40,  3.33it/s][2026-01-17 12:54:22] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  19%|█▊        | 167/900 [01:50<03:33,  3.44it/s][2026-01-17 12:54:22] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  19%|█▊        | 168/900 [01:50<03:43,  3.28it/s][2026-01-17 12:54:23] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  19%|█▉        | 169/900 [01:50<03:35,  3.40it/s][2026-01-17 12:54:23] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  19%|█▉        | 170/900 [01:50<03:24,  3.57it/s][2026-01-17 12:54:23] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:23] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 7.59, #queue-req: 0,
[2026-01-17 12:54:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  19%|█▉        | 171/900 [01:51<03:20,  3.64it/s][2026-01-17 12:54:23] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  19%|█▉        | 172/900 [01:51<03:40,  3.30it/s][2026-01-17 12:54:24] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  19%|█▉        | 173/900 [01:51<03:27,  3.51it/s][2026-01-17 12:54:24] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  19%|█▉        | 174/900 [01:52<03:37,  3.34it/s][2026-01-17 12:54:24] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  19%|█▉        | 175/900 [01:52<03:30,  3.45it/s][2026-01-17 12:54:25] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|█▉        | 176/900 [01:52<03:19,  3.63it/s][2026-01-17 12:54:25] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|█▉        | 177/900 [01:52<03:14,  3.71it/s][2026-01-17 12:54:25] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|█▉        | 178/900 [01:53<03:13,  3.74it/s][2026-01-17 12:54:25] Prefill batch, #new-seq: 1, #new-token: 2304, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:26] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|█▉        | 179/900 [01:53<03:48,  3.16it/s][2026-01-17 12:54:26] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:26] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 180/900 [01:53<03:21,  3.57it/s][2026-01-17 12:54:26] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 181/900 [01:54<04:24,  2.72it/s][2026-01-17 12:54:27] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 182/900 [01:54<04:10,  2.86it/s][2026-01-17 12:54:27] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 183/900 [01:55<04:14,  2.82it/s][2026-01-17 12:54:27] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 184/900 [01:55<03:52,  3.08it/s][2026-01-17 12:54:28] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  21%|██        | 185/900 [01:55<03:52,  3.08it/s][2026-01-17 12:54:28] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  21%|██        | 186/900 [01:55<03:33,  3.35it/s][2026-01-17 12:54:28] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  21%|██        | 187/900 [01:56<03:56,  3.01it/s][2026-01-17 12:54:29] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  21%|██        | 188/900 [01:56<03:52,  3.07it/s][2026-01-17 12:54:29] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  21%|██        | 189/900 [01:57<04:25,  2.68it/s][2026-01-17 12:54:29] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  21%|██        | 190/900 [01:57<03:44,  3.17it/s][2026-01-17 12:54:29] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 6.60, #queue-req: 0,
[2026-01-17 12:54:29] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  21%|██        | 191/900 [01:57<03:31,  3.36it/s][2026-01-17 12:54:30] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  21%|██▏       | 192/900 [01:57<03:27,  3.41it/s][2026-01-17 12:54:30] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  21%|██▏       | 193/900 [01:58<04:03,  2.90it/s][2026-01-17 12:54:30] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 194/900 [01:58<03:41,  3.19it/s][2026-01-17 12:54:31] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 195/900 [01:58<04:03,  2.89it/s][2026-01-17 12:54:31] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 196/900 [01:59<03:54,  3.00it/s][2026-01-17 12:54:31] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 197/900 [01:59<03:39,  3.20it/s][2026-01-17 12:54:32] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 198/900 [01:59<03:27,  3.38it/s][2026-01-17 12:54:32] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 199/900 [02:00<04:41,  2.49it/s][2026-01-17 12:54:33] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 200/900 [02:00<04:23,  2.65it/s][2026-01-17 12:54:33] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 201/900 [02:01<05:08,  2.26it/s][2026-01-17 12:54:34] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 202/900 [02:01<05:10,  2.25it/s][2026-01-17 12:54:34] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  23%|██▎       | 203/900 [02:02<05:48,  2.00it/s][2026-01-17 12:54:35] Prefill batch, #new-seq: 1, #new-token: 1408, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  23%|██▎       | 204/900 [02:02<05:44,  2.02it/s][2026-01-17 12:54:35] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  23%|██▎       | 205/900 [02:03<05:21,  2.16it/s][2026-01-17 12:54:36] Prefill batch, #new-seq: 1, #new-token: 2304, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  23%|██▎       | 206/900 [02:03<05:48,  1.99it/s][2026-01-17 12:54:36] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  23%|██▎       | 207/900 [02:04<05:24,  2.14it/s][2026-01-17 12:54:37] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  23%|██▎       | 208/900 [02:04<05:49,  1.98it/s][2026-01-17 12:54:37] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  23%|██▎       | 209/900 [02:05<05:42,  2.02it/s][2026-01-17 12:54:38] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  23%|██▎       | 210/900 [02:05<05:54,  1.94it/s][2026-01-17 12:54:38] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 4.65, #queue-req: 0,
[2026-01-17 12:54:38] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  23%|██▎       | 211/900 [02:06<05:08,  2.23it/s][2026-01-17 12:54:38] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▎       | 212/900 [02:06<04:26,  2.58it/s][2026-01-17 12:54:39] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▎       | 213/900 [02:06<03:40,  3.12it/s][2026-01-17 12:54:39] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 214/900 [02:06<03:48,  3.00it/s][2026-01-17 12:54:39] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 215/900 [02:07<03:29,  3.27it/s][2026-01-17 12:54:39] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 216/900 [02:07<03:43,  3.06it/s][2026-01-17 12:54:40] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 217/900 [02:07<03:26,  3.31it/s][2026-01-17 12:54:40] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 218/900 [02:07<02:57,  3.84it/s][2026-01-17 12:54:40] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 219/900 [02:08<02:58,  3.82it/s][2026-01-17 12:54:41] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 220/900 [02:08<03:29,  3.25it/s][2026-01-17 12:54:41] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  25%|██▍       | 221/900 [02:08<03:46,  3.00it/s][2026-01-17 12:54:41] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  25%|██▍       | 222/900 [02:09<04:03,  2.78it/s][2026-01-17 12:54:42] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:42] Decode batch, #running-req: 1, #token: 1024, token usage: 0.02, npu graph: True, gen throughput (token/s): 10.08, #queue-req: 0,
[2026-01-17 12:54:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  25%|██▍       | 223/900 [02:09<04:32,  2.49it/s][2026-01-17 12:54:42] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  25%|██▍       | 224/900 [02:10<04:02,  2.79it/s][2026-01-17 12:54:42] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:43] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  25%|██▌       | 225/900 [02:10<03:53,  2.89it/s][2026-01-17 12:54:43] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:43] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  25%|██▌       | 226/900 [02:10<03:33,  3.15it/s][2026-01-17 12:54:43] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:43] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  25%|██▌       | 227/900 [02:11<03:24,  3.30it/s][2026-01-17 12:54:43] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  25%|██▌       | 228/900 [02:11<03:47,  2.95it/s][2026-01-17 12:54:44] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  25%|██▌       | 229/900 [02:11<03:37,  3.09it/s][2026-01-17 12:54:44] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 230/900 [02:11<03:25,  3.26it/s][2026-01-17 12:54:44] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 231/900 [02:12<03:03,  3.64it/s][2026-01-17 12:54:44] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 232/900 [02:12<03:13,  3.45it/s][2026-01-17 12:54:45] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 233/900 [02:12<03:29,  3.19it/s][2026-01-17 12:54:45] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 234/900 [02:13<03:18,  3.35it/s][2026-01-17 12:54:45] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:46] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 10.94, #queue-req: 0,
[2026-01-17 12:54:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 235/900 [02:13<03:22,  3.29it/s][2026-01-17 12:54:46] Prefill batch, #new-seq: 1, #new-token: 1792, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 236/900 [02:13<03:36,  3.07it/s][2026-01-17 12:54:46] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▋       | 237/900 [02:14<03:23,  3.26it/s][2026-01-17 12:54:46] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:47] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▋       | 238/900 [02:14<03:11,  3.46it/s][2026-01-17 12:54:47] Prefill batch, #new-seq: 1, #new-token: 1792, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:47] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  27%|██▋       | 239/900 [02:14<03:25,  3.21it/s][2026-01-17 12:54:47] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:47] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  27%|██▋       | 240/900 [02:14<03:18,  3.33it/s][2026-01-17 12:54:47] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:47] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  27%|██▋       | 241/900 [02:15<03:22,  3.26it/s][2026-01-17 12:54:48] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:48] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  27%|██▋       | 242/900 [02:15<04:19,  2.54it/s][2026-01-17 12:54:49] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  27%|██▋       | 243/900 [02:16<05:39,  1.93it/s][2026-01-17 12:54:49] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  27%|██▋       | 244/900 [02:17<06:16,  1.74it/s][2026-01-17 12:54:50] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  27%|██▋       | 245/900 [02:17<04:58,  2.19it/s][2026-01-17 12:54:50] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  27%|██▋       | 246/900 [02:17<04:30,  2.42it/s][2026-01-17 12:54:50] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  27%|██▋       | 247/900 [02:18<04:07,  2.64it/s][2026-01-17 12:54:50] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 248/900 [02:18<03:52,  2.80it/s][2026-01-17 12:54:51] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 249/900 [02:19<04:18,  2.52it/s][2026-01-17 12:54:51] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 250/900 [02:19<04:01,  2.70it/s][2026-01-17 12:54:52] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 251/900 [02:19<04:00,  2.70it/s][2026-01-17 12:54:52] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 252/900 [02:19<03:34,  3.02it/s][2026-01-17 12:54:52] Prefill batch, #new-seq: 1, #new-token: 1408, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 253/900 [02:20<04:10,  2.58it/s][2026-01-17 12:54:53] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:53] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 5.51, #queue-req: 0,
[2026-01-17 12:54:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 254/900 [02:20<03:50,  2.80it/s][2026-01-17 12:54:53] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 255/900 [02:20<03:30,  3.06it/s][2026-01-17 12:54:53] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 256/900 [02:21<03:24,  3.15it/s][2026-01-17 12:54:54] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  29%|██▊       | 257/900 [02:21<03:22,  3.18it/s][2026-01-17 12:54:54] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  29%|██▊       | 258/900 [02:22<04:13,  2.53it/s][2026-01-17 12:54:54] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  29%|██▉       | 259/900 [02:22<03:38,  2.93it/s][2026-01-17 12:54:55] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  29%|██▉       | 260/900 [02:22<04:18,  2.48it/s][2026-01-17 12:54:55] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  29%|██▉       | 261/900 [02:23<03:51,  2.76it/s][2026-01-17 12:54:55] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  29%|██▉       | 262/900 [02:23<03:27,  3.08it/s][2026-01-17 12:54:56] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  29%|██▉       | 263/900 [02:24<04:24,  2.41it/s][2026-01-17 12:54:56] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  29%|██▉       | 264/900 [02:24<04:03,  2.61it/s][2026-01-17 12:54:57] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  29%|██▉       | 265/900 [02:24<03:48,  2.77it/s][2026-01-17 12:54:57] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|██▉       | 266/900 [02:24<03:29,  3.02it/s][2026-01-17 12:54:57] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|██▉       | 267/900 [02:25<03:24,  3.09it/s][2026-01-17 12:54:58] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|██▉       | 268/900 [02:25<03:20,  3.16it/s][2026-01-17 12:54:58] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|██▉       | 269/900 [02:25<03:31,  2.99it/s][2026-01-17 12:54:58] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 270/900 [02:26<03:14,  3.24it/s][2026-01-17 12:54:58] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 271/900 [02:26<03:06,  3.38it/s][2026-01-17 12:54:59] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 272/900 [02:26<02:58,  3.53it/s][2026-01-17 12:54:59] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:54:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 273/900 [02:27<03:39,  2.85it/s][2026-01-17 12:54:59] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:00] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 5.90, #queue-req: 0,
[2026-01-17 12:55:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 274/900 [02:27<03:31,  2.96it/s][2026-01-17 12:55:00] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  31%|███       | 275/900 [02:27<03:23,  3.07it/s][2026-01-17 12:55:00] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  31%|███       | 276/900 [02:28<03:13,  3.23it/s][2026-01-17 12:55:00] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  31%|███       | 277/900 [02:28<03:01,  3.43it/s][2026-01-17 12:55:01] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  31%|███       | 278/900 [02:28<02:57,  3.51it/s][2026-01-17 12:55:01] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  31%|███       | 279/900 [02:28<03:12,  3.23it/s][2026-01-17 12:55:01] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  31%|███       | 280/900 [02:29<02:45,  3.75it/s][2026-01-17 12:55:01] Prefill batch, #new-seq: 1, #new-token: 1792, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  31%|███       | 281/900 [02:29<03:03,  3.37it/s][2026-01-17 12:55:02] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  31%|███▏      | 282/900 [02:29<02:58,  3.46it/s][2026-01-17 12:55:02] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  31%|███▏      | 283/900 [02:29<02:41,  3.82it/s][2026-01-17 12:55:02] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 284/900 [02:30<02:42,  3.78it/s][2026-01-17 12:55:03] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 285/900 [02:30<03:23,  3.02it/s][2026-01-17 12:55:03] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 286/900 [02:31<03:13,  3.17it/s][2026-01-17 12:55:03] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 287/900 [02:31<03:02,  3.35it/s][2026-01-17 12:55:03] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 288/900 [02:31<02:53,  3.53it/s][2026-01-17 12:55:04] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 289/900 [02:31<02:50,  3.58it/s][2026-01-17 12:55:04] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 290/900 [02:32<03:34,  2.84it/s][2026-01-17 12:55:05] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 291/900 [02:32<03:38,  2.79it/s][2026-01-17 12:55:05] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 292/900 [02:32<03:31,  2.87it/s][2026-01-17 12:55:05] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:06] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 6.85, #queue-req: 0,
[2026-01-17 12:55:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  33%|███▎      | 293/900 [02:33<03:31,  2.86it/s][2026-01-17 12:55:06] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  33%|███▎      | 294/900 [02:33<04:09,  2.43it/s][2026-01-17 12:55:06] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  33%|███▎      | 295/900 [02:34<03:44,  2.69it/s][2026-01-17 12:55:06] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  33%|███▎      | 296/900 [02:34<03:42,  2.71it/s][2026-01-17 12:55:07] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  33%|███▎      | 297/900 [02:34<03:18,  3.04it/s][2026-01-17 12:55:07] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  33%|███▎      | 298/900 [02:35<03:30,  2.87it/s][2026-01-17 12:55:07] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  33%|███▎      | 299/900 [02:35<03:13,  3.11it/s][2026-01-17 12:55:08] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  33%|███▎      | 300/900 [02:35<03:11,  3.14it/s][2026-01-17 12:55:08] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  33%|███▎      | 301/900 [02:36<03:16,  3.04it/s][2026-01-17 12:55:08] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▎      | 302/900 [02:36<02:59,  3.33it/s][2026-01-17 12:55:09] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▎      | 303/900 [02:36<02:47,  3.57it/s][2026-01-17 12:55:09] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 304/900 [02:37<04:29,  2.21it/s][2026-01-17 12:55:10] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 305/900 [02:38<05:07,  1.93it/s][2026-01-17 12:55:10] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 306/900 [02:38<04:29,  2.20it/s][2026-01-17 12:55:11] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 307/900 [02:38<03:49,  2.59it/s][2026-01-17 12:55:12] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:12] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 308/900 [02:39<05:50,  1.69it/s][2026-01-17 12:55:12] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 309/900 [02:40<06:33,  1.50it/s][2026-01-17 12:55:13] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 310/900 [02:41<06:00,  1.64it/s][2026-01-17 12:55:14] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  35%|███▍      | 311/900 [02:41<06:08,  1.60it/s][2026-01-17 12:55:14] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  35%|███▍      | 312/900 [02:42<05:21,  1.83it/s][2026-01-17 12:55:14] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 4.60, #queue-req: 0,
[2026-01-17 12:55:14] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  35%|███▍      | 313/900 [02:42<04:25,  2.21it/s][2026-01-17 12:55:15] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  35%|███▍      | 314/900 [02:42<04:39,  2.10it/s][2026-01-17 12:55:16] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1920, token usage: 0.04, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  35%|███▌      | 315/900 [02:43<06:13,  1.56it/s][2026-01-17 12:55:16] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  35%|███▌      | 316/900 [02:44<05:25,  1.79it/s][2026-01-17 12:55:17] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  35%|███▌      | 317/900 [02:44<05:09,  1.88it/s][2026-01-17 12:55:17] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  35%|███▌      | 318/900 [02:45<05:01,  1.93it/s][2026-01-17 12:55:17] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  35%|███▌      | 319/900 [02:45<04:30,  2.15it/s][2026-01-17 12:55:18] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 320/900 [02:45<03:54,  2.47it/s][2026-01-17 12:55:18] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 321/900 [02:46<03:32,  2.73it/s][2026-01-17 12:55:18] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 322/900 [02:46<03:14,  2.97it/s][2026-01-17 12:55:19] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 323/900 [02:47<04:31,  2.13it/s][2026-01-17 12:55:19] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 324/900 [02:47<03:52,  2.48it/s][2026-01-17 12:55:20] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 325/900 [02:47<03:35,  2.67it/s][2026-01-17 12:55:20] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 326/900 [02:47<03:13,  2.96it/s][2026-01-17 12:55:20] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▋      | 327/900 [02:48<03:28,  2.75it/s][2026-01-17 12:55:21] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▋      | 328/900 [02:48<03:32,  2.69it/s][2026-01-17 12:55:21] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  37%|███▋      | 329/900 [02:49<03:42,  2.57it/s][2026-01-17 12:55:22] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  37%|███▋      | 330/900 [02:49<03:44,  2.54it/s][2026-01-17 12:55:22] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  37%|███▋      | 331/900 [02:50<05:27,  1.74it/s][2026-01-17 12:55:23] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  37%|███▋      | 332/900 [02:50<04:28,  2.12it/s][2026-01-17 12:55:23] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 4.58, #queue-req: 0,
[2026-01-17 12:55:24] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  37%|███▋      | 333/900 [02:52<06:46,  1.39it/s][2026-01-17 12:55:24] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  37%|███▋      | 334/900 [02:52<06:03,  1.56it/s][2026-01-17 12:55:25] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  37%|███▋      | 335/900 [02:52<05:28,  1.72it/s][2026-01-17 12:55:25] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  37%|███▋      | 336/900 [02:53<04:49,  1.95it/s][2026-01-17 12:55:26] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  37%|███▋      | 337/900 [02:54<06:35,  1.42it/s][2026-01-17 12:55:27] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 338/900 [02:54<05:16,  1.78it/s][2026-01-17 12:55:28] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 339/900 [02:55<06:55,  1.35it/s][2026-01-17 12:55:28] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 340/900 [02:56<06:47,  1.37it/s][2026-01-17 12:55:29] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 341/900 [02:57<07:20,  1.27it/s][2026-01-17 12:55:30] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 342/900 [02:57<05:46,  1.61it/s][2026-01-17 12:55:30] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 343/900 [02:58<05:53,  1.58it/s][2026-01-17 12:55:32] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 344/900 [02:59<07:31,  1.23it/s][2026-01-17 12:55:32] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 345/900 [03:00<06:28,  1.43it/s][2026-01-17 12:55:32] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 346/900 [03:00<05:41,  1.62it/s][2026-01-17 12:55:33] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  39%|███▊      | 347/900 [03:00<05:02,  1.83it/s][2026-01-17 12:55:33] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  39%|███▊      | 348/900 [03:01<05:24,  1.70it/s][2026-01-17 12:55:35] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  39%|███▉      | 349/900 [03:02<06:47,  1.35it/s][2026-01-17 12:55:36] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  39%|███▉      | 350/900 [03:03<07:44,  1.18it/s][2026-01-17 12:55:36] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  39%|███▉      | 351/900 [03:03<06:01,  1.52it/s][2026-01-17 12:55:36] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  39%|███▉      | 352/900 [03:04<05:07,  1.78it/s][2026-01-17 12:55:36] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 2.96, #queue-req: 0,
[2026-01-17 12:55:36] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  39%|███▉      | 353/900 [03:04<04:11,  2.17it/s][2026-01-17 12:55:38] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  39%|███▉      | 354/900 [03:05<06:15,  1.45it/s][2026-01-17 12:55:39] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  39%|███▉      | 355/900 [03:06<07:40,  1.18it/s][2026-01-17 12:55:39] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|███▉      | 356/900 [03:07<06:39,  1.36it/s][2026-01-17 12:55:40] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|███▉      | 357/900 [03:08<06:18,  1.43it/s][2026-01-17 12:55:40] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|███▉      | 358/900 [03:08<05:45,  1.57it/s][2026-01-17 12:55:41] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|███▉      | 359/900 [03:08<04:56,  1.82it/s][2026-01-17 12:55:42] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 360/900 [03:10<06:59,  1.29it/s][2026-01-17 12:55:42] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:43] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 361/900 [03:10<05:32,  1.62it/s][2026-01-17 12:55:43] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:43] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 362/900 [03:10<05:09,  1.74it/s][2026-01-17 12:55:43] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:43] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 363/900 [03:11<04:25,  2.03it/s][2026-01-17 12:55:43] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 364/900 [03:11<03:46,  2.37it/s][2026-01-17 12:55:44] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  41%|████      | 365/900 [03:11<03:08,  2.84it/s][2026-01-17 12:55:44] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  41%|████      | 366/900 [03:11<02:55,  3.05it/s][2026-01-17 12:55:44] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  41%|████      | 367/900 [03:12<02:49,  3.15it/s][2026-01-17 12:55:44] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  41%|████      | 368/900 [03:12<02:39,  3.34it/s][2026-01-17 12:55:45] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  41%|████      | 369/900 [03:12<02:40,  3.31it/s][2026-01-17 12:55:45] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  41%|████      | 370/900 [03:13<02:31,  3.50it/s][2026-01-17 12:55:45] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  41%|████      | 371/900 [03:13<02:15,  3.91it/s][2026-01-17 12:55:45] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  41%|████▏     | 372/900 [03:13<02:20,  3.77it/s][2026-01-17 12:55:46] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 4.34, #queue-req: 0,
[2026-01-17 12:55:46] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  41%|████▏     | 373/900 [03:13<02:15,  3.89it/s][2026-01-17 12:55:46] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 374/900 [03:14<02:19,  3.78it/s][2026-01-17 12:55:46] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 375/900 [03:14<02:23,  3.66it/s][2026-01-17 12:55:47] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:47] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 376/900 [03:14<02:10,  4.01it/s][2026-01-17 12:55:47] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:47] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 377/900 [03:14<02:23,  3.64it/s][2026-01-17 12:55:47] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:47] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 378/900 [03:15<02:16,  3.83it/s][2026-01-17 12:55:47] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:48] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 379/900 [03:15<02:18,  3.76it/s][2026-01-17 12:55:48] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:48] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 380/900 [03:15<02:26,  3.54it/s][2026-01-17 12:55:48] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:48] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 381/900 [03:16<02:42,  3.20it/s][2026-01-17 12:55:48] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:48] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 382/900 [03:16<02:36,  3.30it/s][2026-01-17 12:55:49] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 512, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  43%|████▎     | 383/900 [03:16<02:29,  3.46it/s][2026-01-17 12:55:49] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  43%|████▎     | 384/900 [03:16<02:38,  3.26it/s][2026-01-17 12:55:49] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 1024, token usage: 0.02, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  43%|████▎     | 385/900 [03:17<02:30,  3.43it/s][2026-01-17 12:55:49] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  43%|████▎     | 386/900 [03:17<02:30,  3.41it/s][2026-01-17 12:55:50] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  43%|████▎     | 387/900 [03:17<02:19,  3.69it/s][2026-01-17 12:55:50] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  43%|████▎     | 388/900 [03:17<02:21,  3.62it/s][2026-01-17 12:55:50] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  43%|████▎     | 389/900 [03:18<02:37,  3.24it/s][2026-01-17 12:55:51] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  43%|████▎     | 390/900 [03:18<02:25,  3.51it/s][2026-01-17 12:55:51] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  43%|████▎     | 391/900 [03:18<02:20,  3.62it/s][2026-01-17 12:55:51] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:51] Decode batch, #running-req: 1, #token: 768, token usage: 0.02, npu graph: True, gen throughput (token/s): 7.13, #queue-req: 0,
[2026-01-17 12:55:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▎     | 392/900 [03:19<02:22,  3.55it/s][2026-01-17 12:55:51] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▎     | 393/900 [03:19<02:09,  3.93it/s][2026-01-17 12:55:52] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 394/900 [03:19<02:34,  3.29it/s][2026-01-17 12:55:52] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 395/900 [03:20<02:30,  3.36it/s][2026-01-17 12:55:52] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 396/900 [03:20<02:22,  3.53it/s][2026-01-17 12:55:53] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 397/900 [03:20<02:16,  3.70it/s][2026-01-17 12:55:53] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 398/900 [03:20<02:13,  3.77it/s][2026-01-17 12:55:53] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 399/900 [03:21<02:10,  3.85it/s][2026-01-17 12:55:53] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 400/900 [03:21<01:59,  4.19it/s][2026-01-17 12:55:53] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:54] Decode batch, #running-req: 1, #token: 768, token usage: 0.02, npu graph: True, gen throughput (token/s): 16.67, #queue-req: 0,
[2026-01-17 12:55:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  45%|████▍     | 401/900 [03:21<02:18,  3.62it/s][2026-01-17 12:55:54] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  45%|████▍     | 402/900 [03:21<02:15,  3.67it/s][2026-01-17 12:55:54] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  45%|████▍     | 403/900 [03:22<02:10,  3.82it/s][2026-01-17 12:55:54] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  45%|████▍     | 404/900 [03:22<02:07,  3.90it/s][2026-01-17 12:55:55] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  45%|████▌     | 405/900 [03:22<02:01,  4.07it/s][2026-01-17 12:55:55] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  45%|████▌     | 406/900 [03:22<02:03,  3.99it/s][2026-01-17 12:55:55] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  45%|████▌     | 407/900 [03:23<02:03,  4.01it/s][2026-01-17 12:55:55] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  45%|████▌     | 408/900 [03:23<01:58,  4.15it/s][2026-01-17 12:55:55] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  45%|████▌     | 409/900 [03:23<01:54,  4.27it/s][2026-01-17 12:55:56] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 410/900 [03:23<01:51,  4.38it/s][2026-01-17 12:55:56] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 411/900 [03:23<01:56,  4.21it/s][2026-01-17 12:55:56] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 412/900 [03:24<01:56,  4.19it/s][2026-01-17 12:55:56] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 413/900 [03:24<01:48,  4.47it/s][2026-01-17 12:55:57] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:57] Decode batch, #running-req: 1, #token: 768, token usage: 0.02, npu graph: True, gen throughput (token/s): 12.54, #queue-req: 0,
[2026-01-17 12:55:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 414/900 [03:24<02:04,  3.90it/s][2026-01-17 12:55:57] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 415/900 [03:24<02:02,  3.96it/s][2026-01-17 12:55:57] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 416/900 [03:25<02:01,  3.99it/s][2026-01-17 12:55:57] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▋     | 417/900 [03:25<02:03,  3.92it/s][2026-01-17 12:55:58] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▋     | 418/900 [03:25<02:01,  3.97it/s][2026-01-17 12:55:58] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  47%|████▋     | 419/900 [03:25<01:57,  4.11it/s][2026-01-17 12:55:58] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  47%|████▋     | 420/900 [03:26<01:57,  4.09it/s][2026-01-17 12:55:58] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  47%|████▋     | 421/900 [03:26<01:57,  4.09it/s][2026-01-17 12:55:59] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  47%|████▋     | 422/900 [03:26<01:53,  4.20it/s][2026-01-17 12:55:59] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  47%|████▋     | 423/900 [03:26<01:54,  4.18it/s][2026-01-17 12:55:59] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:55:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  47%|████▋     | 424/900 [03:27<01:59,  4.00it/s][2026-01-17 12:55:59] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  47%|████▋     | 425/900 [03:27<01:56,  4.09it/s][2026-01-17 12:56:00] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  47%|████▋     | 426/900 [03:27<01:44,  4.53it/s][2026-01-17 12:56:00] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  47%|████▋     | 427/900 [03:28<02:14,  3.50it/s][2026-01-17 12:56:00] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 428/900 [03:28<02:06,  3.73it/s][2026-01-17 12:56:00] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 429/900 [03:28<02:00,  3.91it/s][2026-01-17 12:56:01] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 430/900 [03:28<02:00,  3.91it/s][2026-01-17 12:56:01] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 431/900 [03:29<02:04,  3.75it/s][2026-01-17 12:56:01] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 9.22, #queue-req: 0,
[2026-01-17 12:56:01] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 432/900 [03:29<02:09,  3.62it/s][2026-01-17 12:56:02] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 433/900 [03:29<02:19,  3.35it/s][2026-01-17 12:56:02] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 434/900 [03:29<02:09,  3.61it/s][2026-01-17 12:56:02] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 435/900 [03:30<01:57,  3.97it/s][2026-01-17 12:56:02] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 436/900 [03:30<02:24,  3.22it/s][2026-01-17 12:56:03] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  49%|████▊     | 437/900 [03:30<02:16,  3.40it/s][2026-01-17 12:56:03] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  49%|████▊     | 438/900 [03:31<02:27,  3.13it/s][2026-01-17 12:56:03] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  49%|████▉     | 439/900 [03:31<02:14,  3.44it/s][2026-01-17 12:56:04] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  49%|████▉     | 440/900 [03:31<02:07,  3.60it/s][2026-01-17 12:56:04] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  49%|████▉     | 441/900 [03:31<01:59,  3.83it/s][2026-01-17 12:56:04] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  49%|████▉     | 442/900 [03:32<01:54,  4.00it/s][2026-01-17 12:56:04] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  49%|████▉     | 443/900 [03:32<01:51,  4.10it/s][2026-01-17 12:56:05] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  49%|████▉     | 444/900 [03:32<01:50,  4.12it/s][2026-01-17 12:56:05] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  49%|████▉     | 445/900 [03:32<02:06,  3.60it/s][2026-01-17 12:56:05] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|████▉     | 446/900 [03:33<02:01,  3.74it/s][2026-01-17 12:56:05] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|████▉     | 447/900 [03:33<01:55,  3.92it/s][2026-01-17 12:56:06] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|████▉     | 448/900 [03:33<01:50,  4.08it/s][2026-01-17 12:56:06] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|████▉     | 449/900 [03:33<01:55,  3.92it/s][2026-01-17 12:56:06] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 450/900 [03:34<01:52,  4.00it/s][2026-01-17 12:56:06] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 451/900 [03:34<01:59,  3.77it/s][2026-01-17 12:56:07] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 7.40, #queue-req: 0,
[2026-01-17 12:56:07] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 452/900 [03:34<01:56,  3.84it/s][2026-01-17 12:56:07] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 453/900 [03:34<01:54,  3.89it/s][2026-01-17 12:56:07] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 454/900 [03:35<01:54,  3.90it/s][2026-01-17 12:56:07] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  51%|█████     | 455/900 [03:35<01:53,  3.93it/s][2026-01-17 12:56:08] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  51%|█████     | 456/900 [03:35<01:52,  3.95it/s][2026-01-17 12:56:08] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  51%|█████     | 457/900 [03:35<01:51,  3.98it/s][2026-01-17 12:56:08] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  51%|█████     | 458/900 [03:36<01:58,  3.71it/s][2026-01-17 12:56:09] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  51%|█████     | 459/900 [03:36<02:03,  3.57it/s][2026-01-17 12:56:09] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  51%|█████     | 460/900 [03:36<02:24,  3.05it/s][2026-01-17 12:56:09] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  51%|█████     | 461/900 [03:37<02:14,  3.27it/s][2026-01-17 12:56:09] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  51%|█████▏    | 462/900 [03:37<02:08,  3.40it/s][2026-01-17 12:56:10] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  51%|█████▏    | 463/900 [03:37<02:04,  3.52it/s][2026-01-17 12:56:10] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 11.99, #queue-req: 0,
[2026-01-17 12:56:10] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 464/900 [03:38<02:15,  3.23it/s][2026-01-17 12:56:10] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 465/900 [03:38<02:06,  3.43it/s][2026-01-17 12:56:11] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 466/900 [03:38<01:59,  3.64it/s][2026-01-17 12:56:11] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 467/900 [03:38<02:04,  3.49it/s][2026-01-17 12:56:11] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 468/900 [03:39<01:58,  3.65it/s][2026-01-17 12:56:11] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:12] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 469/900 [03:39<01:53,  3.79it/s][2026-01-17 12:56:12] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:12] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 470/900 [03:39<02:01,  3.54it/s][2026-01-17 12:56:12] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:12] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 471/900 [03:39<01:50,  3.88it/s][2026-01-17 12:56:12] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:12] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 472/900 [03:40<01:41,  4.22it/s][2026-01-17 12:56:12] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  53%|█████▎    | 473/900 [03:40<01:41,  4.22it/s][2026-01-17 12:56:13] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  53%|█████▎    | 474/900 [03:40<01:35,  4.44it/s][2026-01-17 12:56:13] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:13] Decode batch, #running-req: 1, #token: 1664, token usage: 0.03, npu graph: True, gen throughput (token/s): 12.55, #queue-req: 0,
[2026-01-17 12:56:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  53%|█████▎    | 475/900 [03:40<01:58,  3.58it/s][2026-01-17 12:56:13] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  53%|█████▎    | 476/900 [03:41<01:55,  3.67it/s][2026-01-17 12:56:13] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 512, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  53%|█████▎    | 477/900 [03:41<01:50,  3.83it/s][2026-01-17 12:56:14] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  53%|█████▎    | 478/900 [03:41<01:49,  3.86it/s][2026-01-17 12:56:14] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  53%|█████▎    | 479/900 [03:41<01:48,  3.88it/s][2026-01-17 12:56:14] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 512, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  53%|█████▎    | 480/900 [03:42<01:53,  3.71it/s][2026-01-17 12:56:14] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  53%|█████▎    | 481/900 [03:42<01:48,  3.86it/s][2026-01-17 12:56:15] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▎    | 482/900 [03:42<01:45,  3.97it/s][2026-01-17 12:56:15] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▎    | 483/900 [03:42<01:42,  4.08it/s][2026-01-17 12:56:15] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 484/900 [03:43<02:12,  3.14it/s][2026-01-17 12:56:16] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 485/900 [03:43<02:02,  3.40it/s][2026-01-17 12:56:16] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 486/900 [03:43<01:54,  3.60it/s][2026-01-17 12:56:16] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 487/900 [03:44<01:53,  3.64it/s][2026-01-17 12:56:16] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 488/900 [03:44<01:52,  3.67it/s][2026-01-17 12:56:17] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 489/900 [03:44<01:52,  3.65it/s][2026-01-17 12:56:17] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 490/900 [03:44<01:47,  3.81it/s][2026-01-17 12:56:18] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  55%|█████▍    | 491/900 [03:45<02:33,  2.67it/s][2026-01-17 12:56:18] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:18] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 8.09, #queue-req: 0,
[2026-01-17 12:56:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  55%|█████▍    | 492/900 [03:45<02:21,  2.88it/s][2026-01-17 12:56:18] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  55%|█████▍    | 493/900 [03:46<02:07,  3.19it/s][2026-01-17 12:56:18] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  55%|█████▍    | 494/900 [03:46<02:00,  3.38it/s][2026-01-17 12:56:19] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  55%|█████▌    | 495/900 [03:47<02:39,  2.54it/s][2026-01-17 12:56:20] Prefill batch, #new-seq: 1, #new-token: 1792, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  55%|█████▌    | 496/900 [03:47<03:35,  1.87it/s][2026-01-17 12:56:20] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  55%|█████▌    | 497/900 [03:48<02:51,  2.35it/s][2026-01-17 12:56:20] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  55%|█████▌    | 498/900 [03:48<02:40,  2.50it/s][2026-01-17 12:56:21] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  55%|█████▌    | 499/900 [03:48<02:21,  2.83it/s][2026-01-17 12:56:21] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 500/900 [03:48<02:08,  3.12it/s][2026-01-17 12:56:21] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 501/900 [03:49<02:13,  2.98it/s][2026-01-17 12:56:22] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 502/900 [03:49<02:12,  3.01it/s][2026-01-17 12:56:22] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 503/900 [03:49<02:02,  3.25it/s][2026-01-17 12:56:22] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 504/900 [03:50<01:52,  3.51it/s][2026-01-17 12:56:22] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 505/900 [03:50<01:47,  3.68it/s][2026-01-17 12:56:23] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 506/900 [03:50<01:43,  3.80it/s][2026-01-17 12:56:23] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▋    | 507/900 [03:50<01:40,  3.91it/s][2026-01-17 12:56:23] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▋    | 508/900 [03:51<01:59,  3.29it/s][2026-01-17 12:56:23] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  57%|█████▋    | 509/900 [03:51<01:54,  3.42it/s][2026-01-17 12:56:24] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  57%|█████▋    | 510/900 [03:51<01:53,  3.44it/s][2026-01-17 12:56:24] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:24] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 6.39, #queue-req: 0,
[2026-01-17 12:56:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  57%|█████▋    | 511/900 [03:52<02:08,  3.03it/s][2026-01-17 12:56:25] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  57%|█████▋    | 512/900 [03:52<02:27,  2.63it/s][2026-01-17 12:56:25] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  57%|█████▋    | 513/900 [03:53<02:39,  2.42it/s][2026-01-17 12:56:26] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:26] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  57%|█████▋    | 514/900 [03:53<02:42,  2.37it/s][2026-01-17 12:56:26] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:26] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  57%|█████▋    | 515/900 [03:54<02:50,  2.26it/s][2026-01-17 12:56:27] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  57%|█████▋    | 516/900 [03:54<03:10,  2.02it/s][2026-01-17 12:56:27] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  57%|█████▋    | 517/900 [03:55<03:10,  2.01it/s][2026-01-17 12:56:27] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 518/900 [03:55<02:48,  2.27it/s][2026-01-17 12:56:28] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 519/900 [03:55<02:42,  2.35it/s][2026-01-17 12:56:28] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 520/900 [03:56<02:43,  2.32it/s][2026-01-17 12:56:29] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 521/900 [03:56<02:47,  2.26it/s][2026-01-17 12:56:29] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 522/900 [03:57<02:53,  2.17it/s][2026-01-17 12:56:30] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 523/900 [03:57<02:44,  2.29it/s][2026-01-17 12:56:30] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 524/900 [03:57<02:24,  2.61it/s][2026-01-17 12:56:31] Prefill batch, #new-seq: 1, #new-token: 2944, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 525/900 [03:58<03:03,  2.05it/s][2026-01-17 12:56:31] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 526/900 [03:59<03:05,  2.01it/s][2026-01-17 12:56:31] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  59%|█████▊    | 527/900 [03:59<02:35,  2.40it/s][2026-01-17 12:56:32] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  59%|█████▊    | 528/900 [04:00<02:53,  2.15it/s][2026-01-17 12:56:32] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  59%|█████▉    | 529/900 [04:00<02:39,  2.33it/s][2026-01-17 12:56:33] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  59%|█████▉    | 530/900 [04:00<02:26,  2.53it/s][2026-01-17 12:56:33] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:33] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 4.53, #queue-req: 0,
[2026-01-17 12:56:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  59%|█████▉    | 531/900 [04:01<02:17,  2.68it/s][2026-01-17 12:56:33] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  59%|█████▉    | 532/900 [04:01<02:16,  2.69it/s][2026-01-17 12:56:34] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  59%|█████▉    | 533/900 [04:01<02:13,  2.74it/s][2026-01-17 12:56:34] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  59%|█████▉    | 534/900 [04:02<02:23,  2.55it/s][2026-01-17 12:56:35] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  59%|█████▉    | 535/900 [04:02<02:23,  2.55it/s][2026-01-17 12:56:35] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|█████▉    | 536/900 [04:02<02:21,  2.57it/s][2026-01-17 12:56:35] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|█████▉    | 537/900 [04:03<02:01,  2.98it/s][2026-01-17 12:56:35] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|█████▉    | 538/900 [04:03<02:02,  2.95it/s][2026-01-17 12:56:36] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|█████▉    | 539/900 [04:03<02:04,  2.90it/s][2026-01-17 12:56:36] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 540/900 [04:04<02:11,  2.74it/s][2026-01-17 12:56:37] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 541/900 [04:04<02:07,  2.81it/s][2026-01-17 12:56:37] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 542/900 [04:04<01:57,  3.05it/s][2026-01-17 12:56:37] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 543/900 [04:05<01:52,  3.16it/s][2026-01-17 12:56:37] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 544/900 [04:05<01:50,  3.23it/s][2026-01-17 12:56:38] Prefill batch, #new-seq: 1, #new-token: 1792, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  61%|██████    | 545/900 [04:06<02:43,  2.17it/s][2026-01-17 12:56:39] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  61%|██████    | 546/900 [04:06<02:39,  2.22it/s][2026-01-17 12:56:39] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  61%|██████    | 547/900 [04:06<02:15,  2.61it/s][2026-01-17 12:56:39] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  61%|██████    | 548/900 [04:07<02:37,  2.23it/s][2026-01-17 12:56:40] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  61%|██████    | 549/900 [04:07<02:29,  2.34it/s][2026-01-17 12:56:40] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  61%|██████    | 550/900 [04:08<02:09,  2.70it/s][2026-01-17 12:56:40] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:41] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 5.38, #queue-req: 0,
[2026-01-17 12:56:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  61%|██████    | 551/900 [04:08<02:01,  2.87it/s][2026-01-17 12:56:41] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  61%|██████▏   | 552/900 [04:08<01:59,  2.91it/s][2026-01-17 12:56:41] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  61%|██████▏   | 553/900 [04:09<02:00,  2.88it/s][2026-01-17 12:56:41] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 554/900 [04:09<01:54,  3.02it/s][2026-01-17 12:56:42] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 555/900 [04:09<01:51,  3.10it/s][2026-01-17 12:56:42] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 556/900 [04:09<01:44,  3.30it/s][2026-01-17 12:56:42] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 557/900 [04:10<01:42,  3.34it/s][2026-01-17 12:56:43] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:43] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 558/900 [04:10<01:38,  3.48it/s][2026-01-17 12:56:43] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:43] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 559/900 [04:10<01:37,  3.50it/s][2026-01-17 12:56:43] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:43] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 560/900 [04:11<01:52,  3.03it/s][2026-01-17 12:56:44] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 561/900 [04:11<02:07,  2.67it/s][2026-01-17 12:56:44] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:44] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 562/900 [04:12<02:28,  2.28it/s][2026-01-17 12:56:45] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  63%|██████▎   | 563/900 [04:12<02:07,  2.65it/s][2026-01-17 12:56:45] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  63%|██████▎   | 564/900 [04:12<01:54,  2.93it/s][2026-01-17 12:56:45] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  63%|██████▎   | 565/900 [04:13<01:46,  3.15it/s][2026-01-17 12:56:45] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  63%|██████▎   | 566/900 [04:13<01:49,  3.05it/s][2026-01-17 12:56:46] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  63%|██████▎   | 567/900 [04:13<01:43,  3.21it/s][2026-01-17 12:56:48] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:48] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  63%|██████▎   | 568/900 [04:16<05:32,  1.00s/it][2026-01-17 12:56:49] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  63%|██████▎   | 569/900 [04:16<04:16,  1.29it/s][2026-01-17 12:56:49] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  63%|██████▎   | 570/900 [04:16<03:27,  1.59it/s][2026-01-17 12:56:49] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:49] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 4.60, #queue-req: 0,
[2026-01-17 12:56:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  63%|██████▎   | 571/900 [04:17<02:54,  1.89it/s][2026-01-17 12:56:49] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▎   | 572/900 [04:17<02:26,  2.24it/s][2026-01-17 12:56:50] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▎   | 573/900 [04:17<02:10,  2.51it/s][2026-01-17 12:56:50] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 574/900 [04:17<01:53,  2.88it/s][2026-01-17 12:56:50] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 575/900 [04:18<01:42,  3.17it/s][2026-01-17 12:56:50] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 576/900 [04:18<01:40,  3.22it/s][2026-01-17 12:56:51] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 577/900 [04:18<01:34,  3.43it/s][2026-01-17 12:56:51] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 578/900 [04:19<01:47,  3.00it/s][2026-01-17 12:56:52] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 579/900 [04:19<02:07,  2.52it/s][2026-01-17 12:56:52] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 580/900 [04:19<01:54,  2.79it/s][2026-01-17 12:56:52] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  65%|██████▍   | 581/900 [04:20<01:47,  2.98it/s][2026-01-17 12:56:53] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  65%|██████▍   | 582/900 [04:20<01:54,  2.79it/s][2026-01-17 12:56:53] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  65%|██████▍   | 583/900 [04:20<01:40,  3.14it/s][2026-01-17 12:56:53] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  65%|██████▍   | 584/900 [04:21<01:34,  3.35it/s][2026-01-17 12:56:53] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  65%|██████▌   | 585/900 [04:21<01:31,  3.45it/s][2026-01-17 12:56:54] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  65%|██████▌   | 586/900 [04:21<01:41,  3.10it/s][2026-01-17 12:56:54] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:54] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 8.06, #queue-req: 0,
[2026-01-17 12:56:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  65%|██████▌   | 587/900 [04:22<01:41,  3.10it/s][2026-01-17 12:56:54] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  65%|██████▌   | 588/900 [04:22<01:35,  3.25it/s][2026-01-17 12:56:55] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  65%|██████▌   | 589/900 [04:22<01:29,  3.46it/s][2026-01-17 12:56:55] Prefill batch, #new-seq: 1, #new-token: 1408, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 590/900 [04:22<01:38,  3.15it/s][2026-01-17 12:56:55] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 591/900 [04:23<01:40,  3.07it/s][2026-01-17 12:56:56] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 592/900 [04:23<01:40,  3.07it/s][2026-01-17 12:56:56] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 593/900 [04:23<01:39,  3.09it/s][2026-01-17 12:56:56] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 594/900 [04:24<01:48,  2.82it/s][2026-01-17 12:56:57] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 595/900 [04:24<01:38,  3.10it/s][2026-01-17 12:56:57] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 596/900 [04:24<01:33,  3.26it/s][2026-01-17 12:56:57] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▋   | 597/900 [04:25<01:29,  3.37it/s][2026-01-17 12:56:57] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▋   | 598/900 [04:25<01:26,  3.47it/s][2026-01-17 12:56:58] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1792, token usage: 0.04, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:58] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 10.64, #queue-req: 0,
[2026-01-17 12:56:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  67%|██████▋   | 599/900 [04:25<01:35,  3.16it/s][2026-01-17 12:56:58] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  67%|██████▋   | 600/900 [04:26<01:45,  2.85it/s][2026-01-17 12:56:59] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  67%|██████▋   | 601/900 [04:26<01:50,  2.72it/s][2026-01-17 12:56:59] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:56:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  67%|██████▋   | 602/900 [04:27<01:53,  2.62it/s][2026-01-17 12:56:59] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  67%|██████▋   | 603/900 [04:27<01:48,  2.74it/s][2026-01-17 12:57:00] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  67%|██████▋   | 604/900 [04:27<01:39,  2.97it/s][2026-01-17 12:57:00] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  67%|██████▋   | 605/900 [04:28<01:46,  2.77it/s][2026-01-17 12:57:00] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  67%|██████▋   | 606/900 [04:28<01:52,  2.61it/s][2026-01-17 12:57:01] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  67%|██████▋   | 607/900 [04:28<01:47,  2.72it/s][2026-01-17 12:57:01] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 608/900 [04:29<01:38,  2.95it/s][2026-01-17 12:57:01] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 609/900 [04:29<01:36,  3.01it/s][2026-01-17 12:57:02] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 610/900 [04:29<01:43,  2.80it/s][2026-01-17 12:57:02] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 611/900 [04:30<01:51,  2.59it/s][2026-01-17 12:57:03] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 612/900 [04:30<01:55,  2.50it/s][2026-01-17 12:57:03] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 613/900 [04:31<01:56,  2.46it/s][2026-01-17 12:57:03] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 614/900 [04:31<01:49,  2.62it/s][2026-01-17 12:57:04] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 615/900 [04:31<01:43,  2.75it/s][2026-01-17 12:57:04] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 616/900 [04:32<01:46,  2.66it/s][2026-01-17 12:57:05] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  69%|██████▊   | 617/900 [04:32<01:50,  2.56it/s][2026-01-17 12:57:05] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 5.86, #queue-req: 0,
[2026-01-17 12:57:05] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1792, token usage: 0.04, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  69%|██████▊   | 618/900 [04:33<01:49,  2.57it/s][2026-01-17 12:57:05] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  69%|██████▉   | 619/900 [04:33<01:44,  2.70it/s][2026-01-17 12:57:06] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  69%|██████▉   | 620/900 [04:33<01:45,  2.66it/s][2026-01-17 12:57:06] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 1792, token usage: 0.04, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  69%|██████▉   | 621/900 [04:34<01:49,  2.55it/s][2026-01-17 12:57:06] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  69%|██████▉   | 622/900 [04:34<01:43,  2.69it/s][2026-01-17 12:57:07] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  69%|██████▉   | 623/900 [04:34<01:47,  2.59it/s][2026-01-17 12:57:07] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  69%|██████▉   | 624/900 [04:35<01:49,  2.52it/s][2026-01-17 12:57:08] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  69%|██████▉   | 625/900 [04:35<01:41,  2.72it/s][2026-01-17 12:57:08] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|██████▉   | 626/900 [04:35<01:36,  2.85it/s][2026-01-17 12:57:08] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|██████▉   | 627/900 [04:36<01:37,  2.81it/s][2026-01-17 12:57:09] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|██████▉   | 628/900 [04:36<01:27,  3.11it/s][2026-01-17 12:57:09] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|██████▉   | 629/900 [04:36<01:33,  2.90it/s][2026-01-17 12:57:09] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 630/900 [04:37<01:26,  3.14it/s][2026-01-17 12:57:09] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 631/900 [04:37<01:18,  3.42it/s][2026-01-17 12:57:10] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 632/900 [04:37<01:17,  3.48it/s][2026-01-17 12:57:10] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 633/900 [04:38<01:18,  3.39it/s][2026-01-17 12:57:10] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 634/900 [04:38<01:08,  3.87it/s][2026-01-17 12:57:11] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  71%|███████   | 635/900 [04:38<01:13,  3.59it/s][2026-01-17 12:57:11] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  71%|███████   | 636/900 [04:38<01:09,  3.78it/s][2026-01-17 12:57:11] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  71%|███████   | 637/900 [04:39<01:13,  3.60it/s][2026-01-17 12:57:11] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 6.22, #queue-req: 0,
[2026-01-17 12:57:11] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  71%|███████   | 638/900 [04:39<01:08,  3.84it/s][2026-01-17 12:57:12] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:12] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  71%|███████   | 639/900 [04:39<01:07,  3.88it/s][2026-01-17 12:57:12] Prefill batch, #new-seq: 1, #new-token: 1792, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:12] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  71%|███████   | 640/900 [04:40<01:20,  3.21it/s][2026-01-17 12:57:12] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  71%|███████   | 641/900 [04:40<01:21,  3.17it/s][2026-01-17 12:57:13] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  71%|███████▏  | 642/900 [04:40<01:16,  3.36it/s][2026-01-17 12:57:13] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  71%|███████▏  | 643/900 [04:40<01:17,  3.30it/s][2026-01-17 12:57:13] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 644/900 [04:41<01:08,  3.71it/s][2026-01-17 12:57:13] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 645/900 [04:41<01:10,  3.60it/s][2026-01-17 12:57:14] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 646/900 [04:41<01:13,  3.47it/s][2026-01-17 12:57:14] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 647/900 [04:42<01:16,  3.31it/s][2026-01-17 12:57:14] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 648/900 [04:42<01:16,  3.29it/s][2026-01-17 12:57:15] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 649/900 [04:42<01:16,  3.27it/s][2026-01-17 12:57:15] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 650/900 [04:42<01:16,  3.25it/s][2026-01-17 12:57:15] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 651/900 [04:43<01:13,  3.41it/s][2026-01-17 12:57:16] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 652/900 [04:43<01:14,  3.33it/s][2026-01-17 12:57:16] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  73%|███████▎  | 653/900 [04:43<01:13,  3.37it/s][2026-01-17 12:57:16] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  73%|███████▎  | 654/900 [04:44<01:08,  3.59it/s][2026-01-17 12:57:16] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  73%|███████▎  | 655/900 [04:44<01:07,  3.63it/s][2026-01-17 12:57:17] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  73%|███████▎  | 656/900 [04:44<01:11,  3.39it/s][2026-01-17 12:57:17] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  73%|███████▎  | 657/900 [04:44<01:11,  3.42it/s][2026-01-17 12:57:17] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 6.82, #queue-req: 0,
[2026-01-17 12:57:17] Prefill batch, #new-seq: 1, #new-token: 1792, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  73%|███████▎  | 658/900 [04:45<01:21,  2.97it/s][2026-01-17 12:57:18] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  73%|███████▎  | 659/900 [04:45<01:16,  3.14it/s][2026-01-17 12:57:18] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  73%|███████▎  | 660/900 [04:45<01:15,  3.17it/s][2026-01-17 12:57:18] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  73%|███████▎  | 661/900 [04:46<01:17,  3.08it/s][2026-01-17 12:57:19] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▎  | 662/900 [04:46<01:11,  3.31it/s][2026-01-17 12:57:19] Prefill batch, #new-seq: 1, #new-token: 1408, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▎  | 663/900 [04:46<01:14,  3.20it/s][2026-01-17 12:57:19] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 664/900 [04:47<01:08,  3.46it/s][2026-01-17 12:57:19] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 665/900 [04:47<01:09,  3.38it/s][2026-01-17 12:57:20] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 666/900 [04:47<01:11,  3.27it/s][2026-01-17 12:57:20] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 667/900 [04:48<01:28,  2.62it/s][2026-01-17 12:57:21] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 668/900 [04:48<01:20,  2.90it/s][2026-01-17 12:57:21] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 669/900 [04:48<01:17,  2.99it/s][2026-01-17 12:57:21] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 670/900 [04:49<01:13,  3.14it/s][2026-01-17 12:57:21] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  75%|███████▍  | 671/900 [04:49<01:08,  3.36it/s][2026-01-17 12:57:22] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  75%|███████▍  | 672/900 [04:49<01:06,  3.43it/s][2026-01-17 12:57:22] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  75%|███████▍  | 673/900 [04:50<01:08,  3.33it/s][2026-01-17 12:57:22] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  75%|███████▍  | 674/900 [04:50<01:06,  3.40it/s][2026-01-17 12:57:23] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:23] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 7.02, #queue-req: 0,
[2026-01-17 12:57:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  75%|███████▌  | 675/900 [04:50<01:09,  3.24it/s][2026-01-17 12:57:23] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  75%|███████▌  | 676/900 [04:50<01:07,  3.32it/s][2026-01-17 12:57:23] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  75%|███████▌  | 677/900 [04:51<01:06,  3.34it/s][2026-01-17 12:57:24] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  75%|███████▌  | 678/900 [04:51<01:04,  3.43it/s][2026-01-17 12:57:24] Prefill batch, #new-seq: 1, #new-token: 1408, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  75%|███████▌  | 679/900 [04:51<01:04,  3.44it/s][2026-01-17 12:57:24] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 680/900 [04:52<01:01,  3.57it/s][2026-01-17 12:57:24] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 681/900 [04:52<00:59,  3.69it/s][2026-01-17 12:57:25] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 682/900 [04:52<01:04,  3.38it/s][2026-01-17 12:57:25] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 683/900 [04:52<00:57,  3.76it/s][2026-01-17 12:57:25] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 684/900 [04:53<00:56,  3.83it/s][2026-01-17 12:57:25] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:26] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 685/900 [04:53<00:57,  3.72it/s][2026-01-17 12:57:26] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:26] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 686/900 [04:53<00:59,  3.61it/s][2026-01-17 12:57:26] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:26] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▋  | 687/900 [04:54<01:00,  3.50it/s][2026-01-17 12:57:26] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:26] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▋  | 688/900 [04:54<00:58,  3.61it/s][2026-01-17 12:57:26] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  77%|███████▋  | 689/900 [04:54<00:52,  3.98it/s][2026-01-17 12:57:27] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  77%|███████▋  | 690/900 [04:54<00:55,  3.76it/s][2026-01-17 12:57:27] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  77%|███████▋  | 691/900 [04:54<00:53,  3.92it/s][2026-01-17 12:57:27] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  77%|███████▋  | 692/900 [04:55<00:52,  3.96it/s][2026-01-17 12:57:27] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  77%|███████▋  | 693/900 [04:55<00:52,  3.96it/s][2026-01-17 12:57:28] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  77%|███████▋  | 694/900 [04:55<00:52,  3.96it/s][2026-01-17 12:57:28] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:28] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 7.51, #queue-req: 0,
[2026-01-17 12:57:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  77%|███████▋  | 695/900 [04:56<00:52,  3.92it/s][2026-01-17 12:57:28] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  77%|███████▋  | 696/900 [04:56<00:51,  3.95it/s][2026-01-17 12:57:28] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  77%|███████▋  | 697/900 [04:56<00:50,  4.04it/s][2026-01-17 12:57:29] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 698/900 [04:56<00:49,  4.04it/s][2026-01-17 12:57:29] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 699/900 [04:57<00:55,  3.62it/s][2026-01-17 12:57:29] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 700/900 [04:57<01:02,  3.19it/s][2026-01-17 12:57:30] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 701/900 [04:57<00:57,  3.49it/s][2026-01-17 12:57:30] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 702/900 [04:57<00:54,  3.65it/s][2026-01-17 12:57:30] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 703/900 [04:58<00:52,  3.74it/s][2026-01-17 12:57:30] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 704/900 [04:58<00:50,  3.91it/s][2026-01-17 12:57:31] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 705/900 [04:58<00:49,  3.96it/s][2026-01-17 12:57:31] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 706/900 [04:58<00:52,  3.73it/s][2026-01-17 12:57:31] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  79%|███████▊  | 707/900 [04:59<00:49,  3.90it/s][2026-01-17 12:57:32] Prefill batch, #new-seq: 1, #new-token: 2304, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  79%|███████▊  | 708/900 [04:59<00:59,  3.21it/s][2026-01-17 12:57:32] Prefill batch, #new-seq: 1, #new-token: 1408, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  79%|███████▉  | 709/900 [04:59<01:00,  3.17it/s][2026-01-17 12:57:32] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  79%|███████▉  | 710/900 [05:00<00:54,  3.48it/s][2026-01-17 12:57:32] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  79%|███████▉  | 711/900 [05:00<00:55,  3.40it/s][2026-01-17 12:57:33] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  79%|███████▉  | 712/900 [05:00<00:51,  3.66it/s][2026-01-17 12:57:33] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  79%|███████▉  | 713/900 [05:00<00:48,  3.88it/s][2026-01-17 12:57:33] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  79%|███████▉  | 714/900 [05:01<00:48,  3.81it/s][2026-01-17 12:57:33] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:34] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 7.31, #queue-req: 0,
[2026-01-17 12:57:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  79%|███████▉  | 715/900 [05:01<00:48,  3.84it/s][2026-01-17 12:57:34] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|███████▉  | 716/900 [05:01<00:47,  3.91it/s][2026-01-17 12:57:34] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|███████▉  | 717/900 [05:02<00:48,  3.76it/s][2026-01-17 12:57:34] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|███████▉  | 718/900 [05:02<00:49,  3.70it/s][2026-01-17 12:57:35] Prefill batch, #new-seq: 1, #new-token: 2304, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|███████▉  | 719/900 [05:02<00:58,  3.10it/s][2026-01-17 12:57:35] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 720/900 [05:02<00:54,  3.32it/s][2026-01-17 12:57:35] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 721/900 [05:03<01:01,  2.90it/s][2026-01-17 12:57:36] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 722/900 [05:03<01:00,  2.94it/s][2026-01-17 12:57:36] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 723/900 [05:04<00:55,  3.20it/s][2026-01-17 12:57:36] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 724/900 [05:04<00:53,  3.31it/s][2026-01-17 12:57:37] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  81%|████████  | 725/900 [05:04<00:54,  3.20it/s][2026-01-17 12:57:37] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  81%|████████  | 726/900 [05:04<00:51,  3.39it/s][2026-01-17 12:57:37] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  81%|████████  | 727/900 [05:05<00:54,  3.16it/s][2026-01-17 12:57:37] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  81%|████████  | 728/900 [05:05<00:49,  3.46it/s][2026-01-17 12:57:38] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  81%|████████  | 729/900 [05:05<00:47,  3.61it/s][2026-01-17 12:57:38] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  81%|████████  | 730/900 [05:05<00:44,  3.85it/s][2026-01-17 12:57:38] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  81%|████████  | 731/900 [05:06<00:43,  3.85it/s][2026-01-17 12:57:39] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  81%|████████▏ | 732/900 [05:06<01:06,  2.53it/s][2026-01-17 12:57:39] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  81%|████████▏ | 733/900 [05:07<01:12,  2.31it/s][2026-01-17 12:57:40] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 734/900 [05:07<01:07,  2.47it/s][2026-01-17 12:57:40] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:40] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 6.11, #queue-req: 0,
[2026-01-17 12:57:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 735/900 [05:08<00:59,  2.79it/s][2026-01-17 12:57:40] Prefill batch, #new-seq: 1, #new-token: 1792, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 736/900 [05:08<01:01,  2.65it/s][2026-01-17 12:57:41] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 737/900 [05:08<01:00,  2.68it/s][2026-01-17 12:57:41] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 738/900 [05:09<00:56,  2.89it/s][2026-01-17 12:57:41] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 739/900 [05:09<00:54,  2.96it/s][2026-01-17 12:57:42] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:42] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 740/900 [05:09<00:59,  2.70it/s][2026-01-17 12:57:43] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 741/900 [05:12<02:54,  1.10s/it][2026-01-17 12:57:45] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:45] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 742/900 [05:12<02:09,  1.22it/s][2026-01-17 12:57:45] Prefill batch, #new-seq: 1, #new-token: 3072, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  83%|████████▎ | 743/900 [05:13<02:02,  1.28it/s][2026-01-17 12:57:46] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  83%|████████▎ | 744/900 [05:14<01:48,  1.43it/s][2026-01-17 12:57:46] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:46] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  83%|████████▎ | 745/900 [05:14<01:27,  1.76it/s][2026-01-17 12:57:46] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:47] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  83%|████████▎ | 746/900 [05:14<01:12,  2.11it/s][2026-01-17 12:57:47] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:47] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  83%|████████▎ | 747/900 [05:14<01:10,  2.16it/s][2026-01-17 12:57:47] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:47] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  83%|████████▎ | 748/900 [05:15<01:00,  2.52it/s][2026-01-17 12:57:48] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:48] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  83%|████████▎ | 749/900 [05:16<01:21,  1.84it/s][2026-01-17 12:57:48] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  83%|████████▎ | 750/900 [05:16<01:11,  2.09it/s][2026-01-17 12:57:49] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  83%|████████▎ | 751/900 [05:16<01:00,  2.45it/s][2026-01-17 12:57:49] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▎ | 752/900 [05:16<00:55,  2.67it/s][2026-01-17 12:57:49] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:49] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▎ | 753/900 [05:17<00:50,  2.89it/s][2026-01-17 12:57:49] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 754/900 [05:17<00:43,  3.34it/s][2026-01-17 12:57:50] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 4.25, #queue-req: 0,
[2026-01-17 12:57:50] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 755/900 [05:17<00:41,  3.50it/s][2026-01-17 12:57:50] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 756/900 [05:17<00:38,  3.70it/s][2026-01-17 12:57:50] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:50] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 757/900 [05:18<00:39,  3.66it/s][2026-01-17 12:57:50] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 758/900 [05:18<00:37,  3.79it/s][2026-01-17 12:57:51] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 759/900 [05:18<00:36,  3.87it/s][2026-01-17 12:57:51] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 760/900 [05:18<00:35,  3.89it/s][2026-01-17 12:57:51] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:51] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  85%|████████▍ | 761/900 [05:19<00:37,  3.70it/s][2026-01-17 12:57:51] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  85%|████████▍ | 762/900 [05:19<00:38,  3.62it/s][2026-01-17 12:57:52] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  85%|████████▍ | 763/900 [05:20<00:46,  2.96it/s][2026-01-17 12:57:52] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:52] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  85%|████████▍ | 764/900 [05:20<00:39,  3.41it/s][2026-01-17 12:57:52] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  85%|████████▌ | 765/900 [05:20<00:37,  3.58it/s][2026-01-17 12:57:53] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  85%|████████▌ | 766/900 [05:20<00:35,  3.76it/s][2026-01-17 12:57:53] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  85%|████████▌ | 767/900 [05:20<00:33,  3.92it/s][2026-01-17 12:57:53] Prefill batch, #new-seq: 1, #new-token: 1408, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:53] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  85%|████████▌ | 768/900 [05:21<00:36,  3.61it/s][2026-01-17 12:57:53] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  85%|████████▌ | 769/900 [05:21<00:36,  3.63it/s][2026-01-17 12:57:54] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 770/900 [05:21<00:34,  3.73it/s][2026-01-17 12:57:54] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 771/900 [05:21<00:32,  3.95it/s][2026-01-17 12:57:54] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:54] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 772/900 [05:22<00:31,  4.11it/s][2026-01-17 12:57:54] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 773/900 [05:22<00:30,  4.14it/s][2026-01-17 12:57:55] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 7.98, #queue-req: 0,
[2026-01-17 12:57:55] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 774/900 [05:22<00:29,  4.28it/s][2026-01-17 12:57:55] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 775/900 [05:22<00:29,  4.26it/s][2026-01-17 12:57:55] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 2048, token usage: 0.04, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:55] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 776/900 [05:23<00:35,  3.52it/s][2026-01-17 12:57:56] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▋ | 777/900 [05:23<00:37,  3.26it/s][2026-01-17 12:57:56] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▋ | 778/900 [05:23<00:34,  3.55it/s][2026-01-17 12:57:56] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:56] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  87%|████████▋ | 779/900 [05:24<00:31,  3.80it/s][2026-01-17 12:57:56] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  87%|████████▋ | 780/900 [05:24<00:31,  3.82it/s][2026-01-17 12:57:57] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  87%|████████▋ | 781/900 [05:24<00:30,  3.96it/s][2026-01-17 12:57:57] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  87%|████████▋ | 782/900 [05:24<00:29,  4.00it/s][2026-01-17 12:57:57] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  87%|████████▋ | 783/900 [05:25<00:28,  4.15it/s][2026-01-17 12:57:57] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:57] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  87%|████████▋ | 784/900 [05:25<00:28,  4.09it/s][2026-01-17 12:57:58] Prefill batch, #new-seq: 1, #new-token: 1792, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  87%|████████▋ | 785/900 [05:25<00:33,  3.46it/s][2026-01-17 12:57:58] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  87%|████████▋ | 786/900 [05:25<00:32,  3.51it/s][2026-01-17 12:57:58] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:58] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  87%|████████▋ | 787/900 [05:26<00:30,  3.75it/s][2026-01-17 12:57:58] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 788/900 [05:26<00:27,  4.10it/s][2026-01-17 12:57:59] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 789/900 [05:26<00:26,  4.21it/s][2026-01-17 12:57:59] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 790/900 [05:26<00:26,  4.12it/s][2026-01-17 12:57:59] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:57:59] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 791/900 [05:27<00:26,  4.12it/s][2026-01-17 12:57:59] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 792/900 [05:27<00:26,  4.08it/s][2026-01-17 12:58:00] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 793/900 [05:27<00:27,  3.93it/s][2026-01-17 12:58:00] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 7.71, #queue-req: 0,
[2026-01-17 12:58:00] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 794/900 [05:27<00:26,  3.97it/s][2026-01-17 12:58:00] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:00] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 795/900 [05:28<00:27,  3.81it/s][2026-01-17 12:58:00] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 796/900 [05:28<00:29,  3.55it/s][2026-01-17 12:58:01] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  89%|████████▊ | 797/900 [05:28<00:29,  3.53it/s][2026-01-17 12:58:01] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  89%|████████▊ | 798/900 [05:29<00:27,  3.67it/s][2026-01-17 12:58:01] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:01] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  89%|████████▉ | 799/900 [05:29<00:27,  3.68it/s][2026-01-17 12:58:02] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  89%|████████▉ | 800/900 [05:29<00:26,  3.80it/s][2026-01-17 12:58:02] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  89%|████████▉ | 801/900 [05:29<00:27,  3.56it/s][2026-01-17 12:58:02] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  89%|████████▉ | 802/900 [05:30<00:26,  3.69it/s][2026-01-17 12:58:02] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:02] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  89%|████████▉ | 803/900 [05:30<00:24,  4.02it/s][2026-01-17 12:58:03] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  89%|████████▉ | 804/900 [05:30<00:23,  4.12it/s][2026-01-17 12:58:03] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  89%|████████▉ | 805/900 [05:30<00:23,  4.08it/s][2026-01-17 12:58:03] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:03] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|████████▉ | 806/900 [05:31<00:25,  3.62it/s][2026-01-17 12:58:03] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|████████▉ | 807/900 [05:31<00:25,  3.68it/s][2026-01-17 12:58:04] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|████████▉ | 808/900 [05:31<00:24,  3.75it/s][2026-01-17 12:58:04] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|████████▉ | 809/900 [05:31<00:23,  3.87it/s][2026-01-17 12:58:04] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 810/900 [05:32<00:23,  3.88it/s][2026-01-17 12:58:05] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 811/900 [05:32<00:28,  3.14it/s][2026-01-17 12:58:05] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 812/900 [05:32<00:28,  3.12it/s][2026-01-17 12:58:05] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:05] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 7.08, #queue-req: 0,
[2026-01-17 12:58:05] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 813/900 [05:33<00:28,  3.00it/s][2026-01-17 12:58:06] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 814/900 [05:33<00:27,  3.11it/s][2026-01-17 12:58:06] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:06] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  91%|█████████ | 815/900 [05:34<00:29,  2.85it/s][2026-01-17 12:58:06] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  91%|█████████ | 816/900 [05:34<00:29,  2.84it/s][2026-01-17 12:58:07] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  91%|█████████ | 817/900 [05:34<00:27,  2.97it/s][2026-01-17 12:58:07] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:07] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  91%|█████████ | 818/900 [05:35<00:30,  2.68it/s][2026-01-17 12:58:08] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  91%|█████████ | 819/900 [05:35<00:34,  2.34it/s][2026-01-17 12:58:08] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:08] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  91%|█████████ | 820/900 [05:35<00:30,  2.59it/s][2026-01-17 12:58:08] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  91%|█████████ | 821/900 [05:36<00:30,  2.61it/s][2026-01-17 12:58:09] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  91%|█████████▏| 822/900 [05:36<00:29,  2.66it/s][2026-01-17 12:58:09] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  91%|█████████▏| 823/900 [05:36<00:26,  2.94it/s][2026-01-17 12:58:09] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:09] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 824/900 [05:37<00:26,  2.89it/s][2026-01-17 12:58:10] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 825/900 [05:37<00:24,  3.05it/s][2026-01-17 12:58:10] Prefill batch, #new-seq: 1, #new-token: 2304, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:10] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 826/900 [05:38<00:27,  2.69it/s][2026-01-17 12:58:10] Prefill batch, #new-seq: 1, #new-token: 2304, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 827/900 [05:38<00:28,  2.53it/s][2026-01-17 12:58:11] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 828/900 [05:38<00:25,  2.85it/s][2026-01-17 12:58:11] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:11] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 829/900 [05:39<00:24,  2.94it/s][2026-01-17 12:58:11] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:12] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 830/900 [05:39<00:25,  2.76it/s][2026-01-17 12:58:12] Prefill batch, #new-seq: 1, #new-token: 4096, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:12] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 831/900 [05:40<00:34,  2.00it/s][2026-01-17 12:58:13] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 832/900 [05:40<00:34,  2.00it/s][2026-01-17 12:58:13] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:13] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 5.07, #queue-req: 0,
[2026-01-17 12:58:13] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  93%|█████████▎| 833/900 [05:41<00:30,  2.18it/s][2026-01-17 12:58:14] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  93%|█████████▎| 834/900 [05:41<00:31,  2.08it/s][2026-01-17 12:58:14] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:14] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  93%|█████████▎| 835/900 [05:42<00:29,  2.17it/s][2026-01-17 12:58:14] Prefill batch, #new-seq: 1, #new-token: 1792, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  93%|█████████▎| 836/900 [05:42<00:27,  2.30it/s][2026-01-17 12:58:15] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  93%|█████████▎| 837/900 [05:42<00:24,  2.52it/s][2026-01-17 12:58:15] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:15] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  93%|█████████▎| 838/900 [05:43<00:22,  2.75it/s][2026-01-17 12:58:15] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 2048, token usage: 0.04, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  93%|█████████▎| 839/900 [05:43<00:22,  2.73it/s][2026-01-17 12:58:16] Prefill batch, #new-seq: 1, #new-token: 2304, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  93%|█████████▎| 840/900 [05:43<00:24,  2.42it/s][2026-01-17 12:58:16] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:16] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  93%|█████████▎| 841/900 [05:44<00:21,  2.78it/s][2026-01-17 12:58:16] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▎| 842/900 [05:44<00:18,  3.08it/s][2026-01-17 12:58:17] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▎| 843/900 [05:44<00:17,  3.34it/s][2026-01-17 12:58:17] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 844/900 [05:44<00:16,  3.42it/s][2026-01-17 12:58:17] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:17] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 845/900 [05:45<00:15,  3.52it/s][2026-01-17 12:58:18] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 846/900 [05:45<00:15,  3.51it/s][2026-01-17 12:58:18] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 847/900 [05:45<00:13,  3.93it/s][2026-01-17 12:58:18] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 848/900 [05:45<00:13,  3.84it/s][2026-01-17 12:58:18] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:18] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 849/900 [05:46<00:13,  3.91it/s][2026-01-17 12:58:18] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 850/900 [05:46<00:12,  3.98it/s][2026-01-17 12:58:19] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  95%|█████████▍| 851/900 [05:46<00:12,  4.04it/s][2026-01-17 12:58:19] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  95%|█████████▍| 852/900 [05:46<00:11,  4.08it/s][2026-01-17 12:58:19] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:19] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 6.60, #queue-req: 0,
[2026-01-17 12:58:19] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  95%|█████████▍| 853/900 [05:47<00:11,  3.92it/s][2026-01-17 12:58:19] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  95%|█████████▍| 854/900 [05:47<00:11,  4.00it/s][2026-01-17 12:58:20] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  95%|█████████▌| 855/900 [05:47<00:11,  3.89it/s][2026-01-17 12:58:20] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  95%|█████████▌| 856/900 [05:48<00:12,  3.60it/s][2026-01-17 12:58:20] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:20] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  95%|█████████▌| 857/900 [05:48<00:10,  4.01it/s][2026-01-17 12:58:21] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  95%|█████████▌| 858/900 [05:48<00:12,  3.24it/s][2026-01-17 12:58:21] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  95%|█████████▌| 859/900 [05:48<00:11,  3.56it/s][2026-01-17 12:58:21] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:21] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 860/900 [05:49<00:10,  3.71it/s][2026-01-17 12:58:21] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 861/900 [05:49<00:10,  3.72it/s][2026-01-17 12:58:22] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 862/900 [05:49<00:09,  4.07it/s][2026-01-17 12:58:22] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 863/900 [05:49<00:09,  4.08it/s][2026-01-17 12:58:22] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:22] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 864/900 [05:50<00:09,  3.85it/s][2026-01-17 12:58:22] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 865/900 [05:50<00:08,  4.21it/s][2026-01-17 12:58:23] Prefill batch, #new-seq: 1, #new-token: 1408, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 866/900 [05:50<00:08,  3.79it/s][2026-01-17 12:58:23] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:23] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▋| 867/900 [05:51<00:10,  3.17it/s][2026-01-17 12:58:23] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▋| 868/900 [05:51<00:10,  3.12it/s][2026-01-17 12:58:24] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  97%|█████████▋| 869/900 [05:51<00:09,  3.36it/s][2026-01-17 12:58:24] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.01, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:24] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  97%|█████████▋| 870/900 [05:51<00:08,  3.62it/s][2026-01-17 12:58:25] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  97%|█████████▋| 871/900 [05:52<00:12,  2.37it/s][2026-01-17 12:58:25] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:25] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  97%|█████████▋| 872/900 [05:53<00:11,  2.39it/s][2026-01-17 12:58:25] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:26] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 6.51, #queue-req: 0,
[2026-01-17 12:58:26] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  97%|█████████▋| 873/900 [05:53<00:10,  2.61it/s][2026-01-17 12:58:26] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:26] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  97%|█████████▋| 874/900 [05:53<00:09,  2.64it/s][2026-01-17 12:58:26] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:26] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  97%|█████████▋| 875/900 [05:54<00:10,  2.41it/s][2026-01-17 12:58:27] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  97%|█████████▋| 876/900 [05:54<00:10,  2.35it/s][2026-01-17 12:58:27] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:27] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  97%|█████████▋| 877/900 [05:55<00:09,  2.30it/s][2026-01-17 12:58:28] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:28] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 878/900 [05:55<00:10,  2.13it/s][2026-01-17 12:58:29] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:29] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 879/900 [05:56<00:12,  1.71it/s][2026-01-17 12:58:29] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 880/900 [05:57<00:14,  1.42it/s][2026-01-17 12:58:30] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:30] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 881/900 [05:57<00:11,  1.63it/s][2026-01-17 12:58:30] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 882/900 [05:58<00:09,  1.81it/s][2026-01-17 12:58:31] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:31] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 883/900 [05:59<00:10,  1.67it/s][2026-01-17 12:58:31] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 884/900 [05:59<00:09,  1.77it/s][2026-01-17 12:58:32] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:32] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 885/900 [06:00<00:08,  1.78it/s][2026-01-17 12:58:32] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 886/900 [06:00<00:07,  1.95it/s][2026-01-17 12:58:33] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:33] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  99%|█████████▊| 887/900 [06:01<00:07,  1.81it/s][2026-01-17 12:58:34] Prefill batch, #new-seq: 1, #new-token: 2304, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:34] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  99%|█████████▊| 888/900 [06:02<00:07,  1.51it/s][2026-01-17 12:58:34] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  99%|█████████▉| 889/900 [06:02<00:05,  1.84it/s][2026-01-17 12:58:35] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:35] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  99%|█████████▉| 890/900 [06:03<00:05,  1.71it/s][2026-01-17 12:58:36] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  99%|█████████▉| 891/900 [06:03<00:05,  1.71it/s][2026-01-17 12:58:36] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:36] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  99%|█████████▉| 892/900 [06:03<00:04,  1.99it/s][2026-01-17 12:58:36] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:37] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: True, gen throughput (token/s): 3.60, #queue-req: 0,
[2026-01-17 12:58:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  99%|█████████▉| 893/900 [06:04<00:03,  1.90it/s][2026-01-17 12:58:37] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:37] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  99%|█████████▉| 894/900 [06:04<00:02,  2.05it/s][2026-01-17 12:58:37] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:38] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  99%|█████████▉| 895/900 [06:05<00:02,  1.93it/s][2026-01-17 12:58:39] Prefill batch, #new-seq: 1, #new-token: 1920, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|█████████▉| 896/900 [06:06<00:02,  1.43it/s][2026-01-17 12:58:39] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:39] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|█████████▉| 897/900 [06:06<00:01,  1.66it/s][2026-01-17 12:58:40] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|█████████▉| 898/900 [06:07<00:01,  1.66it/s][2026-01-17 12:58:40] Prefill batch, #new-seq: 1, #new-token: 2176, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:40] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|█████████▉| 899/900 [06:08<00:00,  1.73it/s][2026-01-17 12:58:41] Prefill batch, #new-seq: 1, #new-token: 1536, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 12:58:41] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 900/900 [06:08<00:00,  1.84it/s]
Model Responding: 100%|██████████| 900/900 [06:08<00:00,  2.44it/s]

Postprocessing:   0%|          | 0/900 [00:00<?, ?it/s]
Postprocessing:  71%|███████   | 640/900 [00:00<00:00, 6393.45it/s]
Postprocessing: 100%|██████████| 900/900 [00:00<00:00, 6413.82it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 120, 'acc': 0.675}, 'Art': {'num': 30, 'acc': 0.63333}, 'Art_Theory': {'num': 30, 'acc': 0.86667}, 'Design': {'num': 30, 'acc': 0.8}, 'Music': {'num': 30, 'acc': 0.4}, 'Overall-Business': {'num': 150, 'acc': 0.42667}, 'Accounting': {'num': 30, 'acc': 0.4}, 'Economics': {'num': 30, 'acc': 0.43333}, 'Finance': {'num': 30, 'acc': 0.3}, 'Manage': {'num': 30, 'acc': 0.5}, 'Marketing': {'num': 30, 'acc': 0.5}, 'Overall-Science': {'num': 150, 'acc': 0.43333}, 'Biology': {'num': 30, 'acc': 0.5}, 'Chemistry': {'num': 30, 'acc': 0.36667}, 'Geography': {'num': 30, 'acc': 0.5}, 'Math': {'num': 30, 'acc': 0.5}, 'Physics': {'num': 30, 'acc': 0.3}, 'Overall-Health and Medicine': {'num': 150, 'acc': 0.49333}, 'Basic_Medical_Science': {'num': 30, 'acc': 0.6}, 'Clinical_Medicine': {'num': 30, 'acc': 0.5}, 'Diagnostics_and_Laboratory_Medicine': {'num': 30, 'acc': 0.36667}, 'Pharmacy': {'num': 30, 'acc': 0.5}, 'Public_Health': {'num': 30, 'acc': 0.5}, 'Overall-Humanities and Social Science': {'num': 120, 'acc': 0.65}, 'History': {'num': 30, 'acc': 0.6}, 'Literature': {'num': 30, 'acc': 0.83333}, 'Sociology': {'num': 30, 'acc': 0.6}, 'Psychology': {'num': 30, 'acc': 0.56667}, 'Overall-Tech and Engineering': {'num': 210, 'acc': 0.45714}, 'Agriculture': {'num': 30, 'acc': 0.5}, 'Architecture_and_Engineering': {'num': 30, 'acc': 0.3}, 'Computer_Science': {'num': 30, 'acc': 0.56667}, 'Electronics': {'num': 30, 'acc': 0.36667}, 'Energy_and_Power': {'num': 30, 'acc': 0.53333}, 'Materials': {'num': 30, 'acc': 0.33333}, 'Mechanical_Engineering': {'num': 30, 'acc': 0.6}, 'Overall': {'num': 900, 'acc': 0.50889}}
[32m2026-01-17 12:58:41.389[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 12:58:41.397[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/deepseek-ai/deepseek-vl2",tp=1), gen_kwargs: (), limit: None, num_fewshot: None, batch_size: 64
| Tasks  |Version|Filter|n-shot| Metric |   |Value |   |Stderr|
|--------|------:|------|-----:|--------|---|-----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  |0.5089|±  |   N/A|

/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 114820 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 553.722s

OK
[CI Test Method] TestVLMModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/deepseek-ai/deepseek-vl2
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/deepseek-ai/deepseek-vl2 --trust-remote-code --cuda-graph-max-bs 64 --enable-multimodal --mem-fraction-static 0.95 --log-level info --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/deepseek-ai/deepseek-vl2 achieved accuracy: 0.3000
Cleaning up process 114820
.
.
End (14/23):
filename='ascend/vlm_models/test_ascend_deepseek_vl2.py', elapsed=564, estimated_time=400
.
.

.
.
Begin (15/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_vlm_models_mistral-small_3_1_24b_instruct2503.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:59:06] INFO model_config.py:1016: Downcasting torch.float32 to torch.float16.
[2026-01-17 12:59:06] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/mistralai/Mistral-Small-3.1-24B-Instruct-2503', tokenizer_path='/root/.cache/modelscope/hub/models/mistralai/Mistral-Small-3.1-24B-Instruct-2503', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=935233233, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/mistralai/Mistral-Small-3.1-24B-Instruct-2503', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2026-01-17 12:59:06] Downcasting torch.float32 to torch.float16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:59:09] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:59:16 TP3] Downcasting torch.float32 to torch.float16.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 12:59:16 TP1] Downcasting torch.float32 to torch.float16.
[2026-01-17 12:59:16 TP0] Downcasting torch.float32 to torch.float16.
[2026-01-17 12:59:16 TP2] Downcasting torch.float32 to torch.float16.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:59:17 TP3] Downcasting torch.float32 to torch.float16.
[2026-01-17 12:59:17 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 12:59:17 TP1] Downcasting torch.float32 to torch.float16.
[2026-01-17 12:59:17 TP1] Init torch distributed begin.
[2026-01-17 12:59:17 TP0] Downcasting torch.float32 to torch.float16.
[2026-01-17 12:59:17 TP0] Init torch distributed begin.
[2026-01-17 12:59:18 TP2] Downcasting torch.float32 to torch.float16.
[2026-01-17 12:59:18 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 12:59:19 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:59:19 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:59:19 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:59:19 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 12:59:19 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 12:59:19 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:59:19 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:59:19 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:59:19 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 12:59:20 TP3] Load weight begin. avail mem=61.14 GB
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 12:59:20 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 12:59:20 TP0] Load weight begin. avail mem=60.81 GB
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 12:59:20 TP2] Load weight begin. avail mem=60.86 GB
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 12:59:24 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:59:24 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 12:59:24 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:59:24 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 12:59:24 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:59:24 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 12:59:24 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 12:59:24 TP1] Using sdpa as multimodal attention backend.
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py

Loading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  10% Completed | 1/10 [00:05<00:47,  5.31s/it]

Loading safetensors checkpoint shards:  20% Completed | 2/10 [00:11<00:47,  6.00s/it]

Loading safetensors checkpoint shards:  30% Completed | 3/10 [00:17<00:41,  5.98s/it]

Loading safetensors checkpoint shards:  40% Completed | 4/10 [00:23<00:34,  5.79s/it]

Loading safetensors checkpoint shards:  50% Completed | 5/10 [00:28<00:27,  5.50s/it]

Loading safetensors checkpoint shards:  60% Completed | 6/10 [00:34<00:23,  5.78s/it]

Loading safetensors checkpoint shards:  70% Completed | 7/10 [00:41<00:18,  6.12s/it]

Loading safetensors checkpoint shards:  80% Completed | 8/10 [00:46<00:11,  5.94s/it]

Loading safetensors checkpoint shards:  90% Completed | 9/10 [00:53<00:06,  6.13s/it]

Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:59<00:00,  6.10s/it]

Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:59<00:00,  5.95s/it]

[2026-01-17 13:00:24 TP0] Load weight end. type=LlavaForConditionalGeneration, dtype=torch.float16, avail mem=49.21 GB, mem usage=11.60 GB.
[2026-01-17 13:00:24 TP2] Load weight end. type=LlavaForConditionalGeneration, dtype=torch.float16, avail mem=49.26 GB, mem usage=11.60 GB.
[2026-01-17 13:00:24 TP1] Load weight end. type=LlavaForConditionalGeneration, dtype=torch.float16, avail mem=49.54 GB, mem usage=11.59 GB.
[2026-01-17 13:00:24 TP3] Load weight end. type=LlavaForConditionalGeneration, dtype=torch.float16, avail mem=49.54 GB, mem usage=11.59 GB.
[2026-01-17 13:00:24 TP0] Using KV cache dtype: torch.float16
[2026-01-17 13:00:24 TP0] The available memory for KV cache is 9.69 GB.
[2026-01-17 13:00:24 TP3] The available memory for KV cache is 9.69 GB.
[2026-01-17 13:00:24 TP2] The available memory for KV cache is 9.69 GB.
[2026-01-17 13:00:24 TP1] The available memory for KV cache is 9.69 GB.
[2026-01-17 13:00:24 TP2] KV Cache is allocated. #tokens: 253824, K size: 4.84 GB, V size: 4.84 GB
[2026-01-17 13:00:24 TP1] KV Cache is allocated. #tokens: 253824, K size: 4.84 GB, V size: 4.84 GB
[2026-01-17 13:00:24 TP1] Memory pool end. avail mem=38.85 GB
[2026-01-17 13:00:24 TP2] Memory pool end. avail mem=38.57 GB
[2026-01-17 13:00:24 TP3] KV Cache is allocated. #tokens: 253824, K size: 4.84 GB, V size: 4.84 GB
[2026-01-17 13:00:24 TP3] Memory pool end. avail mem=38.85 GB
[2026-01-17 13:00:24 TP0] KV Cache is allocated. #tokens: 253824, K size: 4.84 GB, V size: 4.84 GB
[2026-01-17 13:00:24 TP0] Memory pool end. avail mem=38.52 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 13:00:26 TP0] max_total_num_tokens=253824, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=38.52 GB
[2026-01-17 13:00:27] INFO:     Started server process [126130]
[2026-01-17 13:00:27] INFO:     Waiting for application startup.
[2026-01-17 13:00:27] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 13:00:27] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 13:00:27] INFO:     Application startup complete.
[2026-01-17 13:00:27] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 13:00:28] INFO:     127.0.0.1:44226 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 13:00:28 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
🚨 Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:00:37] INFO:     127.0.0.1:44240 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:00:47] INFO:     127.0.0.1:43402 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:00:50] INFO:     127.0.0.1:44238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 13:00:50] The server is fired up and ready to roll!
[2026-01-17 13:00:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:00:58] INFO:     127.0.0.1:46012 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 13:01:05.674[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 13:01:07.768[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 13:01:08.516[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 13:01:08.516[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 13:01:08.517[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 13:01:08.519[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 13:01:24.831[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9313.22it/s]
[32m2026-01-17 13:01:24.837[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 13:01:25 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:25] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:00<00:47,  1.02it/s][2026-01-17 13:01:25 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:26 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.31, #queue-req: 0,
[2026-01-17 13:01:26] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:01<00:40,  1.18it/s][2026-01-17 13:01:26 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:27] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:02<00:37,  1.26it/s][2026-01-17 13:01:27 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:28] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:03<00:36,  1.26it/s][2026-01-17 13:01:28 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:28 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 20.09, #queue-req: 0,
[2026-01-17 13:01:28] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:04<00:35,  1.27it/s][2026-01-17 13:01:28 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:29] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:04<00:34,  1.26it/s][2026-01-17 13:01:29 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:30 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 20.68, #queue-req: 0,
[2026-01-17 13:01:30] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:05<00:34,  1.25it/s][2026-01-17 13:01:30 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:31] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:06<00:34,  1.23it/s][2026-01-17 13:01:31 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:32] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:07<00:33,  1.23it/s][2026-01-17 13:01:32 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:32 TP0] Decode batch, #running-req: 1, #token: 640, token usage: 0.00, npu graph: False, gen throughput (token/s): 18.83, #queue-req: 0,
[2026-01-17 13:01:32] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:08<00:32,  1.23it/s][2026-01-17 13:01:32 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:33] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:08<00:27,  1.41it/s][2026-01-17 13:01:33 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:33] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:08<00:20,  1.81it/s][2026-01-17 13:01:33 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:34] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:09<00:23,  1.59it/s][2026-01-17 13:01:34 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:34 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 18.22, #queue-req: 0,
[2026-01-17 13:01:35] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:10<00:24,  1.47it/s][2026-01-17 13:01:35 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:36] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:11<00:25,  1.39it/s][2026-01-17 13:01:36 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:36 TP0] Decode batch, #running-req: 1, #token: 640, token usage: 0.00, npu graph: False, gen throughput (token/s): 20.12, #queue-req: 0,
[2026-01-17 13:01:36] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:12<00:25,  1.32it/s][2026-01-17 13:01:36 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:37] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:12<00:25,  1.28it/s][2026-01-17 13:01:37 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:38] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:13<00:25,  1.27it/s][2026-01-17 13:01:38 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:38 TP0] Decode batch, #running-req: 1, #token: 640, token usage: 0.00, npu graph: False, gen throughput (token/s): 19.30, #queue-req: 0,
[2026-01-17 13:01:39] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:14<00:24,  1.27it/s][2026-01-17 13:01:39 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:40] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:15<00:23,  1.28it/s][2026-01-17 13:01:40 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:40 TP0] Decode batch, #running-req: 1, #token: 512, token usage: 0.00, npu graph: False, gen throughput (token/s): 20.74, #queue-req: 0,
[2026-01-17 13:01:40] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:16<00:22,  1.27it/s][2026-01-17 13:01:40 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:41] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:16<00:22,  1.26it/s][2026-01-17 13:01:41 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:41] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:17<00:16,  1.64it/s][2026-01-17 13:01:41 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:42] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:17<00:17,  1.50it/s][2026-01-17 13:01:42 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:42] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:18<00:13,  1.91it/s][2026-01-17 13:01:42 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:43 TP0] Decode batch, #running-req: 1, #token: 1024, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.62, #queue-req: 0,
[2026-01-17 13:01:43] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:18<00:15,  1.52it/s][2026-01-17 13:01:43 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:44] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:19<00:16,  1.40it/s][2026-01-17 13:01:44 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:45 TP0] Decode batch, #running-req: 1, #token: 768, token usage: 0.00, npu graph: False, gen throughput (token/s): 19.71, #queue-req: 0,
[2026-01-17 13:01:45] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:20<00:16,  1.33it/s][2026-01-17 13:01:45 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:46] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:21<00:15,  1.32it/s][2026-01-17 13:01:46 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:47] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:22<00:15,  1.30it/s][2026-01-17 13:01:49 TP0] Prefill batch, #new-seq: 1, #new-token: 2688, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:52 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 5.51, #queue-req: 0,
[2026-01-17 13:01:52] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:27<00:41,  2.16s/it][2026-01-17 13:01:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1792, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:53] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:28<00:33,  1.84s/it][2026-01-17 13:01:55 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:56] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:31<00:34,  2.02s/it][2026-01-17 13:01:59 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:01:59] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:34<00:40,  2.55s/it][2026-01-17 13:02:00 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:00] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:35<00:28,  1.91s/it][2026-01-17 13:02:00 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:00] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:35<00:19,  1.41s/it][2026-01-17 13:02:00 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:00] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:36<00:14,  1.12s/it][2026-01-17 13:02:03 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:04] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:39<00:20,  1.72s/it][2026-01-17 13:02:06 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:07] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [00:42<00:23,  2.13s/it][2026-01-17 13:02:07 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:07] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [00:42<00:15,  1.55s/it][2026-01-17 13:02:08 TP0] Prefill batch, #new-seq: 1, #new-token: 2304, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:09] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [00:44<00:15,  1.72s/it][2026-01-17 13:02:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:10 TP0] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, npu graph: False, gen throughput (token/s): 2.25, #queue-req: 0,
[2026-01-17 13:02:10] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [00:45<00:12,  1.52s/it][2026-01-17 13:02:12 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:13] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [00:48<00:13,  1.97s/it][2026-01-17 13:02:15 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:16] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [00:51<00:13,  2.32s/it][2026-01-17 13:02:18 TP0] Prefill batch, #new-seq: 1, #new-token: 2304, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:19] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [00:54<00:12,  2.50s/it][2026-01-17 13:02:20 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:21] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [00:56<00:09,  2.27s/it][2026-01-17 13:02:22 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:23] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [00:58<00:06,  2.25s/it][2026-01-17 13:02:25 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:26] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [01:01<00:04,  2.34s/it][2026-01-17 13:02:26 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:27] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [01:02<00:02,  2.12s/it][2026-01-17 13:02:29 TP0] Prefill batch, #new-seq: 1, #new-token: 2816, #cached-token: 128, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:02:30] INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [01:05<00:00,  2.43s/it]
Model Responding: 100%|██████████| 50/50 [01:05<00:00,  1.32s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 4390.29it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.33333}, 'Accounting': {'num': 30, 'acc': 0.33333}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.5}, 'Agriculture': {'num': 20, 'acc': 0.5}, 'Overall': {'num': 50, 'acc': 0.4}}
[32m2026-01-17 13:02:30.820[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 13:02:30.827[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/mistralai/Mistral-Small-3.1-24B-Instruct-2503",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  |  0.4|±  |   N/A|

.
----------------------------------------------------------------------
Ran 1 test in 216.563s

OK
[CI Test Method] TestMistralModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/mistralai/Mistral-Small-3.1-24B-Instruct-2503
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/mistralai/Mistral-Small-3.1-24B-Instruct-2503 --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/mistralai/Mistral-Small-3.1-24B-Instruct-2503 achieved accuracy: 0.3000
Cleaning up process 126130
.
.
End (15/23):
filename='ascend/vlm_models/test_vlm_models_mistral-small_3_1_24b_instruct2503.py', elapsed=227, estimated_time=400
.
.

.
.
Begin (16/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_llava_v1_6_34b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 13:02:53] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/AI-ModelScope/llava-v1.6-34b', tokenizer_path='llava-1.6v-34b-tokenizer/', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.7, max_running_requests=2048, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=976467435, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/AI-ModelScope/llava-v1.6-34b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=60, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 160, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'llava-1.6v-34b-tokenizer/'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 160, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'llava-1.6v-34b-tokenizer/'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 32, in <module>
    run_server(server_args)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 25, in run_server
    launch_server(server_args)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/http_server.py", line 1704, in launch_server
    launch_subprocesses_func(server_args=server_args)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/engine.py", line 151, in _launch_subprocesses
    tokenizer_manager, template_manager = _init_tokenizer_manager(
                                          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/engine.py", line 887, in _init_tokenizer_manager
    tokenizer_manager = TokenizerManagerClass(server_args, port_args)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/tokenizer_manager.py", line 215, in __init__
    _processor = _get_processor_wrapper(server_args)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/tokenizer_manager.py", line 2219, in _get_processor_wrapper
    processor = get_processor(
                ^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 533, in get_processor
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of 'llava-1.6v-34b-tokenizer/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'llava-1.6v-34b-tokenizer/' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-17-13:02:56 (PID:133742, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
E
======================================================================
ERROR: setUpClass (__main__.TestMistral7B)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/gsm8k_ascend_mixin.py", line 42, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 20.006s

FAILED (errors=1)
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/AI-ModelScope/llava-v1.6-34b --trust-remote-code --tp-size 4 --max-running-requests 2048 --mem-fraction-static 0.7 --attention-backend ascend --disable-cuda-graph --mm-per-request-timeout 60 --enable-multimodal --tokenizer-path llava-1.6v-34b-tokenizer/ --device npu --host 127.0.0.1 --port 21000
.
.
End (16/23):
filename='ascend/vlm_models/test_ascend_llava_v1_6_34b.py', elapsed=30, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_llava_v1_6_34b.py failed with retriable pattern: latency
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_llava_v1_6_34b.py

.
.
Begin (16/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_llava_v1_6_34b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 13:04:23] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/AI-ModelScope/llava-v1.6-34b', tokenizer_path='llava-1.6v-34b-tokenizer/', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.7, max_running_requests=2048, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=319588124, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/AI-ModelScope/llava-v1.6-34b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=60, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 160, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'llava-1.6v-34b-tokenizer/'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
                     ^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 160, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'llava-1.6v-34b-tokenizer/'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 32, in <module>
    run_server(server_args)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 25, in run_server
    launch_server(server_args)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/http_server.py", line 1704, in launch_server
    launch_subprocesses_func(server_args=server_args)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/engine.py", line 151, in _launch_subprocesses
    tokenizer_manager, template_manager = _init_tokenizer_manager(
                                          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/engine.py", line 887, in _init_tokenizer_manager
    tokenizer_manager = TokenizerManagerClass(server_args, port_args)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/tokenizer_manager.py", line 215, in __init__
    _processor = _get_processor_wrapper(server_args)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/tokenizer_manager.py", line 2219, in _get_processor_wrapper
    processor = get_processor(
                ^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 533, in get_processor
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of 'llava-1.6v-34b-tokenizer/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'llava-1.6v-34b-tokenizer/' is the correct path to a directory containing a config.json file
[ERROR] 2026-01-17-13:04:25 (PID:134602, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
E
======================================================================
ERROR: setUpClass (__main__.TestMistral7B)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/gsm8k_ascend_mixin.py", line 42, in setUpClass
    cls.process = popen_launch_server(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 660, in popen_launch_server
    raise Exception(
Exception: Server process exited with code 1. Check server logs for errors.

----------------------------------------------------------------------
Ran 0 tests in 20.005s

FAILED (errors=1)
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/AI-ModelScope/llava-v1.6-34b --trust-remote-code --tp-size 4 --max-running-requests 2048 --mem-fraction-static 0.7 --attention-backend ascend --disable-cuda-graph --mm-per-request-timeout 60 --enable-multimodal --tokenizer-path llava-1.6v-34b-tokenizer/ --device npu --host 127.0.0.1 --port 21000
.
.
End (16/23):
filename='ascend/vlm_models/test_ascend_llava_v1_6_34b.py', elapsed=30, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_llava_v1_6_34b.py returned exit code 1

.
.
Begin (17/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_llava_next_72b.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
[2026-01-17 13:04:52] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/lmms-lab/llava-next-72b', tokenizer_path='/root/.cache/modelscope/hub/models/lmms-lab/llava-next-72b', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=16, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=385382463, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/lmms-lab/llava-next-72b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-17 13:04:54] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
`torch_dtype` is deprecated! Use `dtype` instead!
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-17 13:05:05 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-17 13:05:06 TP6] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
[2026-01-17 13:05:06 TP5] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:05:06 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:05:06 TP2] Init torch distributed begin.
[2026-01-17 13:05:06 TP7] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:05:06 TP3] Init torch distributed begin.
[2026-01-17 13:05:06 TP12] Init torch distributed begin.
[2026-01-17 13:05:06 TP15] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:05:06 TP14] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:05:06 TP10] Init torch distributed begin.
[2026-01-17 13:05:06 TP13] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:05:06 TP11] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:05:06 TP4] Init torch distributed begin.
[2026-01-17 13:05:07 TP8] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:05:07 TP9] Init torch distributed begin.
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[2026-01-17 13:05:08 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP13] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP12] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP15] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP11] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP14] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP9] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP10] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP8] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:05:08 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 13:05:08 TP10] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP13] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP15] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP11] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP12] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP8] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP14] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP9] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:05:08 TP13] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:05:08 TP10] Load weight begin. avail mem=60.88 GB
[2026-01-17 13:05:08 TP15] Load weight begin. avail mem=61.13 GB
[2026-01-17 13:05:08 TP11] Load weight begin. avail mem=61.13 GB
[2026-01-17 13:05:08 TP7] Load weight begin. avail mem=61.13 GB
[2026-01-17 13:05:08 TP6] Load weight begin. avail mem=60.88 GB
[2026-01-17 13:05:08 TP12] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:05:08 TP8] Load weight begin. avail mem=60.87 GB
[2026-01-17 13:05:08 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:05:08 TP14] Load weight begin. avail mem=60.88 GB
[2026-01-17 13:05:09 TP9] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:05:09 TP4] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:05:09 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:05:09 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:05:09 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 13:05:09 TP5] Load weight begin. avail mem=61.14 GB
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!

Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:05<02:38,  5.47s/it]

Loading safetensors checkpoint shards:   7% Completed | 2/30 [00:11<02:46,  5.96s/it]

Loading safetensors checkpoint shards:  10% Completed | 3/30 [00:17<02:39,  5.91s/it]

Loading safetensors checkpoint shards:  13% Completed | 4/30 [00:24<02:41,  6.21s/it]

Loading safetensors checkpoint shards:  17% Completed | 5/30 [00:30<02:34,  6.17s/it]

Loading safetensors checkpoint shards:  20% Completed | 6/30 [00:35<02:17,  5.75s/it]

Loading safetensors checkpoint shards:  23% Completed | 7/30 [00:41<02:15,  5.87s/it]

Loading safetensors checkpoint shards:  27% Completed | 8/30 [00:47<02:12,  6.02s/it]

Loading safetensors checkpoint shards:  30% Completed | 9/30 [00:53<02:04,  5.91s/it]

Loading safetensors checkpoint shards:  33% Completed | 10/30 [00:58<01:55,  5.77s/it]

Loading safetensors checkpoint shards:  37% Completed | 11/30 [01:04<01:49,  5.75s/it]

Loading safetensors checkpoint shards:  40% Completed | 12/30 [01:10<01:46,  5.93s/it]

Loading safetensors checkpoint shards:  43% Completed | 13/30 [01:17<01:42,  6.03s/it]

Loading safetensors checkpoint shards:  47% Completed | 14/30 [01:22<01:33,  5.85s/it]

Loading safetensors checkpoint shards:  50% Completed | 15/30 [01:29<01:32,  6.15s/it]

Loading safetensors checkpoint shards:  53% Completed | 16/30 [01:36<01:29,  6.43s/it]

Loading safetensors checkpoint shards:  57% Completed | 17/30 [01:42<01:23,  6.39s/it]

Loading safetensors checkpoint shards:  60% Completed | 18/30 [01:49<01:17,  6.46s/it]

Loading safetensors checkpoint shards:  63% Completed | 19/30 [01:55<01:10,  6.44s/it]

Loading safetensors checkpoint shards:  67% Completed | 20/30 [02:02<01:03,  6.36s/it]

Loading safetensors checkpoint shards:  70% Completed | 21/30 [02:08<00:57,  6.39s/it]

Loading safetensors checkpoint shards:  73% Completed | 22/30 [02:14<00:50,  6.33s/it]

Loading safetensors checkpoint shards:  77% Completed | 23/30 [02:21<00:44,  6.37s/it]

Loading safetensors checkpoint shards:  80% Completed | 24/30 [02:26<00:37,  6.18s/it]

Loading safetensors checkpoint shards:  83% Completed | 25/30 [02:33<00:30,  6.16s/it]

Loading safetensors checkpoint shards:  87% Completed | 26/30 [02:39<00:25,  6.29s/it]

Loading safetensors checkpoint shards:  90% Completed | 27/30 [02:45<00:18,  6.29s/it]

Loading safetensors checkpoint shards:  93% Completed | 28/30 [02:52<00:12,  6.27s/it]

Loading safetensors checkpoint shards:  97% Completed | 29/30 [02:58<00:06,  6.24s/it]

Loading safetensors checkpoint shards: 100% Completed | 30/30 [03:04<00:00,  6.22s/it]

Loading safetensors checkpoint shards: 100% Completed | 30/30 [03:04<00:00,  6.15s/it]

[2026-01-17 13:08:23 TP0] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.66 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP7] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.98 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP2] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.71 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP5] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.99 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP8] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.72 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP4] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.72 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP9] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.99 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP15] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.98 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP10] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.73 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP6] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.73 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP11] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.98 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP13] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.99 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP14] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.73 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP3] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.99 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP12] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.72 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP1] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=51.99 GB, mem usage=9.15 GB.
[2026-01-17 13:08:24 TP0] Using KV cache dtype: torch.float16
[2026-01-17 13:08:24 TP0] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP15] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP14] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP13] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP12] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP11] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP10] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP9] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP8] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP7] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP6] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP5] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP3] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP2] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP1] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:24 TP4] The available memory for KV cache is 39.48 GB.
[2026-01-17 13:08:25 TP4] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:25 TP12] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:25 TP4] Memory pool end. avail mem=11.72 GB
[2026-01-17 13:08:25 TP0] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP8] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP6] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP2] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP12] Memory pool end. avail mem=11.72 GB
[2026-01-17 13:08:26 TP0] Memory pool end. avail mem=11.66 GB
[2026-01-17 13:08:26 TP14] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP8] Memory pool end. avail mem=11.72 GB
[2026-01-17 13:08:26 TP6] Memory pool end. avail mem=11.73 GB
[2026-01-17 13:08:26 TP2] Memory pool end. avail mem=11.71 GB
[2026-01-17 13:08:26 TP3] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP9] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP13] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP15] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP14] Memory pool end. avail mem=11.73 GB
[2026-01-17 13:08:26 TP10] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP5] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP1] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP11] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP3] Memory pool end. avail mem=11.99 GB
[2026-01-17 13:08:26 TP9] Memory pool end. avail mem=11.99 GB
[2026-01-17 13:08:26 TP13] Memory pool end. avail mem=11.99 GB
[2026-01-17 13:08:26 TP15] Memory pool end. avail mem=11.98 GB
[2026-01-17 13:08:26 TP7] KV Cache is allocated. #tokens: 258688, K size: 19.75 GB, V size: 19.75 GB
[2026-01-17 13:08:26 TP10] Memory pool end. avail mem=11.73 GB
[2026-01-17 13:08:26 TP5] Memory pool end. avail mem=11.99 GB
[2026-01-17 13:08:26 TP1] Memory pool end. avail mem=11.99 GB
[2026-01-17 13:08:26 TP11] Memory pool end. avail mem=11.98 GB
[2026-01-17 13:08:26 TP7] Memory pool end. avail mem=11.98 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 13:08:27 TP0] max_total_num_tokens=258688, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4042, context_len=32768, available_gpu_mem=11.66 GB
[2026-01-17 13:08:28] INFO:     Started server process [135462]
[2026-01-17 13:08:28] INFO:     Waiting for application startup.
[2026-01-17 13:08:28] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:08:28] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:08:28] INFO:     Application startup complete.
[2026-01-17 13:08:28] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 13:08:29] INFO:     127.0.0.1:43570 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 13:08:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:08:33] INFO:     127.0.0.1:43574 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:08:43] INFO:     127.0.0.1:54728 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:08:53] INFO:     127.0.0.1:45000 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
.[2026-01-17 13:08:56] INFO:     127.0.0.1:43572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 13:08:56] The server is fired up and ready to roll!
[2026-01-17 13:09:03 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:09:04] INFO:     127.0.0.1:56068 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 13:09:04] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 13:09:04] INFO:     127.0.0.1:56080 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 13:09:04 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:09:04] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/lmms-lab/llava-next-72b --trust-remote-code --tp-size 16 --mem-fraction-static 0.8 --attention-backend ascend --disable-cuda-graph --enable-multimodal --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 13:09:04 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:09:04 TP0] Prefill batch, #new-seq: 23, #new-token: 3200, #cached-token: 17664, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 13:09:06 TP0] Prefill batch, #new-seq: 63, #new-token: 8192, #cached-token: 48384, token usage: 0.02, #running-req: 24, #queue-req: 41,
[2026-01-17 13:09:07 TP0] Prefill batch, #new-seq: 41, #new-token: 5632, #cached-token: 31488, token usage: 0.05, #running-req: 87, #queue-req: 0,
[2026-01-17 13:09:12 TP0] Decode batch, #running-req: 128, #token: 21376, token usage: 0.08, npu graph: False, gen throughput (token/s): 16.08, #queue-req: 0,
[2026-01-17 13:09:12] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:08<27:36,  8.32s/it][2026-01-17 13:09:12 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.08, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:14] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:10<15:53,  4.81s/it][2026-01-17 13:09:14] INFO:     127.0.0.1:56842 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:14 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 13:09:15] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:11<06:19,  1.94s/it][2026-01-17 13:09:15] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:15] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:15] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:15] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:15] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK

  4%|▍         | 8/200 [00:11<02:18,  1.39it/s]
  4%|▍         | 9/200 [00:11<01:07,  2.82it/s][2026-01-17 13:09:15 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 2304, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 13:09:17 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 131, #queue-req: 0,
[2026-01-17 13:09:18] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:18] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:18] INFO:     127.0.0.1:56414 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:14<02:06,  1.49it/s]
  6%|▌         | 12/200 [00:14<02:35,  1.21it/s][2026-01-17 13:09:18 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:19] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:19 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.10, #running-req: 128, #queue-req: 0,
[2026-01-17 13:09:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.10, #running-req: 130, #queue-req: 0,
[2026-01-17 13:09:20] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:20] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:16<02:38,  1.17it/s]
  8%|▊         | 15/200 [00:16<02:22,  1.29it/s][2026-01-17 13:09:20 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 13:09:20] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:20] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 16/200 [00:16<02:06,  1.45it/s]
  8%|▊         | 17/200 [00:16<01:34,  1.94it/s][2026-01-17 13:09:21 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 13:09:21] INFO:     127.0.0.1:57234 - "POST /generate HTTP/1.1" 200 OK

  9%|▉         | 18/200 [00:16<01:23,  2.19it/s][2026-01-17 13:09:21 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:21] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:21] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK

 10%|▉         | 19/200 [00:17<01:12,  2.48it/s]
 10%|█         | 20/200 [00:17<00:52,  3.44it/s][2026-01-17 13:09:21 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 13:09:22] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:22] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:18<01:38,  1.81it/s]
 11%|█         | 22/200 [00:18<01:53,  1.57it/s][2026-01-17 13:09:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 13:09:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 13:09:23] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 23/200 [00:19<01:44,  1.69it/s][2026-01-17 13:09:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 13:09:23] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK

 12%|█▏        | 24/200 [00:19<01:32,  1.90it/s][2026-01-17 13:09:23 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 13:09:23] INFO:     127.0.0.1:56374 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:23] INFO:     127.0.0.1:57116 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:23 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 13:09:23] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:23] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK

 14%|█▎        | 27/200 [00:19<00:55,  3.10it/s]
 14%|█▍        | 28/200 [00:19<00:35,  4.81it/s][2026-01-17 13:09:24] INFO:     127.0.0.1:56572 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:24] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 13:09:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 13:09:24] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:24] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 31/200 [00:20<00:32,  5.14it/s]
 16%|█▌        | 32/200 [00:20<00:28,  5.96it/s][2026-01-17 13:09:24] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:24] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 13:09:24] INFO:     127.0.0.1:56284 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:24] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 35/200 [00:20<00:22,  7.44it/s]
 18%|█▊        | 36/200 [00:20<00:16,  9.91it/s][2026-01-17 13:09:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 13:09:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 130, #queue-req: 0,
[2026-01-17 13:09:25 TP0] Decode batch, #running-req: 128, #token: 29440, token usage: 0.11, npu graph: False, gen throughput (token/s): 403.05, #queue-req: 0,
[2026-01-17 13:09:25] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:25] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK

 19%|█▉        | 38/200 [00:20<00:22,  7.12it/s][2026-01-17 13:09:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:25 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 13:09:27] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:27] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:27] INFO:     127.0.0.1:56418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:27] INFO:     127.0.0.1:56438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:27] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:23<01:04,  2.48it/s]
 22%|██▏       | 43/200 [00:23<01:28,  1.78it/s]
 22%|██▏       | 43/200 [00:23<01:28,  1.78it/s]
 22%|██▏       | 43/200 [00:23<01:28,  1.78it/s][2026-01-17 13:09:27 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:27] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:27 TP0] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 3072, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 13:09:27 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 132, #queue-req: 0,
[2026-01-17 13:09:28] INFO:     127.0.0.1:38430 - "POST /generate HTTP/1.1" 200 OK

 22%|██▎       | 45/200 [00:24<01:16,  2.02it/s][2026-01-17 13:09:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:28] INFO:     127.0.0.1:56436 - "POST /generate HTTP/1.1" 200 OK

 23%|██▎       | 46/200 [00:24<01:10,  2.19it/s][2026-01-17 13:09:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:28] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK

 24%|██▎       | 47/200 [00:24<01:04,  2.39it/s][2026-01-17 13:09:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:28] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:28] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:24<01:01,  2.49it/s]
 24%|██▍       | 49/200 [00:24<00:48,  3.10it/s][2026-01-17 13:09:29 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 13:09:29] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:29] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK

 25%|██▌       | 50/200 [00:24<00:43,  3.41it/s]
 26%|██▌       | 51/200 [00:24<00:32,  4.55it/s][2026-01-17 13:09:29 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 13:09:29] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 52/200 [00:25<00:31,  4.66it/s][2026-01-17 13:09:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:29] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK

 26%|██▋       | 53/200 [00:25<00:31,  4.70it/s][2026-01-17 13:09:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:29] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK

 27%|██▋       | 54/200 [00:25<00:30,  4.71it/s][2026-01-17 13:09:29 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:29] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:29] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:25<00:28,  5.00it/s]
 28%|██▊       | 56/200 [00:25<00:21,  6.67it/s][2026-01-17 13:09:29 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 13:09:30] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:30] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:25<00:21,  6.53it/s]
 29%|██▉       | 58/200 [00:25<00:17,  8.03it/s][2026-01-17 13:09:30 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 13:09:30] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK

 30%|██▉       | 59/200 [00:25<00:19,  7.19it/s][2026-01-17 13:09:30 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:30] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:26<00:26,  5.37it/s][2026-01-17 13:09:30 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:31] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 61/200 [00:26<00:37,  3.69it/s][2026-01-17 13:09:31] INFO:     127.0.0.1:38438 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:31] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:31] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:27<00:26,  5.09it/s]
 32%|███▏      | 64/200 [00:27<00:18,  7.52it/s][2026-01-17 13:09:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 128, #queue-req: 0,
[2026-01-17 13:09:31 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 129, #queue-req: 0,
[2026-01-17 13:09:33] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:33] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:33] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK

 32%|███▎      | 65/200 [00:29<01:06,  2.04it/s]
 34%|███▎      | 67/200 [00:29<01:30,  1.46it/s]
 34%|███▎      | 67/200 [00:29<01:30,  1.46it/s][2026-01-17 13:09:33 TP0] Prefill batch, #new-seq: 3, #new-token: 512, #cached-token: 2304, token usage: 0.11, #running-req: 125, #queue-req: 0,
[2026-01-17 13:09:34] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 68/200 [00:30<01:50,  1.19it/s][2026-01-17 13:09:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:35] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK

 34%|███▍      | 69/200 [00:30<01:32,  1.41it/s][2026-01-17 13:09:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 127, #queue-req: 0,
[2026-01-17 13:09:35] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:35] INFO:     127.0.0.1:39966 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:31<01:16,  1.69it/s]
 36%|███▌      | 71/200 [00:31<00:51,  2.52it/s][2026-01-17 13:09:35 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1536, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 13:09:35] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:35] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:31<00:44,  2.88it/s]
 36%|███▋      | 73/200 [00:31<00:31,  4.06it/s][2026-01-17 13:09:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 768, token usage: 0.11, #running-req: 126, #queue-req: 0,
[2026-01-17 13:09:35] INFO:     127.0.0.1:56150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:35] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:31<00:28,  4.37it/s]
 38%|███▊      | 75/200 [00:31<00:21,  5.78it/s][2026-01-17 13:09:35] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 76/200 [00:31<00:20,  5.93it/s][2026-01-17 13:09:35 TP0] Decode batch, #running-req: 124, #token: 28032, token usage: 0.11, npu graph: False, gen throughput (token/s): 467.20, #queue-req: 0,
[2026-01-17 13:09:35] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK

 38%|███▊      | 77/200 [00:31<00:22,  5.38it/s][2026-01-17 13:09:36] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK

 39%|███▉      | 78/200 [00:32<00:25,  4.81it/s][2026-01-17 13:09:36] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:36] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK

 40%|████      | 80/200 [00:32<00:18,  6.35it/s][2026-01-17 13:09:36] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:36] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:36] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:36] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:32<00:15,  7.71it/s]
 42%|████▏     | 84/200 [00:32<00:08, 14.01it/s]
 42%|████▏     | 84/200 [00:32<00:08, 14.01it/s][2026-01-17 13:09:36] INFO:     127.0.0.1:56758 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:36] INFO:     127.0.0.1:37876 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:36] INFO:     127.0.0.1:38410 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:32<00:07, 15.17it/s][2026-01-17 13:09:36] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:36] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK

 44%|████▍     | 89/200 [00:32<00:07, 14.28it/s][2026-01-17 13:09:37] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:37] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:37] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 91/200 [00:32<00:09, 11.88it/s]
 46%|████▌     | 92/200 [00:32<00:09, 11.90it/s][2026-01-17 13:09:37] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:37] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:37] INFO:     127.0.0.1:37886 - "POST /generate HTTP/1.1" 200 OK

 47%|████▋     | 94/200 [00:33<00:09, 11.28it/s]
 48%|████▊     | 95/200 [00:33<00:08, 12.26it/s][2026-01-17 13:09:37] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:37] INFO:     127.0.0.1:37872 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 97/200 [00:33<00:08, 12.24it/s][2026-01-17 13:09:37] INFO:     127.0.0.1:38420 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:37] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:37] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK

 50%|████▉     | 99/200 [00:33<00:08, 12.07it/s]
 50%|█████     | 100/200 [00:33<00:07, 13.58it/s][2026-01-17 13:09:37] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:37] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:37] INFO:     127.0.0.1:38108 - "POST /generate HTTP/1.1" 200 OK

 51%|█████     | 102/200 [00:33<00:08, 11.41it/s]
 52%|█████▏    | 103/200 [00:33<00:08, 11.51it/s][2026-01-17 13:09:38] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:38] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▎    | 105/200 [00:33<00:08, 11.51it/s][2026-01-17 13:09:38] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:38] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▎    | 107/200 [00:34<00:08, 11.19it/s][2026-01-17 13:09:38] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:38] INFO:     127.0.0.1:40142 - "POST /generate HTTP/1.1" 200 OK

 55%|█████▍    | 109/200 [00:34<00:10,  8.42it/s][2026-01-17 13:09:38] INFO:     127.0.0.1:39952 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:38036 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:34<00:11,  7.67it/s][2026-01-17 13:09:39] INFO:     127.0.0.1:37976 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:38088 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:38130 - "POST /generate HTTP/1.1" 200 OK

 57%|█████▋    | 114/200 [00:35<00:09,  9.49it/s][2026-01-17 13:09:39 TP0] Decode batch, #running-req: 87, #token: 22016, token usage: 0.09, npu graph: False, gen throughput (token/s): 1225.78, #queue-req: 0,
[2026-01-17 13:09:39] INFO:     127.0.0.1:38074 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:35<00:08,  9.97it/s]
 58%|█████▊    | 117/200 [00:35<00:07, 11.79it/s][2026-01-17 13:09:39] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:38104 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 120/200 [00:35<00:07, 10.91it/s][2026-01-17 13:09:39] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:38394 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:35<00:06, 12.66it/s][2026-01-17 13:09:39] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:38016 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:39] INFO:     127.0.0.1:40200 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:40] INFO:     127.0.0.1:56534 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:40] INFO:     127.0.0.1:39978 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 129/200 [00:36<00:05, 12.12it/s]
 65%|██████▌   | 130/200 [00:36<00:05, 12.71it/s][2026-01-17 13:09:40] INFO:     127.0.0.1:56456 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:40] INFO:     127.0.0.1:37964 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:36<00:05, 12.71it/s][2026-01-17 13:09:40] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:40] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:40] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:40] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 136/200 [00:36<00:04, 14.98it/s][2026-01-17 13:09:40] INFO:     127.0.0.1:37912 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:40] INFO:     127.0.0.1:40238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:40] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:40] INFO:     127.0.0.1:38414 - "POST /generate HTTP/1.1" 200 OK

 70%|██████▉   | 139/200 [00:36<00:03, 15.76it/s]
 70%|███████   | 140/200 [00:36<00:03, 17.85it/s][2026-01-17 13:09:41] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:41] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:41] INFO:     127.0.0.1:37888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:41] INFO:     127.0.0.1:40032 - "POST /generate HTTP/1.1" 200 OK

 71%|███████   | 142/200 [00:36<00:03, 14.75it/s]
 72%|███████▏  | 144/200 [00:36<00:03, 14.88it/s]
 72%|███████▏  | 144/200 [00:36<00:03, 14.88it/s][2026-01-17 13:09:41] INFO:     127.0.0.1:39986 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:41] INFO:     127.0.0.1:40022 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:41] INFO:     127.0.0.1:38142 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▎  | 147/200 [00:37<00:03, 15.07it/s][2026-01-17 13:09:41] INFO:     127.0.0.1:40228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:41] INFO:     127.0.0.1:40064 - "POST /generate HTTP/1.1" 200 OK

 74%|███████▍  | 149/200 [00:37<00:03, 12.84it/s][2026-01-17 13:09:41] INFO:     127.0.0.1:40048 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:41] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▌  | 151/200 [00:37<00:04, 11.54it/s][2026-01-17 13:09:41] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:42] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK

 76%|███████▋  | 153/200 [00:37<00:04, 10.55it/s][2026-01-17 13:09:42] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:42] INFO:     127.0.0.1:37948 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:42] INFO:     127.0.0.1:40104 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:37<00:03, 12.49it/s][2026-01-17 13:09:42] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:42] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:38<00:03, 11.91it/s][2026-01-17 13:09:42] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:42] INFO:     127.0.0.1:38110 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:42] INFO:     127.0.0.1:38124 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:42] INFO:     127.0.0.1:40078 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:42 TP0] Decode batch, #running-req: 38, #token: 12160, token usage: 0.05, npu graph: False, gen throughput (token/s): 754.44, #queue-req: 0,
[2026-01-17 13:09:43] INFO:     127.0.0.1:39988 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:43] INFO:     127.0.0.1:40164 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:38<00:04,  9.07it/s]
 82%|████████▏ | 164/200 [00:38<00:04,  8.84it/s][2026-01-17 13:09:43] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:43] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:43] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:39<00:05,  5.83it/s]
 84%|████████▎ | 167/200 [00:39<00:06,  4.85it/s][2026-01-17 13:09:44] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:44] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 168/200 [00:39<00:06,  4.98it/s]
 84%|████████▍ | 169/200 [00:39<00:05,  5.86it/s][2026-01-17 13:09:44] INFO:     127.0.0.1:38382 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:44] INFO:     127.0.0.1:38114 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:44] INFO:     127.0.0.1:40252 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:40<00:07,  3.92it/s]
 86%|████████▌ | 172/200 [00:40<00:07,  3.69it/s]
 86%|████████▌ | 172/200 [00:40<00:07,  3.69it/s][2026-01-17 13:09:44] INFO:     127.0.0.1:37926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:45] INFO:     127.0.0.1:37856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:45] INFO:     127.0.0.1:39954 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:41<00:06,  3.77it/s]
 88%|████████▊ | 175/200 [00:41<00:05,  4.36it/s][2026-01-17 13:09:45 TP0] Decode batch, #running-req: 25, #token: 9216, token usage: 0.04, npu graph: False, gen throughput (token/s): 405.08, #queue-req: 0,
[2026-01-17 13:09:45] INFO:     127.0.0.1:37938 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:41<00:06,  3.66it/s][2026-01-17 13:09:46] INFO:     127.0.0.1:38386 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:42<00:07,  3.19it/s][2026-01-17 13:09:46] INFO:     127.0.0.1:40214 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:42<00:07,  2.91it/s][2026-01-17 13:09:47] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK

 90%|████████▉ | 179/200 [00:43<00:09,  2.30it/s][2026-01-17 13:09:47] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:43<00:07,  2.68it/s][2026-01-17 13:09:48] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:43<00:06,  2.72it/s][2026-01-17 13:09:48] INFO:     127.0.0.1:40006 - "POST /generate HTTP/1.1" 200 OK

 91%|█████████ | 182/200 [00:44<00:06,  2.74it/s][2026-01-17 13:09:48] INFO:     127.0.0.1:40254 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:44<00:06,  2.52it/s][2026-01-17 13:09:49 TP0] Decode batch, #running-req: 17, #token: 6784, token usage: 0.03, npu graph: False, gen throughput (token/s): 256.92, #queue-req: 0,
[2026-01-17 13:09:49] INFO:     127.0.0.1:40118 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:44<00:05,  3.05it/s][2026-01-17 13:09:49] INFO:     127.0.0.1:40148 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:49] INFO:     127.0.0.1:40210 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:45<00:04,  3.02it/s]
 93%|█████████▎| 186/200 [00:45<00:03,  3.89it/s][2026-01-17 13:09:49] INFO:     127.0.0.1:40246 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:49] INFO:     127.0.0.1:40012 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:45<00:02,  4.07it/s][2026-01-17 13:09:50] INFO:     127.0.0.1:38052 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:09:50] INFO:     127.0.0.1:40188 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:46<00:02,  3.66it/s][2026-01-17 13:09:50] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:46<00:02,  4.05it/s][2026-01-17 13:09:52] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:47<00:03,  2.03it/s][2026-01-17 13:09:52] INFO:     127.0.0.1:38054 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:48<00:03,  2.33it/s][2026-01-17 13:09:52 TP0] Decode batch, #running-req: 8, #token: 3712, token usage: 0.01, npu graph: False, gen throughput (token/s): 135.33, #queue-req: 0,
[2026-01-17 13:09:53] INFO:     127.0.0.1:38000 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:49<00:03,  1.70it/s][2026-01-17 13:09:53] INFO:     127.0.0.1:40176 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:49<00:02,  2.07it/s][2026-01-17 13:09:55] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:51<00:03,  1.14it/s][2026-01-17 13:09:55 TP0] Decode batch, #running-req: 4, #token: 2688, token usage: 0.01, npu graph: False, gen throughput (token/s): 69.84, #queue-req: 0,
[2026-01-17 13:09:58] INFO:     127.0.0.1:40040 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:53<00:04,  1.40s/it][2026-01-17 13:09:58 TP0] Decode batch, #running-req: 3, #token: 2304, token usage: 0.01, npu graph: False, gen throughput (token/s): 46.49, #queue-req: 0,
[2026-01-17 13:09:58] INFO:     127.0.0.1:37848 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:54<00:02,  1.23s/it][2026-01-17 13:10:02 TP0] Decode batch, #running-req: 2, #token: 1792, token usage: 0.01, npu graph: False, gen throughput (token/s): 25.78, #queue-req: 0,
[2026-01-17 13:10:05 TP0] Decode batch, #running-req: 2, #token: 2048, token usage: 0.01, npu graph: False, gen throughput (token/s): 25.04, #queue-req: 0,
[2026-01-17 13:10:08 TP0] Decode batch, #running-req: 2, #token: 1408, token usage: 0.01, npu graph: False, gen throughput (token/s): 24.90, #queue-req: 0,
[2026-01-17 13:10:08] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [01:04<00:03,  3.65s/it][2026-01-17 13:10:11 TP0] Decode batch, #running-req: 1, #token: 1408, token usage: 0.01, npu graph: False, gen throughput (token/s): 12.75, #queue-req: 0,
[2026-01-17 13:10:13] INFO:     127.0.0.1:38058 - "POST /generate HTTP/1.1" 200 OK

100%|██████████| 200/200 [01:09<00:00,  4.11s/it]
100%|██████████| 200/200 [01:09<00:00,  2.88it/s]
.
----------------------------------------------------------------------
Ran 1 test in 332.209s

OK
Accuracy: 0.805
Invalid: 0.000
Latency: 69.683 s
Output throughput: 346.568 token/s
.
.
End (17/23):
filename='ascend/vlm_models/test_ascend_llava_next_72b.py', elapsed=343, estimated_time=400
.
.

.
.
Begin (18/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_kimi_vl_a3b_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 13:10:38] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Kimi/Kimi-VL-A3B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Kimi/Kimi-VL-A3B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.7, max_running_requests=2048, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=1047877883, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Kimi/Kimi-VL-A3B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:10:41] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/hf_transformers_utils.py:498: UserWarning: Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:10:49 TP3] Init torch distributed begin.
[2026-01-17 13:10:49 TP0] Init torch distributed begin.
[2026-01-17 13:10:49 TP1] Init torch distributed begin.
[2026-01-17 13:10:49 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 13:10:51 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:10:51 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:10:51 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:10:51 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:10:51 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 13:10:52 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:10:52 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:10:52 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:10:52 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:10:52 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:10:52 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 13:10:52 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:10:52 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:10:52 TP0] Config not support fused shared expert(s). Shared experts fusion optimization is disabled.

Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:06<00:39,  6.66s/it]

Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:13<00:33,  6.64s/it]

Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:19<00:26,  6.63s/it]

Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:27<00:20,  6.86s/it]

Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:31<00:11,  5.87s/it]

Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:37<00:06,  6.01s/it]

Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:44<00:00,  6.49s/it]

Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:44<00:00,  6.43s/it]

[2026-01-17 13:11:50 TP0] Load weight end. type=KimiVLForConditionalGeneration, dtype=torch.bfloat16, avail mem=52.39 GB, mem usage=8.42 GB.
[2026-01-17 13:11:50 TP3] Load weight end. type=KimiVLForConditionalGeneration, dtype=torch.bfloat16, avail mem=52.72 GB, mem usage=8.42 GB.
[2026-01-17 13:11:50 TP1] Load weight end. type=KimiVLForConditionalGeneration, dtype=torch.bfloat16, avail mem=52.72 GB, mem usage=8.42 GB.
[2026-01-17 13:11:50 TP2] Load weight end. type=KimiVLForConditionalGeneration, dtype=torch.bfloat16, avail mem=52.44 GB, mem usage=8.42 GB.
[2026-01-17 13:11:50 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 13:11:50 TP0] The available memory for KV cache is 34.15 GB.
[2026-01-17 13:11:50 TP3] The available memory for KV cache is 34.15 GB.
[2026-01-17 13:11:50 TP2] The available memory for KV cache is 34.15 GB.
[2026-01-17 13:11:50 TP1] The available memory for KV cache is 34.15 GB.
[2026-01-17 13:11:52 TP0] KV Cache is allocated. #tokens: 1178752, KV size: 34.15 GB
[2026-01-17 13:11:52 TP0] Memory pool end. avail mem=17.25 GB
[2026-01-17 13:11:52 TP1] KV Cache is allocated. #tokens: 1178752, KV size: 34.15 GB
[2026-01-17 13:11:52 TP1] Memory pool end. avail mem=17.58 GB
[2026-01-17 13:11:52 TP2] KV Cache is allocated. #tokens: 1178752, KV size: 34.15 GB
[2026-01-17 13:11:52 TP2] Memory pool end. avail mem=17.30 GB
[2026-01-17 13:11:52 TP3] KV Cache is allocated. #tokens: 1178752, KV size: 34.15 GB
[2026-01-17 13:11:52 TP3] Memory pool end. avail mem=17.58 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 13:11:53 TP0] max_total_num_tokens=1178752, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=131072, available_gpu_mem=17.25 GB
[2026-01-17 13:11:53] INFO:     Started server process [173018]
[2026-01-17 13:11:53] INFO:     Waiting for application startup.
[2026-01-17 13:11:53] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.2, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 13:11:53] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.2, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 13:11:53] INFO:     Application startup complete.
[2026-01-17 13:11:53] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 13:11:54] INFO:     127.0.0.1:45994 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 13:11:55 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:11:56] INFO:     127.0.0.1:46006 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:11:59] INFO:     127.0.0.1:46000 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 13:11:59] The server is fired up and ready to roll!
[2026-01-17 13:12:06 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:12:07] INFO:     127.0.0.1:45250 - "GET /health_generate HTTP/1.1" 200 OK
[2026-01-17 13:12:07] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2026-01-17 13:12:07] INFO:     127.0.0.1:48088 - "GET /get_model_info HTTP/1.1" 200 OK
[2026-01-17 13:12:07 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:12:07] INFO:     127.0.0.1:48104 - "POST /generate HTTP/1.1" 200 OK
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Kimi/Kimi-VL-A3B-Instruct --trust-remote-code --max-running-requests 2048 --mem-fraction-static 0.7 --attention-backend ascend --base-gpu-id 0 --port 8010 --tp-size 4 --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
[CI Test Method] TestMistral7B.test_gsm8k

  0%|          | 0/200 [00:00<?, ?it/s][2026-01-17 13:12:07 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:12:07 TP0] Prefill batch, #new-seq: 26, #new-token: 3584, #cached-token: 16640, token usage: 0.00, #running-req: 1, #queue-req: 0,
[2026-01-17 13:12:08 TP0] Prefill batch, #new-seq: 62, #new-token: 8192, #cached-token: 39680, token usage: 0.00, #running-req: 27, #queue-req: 39,
[2026-01-17 13:12:10 TP0] Prefill batch, #new-seq: 39, #new-token: 5248, #cached-token: 24960, token usage: 0.01, #running-req: 89, #queue-req: 0,
[2026-01-17 13:12:13] INFO:     127.0.0.1:48664 - "POST /generate HTTP/1.1" 200 OK

  0%|          | 1/200 [00:06<20:14,  6.11s/it][2026-01-17 13:12:13 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:14] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK

  1%|          | 2/200 [00:06<09:06,  2.76s/it][2026-01-17 13:12:14 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:14] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:14 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 13:12:14] INFO:     127.0.0.1:48318 - "POST /generate HTTP/1.1" 200 OK

  2%|▏         | 4/200 [00:06<03:33,  1.09s/it][2026-01-17 13:12:14 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 13:12:15] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK

  2%|▎         | 5/200 [00:08<03:57,  1.22s/it][2026-01-17 13:12:15 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:16 TP0] Decode batch, #running-req: 128, #token: 21248, token usage: 0.02, npu graph: False, gen throughput (token/s): 45.44, #queue-req: 0,
[2026-01-17 13:12:16] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:16] INFO:     127.0.0.1:48674 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:16] INFO:     127.0.0.1:49200 - "POST /generate HTTP/1.1" 200 OK

  3%|▎         | 6/200 [00:09<03:39,  1.13s/it]
  4%|▍         | 8/200 [00:09<01:48,  1.77it/s]
  4%|▍         | 8/200 [00:09<01:48,  1.77it/s][2026-01-17 13:12:16] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:16] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:16 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 13:12:16] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:16 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 13:12:16] INFO:     127.0.0.1:48874 - "POST /generate HTTP/1.1" 200 OK

  6%|▌         | 11/200 [00:09<01:07,  2.81it/s]
  6%|▌         | 12/200 [00:09<00:41,  4.54it/s][2026-01-17 13:12:16 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 130, #queue-req: 0,
[2026-01-17 13:12:17] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:17] INFO:     127.0.0.1:48550 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:17] INFO:     127.0.0.1:49278 - "POST /generate HTTP/1.1" 200 OK

  7%|▋         | 14/200 [00:09<00:40,  4.61it/s]
  8%|▊         | 15/200 [00:09<00:35,  5.26it/s][2026-01-17 13:12:17] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:17 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 13:12:17] INFO:     127.0.0.1:49080 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:17] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK

  8%|▊         | 17/200 [00:09<00:30,  6.01it/s]
  9%|▉         | 18/200 [00:09<00:23,  7.66it/s][2026-01-17 13:12:17 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 13:12:17 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 13:12:17] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:17] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 20/200 [00:10<00:24,  7.24it/s][2026-01-17 13:12:17 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 13:12:17] INFO:     127.0.0.1:49250 - "POST /generate HTTP/1.1" 200 OK

 10%|█         | 21/200 [00:10<00:26,  6.88it/s][2026-01-17 13:12:18 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:18] INFO:     127.0.0.1:48134 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:18] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:18] INFO:     127.0.0.1:48376 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:18] INFO:     127.0.0.1:49126 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:18] INFO:     127.0.0.1:49128 - "POST /generate HTTP/1.1" 200 OK

 11%|█         | 22/200 [00:10<00:26,  6.61it/s]
 13%|█▎        | 26/200 [00:10<00:10, 17.16it/s]
 13%|█▎        | 26/200 [00:10<00:10, 17.16it/s]
 13%|█▎        | 26/200 [00:10<00:10, 17.16it/s]
 13%|█▎        | 26/200 [00:10<00:10, 17.16it/s][2026-01-17 13:12:18 TP0] Prefill batch, #new-seq: 5, #new-token: 768, #cached-token: 3200, token usage: 0.02, #running-req: 123, #queue-req: 0,
[2026-01-17 13:12:19] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:19] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK

 14%|█▍        | 28/200 [00:12<00:36,  4.72it/s][2026-01-17 13:12:19 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:19] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:19 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 13:12:19 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 13:12:20] INFO:     127.0.0.1:48994 - "POST /generate HTTP/1.1" 200 OK

 15%|█▌        | 30/200 [00:12<00:36,  4.71it/s][2026-01-17 13:12:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:20] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:20] INFO:     127.0.0.1:48482 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:20] INFO:     127.0.0.1:49182 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:20] INFO:     127.0.0.1:49256 - "POST /generate HTTP/1.1" 200 OK

 16%|█▌        | 32/200 [00:12<00:30,  5.42it/s]
 17%|█▋        | 34/200 [00:12<00:17,  9.44it/s]
 17%|█▋        | 34/200 [00:12<00:17,  9.44it/s][2026-01-17 13:12:20 TP0] Prefill batch, #new-seq: 4, #new-token: 512, #cached-token: 2560, token usage: 0.02, #running-req: 124, #queue-req: 0,
[2026-01-17 13:12:21] INFO:     127.0.0.1:48640 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:21] INFO:     127.0.0.1:48888 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:21] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:21] INFO:     127.0.0.1:49224 - "POST /generate HTTP/1.1" 200 OK

 18%|█▊        | 36/200 [00:14<00:39,  4.20it/s]
 19%|█▉        | 38/200 [00:14<00:53,  3.05it/s]
 19%|█▉        | 38/200 [00:14<00:53,  3.05it/s][2026-01-17 13:12:21 TP0] Prefill batch, #new-seq: 4, #new-token: 640, #cached-token: 2560, token usage: 0.02, #running-req: 124, #queue-req: 0,
[2026-01-17 13:12:21] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK

 20%|█▉        | 39/200 [00:14<00:49,  3.23it/s][2026-01-17 13:12:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:22] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 40/200 [00:14<00:46,  3.47it/s][2026-01-17 13:12:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:22] INFO:     127.0.0.1:48758 - "POST /generate HTTP/1.1" 200 OK

 20%|██        | 41/200 [00:14<00:42,  3.73it/s][2026-01-17 13:12:22 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:22] INFO:     127.0.0.1:48522 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:22] INFO:     127.0.0.1:49216 - "POST /generate HTTP/1.1" 200 OK

 21%|██        | 42/200 [00:14<00:39,  3.99it/s]
 22%|██▏       | 43/200 [00:14<00:30,  5.19it/s][2026-01-17 13:12:22 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 13:12:22] INFO:     127.0.0.1:48904 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:22] INFO:     127.0.0.1:48980 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:22] INFO:     127.0.0.1:49050 - "POST /generate HTTP/1.1" 200 OK

 22%|██▏       | 44/200 [00:15<00:29,  5.28it/s]
 23%|██▎       | 46/200 [00:15<00:17,  8.99it/s]
 23%|██▎       | 46/200 [00:15<00:17,  8.99it/s][2026-01-17 13:12:22 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 125, #queue-req: 0,
[2026-01-17 13:12:22] INFO:     127.0.0.1:48720 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:22] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK

 24%|██▍       | 48/200 [00:15<00:19,  7.91it/s][2026-01-17 13:12:23] INFO:     127.0.0.1:48296 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:23] INFO:     127.0.0.1:48432 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:23 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 13:12:23] INFO:     127.0.0.1:48604 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:23] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:23] INFO:     127.0.0.1:49114 - "POST /generate HTTP/1.1" 200 OK

 26%|██▌       | 51/200 [00:15<00:15,  9.83it/s]
 26%|██▋       | 53/200 [00:15<00:08, 16.42it/s]
 26%|██▋       | 53/200 [00:15<00:08, 16.42it/s][2026-01-17 13:12:23 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 13:12:23 TP0] Prefill batch, #new-seq: 3, #new-token: 384, #cached-token: 1920, token usage: 0.02, #running-req: 130, #queue-req: 0,
[2026-01-17 13:12:23] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:23] INFO:     127.0.0.1:49070 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 55/200 [00:16<00:11, 12.26it/s][2026-01-17 13:12:23] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:23 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 13:12:23] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:23] INFO:     127.0.0.1:49066 - "POST /generate HTTP/1.1" 200 OK

 28%|██▊       | 57/200 [00:16<00:11, 11.93it/s]
 29%|██▉       | 58/200 [00:16<00:10, 13.06it/s][2026-01-17 13:12:23 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 13:12:23 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 13:12:24] INFO:     127.0.0.1:49320 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:24] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK

 30%|███       | 60/200 [00:16<00:14,  9.33it/s][2026-01-17 13:12:24 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:24 TP0] Decode batch, #running-req: 127, #token: 26752, token usage: 0.02, npu graph: False, gen throughput (token/s): 622.93, #queue-req: 0,
[2026-01-17 13:12:24] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:24] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 13:12:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 129, #queue-req: 0,
[2026-01-17 13:12:24] INFO:     127.0.0.1:48298 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:24] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK

 32%|███▏      | 63/200 [00:17<00:16,  8.46it/s]
 32%|███▏      | 64/200 [00:17<00:15,  8.84it/s][2026-01-17 13:12:24] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:24 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 13:12:24 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 13:12:24] INFO:     127.0.0.1:48472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:24] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK

 33%|███▎      | 66/200 [00:17<00:18,  7.42it/s]
 34%|███▎      | 67/200 [00:17<00:18,  7.30it/s][2026-01-17 13:12:25 TP0] Prefill batch, #new-seq: 2, #new-token: 384, #cached-token: 1280, token usage: 0.02, #running-req: 126, #queue-req: 0,
[2026-01-17 13:12:25] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:25] INFO:     127.0.0.1:48784 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:25] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK

 35%|███▌      | 70/200 [00:17<00:14,  8.83it/s][2026-01-17 13:12:25 TP0] Prefill batch, #new-seq: 2, #new-token: 256, #cached-token: 1280, token usage: 0.02, #running-req: 128, #queue-req: 0,
[2026-01-17 13:12:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 130, #queue-req: 0,
[2026-01-17 13:12:25] INFO:     127.0.0.1:48816 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:25] INFO:     127.0.0.1:48246 - "POST /generate HTTP/1.1" 200 OK

 36%|███▌      | 72/200 [00:18<00:22,  5.75it/s][2026-01-17 13:12:25 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 640, token usage: 0.02, #running-req: 127, #queue-req: 0,
[2026-01-17 13:12:26] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK

 36%|███▋      | 73/200 [00:18<00:22,  5.70it/s][2026-01-17 13:12:26] INFO:     127.0.0.1:49236 - "POST /generate HTTP/1.1" 200 OK

 37%|███▋      | 74/200 [00:18<00:21,  5.80it/s][2026-01-17 13:12:26] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:49366 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK

 40%|███▉      | 79/200 [00:18<00:11, 10.24it/s][2026-01-17 13:12:26] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:48854 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:48794 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK

 41%|████      | 82/200 [00:19<00:10, 11.58it/s]
 42%|████▏     | 84/200 [00:19<00:06, 17.36it/s]
 42%|████▏     | 84/200 [00:19<00:06, 17.36it/s][2026-01-17 13:12:26] INFO:     127.0.0.1:48236 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK

 44%|████▎     | 87/200 [00:19<00:06, 17.45it/s]
 44%|████▍     | 88/200 [00:19<00:05, 19.07it/s][2026-01-17 13:12:26] INFO:     127.0.0.1:48220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:49036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:48156 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:26] INFO:     127.0.0.1:48388 - "POST /generate HTTP/1.1" 200 OK

 46%|████▌     | 92/200 [00:19<00:05, 20.00it/s]
 46%|████▋     | 93/200 [00:19<00:04, 22.29it/s][2026-01-17 13:12:27] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:27] INFO:     127.0.0.1:48940 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:27] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK

 48%|████▊     | 96/200 [00:19<00:05, 18.59it/s][2026-01-17 13:12:27] INFO:     127.0.0.1:48688 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:27] INFO:     127.0.0.1:49402 - "POST /generate HTTP/1.1" 200 OK

 49%|████▉     | 98/200 [00:19<00:06, 16.78it/s][2026-01-17 13:12:27] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:27] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:27] INFO:     127.0.0.1:48534 - "POST /generate HTTP/1.1" 200 OK

 50%|█████     | 100/200 [00:20<00:06, 14.72it/s][2026-01-17 13:12:27] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:27] INFO:     127.0.0.1:48424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:27] INFO:     127.0.0.1:48222 - "POST /generate HTTP/1.1" 200 OK

 52%|█████▏    | 104/200 [00:20<00:05, 18.27it/s][2026-01-17 13:12:27] INFO:     127.0.0.1:48560 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:27] INFO:     127.0.0.1:58374 - "POST /generate HTTP/1.1" 200 OK

 53%|█████▎    | 106/200 [00:20<00:06, 14.24it/s][2026-01-17 13:12:28] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:28] INFO:     127.0.0.1:49344 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:28] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK

 54%|█████▍    | 108/200 [00:20<00:06, 13.53it/s]
 55%|█████▍    | 109/200 [00:20<00:06, 14.62it/s][2026-01-17 13:12:28] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:28] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:28] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:28] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:28] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK

 56%|█████▌    | 111/200 [00:20<00:07, 12.10it/s]
 57%|█████▋    | 114/200 [00:20<00:05, 15.24it/s]
 57%|█████▋    | 114/200 [00:20<00:05, 15.24it/s]
 57%|█████▋    | 114/200 [00:20<00:05, 15.24it/s][2026-01-17 13:12:28 TP0] Decode batch, #running-req: 90, #token: 20608, token usage: 0.02, npu graph: False, gen throughput (token/s): 1062.63, #queue-req: 0,
[2026-01-17 13:12:28] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:28] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK

 58%|█████▊    | 116/200 [00:21<00:05, 14.65it/s][2026-01-17 13:12:28] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:28] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK

 59%|█████▉    | 118/200 [00:21<00:05, 13.76it/s][2026-01-17 13:12:28] INFO:     127.0.0.1:49308 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:28] INFO:     127.0.0.1:58472 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:28] INFO:     127.0.0.1:48410 - "POST /generate HTTP/1.1" 200 OK

 60%|██████    | 121/200 [00:21<00:05, 14.93it/s][2026-01-17 13:12:28] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:29] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▏   | 123/200 [00:21<00:05, 14.10it/s][2026-01-17 13:12:29] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:29] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:29] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK

 62%|██████▎   | 125/200 [00:21<00:06, 11.77it/s]
 63%|██████▎   | 126/200 [00:21<00:06, 11.76it/s][2026-01-17 13:12:29] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:29] INFO:     127.0.0.1:58194 - "POST /generate HTTP/1.1" 200 OK

 64%|██████▍   | 128/200 [00:21<00:06, 11.78it/s][2026-01-17 13:12:29] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:29] INFO:     127.0.0.1:58294 - "POST /generate HTTP/1.1" 200 OK

 65%|██████▌   | 130/200 [00:22<00:07,  9.01it/s][2026-01-17 13:12:30] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:30] INFO:     127.0.0.1:48800 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▌   | 132/200 [00:22<00:09,  7.24it/s][2026-01-17 13:12:30] INFO:     127.0.0.1:48150 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:30] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK

 66%|██████▋   | 133/200 [00:22<00:09,  7.25it/s]
 67%|██████▋   | 134/200 [00:22<00:07,  8.62it/s][2026-01-17 13:12:30] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:30] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:30] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK

 68%|██████▊   | 137/200 [00:23<00:08,  7.36it/s][2026-01-17 13:12:31] INFO:     127.0.0.1:48576 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:31] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK

 69%|██████▉   | 138/200 [00:23<00:08,  7.32it/s]
 70%|██████▉   | 139/200 [00:23<00:07,  8.63it/s][2026-01-17 13:12:31] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:31] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK

 70%|███████   | 141/200 [00:23<00:07,  8.15it/s][2026-01-17 13:12:31] INFO:     127.0.0.1:48288 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:31] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 143/200 [00:24<00:06,  8.29it/s][2026-01-17 13:12:31] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK

 72%|███████▏  | 144/200 [00:24<00:08,  6.79it/s][2026-01-17 13:12:31] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:31] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:31] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:31] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:31] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:32] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:32] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:32] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:32] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK

 75%|███████▌  | 150/200 [00:24<00:04, 12.05it/s]
 76%|███████▋  | 153/200 [00:24<00:01, 26.00it/s]
 76%|███████▋  | 153/200 [00:24<00:01, 26.00it/s]
 76%|███████▋  | 153/200 [00:24<00:01, 26.00it/s][2026-01-17 13:12:32] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:32] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:32] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK

 78%|███████▊  | 156/200 [00:25<00:03, 14.32it/s][2026-01-17 13:12:32] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:33 TP0] Decode batch, #running-req: 44, #token: 12288, token usage: 0.01, npu graph: False, gen throughput (token/s): 561.47, #queue-req: 0,
[2026-01-17 13:12:33] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK

 79%|███████▉  | 158/200 [00:25<00:03, 11.34it/s][2026-01-17 13:12:33] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:33] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:33] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK

 80%|████████  | 160/200 [00:26<00:04,  9.33it/s]
 80%|████████  | 161/200 [00:26<00:04,  8.71it/s][2026-01-17 13:12:33] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:33] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 163/200 [00:26<00:04,  8.39it/s][2026-01-17 13:12:34] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▏ | 164/200 [00:27<00:07,  4.85it/s][2026-01-17 13:12:35] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK

 82%|████████▎ | 165/200 [00:27<00:08,  4.23it/s][2026-01-17 13:12:35] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK

 83%|████████▎ | 166/200 [00:27<00:07,  4.57it/s][2026-01-17 13:12:35] INFO:     127.0.0.1:48240 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:35] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▎ | 167/200 [00:27<00:06,  4.89it/s]
 84%|████████▍ | 168/200 [00:27<00:05,  6.39it/s][2026-01-17 13:12:35] INFO:     127.0.0.1:49286 - "POST /generate HTTP/1.1" 200 OK

 84%|████████▍ | 169/200 [00:28<00:05,  5.67it/s][2026-01-17 13:12:35] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK

 85%|████████▌ | 170/200 [00:28<00:05,  5.18it/s][2026-01-17 13:12:36] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 171/200 [00:28<00:06,  4.82it/s][2026-01-17 13:12:36] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK

 86%|████████▌ | 172/200 [00:28<00:06,  4.60it/s][2026-01-17 13:12:36] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:36] INFO:     127.0.0.1:48192 - "POST /generate HTTP/1.1" 200 OK

 87%|████████▋ | 174/200 [00:29<00:04,  5.97it/s][2026-01-17 13:12:36] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 175/200 [00:29<00:03,  6.26it/s][2026-01-17 13:12:36] INFO:     127.0.0.1:49022 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 176/200 [00:29<00:04,  5.35it/s][2026-01-17 13:12:37] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK

 88%|████████▊ | 177/200 [00:30<00:07,  3.11it/s][2026-01-17 13:12:38] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:38] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK

 89%|████████▉ | 178/200 [00:30<00:07,  2.89it/s]
 90%|████████▉ | 179/200 [00:30<00:05,  3.51it/s][2026-01-17 13:12:38] INFO:     127.0.0.1:49400 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 180/200 [00:30<00:04,  4.02it/s][2026-01-17 13:12:38 TP0] Decode batch, #running-req: 20, #token: 6400, token usage: 0.01, npu graph: False, gen throughput (token/s): 234.53, #queue-req: 0,
[2026-01-17 13:12:38] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:12:38] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK

 90%|█████████ | 181/200 [00:30<00:04,  3.90it/s]
 91%|█████████ | 182/200 [00:30<00:03,  4.83it/s][2026-01-17 13:12:38] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 183/200 [00:31<00:03,  5.26it/s][2026-01-17 13:12:39] INFO:     127.0.0.1:58400 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▏| 184/200 [00:31<00:05,  3.20it/s][2026-01-17 13:12:40] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK

 92%|█████████▎| 185/200 [00:32<00:07,  2.08it/s][2026-01-17 13:12:40] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK

 93%|█████████▎| 186/200 [00:33<00:06,  2.16it/s][2026-01-17 13:12:40] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▎| 187/200 [00:33<00:04,  2.68it/s][2026-01-17 13:12:41] INFO:     127.0.0.1:58570 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 188/200 [00:33<00:04,  2.60it/s][2026-01-17 13:12:41] INFO:     127.0.0.1:49302 - "POST /generate HTTP/1.1" 200 OK

 94%|█████████▍| 189/200 [00:33<00:03,  2.83it/s][2026-01-17 13:12:41] INFO:     127.0.0.1:58142 - "POST /generate HTTP/1.1" 200 OK

 95%|█████████▌| 190/200 [00:34<00:03,  2.70it/s][2026-01-17 13:12:43 TP0] Decode batch, #running-req: 10, #token: 4352, token usage: 0.00, npu graph: False, gen throughput (token/s): 96.58, #queue-req: 0,
[2026-01-17 13:12:44] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 191/200 [00:36<00:08,  1.10it/s][2026-01-17 13:12:47] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▌| 192/200 [00:39<00:12,  1.50s/it][2026-01-17 13:12:48] INFO:     127.0.0.1:58158 - "POST /generate HTTP/1.1" 200 OK

 96%|█████████▋| 193/200 [00:40<00:10,  1.45s/it][2026-01-17 13:12:49 TP0] Decode batch, #running-req: 7, #token: 3328, token usage: 0.00, npu graph: False, gen throughput (token/s): 64.06, #queue-req: 0,
[2026-01-17 13:12:50] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK

 97%|█████████▋| 194/200 [00:42<00:09,  1.53s/it][2026-01-17 13:12:50] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 195/200 [00:43<00:06,  1.23s/it][2026-01-17 13:12:53] INFO:     127.0.0.1:58348 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 196/200 [00:46<00:07,  1.81s/it][2026-01-17 13:12:54 TP0] Decode batch, #running-req: 4, #token: 2560, token usage: 0.00, npu graph: False, gen throughput (token/s): 39.68, #queue-req: 0,
[2026-01-17 13:13:00 TP0] Decode batch, #running-req: 4, #token: 2560, token usage: 0.00, npu graph: False, gen throughput (token/s): 28.85, #queue-req: 0,
[2026-01-17 13:13:00] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK

 98%|█████████▊| 197/200 [00:53<00:10,  3.35s/it][2026-01-17 13:13:03] INFO:     127.0.0.1:58406 - "POST /generate HTTP/1.1" 200 OK

 99%|█████████▉| 198/200 [00:55<00:06,  3.11s/it][2026-01-17 13:13:05 TP0] Decode batch, #running-req: 2, #token: 1664, token usage: 0.00, npu graph: False, gen throughput (token/s): 20.21, #queue-req: 0,
[2026-01-17 13:13:11 TP0] Decode batch, #running-req: 2, #token: 1792, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.41, #queue-req: 0,
[2026-01-17 13:13:16 TP0] Decode batch, #running-req: 2, #token: 1920, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.60, #queue-req: 0,
[2026-01-17 13:13:22 TP0] Decode batch, #running-req: 2, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.46, #queue-req: 0,
[2026-01-17 13:13:22] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK
[2026-01-17 13:13:22] INFO:     127.0.0.1:49232 - "POST /generate HTTP/1.1" 200 OK

100%|█████████▉| 199/200 [01:14<00:07,  7.83s/it]
100%|██████████| 200/200 [01:14<00:00,  8.57s/it]
100%|██████████| 200/200 [01:14<00:00,  2.68it/s]
.
----------------------------------------------------------------------
Ran 1 test in 176.251s

OK
Accuracy: 0.670
Invalid: 0.000
Latency: 74.738 s
Output throughput: 255.238 token/s
.
.
End (18/23):
filename='ascend/vlm_models/test_ascend_kimi_vl_a3b_instruct.py', elapsed=187, estimated_time=400
.
.

.
.
Begin (19/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_vlm_models_glm_4_5v.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 13:13:43] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/ZhipuAI/GLM-4.5V', tokenizer_path='/root/.cache/modelscope/hub/models/ZhipuAI/GLM-4.5V', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.7, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=716257275, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/ZhipuAI/GLM-4.5V', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:13:46] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:13:55 TP5] Init torch distributed begin.
[2026-01-17 13:13:55 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:13:55 TP6] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:13:55 TP7] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:13:55 TP3] Init torch distributed begin.
[2026-01-17 13:13:55 TP4] Init torch distributed begin.
[2026-01-17 13:13:55 TP0] Init torch distributed begin.
[2026-01-17 13:13:55 TP1] Init torch distributed begin.
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2026-01-17 13:13:57 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:13:57 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:13:57 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:13:57 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:13:57 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:13:57 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:13:57 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:13:57 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:13:58 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:13:58 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:13:58 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:13:58 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:13:58 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:13:58 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:13:58 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:13:58 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:13:58 TP7] Load weight begin. avail mem=61.13 GB
[2026-01-17 13:13:58 TP6] Load weight begin. avail mem=60.88 GB
[2026-01-17 13:13:58 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:13:58 TP5] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:13:58 TP4] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:13:58 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:13:58 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:13:58 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 13:13:58 TP0] Shared experts fusion currently requires CUDA devices. Shared experts fusion optimization is disabled.
[2026-01-17 13:13:59 TP5] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:13:59 TP5] Using sdpa as multimodal attention backend.
[2026-01-17 13:13:59 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:13:59 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 13:13:59 TP7] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:13:59 TP7] Using sdpa as multimodal attention backend.
[2026-01-17 13:13:59 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:13:59 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 13:13:59 TP6] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:13:59 TP6] Using sdpa as multimodal attention backend.
[2026-01-17 13:13:59 TP4] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:13:59 TP4] Using sdpa as multimodal attention backend.
[2026-01-17 13:13:59 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:13:59 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 13:14:00 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:14:00 TP0] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/46 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   2% Completed | 1/46 [00:06<04:48,  6.40s/it]

Loading safetensors checkpoint shards:   4% Completed | 2/46 [00:12<04:28,  6.11s/it]

Loading safetensors checkpoint shards:   7% Completed | 3/46 [00:18<04:17,  6.00s/it]

Loading safetensors checkpoint shards:   9% Completed | 4/46 [00:24<04:18,  6.16s/it]

Loading safetensors checkpoint shards:  11% Completed | 5/46 [00:31<04:17,  6.27s/it]

Loading safetensors checkpoint shards:  13% Completed | 6/46 [00:37<04:12,  6.32s/it]

Loading safetensors checkpoint shards:  15% Completed | 7/46 [00:43<04:06,  6.33s/it]

Loading safetensors checkpoint shards:  17% Completed | 8/46 [00:50<04:00,  6.32s/it]

Loading safetensors checkpoint shards:  20% Completed | 9/46 [00:56<03:53,  6.31s/it]

Loading safetensors checkpoint shards:  22% Completed | 10/46 [01:03<03:56,  6.57s/it]

Loading safetensors checkpoint shards:  24% Completed | 11/46 [01:10<03:49,  6.55s/it]

Loading safetensors checkpoint shards:  26% Completed | 12/46 [01:16<03:41,  6.50s/it]

Loading safetensors checkpoint shards:  28% Completed | 13/46 [01:22<03:33,  6.45s/it]

Loading safetensors checkpoint shards:  30% Completed | 14/46 [01:29<03:24,  6.40s/it]

Loading safetensors checkpoint shards:  33% Completed | 15/46 [01:35<03:19,  6.43s/it]

Loading safetensors checkpoint shards:  35% Completed | 16/46 [01:38<02:37,  5.26s/it]

Loading safetensors checkpoint shards:  37% Completed | 17/46 [01:43<02:34,  5.33s/it]

Loading safetensors checkpoint shards:  39% Completed | 18/46 [01:49<02:33,  5.47s/it]

Loading safetensors checkpoint shards:  41% Completed | 19/46 [01:55<02:30,  5.59s/it]

Loading safetensors checkpoint shards:  43% Completed | 20/46 [02:00<02:24,  5.58s/it]

Loading safetensors checkpoint shards:  46% Completed | 21/46 [02:06<02:20,  5.62s/it]

Loading safetensors checkpoint shards:  48% Completed | 22/46 [02:12<02:18,  5.77s/it]

Loading safetensors checkpoint shards:  50% Completed | 23/46 [02:18<02:13,  5.79s/it]

Loading safetensors checkpoint shards:  52% Completed | 24/46 [02:24<02:09,  5.90s/it]

Loading safetensors checkpoint shards:  54% Completed | 25/46 [02:30<02:06,  6.04s/it]

Loading safetensors checkpoint shards:  57% Completed | 26/46 [02:37<02:02,  6.11s/it]

Loading safetensors checkpoint shards:  59% Completed | 27/46 [02:42<01:53,  5.98s/it]

Loading safetensors checkpoint shards:  61% Completed | 28/46 [02:48<01:45,  5.83s/it]

Loading safetensors checkpoint shards:  63% Completed | 29/46 [02:54<01:39,  5.87s/it]

Loading safetensors checkpoint shards:  65% Completed | 30/46 [03:00<01:34,  5.93s/it]

Loading safetensors checkpoint shards:  67% Completed | 31/46 [03:06<01:27,  5.86s/it]

Loading safetensors checkpoint shards:  70% Completed | 32/46 [03:13<01:28,  6.32s/it]

Loading safetensors checkpoint shards:  72% Completed | 33/46 [03:19<01:20,  6.19s/it]

Loading safetensors checkpoint shards:  74% Completed | 34/46 [03:25<01:14,  6.19s/it]

Loading safetensors checkpoint shards:  76% Completed | 35/46 [03:32<01:09,  6.29s/it]

Loading safetensors checkpoint shards:  78% Completed | 36/46 [03:38<01:02,  6.27s/it]

Loading safetensors checkpoint shards:  80% Completed | 37/46 [03:44<00:55,  6.16s/it]

Loading safetensors checkpoint shards:  83% Completed | 38/46 [03:49<00:47,  5.99s/it]

Loading safetensors checkpoint shards:  85% Completed | 39/46 [03:56<00:42,  6.03s/it]

Loading safetensors checkpoint shards:  87% Completed | 40/46 [04:02<00:36,  6.06s/it]

Loading safetensors checkpoint shards:  89% Completed | 41/46 [04:08<00:30,  6.06s/it]

Loading safetensors checkpoint shards:  91% Completed | 42/46 [04:14<00:24,  6.11s/it]

Loading safetensors checkpoint shards:  93% Completed | 43/46 [04:20<00:18,  6.10s/it]

Loading safetensors checkpoint shards:  96% Completed | 44/46 [04:26<00:11,  5.99s/it]

Loading safetensors checkpoint shards:  98% Completed | 45/46 [04:31<00:05,  5.91s/it]

Loading safetensors checkpoint shards: 100% Completed | 46/46 [04:37<00:00,  5.79s/it]

Loading safetensors checkpoint shards: 100% Completed | 46/46 [04:37<00:00,  6.03s/it]

[2026-01-17 13:18:50 TP1] Load weight end. type=Glm4vMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=35.52 GB, mem usage=25.61 GB.
[2026-01-17 13:18:51 TP2] Load weight end. type=Glm4vMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=35.25 GB, mem usage=25.62 GB.
[2026-01-17 13:18:51 TP0] Load weight end. type=Glm4vMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=35.19 GB, mem usage=25.61 GB.
[2026-01-17 13:18:51 TP7] Load weight end. type=Glm4vMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=35.51 GB, mem usage=25.61 GB.
[2026-01-17 13:18:51 TP4] Load weight end. type=Glm4vMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=35.25 GB, mem usage=25.62 GB.
[2026-01-17 13:18:51 TP3] Load weight end. type=Glm4vMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=35.52 GB, mem usage=25.61 GB.
[2026-01-17 13:18:52 TP5] Load weight end. type=Glm4vMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=35.53 GB, mem usage=25.61 GB.
[2026-01-17 13:18:52 TP6] Load weight end. type=Glm4vMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=35.26 GB, mem usage=25.62 GB.
[2026-01-17 13:18:52 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 13:18:52 TP0] The available memory for KV cache is 16.95 GB.
[2026-01-17 13:18:52 TP7] The available memory for KV cache is 16.95 GB.
[2026-01-17 13:18:52 TP6] The available memory for KV cache is 16.95 GB.
[2026-01-17 13:18:52 TP5] The available memory for KV cache is 16.95 GB.
[2026-01-17 13:18:52 TP4] The available memory for KV cache is 16.95 GB.
[2026-01-17 13:18:52 TP3] The available memory for KV cache is 16.95 GB.
[2026-01-17 13:18:52 TP2] The available memory for KV cache is 16.95 GB.
[2026-01-17 13:18:52 TP1] The available memory for KV cache is 16.95 GB.
[2026-01-17 13:18:53 TP3] KV Cache is allocated. #tokens: 772736, K size: 8.48 GB, V size: 8.48 GB
[2026-01-17 13:18:53 TP3] Memory pool end. avail mem=17.57 GB
[2026-01-17 13:18:53 TP0] KV Cache is allocated. #tokens: 772736, K size: 8.48 GB, V size: 8.48 GB
[2026-01-17 13:18:53 TP0] Memory pool end. avail mem=17.24 GB
[2026-01-17 13:18:53 TP6] KV Cache is allocated. #tokens: 772736, K size: 8.48 GB, V size: 8.48 GB
[2026-01-17 13:18:53 TP5] KV Cache is allocated. #tokens: 772736, K size: 8.48 GB, V size: 8.48 GB
[2026-01-17 13:18:53 TP6] Memory pool end. avail mem=17.31 GB
[2026-01-17 13:18:53 TP5] Memory pool end. avail mem=17.57 GB
[2026-01-17 13:18:53 TP2] KV Cache is allocated. #tokens: 772736, K size: 8.48 GB, V size: 8.48 GB
[2026-01-17 13:18:53 TP1] KV Cache is allocated. #tokens: 772736, K size: 8.48 GB, V size: 8.48 GB
[2026-01-17 13:18:53 TP2] Memory pool end. avail mem=17.29 GB
[2026-01-17 13:18:53 TP1] Memory pool end. avail mem=17.57 GB
[2026-01-17 13:18:53 TP4] KV Cache is allocated. #tokens: 772736, K size: 8.48 GB, V size: 8.48 GB
[2026-01-17 13:18:53 TP7] KV Cache is allocated. #tokens: 772736, K size: 8.48 GB, V size: 8.48 GB
[2026-01-17 13:18:53 TP4] Memory pool end. avail mem=17.29 GB
[2026-01-17 13:18:53 TP7] Memory pool end. avail mem=17.56 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 13:18:54 TP0] max_total_num_tokens=772736, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=65536, available_gpu_mem=17.24 GB
[2026-01-17 13:18:55] INFO:     Started server process [180952]
[2026-01-17 13:18:55] INFO:     Waiting for application startup.
[2026-01-17 13:18:55] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 1, 'top_p': 0.0001}
[2026-01-17 13:18:55] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 1, 'top_p': 0.0001}
[2026-01-17 13:18:55] INFO:     Application startup complete.
[2026-01-17 13:18:55] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 13:18:56] INFO:     127.0.0.1:46276 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 13:18:57 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
........('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:19:03] INFO:     127.0.0.1:33860 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:19:05] INFO:     127.0.0.1:46292 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 13:19:05] The server is fired up and ready to roll!
[2026-01-17 13:19:13 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:14] INFO:     127.0.0.1:60684 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 13:19:22.085[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 13:19:24.174[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 13:19:24.931[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 13:19:24.931[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 13:19:24.932[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 13:19:24.935[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 13:19:33.134[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9140.71it/s]
[32m2026-01-17 13:19:33.140[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 13:19:33 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:34] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:01<01:12,  1.49s/it][2026-01-17 13:19:34 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:35 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.12, #queue-req: 0,
[2026-01-17 13:19:35] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:02<01:07,  1.40s/it][2026-01-17 13:19:36 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:37] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:04<01:03,  1.34s/it][2026-01-17 13:19:37 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:38] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:05<01:02,  1.35s/it][2026-01-17 13:19:38 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:39 TP0] Decode batch, #running-req: 1, #token: 512, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.27, #queue-req: 0,
[2026-01-17 13:19:40] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:07<01:04,  1.44s/it][2026-01-17 13:19:40 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:41] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:08<01:02,  1.43s/it][2026-01-17 13:19:41 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:42 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.93, #queue-req: 0,
[2026-01-17 13:19:42] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:09<00:59,  1.38s/it][2026-01-17 13:19:42 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:44] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:11<00:57,  1.36s/it][2026-01-17 13:19:44 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:45] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:12<00:55,  1.35s/it][2026-01-17 13:19:45 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:46 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.79, #queue-req: 0,
[2026-01-17 13:19:46] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:13<00:53,  1.34s/it][2026-01-17 13:19:46 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:48] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:15<00:51,  1.33s/it][2026-01-17 13:19:48 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:49 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 12.50, #queue-req: 0,
[2026-01-17 13:19:49] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:16<00:50,  1.33s/it][2026-01-17 13:19:49 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:50] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:17<00:48,  1.32s/it][2026-01-17 13:19:50 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:52] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:18<00:47,  1.33s/it][2026-01-17 13:19:52 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:52 TP0] Decode batch, #running-req: 1, #token: 640, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.42, #queue-req: 0,
[2026-01-17 13:19:53] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:20<00:47,  1.36s/it][2026-01-17 13:19:53 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:54] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:21<00:45,  1.34s/it][2026-01-17 13:19:54 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:56 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 12.64, #queue-req: 0,
[2026-01-17 13:19:56] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:22<00:43,  1.32s/it][2026-01-17 13:19:56 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:57] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:24<00:42,  1.31s/it][2026-01-17 13:19:57 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:58] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:25<00:40,  1.31s/it][2026-01-17 13:19:58 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:19:59 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.98, #queue-req: 0,
[2026-01-17 13:20:00] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:26<00:39,  1.31s/it][2026-01-17 13:20:00 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:01] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:28<00:37,  1.31s/it][2026-01-17 13:20:01 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:02 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 12.56, #queue-req: 0,
[2026-01-17 13:20:02] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:29<00:36,  1.31s/it][2026-01-17 13:20:02 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:03] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:30<00:35,  1.30s/it][2026-01-17 13:20:03 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:05] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:32<00:33,  1.30s/it][2026-01-17 13:20:05 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:05 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 12.15, #queue-req: 0,
[2026-01-17 13:20:06] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:33<00:32,  1.29s/it][2026-01-17 13:20:06 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:07] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:34<00:31,  1.32s/it][2026-01-17 13:20:07 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:09 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 12.24, #queue-req: 0,
[2026-01-17 13:20:09] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:36<00:30,  1.32s/it][2026-01-17 13:20:09 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:10] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:37<00:29,  1.33s/it][2026-01-17 13:20:10 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:11] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:38<00:27,  1.32s/it][2026-01-17 13:20:11 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:12 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.84, #queue-req: 0,
[2026-01-17 13:20:13] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:40<00:26,  1.31s/it][2026-01-17 13:20:16 TP0] Prefill batch, #new-seq: 1, #new-token: 6144, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:22] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:49<01:12,  3.83s/it][2026-01-17 13:20:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:25 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.18, #queue-req: 0,
[2026-01-17 13:20:25] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:52<01:00,  3.38s/it][2026-01-17 13:20:27 TP0] Prefill batch, #new-seq: 1, #new-token: 4096, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:30] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:57<01:08,  4.02s/it][2026-01-17 13:20:34 TP0] Prefill batch, #new-seq: 1, #new-token: 6144, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:40] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [01:07<01:34,  5.89s/it][2026-01-17 13:20:41 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:41 TP0] Decode batch, #running-req: 1, #token: 640, token usage: 0.00, npu graph: False, gen throughput (token/s): 2.39, #queue-req: 0,
[2026-01-17 13:20:42] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [01:09<01:08,  4.56s/it][2026-01-17 13:20:42 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:43] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [01:10<00:50,  3.58s/it][2026-01-17 13:20:43 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:45 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 12.09, #queue-req: 0,
[2026-01-17 13:20:45] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [01:12<00:38,  2.96s/it][2026-01-17 13:20:48 TP0] Prefill batch, #new-seq: 1, #new-token: 6144, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:20:54] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [01:20<00:56,  4.75s/it][2026-01-17 13:20:56 TP0] Prefill batch, #new-seq: 1, #new-token: 6272, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:21:02] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [01:29<01:04,  5.90s/it][2026-01-17 13:21:02 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:21:03 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 2.19, #queue-req: 0,
[2026-01-17 13:21:04] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [01:30<00:45,  4.52s/it][2026-01-17 13:21:06 TP0] Prefill batch, #new-seq: 1, #new-token: 5632, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:21:11] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [01:38<00:49,  5.54s/it][2026-01-17 13:21:12 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:21:13 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.96, #queue-req: 0,
[2026-01-17 13:21:13] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [01:40<00:34,  4.36s/it][2026-01-17 13:21:16 TP0] Prefill batch, #new-seq: 1, #new-token: 6272, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:21:22] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [01:49<00:40,  5.76s/it][2026-01-17 13:21:25 TP0] Prefill batch, #new-seq: 1, #new-token: 6144, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:21:31] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:58<00:41,  6.84s/it][2026-01-17 13:21:34 TP0] Prefill batch, #new-seq: 1, #new-token: 5760, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:21:40 TP0] Decode batch, #running-req: 1, #token: 5760, token usage: 0.01, npu graph: False, gen throughput (token/s): 1.50, #queue-req: 0,
[2026-01-17 13:21:40] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [02:07<00:37,  7.43s/it][2026-01-17 13:21:41 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:21:44] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [02:10<00:24,  6.20s/it][2026-01-17 13:21:45 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:21:49 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 4.27, #queue-req: 0,
[2026-01-17 13:21:49] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [02:16<00:17,  5.96s/it][2026-01-17 13:21:51 TP0] Prefill batch, #new-seq: 1, #new-token: 4096, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:21:55] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [02:21<00:11,  5.86s/it][2026-01-17 13:21:56 TP0] Prefill batch, #new-seq: 1, #new-token: 2688, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:21:58] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [02:25<00:05,  5.05s/it][2026-01-17 13:22:01 TP0] Prefill batch, #new-seq: 1, #new-token: 6144, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:22:13 TP0] Decode batch, #running-req: 1, #token: 6272, token usage: 0.01, npu graph: False, gen throughput (token/s): 1.65, #queue-req: 0,
[2026-01-17 13:22:14] INFO:     127.0.0.1:48234 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [02:41<00:00,  8.35s/it]
Model Responding: 100%|██████████| 50/50 [02:41<00:00,  3.22s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 2713.21it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.4}, 'Accounting': {'num': 30, 'acc': 0.4}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.3}, 'Agriculture': {'num': 20, 'acc': 0.3}, 'Overall': {'num': 50, 'acc': 0.36}}
[32m2026-01-17 13:22:14.339[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 13:22:14.346[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/ZhipuAI/GLM-4.5V",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  | 0.36|±  |   N/A|

.
----------------------------------------------------------------------
Ran 1 test in 524.636s

OK
[CI Test Method] TestGLM4Models.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/ZhipuAI/GLM-4.5V
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/ZhipuAI/GLM-4.5V --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.7 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 8 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/ZhipuAI/GLM-4.5V achieved accuracy: 0.3000
Cleaning up process 180952
.
.
End (19/23):
filename='ascend/vlm_models/test_vlm_models_glm_4_5v.py', elapsed=535, estimated_time=400
.
.

.
.
Begin (20/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_4b_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 13:22:38] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-4B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-4B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=233732871, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-4B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:22:40] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:22:48 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:22:48 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:22:48 TP3] Init torch distributed begin.
[2026-01-17 13:22:48 TP2] Init torch distributed begin.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 13:22:49 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:22:49 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:22:49 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:22:49 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:22:49 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 13:22:50 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:22:50 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:22:50 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:22:50 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:22:50 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:22:50 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 13:22:50 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:22:50 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:22:50 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:22:50 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:22:50 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:22:50 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 13:22:50 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 13:22:50 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 13:22:50 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:22:50 TP1] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:05<00:05,  5.98s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:11<00:00,  5.70s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:11<00:00,  5.74s/it]

[2026-01-17 13:23:02 TP1] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=58.91 GB, mem usage=2.22 GB.
[2026-01-17 13:23:02 TP3] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=58.91 GB, mem usage=2.22 GB.
[2026-01-17 13:23:02 TP2] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=58.64 GB, mem usage=2.23 GB.
[2026-01-17 13:23:02 TP0] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=58.58 GB, mem usage=2.23 GB.
[2026-01-17 13:23:02 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 13:23:02 TP0] The available memory for KV cache is 18.96 GB.
[2026-01-17 13:23:02 TP3] The available memory for KV cache is 18.96 GB.
[2026-01-17 13:23:02 TP2] The available memory for KV cache is 18.96 GB.
[2026-01-17 13:23:02 TP1] The available memory for KV cache is 18.96 GB.
[2026-01-17 13:23:03 TP3] KV Cache is allocated. #tokens: 552320, K size: 9.48 GB, V size: 9.48 GB
[2026-01-17 13:23:03 TP0] KV Cache is allocated. #tokens: 552320, K size: 9.48 GB, V size: 9.48 GB
[2026-01-17 13:23:03 TP3] Memory pool end. avail mem=37.85 GB
[2026-01-17 13:23:03 TP0] Memory pool end. avail mem=37.52 GB
[2026-01-17 13:23:03 TP2] KV Cache is allocated. #tokens: 552320, K size: 9.48 GB, V size: 9.48 GB
[2026-01-17 13:23:03 TP2] Memory pool end. avail mem=37.57 GB
[2026-01-17 13:23:03 TP1] KV Cache is allocated. #tokens: 552320, K size: 9.48 GB, V size: 9.48 GB
[2026-01-17 13:23:03 TP1] Memory pool end. avail mem=37.85 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 13:23:04 TP0] max_total_num_tokens=552320, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=262144, available_gpu_mem=37.52 GB
[2026-01-17 13:23:05] INFO:     Started server process [195438]
[2026-01-17 13:23:05] INFO:     Waiting for application startup.
[2026-01-17 13:23:05] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:23:05] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:23:05] INFO:     Application startup complete.
[2026-01-17 13:23:05] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 13:23:06] INFO:     127.0.0.1:54612 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 13:23:06 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:23:08] INFO:     127.0.0.1:59296 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:23:18] INFO:     127.0.0.1:56504 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:23:26] INFO:     127.0.0.1:54624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 13:23:26] The server is fired up and ready to roll!
[2026-01-17 13:23:28 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:23:29] INFO:     127.0.0.1:50204 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 13:23:36.966[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 13:23:39.067[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 13:23:39.829[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 13:23:39.829[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 13:23:39.830[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 13:23:39.833[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 13:24:04.013[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9306.20it/s]
[32m2026-01-17 13:24:04.019[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 13:24:04 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:05] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:01<00:52,  1.08s/it][2026-01-17 13:24:05 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:05 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.51, #queue-req: 0,
[2026-01-17 13:24:05] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:01<00:46,  1.04it/s][2026-01-17 13:24:06 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:06] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:02<00:43,  1.07it/s][2026-01-17 13:24:06 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:07] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:03<00:31,  1.46it/s][2026-01-17 13:24:07 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:08] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:04<00:34,  1.29it/s][2026-01-17 13:24:08 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:10] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:06<01:00,  1.37s/it][2026-01-17 13:24:10 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:10 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 8.02, #queue-req: 0,
[2026-01-17 13:24:11] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:07<00:53,  1.24s/it][2026-01-17 13:24:11 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:12] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:08<00:47,  1.12s/it][2026-01-17 13:24:12 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:13 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 17.61, #queue-req: 0,
[2026-01-17 13:24:13] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:09<00:44,  1.07s/it][2026-01-17 13:24:13 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:14] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:10<00:41,  1.04s/it][2026-01-17 13:24:14 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:14] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:10<00:34,  1.12it/s][2026-01-17 13:24:15 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:15] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:11<00:26,  1.44it/s][2026-01-17 13:24:15 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:15 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 15.26, #queue-req: 0,
[2026-01-17 13:24:16] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:12<00:28,  1.30it/s][2026-01-17 13:24:16 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:16] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:12<00:22,  1.60it/s][2026-01-17 13:24:16 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:17] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:13<00:22,  1.58it/s][2026-01-17 13:24:17 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:18] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:14<00:24,  1.38it/s][2026-01-17 13:24:18 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:18 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.90, #queue-req: 0,
[2026-01-17 13:24:18] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:14<00:26,  1.27it/s][2026-01-17 13:24:19 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:19] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:15<00:27,  1.18it/s][2026-01-17 13:24:20 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:20 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 18.02, #queue-req: 0,
[2026-01-17 13:24:20] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:16<00:26,  1.17it/s][2026-01-17 13:24:20 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:21] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:17<00:26,  1.13it/s][2026-01-17 13:24:21 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:22] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:18<00:26,  1.10it/s][2026-01-17 13:24:22 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:23 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.32, #queue-req: 0,
[2026-01-17 13:24:23] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:19<00:25,  1.09it/s][2026-01-17 13:24:23 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:23] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:19<00:19,  1.40it/s][2026-01-17 13:24:23 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:24] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:20<00:20,  1.28it/s][2026-01-17 13:24:24 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:25 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.73, #queue-req: 0,
[2026-01-17 13:24:25] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:21<00:20,  1.21it/s][2026-01-17 13:24:25 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:26] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:22<00:16,  1.48it/s][2026-01-17 13:24:26 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:26] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:22<00:15,  1.53it/s][2026-01-17 13:24:26 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:27] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:23<00:15,  1.40it/s][2026-01-17 13:24:27 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:27] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:23<00:12,  1.74it/s][2026-01-17 13:24:27 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:28 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.84, #queue-req: 0,
[2026-01-17 13:24:28] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:24<00:13,  1.51it/s][2026-01-17 13:24:31 TP0] Prefill batch, #new-seq: 1, #new-token: 5504, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:35 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:24:35 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:24:35 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:24:35 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:24:35] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:31<00:50,  2.63s/it][2026-01-17 13:24:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:37] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:32<00:39,  2.17s/it][2026-01-17 13:24:39 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:40] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:36<00:44,  2.64s/it][2026-01-17 13:24:44 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:48] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:44<01:04,  4.05s/it][2026-01-17 13:24:48 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:48] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:44<00:44,  2.97s/it][2026-01-17 13:24:48 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:48] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:44<00:30,  2.15s/it][2026-01-17 13:24:49 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:49] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:45<00:21,  1.65s/it][2026-01-17 13:24:51 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:24:55] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:51<00:36,  3.06s/it][2026-01-17 13:24:58 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:02] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [00:57<00:44,  4.07s/it][2026-01-17 13:25:02 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:02] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [00:58<00:29,  2.92s/it][2026-01-17 13:25:04 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:06] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [01:02<00:30,  3.35s/it][2026-01-17 13:25:06 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:07] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [01:03<00:19,  2.49s/it][2026-01-17 13:25:09 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:13] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [01:09<00:25,  3.69s/it][2026-01-17 13:25:16 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:20] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:16<00:27,  4.56s/it][2026-01-17 13:25:22 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:25] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [01:21<00:23,  4.72s/it][2026-01-17 13:25:26 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:27 TP0] Decode batch, #running-req: 1, #token: 2048, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.68, #queue-req: 0,
[2026-01-17 13:25:27] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:23<00:15,  3.90s/it][2026-01-17 13:25:28 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:30] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:26<00:11,  3.73s/it][2026-01-17 13:25:32 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:34] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [01:30<00:07,  3.73s/it][2026-01-17 13:25:35 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:36] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [01:32<00:03,  3.14s/it][2026-01-17 13:25:38 TP0] Prefill batch, #new-seq: 1, #new-token: 5248, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:25:41 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:25:41 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:25:41 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:25:41 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:25:42] INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [01:38<00:00,  4.05s/it]
Model Responding: 100%|██████████| 50/50 [01:38<00:00,  1.96s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 3690.48it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.2}, 'Accounting': {'num': 30, 'acc': 0.2}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.45}, 'Agriculture': {'num': 20, 'acc': 0.45}, 'Overall': {'num': 50, 'acc': 0.3}}
[32m2026-01-17 13:25:42.258[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 13:25:42.264[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-4B-Instruct",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  |  0.3|±  |   N/A|

/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 195438 is still running
  _warn("subprocess %s is still running" % self.pid,
.
----------------------------------------------------------------------
Ran 1 test in 196.689s

OK
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-4B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-4B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-4B-Instruct achieved accuracy: 0.3000
Cleaning up process 195438
.
.
End (20/23):
filename='ascend/vlm_models/test_ascend_qwen3_vl_4b_instruct.py', elapsed=207, estimated_time=400
.
.

.
.
Begin (21/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 13:26:05] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=208321390, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:26:07] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:26:16 TP3] Init torch distributed begin.
[2026-01-17 13:26:16 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:26:16 TP1] Init torch distributed begin.
[2026-01-17 13:26:16 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 13:26:18 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:26:18 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:26:18 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:26:18 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:26:18 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 13:26:18 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:26:18 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:26:19 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:26:19 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:26:19 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:26:19 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 13:26:19 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:26:19 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:26:19 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:26:19 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 13:26:19 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:26:19 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 13:26:19 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:26:19 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 13:26:19 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:26:19 TP2] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.14s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.20s/it]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:01,  1.89s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:07<00:00,  1.35s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:07<00:00,  1.88s/it]

[2026-01-17 13:26:27 TP3] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.70 GB, mem usage=4.43 GB.
[2026-01-17 13:26:27 TP0] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.37 GB, mem usage=4.44 GB.
[2026-01-17 13:26:27 TP2] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.43 GB, mem usage=4.44 GB.
[2026-01-17 13:26:27 TP1] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.70 GB, mem usage=4.43 GB.
[2026-01-17 13:26:27 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 13:26:27 TP0] The available memory for KV cache is 16.74 GB.
[2026-01-17 13:26:27 TP3] The available memory for KV cache is 16.74 GB.
[2026-01-17 13:26:27 TP2] The available memory for KV cache is 16.74 GB.
[2026-01-17 13:26:27 TP1] The available memory for KV cache is 16.74 GB.
[2026-01-17 13:26:28 TP0] KV Cache is allocated. #tokens: 487552, K size: 8.37 GB, V size: 8.37 GB
[2026-01-17 13:26:28 TP0] Memory pool end. avail mem=37.52 GB
[2026-01-17 13:26:28 TP3] KV Cache is allocated. #tokens: 487552, K size: 8.37 GB, V size: 8.37 GB
[2026-01-17 13:26:28 TP1] KV Cache is allocated. #tokens: 487552, K size: 8.37 GB, V size: 8.37 GB
[2026-01-17 13:26:28 TP3] Memory pool end. avail mem=37.85 GB
[2026-01-17 13:26:28 TP1] Memory pool end. avail mem=37.85 GB
[2026-01-17 13:26:28 TP2] KV Cache is allocated. #tokens: 487552, K size: 8.37 GB, V size: 8.37 GB
[2026-01-17 13:26:28 TP2] Memory pool end. avail mem=37.58 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 13:26:29 TP0] max_total_num_tokens=487552, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=262144, available_gpu_mem=37.52 GB
[2026-01-17 13:26:30] INFO:     Started server process [202146]
[2026-01-17 13:26:30] INFO:     Waiting for application startup.
[2026-01-17 13:26:30] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:26:30] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:26:30] INFO:     Application startup complete.
[2026-01-17 13:26:30] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 13:26:31] INFO:     127.0.0.1:58482 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 13:26:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:26:35] INFO:     127.0.0.1:58498 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:26:45] INFO:     127.0.0.1:43562 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:26:47] INFO:     127.0.0.1:58494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 13:26:47] The server is fired up and ready to roll!
[2026-01-17 13:26:55 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:26:56] INFO:     127.0.0.1:52390 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 13:27:04.557[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 13:27:06.659[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 13:27:07.436[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 13:27:07.436[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 13:27:07.437[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 13:27:07.441[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 13:27:16.042[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9313.22it/s]
[32m2026-01-17 13:27:16.048[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 13:27:16 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:16] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:00<00:22,  2.15it/s][2026-01-17 13:27:16 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:17] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:01<00:34,  1.38it/s][2026-01-17 13:27:17 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:18 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.64, #queue-req: 0,
[2026-01-17 13:27:18] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:02<00:37,  1.25it/s][2026-01-17 13:27:18 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:18] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:02<00:27,  1.69it/s][2026-01-17 13:27:18 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:19] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:03<00:31,  1.41it/s][2026-01-17 13:27:19 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:19] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:03<00:24,  1.77it/s][2026-01-17 13:27:19 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:20] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:04<00:28,  1.50it/s][2026-01-17 13:27:20 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:20 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.81, #queue-req: 0,
[2026-01-17 13:27:21] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:05<00:31,  1.34it/s][2026-01-17 13:27:21 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:22] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:06<00:33,  1.23it/s][2026-01-17 13:27:22 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:22] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:06<00:25,  1.55it/s][2026-01-17 13:27:22 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:23 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.54, #queue-req: 0,
[2026-01-17 13:27:23] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:07<00:23,  1.64it/s][2026-01-17 13:27:23 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:24] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:08<00:25,  1.51it/s][2026-01-17 13:27:24 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:24] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:08<00:24,  1.54it/s][2026-01-17 13:27:24 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:25] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:09<00:23,  1.54it/s][2026-01-17 13:27:25 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:25] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:09<00:19,  1.82it/s][2026-01-17 13:27:25 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:25] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:09<00:15,  2.20it/s][2026-01-17 13:27:25 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:26 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 13.92, #queue-req: 0,
[2026-01-17 13:27:26] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:10<00:16,  1.96it/s][2026-01-17 13:27:26 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:27] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:11<00:19,  1.62it/s][2026-01-17 13:27:27 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:27] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:11<00:15,  2.02it/s][2026-01-17 13:27:27 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:28 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 17.83, #queue-req: 0,
[2026-01-17 13:27:28] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:12<00:18,  1.63it/s][2026-01-17 13:27:28 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:29] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:13<00:20,  1.44it/s][2026-01-17 13:27:29 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:29] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:13<00:15,  1.81it/s][2026-01-17 13:27:29 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:29] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:13<00:12,  2.21it/s][2026-01-17 13:27:29 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:30] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:14<00:14,  1.74it/s][2026-01-17 13:27:30 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:30 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 15.91, #queue-req: 0,
[2026-01-17 13:27:31] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:15<00:14,  1.73it/s][2026-01-17 13:27:31 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:31] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:15<00:11,  2.01it/s][2026-01-17 13:27:31 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:32] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:16<00:12,  1.88it/s][2026-01-17 13:27:32 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:32] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:16<00:12,  1.71it/s][2026-01-17 13:27:32 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:33] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:17<00:10,  2.09it/s][2026-01-17 13:27:33 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:33 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.39, #queue-req: 0,
[2026-01-17 13:27:34] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:18<00:12,  1.65it/s][2026-01-17 13:27:37 TP0] Prefill batch, #new-seq: 1, #new-token: 5504, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:40 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:40 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:40 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:40 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:40] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:24<00:47,  2.48s/it][2026-01-17 13:27:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:42] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:26<00:37,  2.07s/it][2026-01-17 13:27:43 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:45] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:29<00:40,  2.37s/it][2026-01-17 13:27:48 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:50 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:50 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:50 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:50 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:51] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:34<00:54,  3.43s/it][2026-01-17 13:27:51 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:51] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:35<00:38,  2.54s/it][2026-01-17 13:27:51 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:51] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:35<00:25,  1.85s/it][2026-01-17 13:27:51 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:52] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:36<00:18,  1.43s/it][2026-01-17 13:27:54 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:27:57 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:57 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:57 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:57 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:27:57] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:41<00:30,  2.54s/it][2026-01-17 13:27:59 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:02 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:02 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:02 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:02 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:02] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [00:46<00:36,  3.29s/it][2026-01-17 13:28:02 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:02] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [00:46<00:23,  2.38s/it][2026-01-17 13:28:04 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:06 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:06 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:06 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:06 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:06] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [00:50<00:25,  2.88s/it][2026-01-17 13:28:06 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:07] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [00:51<00:17,  2.17s/it][2026-01-17 13:28:09 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:12 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:12 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:12 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:12 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:12] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [00:56<00:21,  3.10s/it][2026-01-17 13:28:15 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:17 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:17 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:17 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:17 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:17] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:01<00:22,  3.79s/it][2026-01-17 13:28:20 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:22 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:22 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:22 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:22 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:22] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [01:06<00:20,  4.11s/it][2026-01-17 13:28:23 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:24 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.79, #queue-req: 0,
[2026-01-17 13:28:24] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:08<00:13,  3.39s/it][2026-01-17 13:28:26 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:27] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:11<00:09,  3.23s/it][2026-01-17 13:28:29 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:30] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [01:14<00:06,  3.22s/it][2026-01-17 13:28:31 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:32] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [01:16<00:02,  2.75s/it][2026-01-17 13:28:34 TP0] Prefill batch, #new-seq: 1, #new-token: 5248, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:28:37 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:37 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:37 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:37 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:28:38] INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [01:21<00:00,  3.71s/it]
Model Responding: 100%|██████████| 50/50 [01:21<00:00,  1.64s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 5815.57it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.43333}, 'Accounting': {'num': 30, 'acc': 0.43333}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.45}, 'Agriculture': {'num': 20, 'acc': 0.45}, 'Overall': {'num': 50, 'acc': 0.44}}
[32m2026-01-17 13:28:38.053[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 13:28:38.060[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  | 0.44|±  |   N/A|

Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1277, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 177, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct accuracy (0.3000) below expected threshold (0.4400)
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 202146 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
AssertionError: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 164.933s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct achieved accuracy: 0.3000
Error testing /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct accuracy (0.3000) below expected threshold (0.4400)
Cleaning up process 202146
.
.
End (21/23):
filename='ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py', elapsed=176, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py

.
.
Begin (21/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 13:30:01] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=7319359, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:30:03] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:30:12 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:30:12 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:30:12 TP2] Init torch distributed begin.
[2026-01-17 13:30:12 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 13:30:13 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:30:13 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:30:13 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:30:13 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:30:13 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 13:30:14 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:30:14 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:30:14 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:30:14 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:30:14 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 13:30:14 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:30:14 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:30:14 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:30:14 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:30:14 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 13:30:14 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:30:14 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 13:30:14 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:30:14 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 13:30:14 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:30:14 TP1] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.65it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.25it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.55it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.62it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.56it/s]

[2026-01-17 13:30:16 TP1] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.70 GB, mem usage=4.44 GB.
[2026-01-17 13:30:16 TP0] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.37 GB, mem usage=4.44 GB.
[2026-01-17 13:30:16 TP3] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.70 GB, mem usage=4.44 GB.
[2026-01-17 13:30:16 TP2] Load weight end. type=Qwen3VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=56.43 GB, mem usage=4.44 GB.
[2026-01-17 13:30:16 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 13:30:16 TP0] The available memory for KV cache is 16.74 GB.
[2026-01-17 13:30:16 TP3] The available memory for KV cache is 16.74 GB.
[2026-01-17 13:30:16 TP2] The available memory for KV cache is 16.74 GB.
[2026-01-17 13:30:16 TP1] The available memory for KV cache is 16.74 GB.
[2026-01-17 13:30:17 TP0] KV Cache is allocated. #tokens: 487552, K size: 8.37 GB, V size: 8.37 GB
[2026-01-17 13:30:17 TP3] KV Cache is allocated. #tokens: 487552, K size: 8.37 GB, V size: 8.37 GB
[2026-01-17 13:30:17 TP0] Memory pool end. avail mem=37.52 GB
[2026-01-17 13:30:17 TP3] Memory pool end. avail mem=37.85 GB
[2026-01-17 13:30:17 TP2] KV Cache is allocated. #tokens: 487552, K size: 8.37 GB, V size: 8.37 GB
[2026-01-17 13:30:17 TP1] KV Cache is allocated. #tokens: 487552, K size: 8.37 GB, V size: 8.37 GB
[2026-01-17 13:30:17 TP2] Memory pool end. avail mem=37.58 GB
[2026-01-17 13:30:17 TP1] Memory pool end. avail mem=37.85 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 13:30:18 TP0] max_total_num_tokens=487552, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=262144, available_gpu_mem=37.52 GB
[2026-01-17 13:30:19] INFO:     Started server process [207941]
[2026-01-17 13:30:19] INFO:     Waiting for application startup.
[2026-01-17 13:30:19] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:30:19] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:30:19] INFO:     Application startup complete.
[2026-01-17 13:30:19] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 13:30:20] INFO:     127.0.0.1:33664 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 13:30:20 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:30:21] INFO:     127.0.0.1:33686 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:30:31] INFO:     127.0.0.1:41598 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:30:35] INFO:     127.0.0.1:33672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 13:30:35] The server is fired up and ready to roll!
[2026-01-17 13:30:41 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:30:42] INFO:     127.0.0.1:35234 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 13:30:50.358[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 13:30:52.443[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 13:30:53.197[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 13:30:53.197[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 13:30:53.198[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 13:30:53.201[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 13:31:00.492[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9173.09it/s]
[32m2026-01-17 13:31:00.498[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 13:31:00 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:00] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:00<00:23,  2.13it/s][2026-01-17 13:31:01 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:01] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:01<00:34,  1.38it/s][2026-01-17 13:31:01 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:02 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.78, #queue-req: 0,
[2026-01-17 13:31:02] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:02<00:37,  1.26it/s][2026-01-17 13:31:02 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:03] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:02<00:27,  1.68it/s][2026-01-17 13:31:03 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:03] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:03<00:32,  1.39it/s][2026-01-17 13:31:04 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:04] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:03<00:25,  1.74it/s][2026-01-17 13:31:04 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:05] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:04<00:29,  1.45it/s][2026-01-17 13:31:05 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:05 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.32, #queue-req: 0,
[2026-01-17 13:31:06] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:05<00:32,  1.30it/s][2026-01-17 13:31:06 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:07] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:06<00:33,  1.23it/s][2026-01-17 13:31:07 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:07] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:06<00:25,  1.57it/s][2026-01-17 13:31:07 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:07 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.76, #queue-req: 0,
[2026-01-17 13:31:07] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:07<00:23,  1.63it/s][2026-01-17 13:31:07 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:08] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:08<00:25,  1.50it/s][2026-01-17 13:31:08 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:09] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:08<00:24,  1.51it/s][2026-01-17 13:31:09 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:09] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:09<00:24,  1.49it/s][2026-01-17 13:31:10 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:10] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:09<00:19,  1.78it/s][2026-01-17 13:31:10 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:10] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:10<00:15,  2.14it/s][2026-01-17 13:31:10 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:10 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 13.30, #queue-req: 0,
[2026-01-17 13:31:11] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:10<00:17,  1.86it/s][2026-01-17 13:31:11 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:12] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:11<00:21,  1.52it/s][2026-01-17 13:31:12 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:12] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:11<00:16,  1.90it/s][2026-01-17 13:31:12 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:13 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 16.78, #queue-req: 0,
[2026-01-17 13:31:13] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:12<00:19,  1.54it/s][2026-01-17 13:31:13 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:14] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:13<00:21,  1.36it/s][2026-01-17 13:31:14 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:14] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:13<00:16,  1.71it/s][2026-01-17 13:31:14 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:14] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:14<00:12,  2.08it/s][2026-01-17 13:31:14 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:15] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:15<00:15,  1.64it/s][2026-01-17 13:31:15 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:15 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 15.09, #queue-req: 0,
[2026-01-17 13:31:16] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:15<00:14,  1.67it/s][2026-01-17 13:31:16 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:16] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:16<00:12,  1.92it/s][2026-01-17 13:31:16 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:17] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:16<00:12,  1.79it/s][2026-01-17 13:31:17 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:17] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:17<00:13,  1.64it/s][2026-01-17 13:31:17 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:18] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:17<00:10,  2.02it/s][2026-01-17 13:31:18 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:18 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 14.03, #queue-req: 0,
[2026-01-17 13:31:19] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:18<00:12,  1.62it/s][2026-01-17 13:31:21 TP0] Prefill batch, #new-seq: 1, #new-token: 5504, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:25 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:25 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:25 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:25 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:26] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:25<00:49,  2.61s/it][2026-01-17 13:31:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:27] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:26<00:38,  2.15s/it][2026-01-17 13:31:29 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:30] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:30<00:42,  2.47s/it][2026-01-17 13:31:33 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:37 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:37 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:37 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:37 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:37] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:36<01:00,  3.76s/it][2026-01-17 13:31:37 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:37] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:37<00:41,  2.77s/it][2026-01-17 13:31:37 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:38] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:37<00:28,  2.01s/it][2026-01-17 13:31:38 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:38] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:38<00:20,  1.55s/it][2026-01-17 13:31:41 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:44 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:44 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:44 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:44 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:44] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:44<00:34,  2.90s/it][2026-01-17 13:31:47 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:50 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:50 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:50 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:50 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:50] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [00:50<00:42,  3.84s/it][2026-01-17 13:31:50 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:50] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [00:50<00:27,  2.76s/it][2026-01-17 13:31:52 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:55 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:55 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:55 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:55 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:31:55] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [00:55<00:30,  3.42s/it][2026-01-17 13:31:56 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:31:56] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [00:55<00:20,  2.55s/it][2026-01-17 13:31:59 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:32:02 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:02 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:02 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:02 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:02] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [01:01<00:25,  3.61s/it][2026-01-17 13:32:05 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:32:08 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:08 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:08 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:08 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:08] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:08<00:26,  4.36s/it][2026-01-17 13:32:11 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:32:13 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:13 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:13 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:13 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:14] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [01:13<00:23,  4.74s/it][2026-01-17 13:32:15 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:32:16] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:15<00:15,  3.91s/it][2026-01-17 13:32:16 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.70, #queue-req: 0,
[2026-01-17 13:32:17 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:32:19] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:18<00:11,  3.67s/it][2026-01-17 13:32:21 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:32:22] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [01:22<00:07,  3.60s/it][2026-01-17 13:32:23 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:32:24] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [01:23<00:03,  3.06s/it][2026-01-17 13:32:27 TP0] Prefill batch, #new-seq: 1, #new-token: 5248, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:32:30 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:30 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:30 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:30 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:32:30] INFO:     127.0.0.1:39116 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [01:30<00:00,  4.06s/it]
Model Responding: 100%|██████████| 50/50 [01:30<00:00,  1.81s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 4458.14it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.43333}, 'Accounting': {'num': 30, 'acc': 0.43333}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.45}, 'Agriculture': {'num': 20, 'acc': 0.45}, 'Overall': {'num': 50, 'acc': 0.44}}
[32m2026-01-17 13:32:30.909[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 13:32:30.916[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  | 0.44|±  |   N/A|

Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1277, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 177, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct accuracy (0.3000) below expected threshold (0.4400)
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 207941 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
AssertionError: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 162.020s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct achieved accuracy: 0.3000
Error testing /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-8B-Instruct accuracy (0.3000) below expected threshold (0.4400)
Cleaning up process 207941
.
.
End (21/23):
filename='ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py', elapsed=172, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py returned exit code 1

.
.
Begin (22/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 13:32:53] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=1053819758, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:32:55] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:33:04 TP1] Init torch distributed begin.
[2026-01-17 13:33:04 TP2] Init torch distributed begin.
[2026-01-17 13:33:04 TP3] Init torch distributed begin.
[2026-01-17 13:33:04 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 13:33:06 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:33:06 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:33:06 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:33:06 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:33:07 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:33:07 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:33:07 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:33:07 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:33:07 TP1] Load weight begin. avail mem=61.11 GB
[2026-01-17 13:33:07 TP2] Load weight begin. avail mem=60.84 GB
[2026-01-17 13:33:07 TP3] Load weight begin. avail mem=61.11 GB
[2026-01-17 13:33:07 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:33:07 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 13:33:07 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:33:07 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 13:33:07 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:33:07 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 13:33:07 TP0] Load weight begin. avail mem=60.78 GB
[2026-01-17 13:33:07 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:33:07 TP0] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/13 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   8% Completed | 1/13 [00:11<02:13, 11.11s/it]

Loading safetensors checkpoint shards:  15% Completed | 2/13 [00:19<01:45,  9.58s/it]

Loading safetensors checkpoint shards:  23% Completed | 3/13 [00:30<01:40, 10.07s/it]

Loading safetensors checkpoint shards:  31% Completed | 4/13 [00:40<01:31, 10.21s/it]

Loading safetensors checkpoint shards:  38% Completed | 5/13 [00:51<01:22, 10.26s/it]

Loading safetensors checkpoint shards:  46% Completed | 6/13 [01:01<01:12, 10.35s/it]

Loading safetensors checkpoint shards:  54% Completed | 7/13 [01:13<01:04, 10.71s/it]

Loading safetensors checkpoint shards:  62% Completed | 8/13 [01:24<00:55, 11.04s/it]

Loading safetensors checkpoint shards:  69% Completed | 9/13 [01:36<00:44, 11.14s/it]

Loading safetensors checkpoint shards:  77% Completed | 10/13 [01:46<00:32, 10.96s/it]

Loading safetensors checkpoint shards:  85% Completed | 11/13 [01:57<00:21, 10.94s/it]

Loading safetensors checkpoint shards:  92% Completed | 12/13 [02:02<00:09,  9.03s/it]

Loading safetensors checkpoint shards: 100% Completed | 13/13 [02:11<00:00,  9.22s/it]

Loading safetensors checkpoint shards: 100% Completed | 13/13 [02:11<00:00, 10.15s/it]

[2026-01-17 13:35:31 TP0] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=45.94 GB, mem usage=14.85 GB.
[2026-01-17 13:35:32 TP1] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=46.26 GB, mem usage=14.85 GB.
[2026-01-17 13:35:32 TP3] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=46.27 GB, mem usage=14.85 GB.
[2026-01-17 13:35:32 TP2] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=45.99 GB, mem usage=14.85 GB.
[2026-01-17 13:35:32 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 13:35:32 TP0] The available memory for KV cache is 6.39 GB.
[2026-01-17 13:35:32 TP1] The available memory for KV cache is 6.39 GB.
[2026-01-17 13:35:32 TP2] The available memory for KV cache is 6.39 GB.
[2026-01-17 13:35:32 TP3] The available memory for KV cache is 6.39 GB.
[2026-01-17 13:35:32 TP1] KV Cache is allocated. #tokens: 279168, K size: 3.20 GB, V size: 3.20 GB
[2026-01-17 13:35:32 TP2] KV Cache is allocated. #tokens: 279168, K size: 3.20 GB, V size: 3.20 GB
[2026-01-17 13:35:32 TP0] KV Cache is allocated. #tokens: 279168, K size: 3.20 GB, V size: 3.20 GB
[2026-01-17 13:35:32 TP3] KV Cache is allocated. #tokens: 279168, K size: 3.20 GB, V size: 3.20 GB
[2026-01-17 13:35:32 TP1] Memory pool end. avail mem=37.85 GB
[2026-01-17 13:35:32 TP2] Memory pool end. avail mem=37.58 GB
[2026-01-17 13:35:32 TP0] Memory pool end. avail mem=37.52 GB
[2026-01-17 13:35:32 TP3] Memory pool end. avail mem=37.85 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 13:35:33 TP0] max_total_num_tokens=279168, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=262144, available_gpu_mem=37.52 GB
[2026-01-17 13:35:34] INFO:     Started server process [213736]
[2026-01-17 13:35:34] INFO:     Waiting for application startup.
[2026-01-17 13:35:34] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:35:34] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:35:34] INFO:     Application startup complete.
[2026-01-17 13:35:34] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 13:35:35] INFO:     127.0.0.1:40930 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 13:35:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
...('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:35:44] INFO:     127.0.0.1:41968 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:35:44] INFO:     127.0.0.1:40944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 13:35:44] The server is fired up and ready to roll!
[2026-01-17 13:35:54 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:35:55] INFO:     127.0.0.1:42402 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 13:36:02.940[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 13:36:05.033[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 13:36:05.796[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 13:36:05.796[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 13:36:05.797[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 13:36:05.799[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 13:36:14.087[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9162.27it/s]
[32m2026-01-17 13:36:14.093[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 13:36:14 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:15] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:01<01:17,  1.59s/it][2026-01-17 13:36:15 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:17 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.21, #queue-req: 0,
[2026-01-17 13:36:17] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:03<01:11,  1.49s/it][2026-01-17 13:36:17 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:18] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:04<01:07,  1.44s/it][2026-01-17 13:36:18 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:19] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:05<01:06,  1.45s/it][2026-01-17 13:36:20 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:20 TP0] Decode batch, #running-req: 1, #token: 512, token usage: 0.00, npu graph: False, gen throughput (token/s): 10.88, #queue-req: 0,
[2026-01-17 13:36:21] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:07<01:05,  1.45s/it][2026-01-17 13:36:21 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:25] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:11<01:41,  2.31s/it][2026-01-17 13:36:25 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:26 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 6.69, #queue-req: 0,
[2026-01-17 13:36:26] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:12<01:26,  2.00s/it][2026-01-17 13:36:26 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:28] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:14<01:16,  1.82s/it][2026-01-17 13:36:28 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:29] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:15<01:09,  1.69s/it][2026-01-17 13:36:29 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:30 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.03, #queue-req: 0,
[2026-01-17 13:36:30] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:16<01:04,  1.60s/it][2026-01-17 13:36:31 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:31] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:17<00:53,  1.37s/it][2026-01-17 13:36:31 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:32] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:18<00:49,  1.31s/it][2026-01-17 13:36:33 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:34 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 10.96, #queue-req: 0,
[2026-01-17 13:36:34] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:20<00:49,  1.34s/it][2026-01-17 13:36:34 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:35] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:21<00:44,  1.23s/it][2026-01-17 13:36:35 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:36] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:22<00:45,  1.30s/it][2026-01-17 13:36:36 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:37 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 10.80, #queue-req: 0,
[2026-01-17 13:36:38] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:24<00:45,  1.34s/it][2026-01-17 13:36:38 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:39] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:25<00:45,  1.37s/it][2026-01-17 13:36:39 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:41] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:27<00:44,  1.39s/it][2026-01-17 13:36:41 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.68, #queue-req: 0,
[2026-01-17 13:36:41 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:42] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:28<00:43,  1.40s/it][2026-01-17 13:36:42 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:44] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:29<00:42,  1.41s/it][2026-01-17 13:36:44 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:44 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 10.66, #queue-req: 0,
[2026-01-17 13:36:45] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:31<00:41,  1.42s/it][2026-01-17 13:36:45 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:46] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:32<00:39,  1.43s/it][2026-01-17 13:36:46 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:48] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:34<00:38,  1.43s/it][2026-01-17 13:36:48 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.67, #queue-req: 0,
[2026-01-17 13:36:48 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:49] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:35<00:37,  1.43s/it][2026-01-17 13:36:49 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:50] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:36<00:31,  1.27s/it][2026-01-17 13:36:50 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:52 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 10.37, #queue-req: 0,
[2026-01-17 13:36:52] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:38<00:32,  1.35s/it][2026-01-17 13:36:52 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:53] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:39<00:31,  1.38s/it][2026-01-17 13:36:53 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:55] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:41<00:31,  1.45s/it][2026-01-17 13:36:55 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:56 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 10.17, #queue-req: 0,
[2026-01-17 13:36:56] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:42<00:31,  1.48s/it][2026-01-17 13:36:56 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:36:58] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:44<00:30,  1.51s/it][2026-01-17 13:37:01 TP0] Prefill batch, #new-seq: 1, #new-token: 5504, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:06] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:52<01:08,  3.62s/it][2026-01-17 13:37:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:08 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.13, #queue-req: 0,
[2026-01-17 13:37:08] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:54<00:56,  3.13s/it][2026-01-17 13:37:11 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:13] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:58<00:58,  3.44s/it][2026-01-17 13:37:16 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:20] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [01:06<01:16,  4.78s/it][2026-01-17 13:37:21 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:21] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [01:07<00:53,  3.56s/it][2026-01-17 13:37:21 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:22] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [01:08<00:38,  2.74s/it][2026-01-17 13:37:22 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:23] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [01:09<00:29,  2.23s/it][2026-01-17 13:37:26 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:30] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [01:16<00:43,  3.62s/it][2026-01-17 13:37:33 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:37 TP0] Decode batch, #running-req: 1, #token: 4992, token usage: 0.02, npu graph: False, gen throughput (token/s): 1.42, #queue-req: 0,
[2026-01-17 13:37:37] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [01:23<00:50,  4.57s/it][2026-01-17 13:37:37 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:37] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [01:23<00:33,  3.38s/it][2026-01-17 13:37:39 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:43] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [01:29<00:35,  3.98s/it][2026-01-17 13:37:43 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:44] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [01:30<00:24,  3.06s/it][2026-01-17 13:37:47 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:51] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [01:37<00:31,  4.50s/it][2026-01-17 13:37:55 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:37:58 TP0] Decode batch, #running-req: 1, #token: 4992, token usage: 0.02, npu graph: False, gen throughput (token/s): 1.83, #queue-req: 0,
[2026-01-17 13:37:59] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:45<00:32,  5.40s/it][2026-01-17 13:38:02 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:38:05] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [01:51<00:28,  5.69s/it][2026-01-17 13:38:06 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:38:07] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:53<00:18,  4.61s/it][2026-01-17 13:38:09 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:38:11] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:57<00:12,  4.30s/it][2026-01-17 13:38:13 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:38:15] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [02:01<00:08,  4.14s/it][2026-01-17 13:38:16 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:38:17 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 2.12, #queue-req: 0,
[2026-01-17 13:38:17] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [02:03<00:03,  3.65s/it][2026-01-17 13:38:20 TP0] Prefill batch, #new-seq: 1, #new-token: 5248, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:38:24] INFO:     127.0.0.1:58712 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [02:10<00:00,  4.72s/it]
Model Responding: 100%|██████████| 50/50 [02:10<00:00,  2.62s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 2550.54it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.33333}, 'Accounting': {'num': 30, 'acc': 0.33333}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.6}, 'Agriculture': {'num': 20, 'acc': 0.6}, 'Overall': {'num': 50, 'acc': 0.44}}
[32m2026-01-17 13:38:25.029[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 13:38:25.035[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  | 0.44|±  |   N/A|

Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1277, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 177, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct accuracy (0.3000) below expected threshold (0.4400)
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 213736 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
AssertionError: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 344.723s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct achieved accuracy: 0.3000
Error testing /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct accuracy (0.3000) below expected threshold (0.4400)
Cleaning up process 213736
.
.
End (22/23):
filename='ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py', elapsed=355, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py

.
.
Begin (22/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 13:39:49] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.35, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=440259744, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:39:50] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:39:59 TP1] Init torch distributed begin.
[2026-01-17 13:39:59 TP2] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:39:59 TP0] Init torch distributed begin.
[2026-01-17 13:39:59 TP3] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2026-01-17 13:40:01 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:40:01 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:40:01 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:40:01 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:40:02 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:40:02 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:40:02 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:40:02 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:40:03 TP2] Load weight begin. avail mem=60.84 GB
[2026-01-17 13:40:03 TP1] Load weight begin. avail mem=61.11 GB
[2026-01-17 13:40:03 TP0] Load weight begin. avail mem=60.78 GB
[2026-01-17 13:40:03 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:40:03 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 13:40:03 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:40:03 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 13:40:03 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:40:03 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 13:40:03 TP3] Load weight begin. avail mem=61.11 GB
[2026-01-17 13:40:03 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:40:03 TP3] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/13 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   8% Completed | 1/13 [00:01<00:20,  1.70s/it]

Loading safetensors checkpoint shards:  15% Completed | 2/13 [00:03<00:18,  1.71s/it]

Loading safetensors checkpoint shards:  23% Completed | 3/13 [00:05<00:19,  1.95s/it]

Loading safetensors checkpoint shards:  31% Completed | 4/13 [00:08<00:20,  2.26s/it]

Loading safetensors checkpoint shards:  38% Completed | 5/13 [00:10<00:18,  2.30s/it]

Loading safetensors checkpoint shards:  46% Completed | 6/13 [00:13<00:15,  2.28s/it]

Loading safetensors checkpoint shards:  54% Completed | 7/13 [00:15<00:13,  2.25s/it]

Loading safetensors checkpoint shards:  62% Completed | 8/13 [00:17<00:11,  2.24s/it]

Loading safetensors checkpoint shards:  69% Completed | 9/13 [00:19<00:09,  2.26s/it]

Loading safetensors checkpoint shards:  77% Completed | 10/13 [00:22<00:06,  2.27s/it]

Loading safetensors checkpoint shards:  85% Completed | 11/13 [00:24<00:04,  2.25s/it]

Loading safetensors checkpoint shards:  92% Completed | 12/13 [00:25<00:01,  1.81s/it]

Loading safetensors checkpoint shards: 100% Completed | 13/13 [00:27<00:00,  1.89s/it]

Loading safetensors checkpoint shards: 100% Completed | 13/13 [00:27<00:00,  2.08s/it]

[2026-01-17 13:40:40 TP2] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=45.99 GB, mem usage=14.85 GB.
[2026-01-17 13:40:40 TP1] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=46.27 GB, mem usage=14.85 GB.
[2026-01-17 13:40:42 TP3] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=46.26 GB, mem usage=14.85 GB.
[2026-01-17 13:40:42 TP0] Load weight end. type=Qwen3VLMoeForConditionalGeneration, dtype=torch.bfloat16, avail mem=45.94 GB, mem usage=14.85 GB.
[2026-01-17 13:40:42 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 13:40:42 TP0] The available memory for KV cache is 6.39 GB.
[2026-01-17 13:40:42 TP2] The available memory for KV cache is 6.39 GB.
[2026-01-17 13:40:42 TP1] The available memory for KV cache is 6.39 GB.
[2026-01-17 13:40:42 TP3] The available memory for KV cache is 6.39 GB.
[2026-01-17 13:40:42 TP0] KV Cache is allocated. #tokens: 279296, K size: 3.20 GB, V size: 3.20 GB
[2026-01-17 13:40:42 TP0] Memory pool end. avail mem=37.52 GB
[2026-01-17 13:40:42 TP2] KV Cache is allocated. #tokens: 279296, K size: 3.20 GB, V size: 3.20 GB
[2026-01-17 13:40:42 TP3] KV Cache is allocated. #tokens: 279296, K size: 3.20 GB, V size: 3.20 GB
[2026-01-17 13:40:42 TP1] KV Cache is allocated. #tokens: 279296, K size: 3.20 GB, V size: 3.20 GB
[2026-01-17 13:40:42 TP2] Memory pool end. avail mem=37.57 GB
[2026-01-17 13:40:42 TP3] Memory pool end. avail mem=37.85 GB
[2026-01-17 13:40:42 TP1] Memory pool end. avail mem=37.85 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 13:40:44 TP0] max_total_num_tokens=279296, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=262144, available_gpu_mem=37.52 GB
[2026-01-17 13:40:44] INFO:     Started server process [220754]
[2026-01-17 13:40:44] INFO:     Waiting for application startup.
[2026-01-17 13:40:44] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:40:44] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2026-01-17 13:40:44] INFO:     Application startup complete.
[2026-01-17 13:40:44] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 13:40:45] INFO:     127.0.0.1:59374 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 13:40:45 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:40:49] INFO:     127.0.0.1:52704 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:40:49] INFO:     127.0.0.1:59390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 13:40:49] The server is fired up and ready to roll!
[2026-01-17 13:40:59 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:00] INFO:     127.0.0.1:38490 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 13:41:08.531[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 13:41:10.608[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 13:41:11.368[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 13:41:11.368[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 13:41:11.369[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 13:41:11.373[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 13:41:32.369[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9156.27it/s]
[32m2026-01-17 13:41:32.376[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 13:41:32 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:33] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:01<01:18,  1.59s/it][2026-01-17 13:41:34 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:35 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.42, #queue-req: 0,
[2026-01-17 13:41:35] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:02<01:10,  1.47s/it][2026-01-17 13:41:35 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:36] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:04<01:07,  1.43s/it][2026-01-17 13:41:36 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:38] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:05<01:05,  1.42s/it][2026-01-17 13:41:38 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:38 TP0] Decode batch, #running-req: 1, #token: 512, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.09, #queue-req: 0,
[2026-01-17 13:41:39] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:07<01:03,  1.41s/it][2026-01-17 13:41:39 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:40] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:08<01:02,  1.41s/it][2026-01-17 13:41:40 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:42 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.59, #queue-req: 0,
[2026-01-17 13:41:42] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:10<01:01,  1.43s/it][2026-01-17 13:41:42 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:43] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:11<01:00,  1.43s/it][2026-01-17 13:41:43 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:45] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:12<00:57,  1.41s/it][2026-01-17 13:41:45 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:45 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.17, #queue-req: 0,
[2026-01-17 13:41:46] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:14<00:55,  1.39s/it][2026-01-17 13:41:46 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:47] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:15<00:48,  1.24s/it][2026-01-17 13:41:47 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:48] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:16<00:46,  1.22s/it][2026-01-17 13:41:48 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:49 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 10.96, #queue-req: 0,
[2026-01-17 13:41:50] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:17<00:46,  1.27s/it][2026-01-17 13:41:50 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:51] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:18<00:42,  1.18s/it][2026-01-17 13:41:51 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:52] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:20<00:45,  1.30s/it][2026-01-17 13:41:52 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:53 TP0] Decode batch, #running-req: 1, #token: 384, token usage: 0.00, npu graph: False, gen throughput (token/s): 10.57, #queue-req: 0,
[2026-01-17 13:41:53] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:21<00:44,  1.32s/it][2026-01-17 13:41:53 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:55] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:22<00:43,  1.32s/it][2026-01-17 13:41:55 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:56] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:24<00:42,  1.33s/it][2026-01-17 13:41:56 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 12.46, #queue-req: 0,
[2026-01-17 13:41:56 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:57] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:25<00:41,  1.34s/it][2026-01-17 13:41:58 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:41:59] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:26<00:40,  1.34s/it][2026-01-17 13:41:59 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:00 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 11.29, #queue-req: 0,
[2026-01-17 13:42:00] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:28<00:39,  1.35s/it][2026-01-17 13:42:00 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:02] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:29<00:37,  1.35s/it][2026-01-17 13:42:02 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:03 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 12.47, #queue-req: 0,
[2026-01-17 13:42:03] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:31<00:36,  1.34s/it][2026-01-17 13:42:03 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:04] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:32<00:35,  1.36s/it][2026-01-17 13:42:04 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:05] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:33<00:30,  1.21s/it][2026-01-17 13:42:05 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:07 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 10.57, #queue-req: 0,
[2026-01-17 13:42:07] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:34<00:31,  1.31s/it][2026-01-17 13:42:07 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:08] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:36<00:31,  1.35s/it][2026-01-17 13:42:08 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:10] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:37<00:30,  1.39s/it][2026-01-17 13:42:10 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:10 TP0] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, npu graph: False, gen throughput (token/s): 10.71, #queue-req: 0,
[2026-01-17 13:42:11] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:39<00:29,  1.40s/it][2026-01-17 13:42:11 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:12] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:40<00:28,  1.41s/it][2026-01-17 13:42:16 TP0] Prefill batch, #new-seq: 1, #new-token: 5504, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:21] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:49<01:08,  3.62s/it][2026-01-17 13:42:22 TP0] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:23 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 3.10, #queue-req: 0,
[2026-01-17 13:42:23] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:51<00:56,  3.16s/it][2026-01-17 13:42:25 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:27] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:55<00:58,  3.41s/it][2026-01-17 13:42:31 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:35] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [01:03<01:16,  4.77s/it][2026-01-17 13:42:36 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:36] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [01:04<00:53,  3.55s/it][2026-01-17 13:42:36 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:37] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [01:04<00:38,  2.73s/it][2026-01-17 13:42:37 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:38] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [01:05<00:28,  2.22s/it][2026-01-17 13:42:41 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:45] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [01:13<00:45,  3.77s/it][2026-01-17 13:42:48 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:52 TP0] Decode batch, #running-req: 1, #token: 4992, token usage: 0.02, npu graph: False, gen throughput (token/s): 1.39, #queue-req: 0,
[2026-01-17 13:42:52] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [01:20<00:52,  4.77s/it][2026-01-17 13:42:52 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:53] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [01:20<00:35,  3.51s/it][2026-01-17 13:42:55 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:42:59] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [01:27<00:38,  4.33s/it][2026-01-17 13:42:59 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:43:00] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [01:28<00:26,  3.32s/it][2026-01-17 13:43:03 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:43:08] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [01:36<00:32,  4.68s/it][2026-01-17 13:43:11 TP0] Prefill batch, #new-seq: 1, #new-token: 4992, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:43:15 TP0] Decode batch, #running-req: 1, #token: 4992, token usage: 0.02, npu graph: False, gen throughput (token/s): 1.73, #queue-req: 0,
[2026-01-17 13:43:16] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:43<00:33,  5.63s/it][2026-01-17 13:43:19 TP0] Prefill batch, #new-seq: 1, #new-token: 4352, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:43:23] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [01:50<00:30,  6.06s/it][2026-01-17 13:43:24 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:43:25] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:53<00:20,  5.00s/it][2026-01-17 13:43:27 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:43:29] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:57<00:13,  4.61s/it][2026-01-17 13:43:31 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:43:33] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [02:01<00:08,  4.42s/it][2026-01-17 13:43:34 TP0] Prefill batch, #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:43:36 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 1.93, #queue-req: 0,
[2026-01-17 13:43:36] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [02:04<00:03,  3.98s/it][2026-01-17 13:43:39 TP0] Prefill batch, #new-seq: 1, #new-token: 5248, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:43:44] INFO:     127.0.0.1:47296 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [02:12<00:00,  5.18s/it]
Model Responding: 100%|██████████| 50/50 [02:12<00:00,  2.64s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 3377.38it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.33333}, 'Accounting': {'num': 30, 'acc': 0.33333}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.6}, 'Agriculture': {'num': 20, 'acc': 0.6}, 'Overall': {'num': 50, 'acc': 0.44}}
[32m2026-01-17 13:43:44.526[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 13:43:44.534[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  | 0.44|±  |   N/A|

Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1277, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py", line 14, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 177, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct accuracy (0.3000) below expected threshold (0.4400)
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 220754 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
AssertionError: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct accuracy (0.3000) below expected threshold (0.4400)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 248.898s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.35 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 4 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct achieved accuracy: 0.3000
Error testing /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct: 0.3 not greater than or equal to 0.44 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen3-VL-30B-A3B-Instruct accuracy (0.3000) below expected threshold (0.4400)
Cleaning up process 220754
.
.
End (22/23):
filename='ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py', elapsed=259, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py returned exit code 1

.
.
Begin (23/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 13:44:08] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.6, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=154751757, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:44:10] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 13:44:19 TP6] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:44:19 TP7] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:44:20 TP5] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:44:20 TP0] Init torch distributed begin.
[2026-01-17 13:44:20 TP2] Init torch distributed begin.
[2026-01-17 13:44:20 TP3] Init torch distributed begin.
[2026-01-17 13:44:20 TP4] Init torch distributed begin.
[2026-01-17 13:44:20 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2026-01-17 13:44:21 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:44:21 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:44:21 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:44:21 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:44:21 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:44:21 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:44:21 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:44:21 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:44:21 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 13:44:22 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:44:22 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:44:22 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:44:22 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:44:22 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:44:22 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:44:22 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:44:22 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:44:22 TP5] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:44:22 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:44:22 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:44:22 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 13:44:22 TP6] Load weight begin. avail mem=60.88 GB
[2026-01-17 13:44:22 TP4] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:44:22 TP7] Load weight begin. avail mem=61.12 GB
[2026-01-17 13:44:22 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:44:23 TP5] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:44:23 TP5] Using sdpa as multimodal attention backend.
[2026-01-17 13:44:23 TP6] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:44:23 TP6] Using sdpa as multimodal attention backend.
[2026-01-17 13:44:23 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:44:23 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 13:44:23 TP7] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:44:23 TP7] Using sdpa as multimodal attention backend.
[2026-01-17 13:44:23 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:44:23 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 13:44:23 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:44:23 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 13:44:23 TP4] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:44:23 TP4] Using sdpa as multimodal attention backend.
[2026-01-17 13:44:23 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:44:23 TP1] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/38 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:   3% Completed | 1/38 [00:05<03:09,  5.13s/it]

Loading safetensors checkpoint shards:   5% Completed | 2/38 [00:09<02:40,  4.46s/it]

Loading safetensors checkpoint shards:   8% Completed | 3/38 [00:14<02:48,  4.80s/it]

Loading safetensors checkpoint shards:  11% Completed | 4/38 [00:19<02:46,  4.91s/it]

Loading safetensors checkpoint shards:  13% Completed | 5/38 [00:24<02:44,  4.98s/it]

Loading safetensors checkpoint shards:  16% Completed | 6/38 [00:30<02:46,  5.21s/it]

Loading safetensors checkpoint shards:  18% Completed | 7/38 [00:35<02:38,  5.11s/it]

Loading safetensors checkpoint shards:  21% Completed | 8/38 [00:40<02:34,  5.15s/it]

Loading safetensors checkpoint shards:  24% Completed | 9/38 [00:46<02:35,  5.36s/it]

Loading safetensors checkpoint shards:  26% Completed | 10/38 [00:51<02:25,  5.21s/it]

Loading safetensors checkpoint shards:  29% Completed | 11/38 [00:56<02:25,  5.39s/it]

Loading safetensors checkpoint shards:  32% Completed | 12/38 [01:00<02:04,  4.79s/it]

Loading safetensors checkpoint shards:  34% Completed | 13/38 [01:05<02:00,  4.82s/it]

Loading safetensors checkpoint shards:  37% Completed | 14/38 [01:10<01:56,  4.86s/it]

Loading safetensors checkpoint shards:  39% Completed | 15/38 [01:15<01:53,  4.92s/it]

Loading safetensors checkpoint shards:  42% Completed | 16/38 [01:20<01:50,  5.00s/it]

Loading safetensors checkpoint shards:  45% Completed | 17/38 [01:25<01:45,  5.00s/it]

Loading safetensors checkpoint shards:  47% Completed | 18/38 [01:30<01:39,  4.96s/it]

Loading safetensors checkpoint shards:  50% Completed | 19/38 [01:34<01:32,  4.86s/it]

Loading safetensors checkpoint shards:  53% Completed | 20/38 [01:39<01:28,  4.89s/it]

Loading safetensors checkpoint shards:  55% Completed | 21/38 [01:44<01:23,  4.93s/it]

Loading safetensors checkpoint shards:  58% Completed | 22/38 [01:49<01:19,  4.99s/it]

Loading safetensors checkpoint shards:  61% Completed | 23/38 [01:54<01:14,  4.97s/it]

Loading safetensors checkpoint shards:  63% Completed | 24/38 [01:59<01:09,  4.94s/it]

Loading safetensors checkpoint shards:  66% Completed | 25/38 [02:01<00:50,  3.88s/it]

Loading safetensors checkpoint shards:  68% Completed | 26/38 [02:07<00:55,  4.60s/it]

Loading safetensors checkpoint shards:  71% Completed | 27/38 [02:13<00:54,  4.91s/it]

Loading safetensors checkpoint shards:  74% Completed | 28/38 [02:18<00:51,  5.12s/it]

Loading safetensors checkpoint shards:  76% Completed | 29/38 [02:23<00:45,  5.05s/it]

Loading safetensors checkpoint shards:  79% Completed | 30/38 [02:27<00:38,  4.87s/it]

Loading safetensors checkpoint shards:  82% Completed | 31/38 [02:32<00:33,  4.84s/it]

Loading safetensors checkpoint shards:  84% Completed | 32/38 [02:37<00:29,  4.86s/it]

Loading safetensors checkpoint shards:  87% Completed | 33/38 [02:43<00:25,  5.02s/it]

Loading safetensors checkpoint shards:  89% Completed | 34/38 [02:48<00:20,  5.15s/it]

Loading safetensors checkpoint shards:  92% Completed | 35/38 [02:53<00:15,  5.05s/it]

Loading safetensors checkpoint shards:  95% Completed | 36/38 [02:58<00:09,  4.98s/it]

Loading safetensors checkpoint shards:  97% Completed | 37/38 [03:02<00:04,  4.88s/it]

Loading safetensors checkpoint shards: 100% Completed | 38/38 [03:07<00:00,  4.88s/it]

Loading safetensors checkpoint shards: 100% Completed | 38/38 [03:07<00:00,  4.94s/it]

[2026-01-17 13:47:31 TP0] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.37 GB, mem usage=17.44 GB.
[2026-01-17 13:47:31 TP1] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.70 GB, mem usage=17.44 GB.
[2026-01-17 13:47:31 TP2] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.42 GB, mem usage=17.44 GB.
[2026-01-17 13:47:31 TP7] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.69 GB, mem usage=17.44 GB.
[2026-01-17 13:47:31 TP4] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.42 GB, mem usage=17.44 GB.
[2026-01-17 13:47:31 TP3] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.70 GB, mem usage=17.44 GB.
[2026-01-17 13:47:31 TP5] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.70 GB, mem usage=17.44 GB.
[2026-01-17 13:47:31 TP6] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.44 GB, mem usage=17.44 GB.
[2026-01-17 13:47:31 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 13:47:31 TP0] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:47:31 TP7] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:47:31 TP6] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:47:31 TP5] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:47:31 TP4] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:47:31 TP3] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:47:31 TP1] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:47:31 TP2] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:47:32 TP0] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:47:32 TP5] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:47:32 TP0] Memory pool end. avail mem=23.34 GB
[2026-01-17 13:47:32 TP5] Memory pool end. avail mem=23.67 GB
[2026-01-17 13:47:32 TP3] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:47:32 TP7] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:47:32 TP4] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:47:32 TP1] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:47:32 TP3] Memory pool end. avail mem=23.67 GB
[2026-01-17 13:47:32 TP2] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:47:32 TP7] Memory pool end. avail mem=23.66 GB
[2026-01-17 13:47:32 TP4] Memory pool end. avail mem=23.40 GB
[2026-01-17 13:47:32 TP6] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:47:32 TP1] Memory pool end. avail mem=23.67 GB
[2026-01-17 13:47:32 TP2] Memory pool end. avail mem=23.40 GB
[2026-01-17 13:47:32 TP6] Memory pool end. avail mem=23.41 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 13:47:33 TP0] max_total_num_tokens=498304, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=128000, available_gpu_mem=23.34 GB
[2026-01-17 13:47:34] INFO:     Started server process [226845]
[2026-01-17 13:47:34] INFO:     Waiting for application startup.
[2026-01-17 13:47:34] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 1e-06, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 13:47:34] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 1e-06, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 13:47:34] INFO:     Application startup complete.
[2026-01-17 13:47:34] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 13:47:35] INFO:     127.0.0.1:45784 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 13:47:35 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:47:38] INFO:     127.0.0.1:40352 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:47:48] INFO:     127.0.0.1:60560 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:47:55] INFO:     127.0.0.1:45788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 13:47:55] The server is fired up and ready to roll!
[2026-01-17 13:47:58 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:47:59] INFO:     127.0.0.1:55342 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 13:48:08.346[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 13:48:10.441[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 13:48:11.209[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 13:48:11.209[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 13:48:11.210[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 13:48:11.213[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 13:48:24.930[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9143.10it/s]
[32m2026-01-17 13:48:24.936[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 13:48:25 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:25] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:00<00:31,  1.57it/s][2026-01-17 13:48:25 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:25] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:00<00:22,  2.16it/s][2026-01-17 13:48:25 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:26] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:01<00:18,  2.49it/s][2026-01-17 13:48:26 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:26] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:01<00:18,  2.45it/s][2026-01-17 13:48:26 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:27] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:02<00:18,  2.44it/s][2026-01-17 13:48:27 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:27] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:02<00:18,  2.42it/s][2026-01-17 13:48:27 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:27] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:02<00:16,  2.60it/s][2026-01-17 13:48:27 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:28] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:03<00:16,  2.62it/s][2026-01-17 13:48:28 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:28] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:03<00:14,  2.75it/s][2026-01-17 13:48:28 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:28] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:03<00:14,  2.77it/s][2026-01-17 13:48:28 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:29] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:04<00:14,  2.77it/s][2026-01-17 13:48:29 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:29] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:04<00:13,  2.81it/s][2026-01-17 13:48:29 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:29] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:04<00:13,  2.82it/s][2026-01-17 13:48:30 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:30] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:05<00:13,  2.67it/s][2026-01-17 13:48:30 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:30] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:05<00:13,  2.51it/s][2026-01-17 13:48:30 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:31 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.16, #queue-req: 0,
[2026-01-17 13:48:31] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:06<00:13,  2.59it/s][2026-01-17 13:48:31 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:31] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:06<00:12,  2.72it/s][2026-01-17 13:48:31 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:31] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:06<00:11,  2.75it/s][2026-01-17 13:48:31 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:32] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:07<00:10,  2.90it/s][2026-01-17 13:48:32 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:32] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:07<00:10,  2.94it/s][2026-01-17 13:48:32 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:32] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:07<00:09,  3.00it/s][2026-01-17 13:48:32 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:33] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:08<00:09,  3.04it/s][2026-01-17 13:48:33 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:33] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:08<00:08,  3.10it/s][2026-01-17 13:48:33 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:33] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:08<00:08,  3.11it/s][2026-01-17 13:48:33 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:34] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:09<00:08,  3.03it/s][2026-01-17 13:48:34 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:34] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:09<00:09,  2.64it/s][2026-01-17 13:48:34 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:34] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:10<00:08,  2.57it/s][2026-01-17 13:48:35 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:35] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:10<00:08,  2.55it/s][2026-01-17 13:48:35 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:35] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:10<00:07,  2.72it/s][2026-01-17 13:48:35 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:36] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:11<00:07,  2.80it/s][2026-01-17 13:48:39 TP0] Prefill batch, #new-seq: 1, #new-token: 7040, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:47 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:48:47 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:48:47 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:48:47 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:48:47 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:48:47 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:48:47 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:48:47 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:48:48] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:23<01:14,  3.94s/it][2026-01-17 13:48:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:50] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:25<00:59,  3.28s/it][2026-01-17 13:48:52 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:48:55] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:30<01:06,  3.93s/it][2026-01-17 13:48:59 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:49:07] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:42<01:39,  6.20s/it][2026-01-17 13:49:07 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:49:07] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:42<01:08,  4.54s/it][2026-01-17 13:49:07 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:49:08 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 1.08, #queue-req: 0,
[2026-01-17 13:49:08] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:43<00:46,  3.30s/it][2026-01-17 13:49:08 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:49:08] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:43<00:32,  2.52s/it][2026-01-17 13:49:11 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:49:19] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:54<00:59,  4.94s/it][2026-01-17 13:49:22 TP0] Prefill batch, #new-seq: 1, #new-token: 6528, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:49:29] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [01:04<01:12,  6.60s/it][2026-01-17 13:49:29 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:49:30] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [01:05<00:47,  4.73s/it][2026-01-17 13:49:32 TP0] Prefill batch, #new-seq: 1, #new-token: 5632, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:49:38] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [01:13<00:52,  5.81s/it][2026-01-17 13:49:38 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:49:39] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [01:14<00:34,  4.30s/it][2026-01-17 13:49:42 TP0] Prefill batch, #new-seq: 1, #new-token: 6528, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:49:49] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [01:24<00:42,  6.11s/it][2026-01-17 13:49:52 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:50:00] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:35<00:45,  7.52s/it][2026-01-17 13:50:03 TP0] Prefill batch, #new-seq: 1, #new-token: 5760, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:50:09] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [01:44<00:39,  7.97s/it][2026-01-17 13:50:10 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:50:12] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:47<00:26,  6.57s/it][2026-01-17 13:50:14 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:50:17] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:52<00:18,  6.08s/it][2026-01-17 13:50:19 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:50:23] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [01:58<00:11,  5.85s/it][2026-01-17 13:50:24 TP0] Prefill batch, #new-seq: 1, #new-token: 2688, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:50:26] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [02:01<00:04,  4.99s/it][2026-01-17 13:50:28 TP0] Prefill batch, #new-seq: 1, #new-token: 6912, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:50:35 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:50:35 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:50:35 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:50:35 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:50:35 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:50:35 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:50:35 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:50:35 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:50:36] INFO:     127.0.0.1:45158 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [02:11<00:00,  6.72s/it]
Model Responding: 100%|██████████| 50/50 [02:11<00:00,  2.64s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 4577.64it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.56667}, 'Accounting': {'num': 30, 'acc': 0.56667}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.55}, 'Agriculture': {'num': 20, 'acc': 0.55}, 'Overall': {'num': 50, 'acc': 0.56}}
[32m2026-01-17 13:50:36.793[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 13:50:36.804[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  | 0.56|±  |   N/A|

Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1277, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: 0.3 not greater than or equal to 0.56 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct accuracy (0.3000) below expected threshold (0.5600)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py", line 29, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 177, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct: 0.3 not greater than or equal to 0.56 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct accuracy (0.3000) below expected threshold (0.5600)
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 226845 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
AssertionError: 0.3 not greater than or equal to 0.56 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct accuracy (0.3000) below expected threshold (0.5600)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct: 0.3 not greater than or equal to 0.56 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct accuracy (0.3000) below expected threshold (0.5600)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 402.153s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.6 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 8 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct achieved accuracy: 0.3000
Error testing /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct: 0.3 not greater than or equal to 0.56 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct accuracy (0.3000) below expected threshold (0.5600)
Cleaning up process 226845
.
.
End (23/23):
filename='ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py', elapsed=413, estimated_time=400
.
.


[CI Retry] ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py failed with retriable pattern: AssertionError:.*not greater than
[CI Retry] Waiting 60s before retry...


[CI Retry] Attempt 2/2 for ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py

.
.
Begin (23/23):
python3 /data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py
.
.

/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2026-01-17 13:52:01] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct', tokenizer_path='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=True, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.6, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=273437535, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=50000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:52:04] Using default HuggingFace chat template with detected content format: openai
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:52:13 TP1] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:52:13 TP0] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:52:13 TP6] Init torch distributed begin.
[2026-01-17 13:52:13 TP4] Init torch distributed begin.
[2026-01-17 13:52:13 TP3] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning:
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2026-01-17 13:52:13 TP5] Init torch distributed begin.
[2026-01-17 13:52:13 TP2] Init torch distributed begin.
[2026-01-17 13:52:13 TP7] Init torch distributed begin.
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2026-01-17 13:52:14 TP0] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:52:14 TP4] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:52:14 TP7] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:52:14 TP3] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:52:14 TP1] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:52:14 TP5] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:52:14 TP2] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:52:14 TP6] Init torch distributed ends. mem usage=0.00 GB
[2026-01-17 13:52:14 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2026-01-17 13:52:15 TP4] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:52:15 TP3] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:52:15 TP2] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:52:15 TP6] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:52:15 TP1] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:52:15 TP0] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:52:15 TP7] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:52:15 TP5] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2026-01-17 13:52:15 TP2] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:52:15 TP4] Load weight begin. avail mem=60.86 GB
[2026-01-17 13:52:15 TP3] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:52:15 TP6] Load weight begin. avail mem=60.88 GB
[2026-01-17 13:52:15 TP1] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:52:15 TP0] Load weight begin. avail mem=60.81 GB
[2026-01-17 13:52:15 TP7] Load weight begin. avail mem=61.13 GB
[2026-01-17 13:52:15 TP5] Load weight begin. avail mem=61.14 GB
[2026-01-17 13:52:16 TP4] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:52:16 TP4] Using sdpa as multimodal attention backend.
[2026-01-17 13:52:16 TP1] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:52:16 TP1] Using sdpa as multimodal attention backend.
[2026-01-17 13:52:16 TP6] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:52:16 TP6] Using sdpa as multimodal attention backend.
[2026-01-17 13:52:16 TP2] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:52:16 TP2] Using sdpa as multimodal attention backend.
[2026-01-17 13:52:16 TP3] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:52:16 TP3] Using sdpa as multimodal attention backend.
[2026-01-17 13:52:16 TP0] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:52:16 TP0] Using sdpa as multimodal attention backend.
[2026-01-17 13:52:16 TP7] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:52:16 TP7] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   0% Completed | 0/38 [00:00<?, ?it/s]
[2026-01-17 13:52:16 TP5] Multimodal attention backend not set. Use sdpa.
[2026-01-17 13:52:16 TP5] Using sdpa as multimodal attention backend.

Loading safetensors checkpoint shards:   3% Completed | 1/38 [00:00<00:08,  4.62it/s]

Loading safetensors checkpoint shards:   5% Completed | 2/38 [00:00<00:08,  4.01it/s]

Loading safetensors checkpoint shards:   8% Completed | 3/38 [00:00<00:08,  4.09it/s]

Loading safetensors checkpoint shards:  11% Completed | 4/38 [00:00<00:08,  3.96it/s]

Loading safetensors checkpoint shards:  13% Completed | 5/38 [00:01<00:08,  4.04it/s]

Loading safetensors checkpoint shards:  16% Completed | 6/38 [00:01<00:08,  3.87it/s]

Loading safetensors checkpoint shards:  18% Completed | 7/38 [00:01<00:07,  4.02it/s]

Loading safetensors checkpoint shards:  21% Completed | 8/38 [00:02<00:07,  3.88it/s]

Loading safetensors checkpoint shards:  24% Completed | 9/38 [00:02<00:07,  3.76it/s]

Loading safetensors checkpoint shards:  26% Completed | 10/38 [00:02<00:07,  3.91it/s]

Loading safetensors checkpoint shards:  29% Completed | 11/38 [00:02<00:06,  4.12it/s]

Loading safetensors checkpoint shards:  32% Completed | 12/38 [00:02<00:05,  4.44it/s]

Loading safetensors checkpoint shards:  34% Completed | 13/38 [00:03<00:05,  4.51it/s]

Loading safetensors checkpoint shards:  37% Completed | 14/38 [00:03<00:05,  4.33it/s]

Loading safetensors checkpoint shards:  39% Completed | 15/38 [00:03<00:05,  4.30it/s]

Loading safetensors checkpoint shards:  42% Completed | 16/38 [00:03<00:05,  4.07it/s]

Loading safetensors checkpoint shards:  45% Completed | 17/38 [00:04<00:05,  3.99it/s]

Loading safetensors checkpoint shards:  47% Completed | 18/38 [00:04<00:04,  4.07it/s]

Loading safetensors checkpoint shards:  50% Completed | 19/38 [00:04<00:04,  4.12it/s]

Loading safetensors checkpoint shards:  53% Completed | 20/38 [00:04<00:04,  4.17it/s]

Loading safetensors checkpoint shards:  55% Completed | 21/38 [00:05<00:03,  4.31it/s]

Loading safetensors checkpoint shards:  58% Completed | 22/38 [00:05<00:03,  4.20it/s]

Loading safetensors checkpoint shards:  61% Completed | 23/38 [00:05<00:03,  4.21it/s]

Loading safetensors checkpoint shards:  63% Completed | 24/38 [00:05<00:03,  4.03it/s]

Loading safetensors checkpoint shards:  66% Completed | 25/38 [00:05<00:02,  4.75it/s]

Loading safetensors checkpoint shards:  68% Completed | 26/38 [00:06<00:02,  4.57it/s]

Loading safetensors checkpoint shards:  71% Completed | 27/38 [00:06<00:02,  4.11it/s]

Loading safetensors checkpoint shards:  74% Completed | 28/38 [00:06<00:02,  3.91it/s]

Loading safetensors checkpoint shards:  76% Completed | 29/38 [00:07<00:02,  3.94it/s]

Loading safetensors checkpoint shards:  79% Completed | 30/38 [00:07<00:01,  4.03it/s]

Loading safetensors checkpoint shards:  82% Completed | 31/38 [00:07<00:01,  3.99it/s]

Loading safetensors checkpoint shards:  84% Completed | 32/38 [00:07<00:01,  3.97it/s]

Loading safetensors checkpoint shards:  87% Completed | 33/38 [00:08<00:01,  3.83it/s]

Loading safetensors checkpoint shards:  89% Completed | 34/38 [00:08<00:01,  3.71it/s]

Loading safetensors checkpoint shards:  92% Completed | 35/38 [00:08<00:00,  3.80it/s]

Loading safetensors checkpoint shards:  95% Completed | 36/38 [00:08<00:00,  3.92it/s]

Loading safetensors checkpoint shards:  97% Completed | 37/38 [00:09<00:00,  3.97it/s]

Loading safetensors checkpoint shards: 100% Completed | 38/38 [00:09<00:00,  4.00it/s]

Loading safetensors checkpoint shards: 100% Completed | 38/38 [00:09<00:00,  4.07it/s]

[2026-01-17 13:52:26 TP0] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.37 GB, mem usage=17.44 GB.
[2026-01-17 13:52:26 TP7] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.69 GB, mem usage=17.44 GB.
[2026-01-17 13:52:26 TP1] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.70 GB, mem usage=17.44 GB.
[2026-01-17 13:52:27 TP3] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.70 GB, mem usage=17.44 GB.
[2026-01-17 13:52:27 TP6] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.44 GB, mem usage=17.44 GB.
[2026-01-17 13:52:27 TP4] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.43 GB, mem usage=17.44 GB.
[2026-01-17 13:52:27 TP2] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.42 GB, mem usage=17.44 GB.
[2026-01-17 13:52:27 TP5] Load weight end. type=Qwen2_5_VLForConditionalGeneration, dtype=torch.bfloat16, avail mem=43.70 GB, mem usage=17.44 GB.
[2026-01-17 13:52:27 TP0] Using KV cache dtype: torch.bfloat16
[2026-01-17 13:52:27 TP0] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:52:27 TP5] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:52:27 TP6] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:52:27 TP7] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:52:27 TP4] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:52:27 TP3] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:52:27 TP1] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:52:27 TP2] The available memory for KV cache is 19.01 GB.
[2026-01-17 13:52:28 TP0] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:52:28 TP0] Memory pool end. avail mem=23.34 GB
[2026-01-17 13:52:28 TP6] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:52:28 TP5] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:52:28 TP6] Memory pool end. avail mem=23.41 GB
[2026-01-17 13:52:28 TP5] Memory pool end. avail mem=23.68 GB
[2026-01-17 13:52:28 TP2] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:52:28 TP4] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:52:28 TP7] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:52:28 TP3] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:52:28 TP1] KV Cache is allocated. #tokens: 498304, K size: 9.51 GB, V size: 9.51 GB
[2026-01-17 13:52:28 TP2] Memory pool end. avail mem=23.40 GB
[2026-01-17 13:52:28 TP7] Memory pool end. avail mem=23.66 GB
[2026-01-17 13:52:28 TP4] Memory pool end. avail mem=23.40 GB
[2026-01-17 13:52:28 TP1] Memory pool end. avail mem=23.67 GB
[2026-01-17 13:52:28 TP3] Memory pool end. avail mem=23.67 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2026-01-17 13:52:29 TP0] max_total_num_tokens=498304, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2048, context_len=128000, available_gpu_mem=23.34 GB
[2026-01-17 13:52:30] INFO:     Started server process [237124]
[2026-01-17 13:52:30] INFO:     Waiting for application startup.
[2026-01-17 13:52:30] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 1e-06, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 13:52:30] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 1e-06, 'top_k': 50, 'top_p': 1.0}
[2026-01-17 13:52:30] INFO:     Application startup complete.
[2026-01-17 13:52:30] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2026-01-17 13:52:31] INFO:     127.0.0.1:46846 - "GET /model_info HTTP/1.1" 200 OK
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/serving_chat.py:130: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  max_output_tokens = request.max_completion_tokens or request.max_tokens
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/entrypoints/openai/protocol.py:641: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
  "max_new_tokens": self.max_tokens or self.max_completion_tokens,
[2026-01-17 13:52:31 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:52:32] INFO:     127.0.0.1:46858 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
('Warning: torch.save with "_use_new_zipfile_serialization = False" is not recommended for npu tensor, which may bring unexpected errors and hopefully set "_use_new_zipfile_serialization = True"', 'if it is necessary to use this, please convert the npu tensor to cpu tensor for saving')
[2026-01-17 13:52:42] INFO:     127.0.0.1:43012 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2026-01-17 13:52:51] INFO:     127.0.0.1:46848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 13:52:51] The server is fired up and ready to roll!
[2026-01-17 13:52:52 TP0] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:52:53] INFO:     127.0.0.1:48486 - "GET /health_generate HTTP/1.1" 200 OK
[32m2026-01-17 13:53:00.697[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m295[0m - [1mVerbosity set to INFO[0m
[32m2026-01-17 13:53:02.840[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2026-01-17 13:53:03.609[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m378[0m - [1mEvaluation tracker args: {'output_path': './logs'}[0m
[32m2026-01-17 13:53:03.609[0m | [33m[1mWARNING [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [33m[1m --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.[0m
[32m2026-01-17 13:53:03.610[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m467[0m - [1mSelected Tasks: ['mmmu_val'][0m
[32m2026-01-17 13:53:03.612[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2026-01-17 13:53:15.249[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmmu_val on rank 0...[0m

  0%|          | 0/50 [00:00<?, ?it/s]
100%|██████████| 50/50 [00:00<00:00, 9268.36it/s]
[32m2026-01-17 13:53:15.255[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m447[0m - [1mRunning generate_until requests[0m

Model Responding:   0%|          | 0/50 [00:00<?, ?it/s][2026-01-17 13:53:15 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:15] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   2%|▏         | 1/50 [00:00<00:30,  1.60it/s][2026-01-17 13:53:15 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:16] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   4%|▍         | 2/50 [00:00<00:21,  2.19it/s][2026-01-17 13:53:16 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:16] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   6%|▌         | 3/50 [00:01<00:18,  2.54it/s][2026-01-17 13:53:16 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:16] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:   8%|▊         | 4/50 [00:01<00:18,  2.50it/s][2026-01-17 13:53:17 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:17] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  10%|█         | 5/50 [00:02<00:17,  2.50it/s][2026-01-17 13:53:17 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:17] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  12%|█▏        | 6/50 [00:02<00:17,  2.46it/s][2026-01-17 13:53:17 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:18] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  14%|█▍        | 7/50 [00:02<00:16,  2.65it/s][2026-01-17 13:53:18 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:18] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  16%|█▌        | 8/50 [00:03<00:15,  2.68it/s][2026-01-17 13:53:18 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:18] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  18%|█▊        | 9/50 [00:03<00:14,  2.77it/s][2026-01-17 13:53:18 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:19] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  20%|██        | 10/50 [00:03<00:14,  2.83it/s][2026-01-17 13:53:19 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:19] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  22%|██▏       | 11/50 [00:04<00:13,  2.84it/s][2026-01-17 13:53:19 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:19] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  24%|██▍       | 12/50 [00:04<00:13,  2.86it/s][2026-01-17 13:53:19 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:20] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  26%|██▌       | 13/50 [00:04<00:12,  2.90it/s][2026-01-17 13:53:20 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:20] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  28%|██▊       | 14/50 [00:05<00:13,  2.77it/s][2026-01-17 13:53:20 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:20] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  30%|███       | 15/50 [00:05<00:13,  2.61it/s][2026-01-17 13:53:21 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:21 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 0.58, #queue-req: 0,
[2026-01-17 13:53:21] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  32%|███▏      | 16/50 [00:06<00:12,  2.69it/s][2026-01-17 13:53:21 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:21] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  34%|███▍      | 17/50 [00:06<00:11,  2.81it/s][2026-01-17 13:53:21 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:21] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  36%|███▌      | 18/50 [00:06<00:11,  2.86it/s][2026-01-17 13:53:22 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:22] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  38%|███▊      | 19/50 [00:07<00:10,  3.02it/s][2026-01-17 13:53:22 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:22] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  40%|████      | 20/50 [00:07<00:09,  3.03it/s][2026-01-17 13:53:22 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:22] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  42%|████▏     | 21/50 [00:07<00:09,  2.98it/s][2026-01-17 13:53:22 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:23] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  44%|████▍     | 22/50 [00:08<00:09,  3.01it/s][2026-01-17 13:53:23 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:23] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  46%|████▌     | 23/50 [00:08<00:08,  3.07it/s][2026-01-17 13:53:23 TP0] Prefill batch, #new-seq: 1, #new-token: 384, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:23] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  48%|████▊     | 24/50 [00:08<00:08,  2.95it/s][2026-01-17 13:53:23 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:24] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  50%|█████     | 25/50 [00:09<00:08,  3.01it/s][2026-01-17 13:53:24 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:24] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  52%|█████▏    | 26/50 [00:09<00:09,  2.64it/s][2026-01-17 13:53:24 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:25] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  54%|█████▍    | 27/50 [00:09<00:08,  2.61it/s][2026-01-17 13:53:25 TP0] Prefill batch, #new-seq: 1, #new-token: 512, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:25] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  56%|█████▌    | 28/50 [00:10<00:08,  2.60it/s][2026-01-17 13:53:25 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:25] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  58%|█████▊    | 29/50 [00:10<00:07,  2.72it/s][2026-01-17 13:53:25 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:26] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  60%|██████    | 30/50 [00:10<00:07,  2.82it/s][2026-01-17 13:53:29 TP0] Prefill batch, #new-seq: 1, #new-token: 7040, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:34 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:53:34 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:53:34 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:53:34 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:53:34 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:53:34 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:53:34 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:53:34 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:53:35] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  62%|██████▏   | 31/50 [00:20<01:00,  3.17s/it][2026-01-17 13:53:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1664, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:37] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  64%|██████▍   | 32/50 [00:22<00:48,  2.68s/it][2026-01-17 13:53:39 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:42] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  66%|██████▌   | 33/50 [00:27<00:59,  3.50s/it][2026-01-17 13:53:46 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:53] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  68%|██████▊   | 34/50 [00:38<01:30,  5.65s/it][2026-01-17 13:53:53 TP0] Prefill batch, #new-seq: 1, #new-token: 640, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:54] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  70%|███████   | 35/50 [00:38<01:02,  4.15s/it][2026-01-17 13:53:54 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:54 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, npu graph: False, gen throughput (token/s): 1.20, #queue-req: 0,
[2026-01-17 13:53:54] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  72%|███████▏  | 36/50 [00:39<00:42,  3.01s/it][2026-01-17 13:53:54 TP0] Prefill batch, #new-seq: 1, #new-token: 768, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:53:55] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  74%|███████▍  | 37/50 [00:39<00:29,  2.30s/it][2026-01-17 13:53:58 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:54:04] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  76%|███████▌  | 38/50 [00:49<00:53,  4.44s/it][2026-01-17 13:54:07 TP0] Prefill batch, #new-seq: 1, #new-token: 6528, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:54:13] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  78%|███████▊  | 39/50 [00:58<01:03,  5.80s/it][2026-01-17 13:54:13 TP0] Prefill batch, #new-seq: 1, #new-token: 256, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:54:13] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  80%|████████  | 40/50 [00:58<00:41,  4.17s/it][2026-01-17 13:54:16 TP0] Prefill batch, #new-seq: 1, #new-token: 5632, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:54:21] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  82%|████████▏ | 41/50 [01:06<00:47,  5.30s/it][2026-01-17 13:54:22 TP0] Prefill batch, #new-seq: 1, #new-token: 896, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:54:22] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  84%|████████▍ | 42/50 [01:07<00:31,  3.93s/it][2026-01-17 13:54:25 TP0] Prefill batch, #new-seq: 1, #new-token: 6528, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:54:32] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  86%|████████▌ | 43/50 [01:16<00:39,  5.58s/it][2026-01-17 13:54:35 TP0] Prefill batch, #new-seq: 1, #new-token: 6400, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:54:41] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  88%|████████▊ | 44/50 [01:26<00:40,  6.68s/it][2026-01-17 13:54:44 TP0] Prefill batch, #new-seq: 1, #new-token: 5760, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:54:49] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  90%|█████████ | 45/50 [01:34<00:36,  7.23s/it][2026-01-17 13:54:51 TP0] Prefill batch, #new-seq: 1, #new-token: 2560, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:54:52] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  92%|█████████▏| 46/50 [01:37<00:23,  5.93s/it][2026-01-17 13:54:54 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:54:57] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  94%|█████████▍| 47/50 [01:42<00:16,  5.64s/it][2026-01-17 13:54:59 TP0] Prefill batch, #new-seq: 1, #new-token: 4224, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:55:02] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  96%|█████████▌| 48/50 [01:47<00:11,  5.54s/it][2026-01-17 13:55:04 TP0] Prefill batch, #new-seq: 1, #new-token: 2688, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:55:05] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding:  98%|█████████▊| 49/50 [01:50<00:04,  4.69s/it][2026-01-17 13:55:08 TP0] Prefill batch, #new-seq: 1, #new-token: 6912, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,
[2026-01-17 13:55:15 TP4] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:55:15 TP0] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:55:15 TP5] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:55:15 TP1] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:55:15 TP3] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:55:15 TP2] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:55:15 TP6] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:55:15 TP7] Multimodal embedding cache is full. This typically occurs when a single embedding exceeds the cache size limit. Consider increasing the `SGLANG_VLM_CACHE_SIZE_MB` environment variable or reducing the input embedding size.
[2026-01-17 13:55:16] INFO:     127.0.0.1:43546 - "POST /v1/chat/completions HTTP/1.1" 200 OK

Model Responding: 100%|██████████| 50/50 [02:01<00:00,  6.58s/it]
Model Responding: 100%|██████████| 50/50 [02:01<00:00,  2.43s/it]

Postprocessing:   0%|          | 0/50 [00:00<?, ?it/s]
Postprocessing: 100%|██████████| 50/50 [00:00<00:00, 5017.23it/s]
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{'Overall-Art and Design': {'num': 0, 'acc': 0}, 'Overall-Business': {'num': 30, 'acc': 0.56667}, 'Accounting': {'num': 30, 'acc': 0.56667}, 'Overall-Science': {'num': 0, 'acc': 0}, 'Overall-Health and Medicine': {'num': 0, 'acc': 0}, 'Overall-Humanities and Social Science': {'num': 0, 'acc': 0}, 'Overall-Tech and Engineering': {'num': 20, 'acc': 0.55}, 'Agriculture': {'num': 20, 'acc': 0.55}, 'Overall': {'num': 50, 'acc': 0.56}}
[32m2026-01-17 13:55:16.697[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_aggregated[0m:[36m188[0m - [1mSaving results aggregated[0m
[32m2026-01-17 13:55:16.704[0m | [1mINFO    [0m | [36mlmms_eval.loggers.evaluation_tracker[0m:[36msave_results_samples[0m:[36m255[0m - [1mSaving per-sample results for: mmmu_val[0m
openai_compatible (model_version="/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct",tp=1), gen_kwargs: (), limit: 50.0, num_fewshot: None, batch_size: 2
| Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|--------|------:|------|-----:|--------|---|----:|---|------|
|mmmu_val|      0|none  |     0|mmmu_acc|↑  | 0.56|±  |   N/A|

Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 1277, in assertGreaterEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: 0.3 not greater than or equal to 0.56 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct accuracy (0.3000) below expected threshold (0.5600)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/c30044170/code/newHDK/ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py", line 29, in test_vlm_mmmu_benchmark
    self._run_vlm_mmmu_test()
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 177, in _run_vlm_mmmu_test
    self.fail(f"Test failed for {self.model}{test_name}: {e}")
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 703, in fail
    raise self.failureException(msg)
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct: 0.3 not greater than or equal to 0.56 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct accuracy (0.3000) below expected threshold (0.5600)
E/usr/local/python3.11.13/lib/python3.11/subprocess.py:1127: ResourceWarning: subprocess 237124 is still running
  _warn("subprocess %s is still running" % self.pid,

======================================================================
ERROR: test_vlm_mmmu_benchmark (__main__.TestGemmaModels.test_vlm_mmmu_benchmark)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/data/c30044170/code/newHDK/ascend/vlm_models/vlm_utils.py", line 167, in _run_vlm_mmmu_test
    self.assertGreaterEqual(
AssertionError: 0.3 not greater than or equal to 0.56 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct accuracy (0.3000) below expected threshold (0.5600)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1705, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Test failed for /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct: 0.3 not greater than or equal to 0.56 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct accuracy (0.3000) below expected threshold (0.5600)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2535, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 1 test in 208.415s

FAILED (errors=1)
[CI Test Method] TestGemmaModels.test_vlm_mmmu_benchmark

Testing model: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct --trust-remote-code --cuda-graph-max-bs 32 --enable-multimodal --mem-fraction-static 0.6 --log-level info --attention-backend ascend --disable-cuda-graph --tp-size 8 --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
Result
: {'results': {'mmmu_val': {'alias': 'mmmu_val', 'mmmu_acc,none': 0.3, 'mmmu_acc_stderr,none': 'N/A', 'mmmu_acc_pass_at_k,none': [], 'mmmu_acc_pass_at_k_stderr,none': []}}, 'group_subtasks': {'mmmu_val': []}, 'configs': {'mmmu_val': {'task': 'mmmu_val', 'dataset_path': 'lmms-lab/MMMU', 'test_split': 'validation', 'full_docs': False, 'process_results_use_image': False, 'doc_to_visual': '<function mmmu_doc_to_visual at 0xfffca0766700>', 'doc_to_text': '<function mmmu_doc_to_text at 0xfffca0767560>', 'doc_to_target': 'answer', 'process_results': '<function mmmu_process_results at 0xfffca07745e0>', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'mmmu_acc', 'aggregation': '<function mmmu_aggregate_results at 0xfffca0775800>', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'max_new_tokens': 16, 'until': ['\n\n']}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0, 'interleaved_format': False}, 'lmms_eval_specific_kwargs': {'default': {'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}, 'prompt_type': 'format', 'multiple_choice_prompt': "Answer with the option's letter from the given choices directly.", 'open_ended_prompt': 'Answer the question using a single word or phrase.'}}}, 'versions': {'mmmu_val': 0.0}, 'n-shot': {'mmmu_val': 0}, 'higher_is_better': {'mmmu_val': {'mmmu_acc': True}}, 'n-samples': {'mmmu_val': {'original': 900, 'effective': 50}}, 'config': {'model': 'openai_compatible', 'model_args': 'model_version="/root/.cache/modelscope/hub/models/microsoft/Phi-4-multimodal-instruct",tp=1', 'batch_size': '2', 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': 50.0, 'bootstrap_iters': 100000, 'gen_kwargs': '', 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': None, 'date': '20260117_202550', 'task_hashes': {'mmmu_val': '9a212e874a07fe1eec2f5e5272c9ddf550977df7f30941c9576ec05ea73af223'}, 'model_source': 'openai_compatible', 'model_name': '', 'model_name_sanitized': '', 'system_instruction': None, 'system_instruction_sha': None, 'fewshot_as_multiturn': False, 'chat_template': None, 'chat_template_sha': None, 'start_time': 372954.399583059, 'end_time': 373034.312507507, 'total_evaluation_time_seconds': '79.91292444797'}
Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct achieved accuracy: 0.3000
Error testing /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct: 0.3 not greater than or equal to 0.56 : Model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-VL-72B-Instruct accuracy (0.3000) below expected threshold (0.5600)
Cleaning up process 237124
.
.
End (23/23):
filename='ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py', elapsed=219, estimated_time=400
.
.


✗ FAILED: ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py returned exit code 1

Fail. Time elapsed: 7311.50s

============================================================
Test Summary: 20/24 passed
Retries: 5 test(s) were retried
============================================================
✓ PASSED:
  ascend/rerank_models/test_ascend_cross_encoder_models.py
  ascend/reward_models/test_ascend_reward_internlm2_7b.py
  ascend/reward_models/test_ascend_llama_3_1_8b.py
  ascend/reward_models/test_ascend_qwen2_5_math_rm_72b.py
  ascend/reward_models/test_ascend_Qwen2_5_1_5B_apeach.py
  ascend/vlm_models/test_ascend_gemma_3_4b_it.py
  ascend/vlm_models/test_ascend_janus_pro_7b.py
  ascend/vlm_models/test_ascend_mimo_vl_7b_rl.py
  ascend/vlm_models/test_ascend_janus_pro_1b.py
  ascend/vlm_models/test_ascend_minicpm_v_2_6.py
  ascend/vlm_models/test_ascend_minicpm_o_2_6.py
  ascend/vlm_models/test_ascend_qwen2_5_vl_3b_instruct.py
  ascend/vlm_models/test_ascend_phi4_multimodal_instruct.py
  ascend/vlm_models/test_ascend_qwen3_vl_235.py
  ascend/vlm_models/test_ascend_deepseek_vl2.py
  ascend/vlm_models/test_vlm_models_mistral-small_3_1_24b_instruct2503.py
  ascend/vlm_models/test_ascend_llava_next_72b.py
  ascend/vlm_models/test_ascend_kimi_vl_a3b_instruct.py
  ascend/vlm_models/test_vlm_models_glm_4_5v.py
  ascend/vlm_models/test_ascend_qwen3_vl_4b_instruct.py

✗ FAILED (with reason):
  ascend/vlm_models/test_ascend_llava_v1_6_34b.py (exit code 1)
  ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py (exit code 1)
  ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py (exit code 1)
  ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py (exit code 1)

↻ RETRIED:
  ascend/vlm_models/test_ascend_qwen3_vl_235.py (2 attempts, passed)
  ascend/vlm_models/test_ascend_llava_v1_6_34b.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py (2 attempts, failed)
  ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py (2 attempts, failed)
============================================================


============================================================
Failed Tests List (names only):
============================================================
  1. ascend/vlm_models/test_ascend_llava_v1_6_34b.py
  2. ascend/vlm_models/test_ascend_qwen3_vl_8b_instruct.py
  3. ascend/vlm_models/test_ascend_qwen3_vl_30b_a3b_instruct.py
  4. ascend/vlm_models/test_ascend_qwen2_5_vl_72b_instruct.py
============================================================

2026-01-17 13:55:23,484 - INFO - Test suite finished with exit code: -1
Test suite finished with exit code: -1
2026-01-17 13:55:23,485 - INFO - All logs saved to: /data/c30044170/log/per-commit-1-npu-a3/20260117_115331
All logs saved to: /data/c30044170/log/per-commit-1-npu-a3/20260117_115331
